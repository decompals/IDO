#EDIR DATA#
LANG=CDeveloper Magic‘: Performance Analyzer User's GuideDocument Number 007-2581-003CONTRIBUTORSWritten and Illustrated by John C. Stearns Edited by Christina CaryProduction by Laura CooperEngineering contributions by Lia Adams, Jim Ambras, Trevor Bechtel, Alan Foster, Christine Hanna, David Henke, Marty Itzkowitz, Mahadevan Iyer, Lisa Kvarda, Allan McNaughton, Ashok Mouli, Sudhir Mohan, Anil Pal, Andrew Palay, Kim Rachmeler, Jack Repenning, Paul Sanville, Ravi Shankar, Shankar Unni, Mike Yang, Jun Yu, and Doug Young.© Copyright 1995 Silicon Graphics, Inc.name='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]' All Rights ReservedThis document contains proprietary and confidential information of Silicon Graphics, Inc. The contents of this document may not be disclosed to third parties, copied, or duplicated in any form, in whole or in part, without the prior written permission of Silicon Graphics, Inc.RESTRICTED RIGHTS LEGENDUse, duplication, or disclosure of the technical data contained in this document by the Government is subject to restrictions as set forth in subdivision (c) (1) (ii) of the Rights in Technical Data and Computer Software clause at DFARS 52.227-7013 and/or in similar or successor clauses in the FAR, or in the DOD or NASA FAR Supplement. Unpublished rights reserved under the Copyright Laws of the United States. Contractor/manufacturer is Silicon Graphics, Inc., 2011 N. Shoreline Blvd., Mountain View, CA 94039-7311.Silicon Graphics is a registered trademark, and CASEVision and Graphics Library are trademarks of Silicon Graphics, Inc. ClearCase is a trademark of Atria Software, Inc. UNIX is a registered trademark of UNIX System Laboratories. X Window System is a trademark of the Massachusetts Institute of Technology. OSF/Motif is a trademark of the Open Software Foundation.About This GuideThis manual is a user's guide for the ProDev WorkShop Performance Analyzer and Tester, Release 2.5.1. It contains the following chapters:IDREF="40590" TYPE="TITLE"Chapter 1, "Introduction to the Performance Analyzer" describes the WorkShop Performance Analyzer, which helps you gain an understanding of your program's use of resources and determine if performance can be improved.IDREF="11628" TYPE="TITLE"Chapter 2, "Performance Analyzer Tutorial" provides a short tutorial that introduces you to the major features of the Performance Analyzer.IDREF="27257" TYPE="TITLE"Chapter 3, "Setting Up Performance Analysis Experiments" describes the process of setting up a performance analysis experiment, including sample trap strategy and specifying the type of performance task to capture the relevant performance data.IDREF="90728" TYPE="TITLE"Chapter 4, "Performance Analyzer Reference" provides detailed information on the Performance Analyzer and its associated facilities.IDREF="37011" TYPE="TITLE"Chapter 5, "Using Tester" provides an overview of Tester, which is used for coverage analysis, and provides a model of the different ways in which Tester can be applied.IDREF="40637" TYPE="TITLE"Chapter 6, "Tester Command Line Interface Tutorial" provides a tutorial which demonstrates how the command line version of Tester can be applied. IDREF="37824" TYPE="TITLE"Chapter 7, "Tester Command Line Reference" describes in detail each of the commands available in the Tester command line interface.IDREF="47213" TYPE="TITLE"Chapter 8, "Tester Graphical User Interface Tutorial" provides a tutorial which demonstrates how the graphical user interface version of Tester can be applied.IDREF="24089" TYPE="TITLE"Chapter 9, "Tester Graphical User Interface Reference" describes in detail each of the windows and their associated features available in the Tester graphical user interface.LBL="1"ID="40590"Introduction to the Performance AnalyzerThe Performance Analyzer helps you understand your program in terms of performance, determine if there are problems, and correct them. This chapter provides a brief introduction to the Performance Analyzer tools and describes how to use them to solve performance problems. It includes the following sections:IDREF="25048" TYPE="TITLE""Performance Analyzer Overview"IDREF="41377" TYPE="TITLE""The Performance Analyzer Tools"IDREF="10636" TYPE="TITLE""Sources of Performance Problems"IDREF="22183" TYPE="TITLE""Interpreting Performance Analyzer Results"LBL="" HELPID=""ID="25048"Performance Analyzer OverviewTo conduct performance analysis, you run a series of experiments to collect performance data. Prior to running an experiment, you specify the objective of your experiment through a task menu. The Performance Analyzer collects the required data and provides charts, tables, and annotated code to help you analyze the results. The Performance Analyzer has three general techniques for collecting performance data:Countingname='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'It can count the exact number of times each function and/or basic block has been executed. This requires instrumenting the program, that is, inserting code into the executable to collect counts.Profilingname='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'It can periodically examine and record the program's PC (program counter), call stack, and resource consumption. Tracingname='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'It can trace events that impact performance, such as reads and writes, system calls, page faults, floating point exceptions, and mallocs, reallocs, and frees. The Performance Analyzer processes the data to provide 28 different types of performance metrics.LBL="" HELPID=""ID="41377"The Performance Analyzer ToolsThese are the major windows in the Performance Analyzer toolset:Performance Analyzer main window (see IDREF="49629" TYPE="GRAPHIC"Figure 1-1)name='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'contains: the function list, which shows functions with their performance metricsthe system resource usage chartthe time line, which shows when sample events occurred in the experiment and controls the scope of analysis for the Performance Analyzer viewsFILE="ch01-11.gif" POSITION="INLINE" SCALE="FALSE"LBL="1-1"Figure 1-1 ID="49629"Performance Analyzer Main WindowUsage View (Graphical)name='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'contains charts that indicate resource usage and the occurrence of sample corresponding to time intervals set by the time line calipers (see IDREF="49865" TYPE="GRAPHIC"Figure 1-3)Usage View (Textual)name='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'provides the actual resource usage values corresponding to time intervals set by the time line calipers (see IDREF="32936" TYPE="GRAPHIC"Figure 1-4)Call Graph Viewname='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'displays the functions (with their metrics) in a graphical format showing where the calls were made (see IDREF="66889" TYPE="GRAPHIC"Figure 1-6)Call Stackname='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'displays the contents of the call stack at the selected event (see IDREF="10883" TYPE="GRAPHIC"Figure 1-10)Malloc Error Viewname='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'displays each malloc error (leaks and bad frees) that occurred in the experiment, the number of times the malloc occurred (a count is kept of mallocs with identical call stacks), and the call stack corresponding to the selected malloc error. Leak Viewname='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'displays each memory leak that occurred in your experiment, its size, the number of times the leak occurred at that location during the experiment, and the call stack corresponding to the selected leak. Malloc Viewname='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'displays each malloc (whether or not it caused a problem) that occurred in your experiment, its size, the number of times the malloc occurred (a count is kept of mallocs with identical call stacks), and the call stack corresponding to the selected malloc. Heap Viewname='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'displays a map of memory indicating how blocks of memory were used in the time interval set by the time line calipers (see IDREF="89972" TYPE="GRAPHIC"Figure 1-9)I/O Viewname='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'displays a chart devoted to I/O system calls. Identifies up to 10 files involved in I/O (see IDREF="52155" TYPE="GRAPHIC"Figure 1-11)Working Set Viewname='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'measures the coverage of the dynamic shared objects (DSOs) that make up your executable. It indicates instructions, functions, and pages that were not used when the experiment was run (see IDREF="54751" TYPE="GRAPHIC"Figure 1-12).Cord Analyzer (accessed from cvcord)name='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'works in conjunction with Working Set View to let you try out different working set configurations to improve performance (see IDREF="79264" TYPE="GRAPHIC"Figure 1-13).Source View with performance annotationsname='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'displays performance metrics adjacent to the corresponding line of source code (see IDREF="49404" TYPE="GRAPHIC"Figure 1-7) Disassembly View with performance annotationsname='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'displays the performance metrics adjacent to the corresponding machine code. For the "Get Ideal Time (pixie) per function & source line" experiment, Source View can show where and why a clock may have stalled during an instruction.LBL="" HELPID=""ID="10636"Sources of Performance ProblemsID="ch011"To tune a program's performance, you need to determine its consumption of machine resources. At any point (or phase) in a process, there is one limiting resource controlling the speed of execution. Processes can be slowed down by:CPU speed and availabilityI/O processingmemory size and availabilitybugsinstruction and data cache sizeany of the above in different phasesLBL="" HELPID=""ID="28851"CPU-bound ProcessesA CPU-bound process spends its time in the CPU and is limited by CPU speed and availability. To improve its performance on CPU-bound processes, you may need to streamline your code. This can entail modifying algorithms, reordering code to avoid interlocks, removing nonessential steps, blocking to keep data in cache and registers, or using alternative algorithms. LBL="" HELPID=""ID="11939"I/O-bound ProcessesAn I/O-bound process has to wait for I/O to complete and may be limited by disk access speeds or memory caching. To improve the performance of I/O-bound processes, you can try one of the following techniques:improve overlap of I/O with computationoptimize data usage to minimize disk accessuse data compressionLBL="" HELPID=""ID="16388"Memory-bound ProcessesA program that continuously needs to swap out pages of memory is called memory-bound. Page thrashing is often due to accessing virtual memory on a haphazard rather than strategic basis. One telltale indication of a page-thrashing condition is noise due to disk accesses. To fix a memory-bound process, you can try to improve the memory reference patterns or, if possible, decrease the memory used by the program.LBL="" HELPID=""ID="39935"BugsYou may find that a bug is causing the performance problem. For example, you may find that you are reading in the same file twice in different parts of the program, that floating point exceptions are slowing down your program, that old code has not been completely removed, or that you are leaking memory (making malloc calls without the corresponding calls to free).LBL="" HELPID=""Performance Phases in ProgramsSince programs exhibit different behavior during different phases of operation, you need to identify the limiting resource during each phase. A program can be I/O-bound while it reads in data, CPU-bound while it performs computation, and I/O-bound again in its final stage while it writes out data. Once you've identified the limiting resource in a phase, you can perform an in-depth analysis to find the problem. And after you have solved that problem, you can check for other problems within the phasename='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'performance analysis is an iterative process. LBL="" HELPID=""ID="22183"Interpreting Performance Analyzer ResultsBefore we discuss the mechanics of using the Performance Analyzer, let's look at these features that help you understand the behavior of your processes:IDREF="17061" TYPE="TITLE""The Time Line Display"IDREF="40481" TYPE="TITLE""Resource Usage Graphs"IDREF="24531" TYPE="TITLE""Textual Usage View"IDREF="38650" TYPE="TITLE""The Function List Area"IDREF="21184" TYPE="TITLE""Call Graph View"IDREF="14832" TYPE="TITLE""Source View with Performance Annotations"IDREF="14920" TYPE="TITLE""Malloc Error View, Leak View, Malloc View, and Heap View"IDREF="18232" TYPE="TITLE""Call Stack View"IDREF="10097" TYPE="TITLE""I/O View"IDREF="51760" TYPE="TITLE""Working Set View"LBL="" HELPID=""ID="17061"The Time Line DisplayHave you ever considered timing a program with a stopwatch? The Performance Analyzer time line serves the same function. The time line shows where each sample event in the experiment occurred. By setting sample traps at phase boundaries, you can analyze metrics on a phase-by-phase basis. The simplest metric, time, is easily recognized as the space between events. The triangular icons are calipers; they let you set the scope of analysis to the interval between the selected events.IDREF="89289" TYPE="GRAPHIC"Figure 1-2 shows the time line portion of the Performance Analyzer window with typical results. Events #3 and #4 are labeled. By looking at the distance between them and counting tick marks on the scale, you can see that this phase lasted for approximately 6 seconds.FILE="ch01-2.gif" POSITION="INLINE" SCALE="FALSE"LBL="1-2"Figure 1-2 ID="89289"Typical Performance Analyzer Time LineLBL="" HELPID=""ID="40481"Resource Usage GraphsThe Performance Analyzer lets you look at how different resources are consumed over time. It produces a number of resource usage graphs that are tied to the time line (see IDREF="49865" TYPE="GRAPHIC"Figure 1-3, which shows six of the graphs available). These resource usage graphs indicate trends and let you pinpoint problems within phases. Resource usage data refers to items that consume system resources. They includeID="ch012"user and system timepage faultscontext switchesthe size of reads and writesread and write countspoll and I/O callstotal system callsprocess signalsprocess sizeResource usage data is always recorded (written to file) at each sample point. In addition, setting a time in the Fine Grained Usage field enables you to record resource usage data at regular intervals. Fine-grained usage allows you to see fluctuations at a finer gradation than the phases defined by sample points. If you discover inconsistent behavior within a phase, you can set new sample points and break the phase down into smaller phases.You can analyze resource usage trends in the charts in Graphical Usage View and can view the numerical values in the Textual Usage View.Fine grained usage has little effect on the execution of the target process during data collection. It is of limited use if the program is divided into phases of uniform behavior by the placement of the sample points.LBL="" HELPID=""ID="24531"Textual Usage ViewThe usage graphs show the patterns; the textual usage views let you view the aggregate values for the interval specified by the time line calipers. IDREF="32936" TYPE="GRAPHIC"Figure 1-4 shows a typical Textual Usage View window. FILE="ch01-3.gif" POSITION="INLINE" SCALE="FALSE"LBL="1-3"Figure 1-3 ID="49865"Typical Resource Usage GraphsFILE="ch01-5.gif" POSITION="INLINE" SCALE="FALSE"LBL="1-4"Figure 1-4 ID="32936"Typical Textual Usage ViewLBL="" HELPID=""ID="38650"The Function List AreaThe function list displays all functions in the source code, annotated by performance metrics and ranked by the criterion of your choice, such as counts or one of the time metrics. IDREF="23690" TYPE="GRAPHIC"Figure 1-5 is an example of the function list, ranked by exclusive CPU time (defined as the time this function spent in the CPU, excluding jumps to other blocks).FILE="ch01-6.gif" POSITION="INLINE" SCALE="FALSE"LBL="1-5"Figure 1-5 ID="23690"Typical Performance Analyzer Function List AreaYou can configure how functions appear in the function list area by selecting "Preferences..." in the Config menu. It lets you select which performance metrics display, whether they display as percentages or absolute values, and the style of the function name. The "Sort..." selection in the Config menu lets you order the functions in the list by the selected metric. Both selections disable those metric selections that were not collected in the current experiment.LBL="" HELPID=""ID="21184"Call Graph ViewIn contrast to the function list which provides the performance metrics for functions, the call graph puts this information into context by showing you where the calls are made. The call graph displays functions as nodes and calls as arcs. The nodes are annotated with the performance metrics; the arcs come with counts by default and can include other metrics as well. In IDREF="66889" TYPE="GRAPHIC"Figure 1-6, for example, the inclusive time spent by the function main is 8.107 seconds. Its exclusive time was 0 seconds, meaning that the time was actually spent in called functions. main can potentially call three functions. Call Graph View indicates that in the experiment main called three functions: getArray which consumed 1.972 seconds, sum1 which consumed 3.287 seconds, and sum2 which consumed 2.848 seconds. FILE="ch01-7.gif" POSITION="INLINE" SCALE="FALSE"LBL="1-6"Figure 1-6 ID="66889"Typical Performance Analyzer Call GraphLBL="" HELPID=""ID="14832"Source View with Performance AnnotationsThe Performance Analyzer lets you view performance metrics by source line in Source View (see IDREF="49404" TYPE="GRAPHIC"Figure 1-7) or by machine instruction in Disassembly View. Displaying performance metrics is set in the Preferences dialog box, accessed from the Display menu in Source View and Disassembly View. The Performance Analyzer sets thresholds to flag lines that consume more than 90% of a total resource. These indicators appear in the metrics column and on the scroll bar. FILE="ch01-8.gif" POSITION="INLINE" SCALE="FALSE"LBL="1-7"Figure 1-7 ID="49404"Detailed Performance Metrics by Source LineLBL="" HELPID=""Disassembled Code with Performance AnnotationsThe Performance Analyzer also lets you view performance metrics by machine instruction. You can view any of the performance metrics that were measured in your experiment. If you ran a "Get Ideal Time (pixie) per function & source line" experiment, you can get a special three-part annotation that providing information about on stalled instructions (see IDREF="48745" TYPE="GRAPHIC"Figure 1-8). The yellow bar spanning the top of three columns in this annotation indicates the first instruction in each basic block. The first column labelled Clock in the annotation displays the clock number in which the instruction issues relative to the start of a basic block. If you see clock numbers replaced by ditto marks ("), it means that multiple instructions were issued in the same cycle. The second column is labelled Stall and shows how many clocks elapsed during the stall before the instruction was issued. The third column labelled Why shows the reason for the stall. There are three possibilities: B - branch delayF - function unit delayO - operand hasn't arrived yetFILE="ch01-12.gif" POSITION="INLINE" SCALE="FALSE"LBL="1-8"Figure 1-8 ID="48745"Disassembled Code with Stalled Clock AnnotationsLBL="" HELPID=""ID="14920"Malloc Error View, Leak View, Malloc View, and Heap ViewThe Performance Analyzer lets you look for memory problems. The Malloc Error View, Leak View, Malloc View, and Heap View windows address two common types of memory problems that can inhibit performance:ID="ch013"IDREF="40666" TYPE="TITLE""Memory Leakage"IDREF="10453" TYPE="TITLE""Bad Frees"The difference between these windows lies in the set of data that they collect. Malloc Error View displays all malloc errors: both memory leaks and bad frees. When you run a memory leak experiment and problems are found, a dialog box displays suggesting you use Malloc Error View to see the problems. Leak View shows memory leak errors only. Malloc View shows each malloc operation whether faulty or not. Heap View displays a map of heap memory indicating where both problems and normal memory allocations occur and can tie allocations to memory addresses. The first two views are better for focusing on problems; the latter two views show the big picture. LBL="" HELPID=""ID="40666"Memory LeakageMemory leakage occurs when a program dynamically allocates memory and fails to deallocate that memory when it is through using the space. This causes the program size to increase continuously as the process runs. A simple indicator of this condition is the Process Size stripchart in Process View. The strip chart only indicates the size; it does not show the reasons for an increase. ID="ch014"Leak View displays each memory leak in the executable, its size, the number of times the leak occurred at that location, and the corresponding call stack (when you select the leak), and is thus the most appropriate view for focusing on memory leaks. A region allocated but not freed is not necessarily a leak. If the calipers are not set to cover the entire experiment, the allocated region may still be in use later in the experiment. In fact, even when the calipers cover the entire experiment, it is not necessarily wrong if the program does not explicitly free memory before exiting, since all memory is freed anyway on program termination. The best way to look for leaks is to set sample points to bracket a specific operation that should have no effect on allocated memory. Then any area that is allocated but not freed is a leak.LBL="" HELPID=""ID="10453"Bad FreesA bad ID="ch015"free (also referred to as an anti-leak condition) occurs when a program frees some structure that it had already freed. In many such cases, a subsequent reference picks up a meaningless pointer, causing a segmentation violation. Bad frees are indicated in both Malloc Error View and in Heap View. Heap View identifies bad frees in its memory map display. It helps you find the address of the freed structure, search for the malloc event that created it, and the free event that released it. Hopefully, you can determine why it was prematurely freed or why a pointer to it was referenced after it had been freed.Heap View also identifies unmatched frees in an information window. An unmatched ID="ch016"free is a free that does not have a corresponding allocation in the same interval. As with leaks, the caliper settings may cause false indications. An unmatched free that occurs in any region not starting at the beginning of the experiment may not be an error. The region may have been allocated before the current interval and the unmatched free in the current interval may not be a problem after all. A segment identified as a bad free is definitely a problem; it has been freed more than once in the same interval.A search facility is provided in Heap View that allows the user to find the allocation and deallocation events for all blocks containing a particular virtual address.The Heap View window lets you analyze memory allocation and frees between selected sample events in your experiment. Heap View displays a memory map that indicates mallocs, reallocs, bad frees, and valid frees during the selected period, as shown in IDREF="89972" TYPE="GRAPHIC"Figure 1-9. Clicking an area in the memory map displays the address.FILE="f18heapv.gif" POSITION="INLINE" SCALE="FALSE"LBL="1-9"Figure 1-9 ID="89972"Typical Heap View Display AreaLBL="" HELPID=""ID="18232"Call Stack ViewThe Performance Analyzer enables you to recall call stacks at sample events, which helps you reconstruct the calls leading up to an event so that you can relate the event back to your code. IDREF="10883" TYPE="GRAPHIC"Figure 1-10 shows a typical call stack. It corresponds to sample event #2 in an experiment. FILE="ch01-9.gif" POSITION="INLINE" SCALE="FALSE"LBL="1-10"Figure 1-10 ID="10883" Typical Call StackLBL="" HELPID=""ID="10097"I/O ViewI/O View helps you determine the problems in an I/O-bound process. It produces a graph of all I/O system calls and identifies up to 10 files involved in I/O. See IDREF="52155" TYPE="GRAPHIC"Figure 1-11.FILE="ch01-10.gif" POSITION="INLINE" SCALE="FALSE"LBL="1-11"Figure 1-11 ID="52155" I/O ViewLBL="" HELPID=""ID="51760"Working Set ViewWorking Set View measures the coverage of the dynamic shared objects (DSOs) that make up your executable (see ID="ch017"IDREF="54751" TYPE="GRAPHIC"Figure 1-12). It indicates instructions, functions, and pages that were not used when the experiment was run. It shows the coverage results for each DSO in the DSO list area. Clicking a DSO in the list displays its pages with color-coding to indicate the coverage of the page.FILE="ch01-13.gif" POSITION="INLINE" SCALE="FALSE"LBL="1-12"Figure 1-12 ID="54751"Working Set ViewLBL="" HELPID=""Cord AnalyzerThe Cord Analyzer is not actually part of the Performance Analyzer and is invoked by typing ID="ch018"cvcord at the command line. The Cord Analyzer (see IDREF="79264" TYPE="GRAPHIC"Figure 1-13) lets you explore the working set behavior of an executable or dynamic shared library (DSO). With it you can construct a feedback file for input to cord to generate an executable with improved working-set behavior. FILE="ch01-4.gif" POSITION="INLINE" SCALE="FALSE"LBL="1-13"Figure 1-13 ID="79264"Cord AnalyzerLBL="2"ID="11628"Performance Analyzer TutorialThis chapter presents a tutorial for using the Performance Analyzer and covers these topics:ID="ch021"IDREF="33378" TYPE="TITLE""Tutorial Overview"IDREF="15600" TYPE="TITLE""Tutorial Setup"IDREF="31765" TYPE="TITLE""Analyzing the Performance Data"NoteBecause of inherent differences between systems and also due to concurrent processes that may be running on your system, your experiment will produce different results from the one in this tutorial. However, the basic form of the results should be the same.LBL="" HELPID=""ID="33378"Tutorial OverviewThis tutorial is based on a sample program called arraysum. The arraysum program goes through the following steps:defines the size of an array (2,000 by 2,000)creates a 2,000-by-2,000 element array, gets the size of the array, and reads in the elementscalculates the array total by adding up elements in each columnrecalculates the array total differently, by adding up elements in each rowAs you probably can already guess, it is more efficient to add the elements in an array row-by-row, as in step 4, than column-by-column, as in step 3. Because the elements in an array are stored sequentially by rows, adding the elements by columns potentially causes context switches, page faults, and cache misses. The tutorial shows you how you can detect symptoms of problems like this and then zero in on the problem. The source code is located in /usr/demos/WorkShop/performance/tutorial if you wish to examine it.LBL="" HELPID=""ID="15600"Tutorial SetupYou need to compile the program first so that you can use it in the tutorial.Change to the /usr/demos/WorkShop/performance directory.You can run the experiment in this directory or set up your own directory. You'll need the arraysum.c file in either case.Compile the arraysum.c file by typing make arraysumThis will provide you with an executable for the experiment.From the command line, type cvd arraysum&The Debugger Main View window is displayed. You need the Debugger to specify the data to be collected and run the experiment. Choose "Identify bottleneck resources & phases" from the "Select Task..." submenu in the Perf menu.This is a general-purpose performance task that will help us determine the phases of the program and view basic resource usage. Click Run in the Debugger Main View window.This starts the experiment. When the status line indicates that the process has terminated, the experiment has completed and the main Performance Analyzer window is displayed automatically. The experiment may take one to three minutes, depending on your system.LBL="" HELPID=""ID="31765"Analyzing the Performance DataPerformance analysis experiments are set up and run in the Debugger window; the data is analyzed in the main Performance Analyzer window.Examine the main Performance Analyzer window.The Performance Analyzer window now displays the information from the new experiment (see IDREF="76072" TYPE="GRAPHIC"Figure 2-1).Look at the Usage Chart in the Performance Analyzer window.There are three general phases. The first phase is I/O-intensive, as evidenced by the high system time. The middle phase takes up most of the experiment. We do not have enough information yet, however, to characterize it. The third phase shows high user time and is CPU-intensive.Select "Usage View (Graphs)" from the Views menu.The Usage View (Graphs) window displays as in IDREF="84137" TYPE="GRAPHIC"Figure 2-2. This indicates that there are significant page faults and context switches in the middle phase. It also shows high read activity and system calls in the first phase, confirming our hypothesis that it is I/O -intensive.As a side note, notice that the last chart indicates that the maximum total size of the process is reached at the end of the first phase and does not grow thereafter.Select "Call Stack" from the Views menu.The call stack displays for the selected event. An event refers to a sample point on the time line (or any usage chart). FILE="ch024.gif" POSITION="INLINE" SCALE="FALSE"LBL="2-1"Figure 2-1 ID="76072"Performance Analyzer Main Windowname='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'arraysum ExperimentFILE="ch025.gif" POSITION="INLINE" SCALE="FALSE"LBL="2-2"Figure 2-2 ID="84137"Usage View (Graphs)name='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'arraysum ExperimentAt this point, no events have been selected so the call stack is empty. To select events, you can click in the time line or usage chart. You can also click the event selector controls to make one event at a time (see IDREF="76072" TYPE="GRAPHIC"Figure 2-1).The call stack window indicates the state of the call stack when the event occurred. The significance of the call stack is that it lets you map events to the functions in which they occurred.Select some random events and watch the call stack.This exercise helps you see the connection between events and call stacks.The important call stacks are the ones that occur at the beginning and end of phases. The general approach is to click in the vicinity of a usage chart where you think a phase boundary may occur and then check the call stack at that point. In this example, events #2, #3, #8, and #13 are important. The call stacks for these events are shown in IDREF="46351" TYPE="GRAPHIC"Figure 2-3, which is drawn to illustrate the relationships, although you can't actually display multiple call stacks at the same time. Remember that your results will be different.Event #2 is the last event in the first phase. Events #3 and #7 are the first and last events in the sum1 function. Event #8 shows the switch from sum1 to sum2 and represents the beginning of the last phase. The length of time in sum1 indicates potential problems.FILE="ch026.gif" POSITION="INLINE" SCALE="FALSE"LBL="2-3"Figure 2-3 ID="46351"Significant Call Stacks in the arraysum ExperimentReturn to the Performance Analyzer window and pull down the sash to expose the complete function list.This shows the inclusive time (that is, time spent in the function and its called functions) and exclusive time (time in the function itself only) for each function. As you can see, 5.645 seconds are spent in sum1 and 5.536 seconds in sum2.FILE="f19funcs.gif" POSITION="INLINE" SCALE="FALSE"LBL="2-4"Figure 2-4 Function List Portion of Performance Analyzer WindowSelect "Call Graph View" from the Views menu and click the Butterfly button.The call graph provides an alternate means of viewing function performance data. It also shows the relationships, that is, which functions call which functions. After the Butterfly button is clicked, Call Graph View displays as in IDREF="67932" TYPE="GRAPHIC"Figure 2-5. The Butterfly button takes the selected function (or most active function if none is selected) and displays it with the functions that call it and those that it calls. FILE="f19cgrph.gif" POSITION="INLINE" SCALE="FALSE"LBL="2-5"Figure 2-5 ID="67932"Call Graph Viewname='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'arraysum ExperimentSelect "Close" from the Admin menu in the Call Graph View to close it. Return to the main Performance Analyzer window and move the left caliper (Begin) to event #3 and the right caliper (End) to event #8. This is shown in IDREF="71192" TYPE="GRAPHIC"Figure 2-6. Moving the calipers like this lets us focus on the data between event #3 and event #8.FILE="f19clprs.gif" POSITION="INLINE" SCALE="FALSE"LBL="2-6"Figure 2-6 ID="71192"Defining a Phase with Calipersname='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'arraysum ExperimentSelect "Usage View (Numerical)" from the Views menu.The Usage View (Numerical) window displays as shown in IDREF="97313" TYPE="GRAPHIC"Figure 2-7.FILE="f19usnum.gif" POSITION="INLINE" SCALE="FALSE"LBL="2-7"Figure 2-7 ID="97313"Viewing a Phase in the Usage View (Numerical)This view provides the performance metrics for the interval defined by the calipers, in this case the sum1 phase. Return to the main Performance Analyzer window, select sum1 from the function list, and click Source.The Source View window displays as in IDREF="17009" TYPE="GRAPHIC"Figure 2-8, scrolled to sum1, the selected function. The annotation column to the left of the display area shows the performance metrics by line. Lines consuming more than 90% of a particular resource appear with highlighted annotations.Notice that the line where the total is computed in sum1 is seen to be the culprit, consuming 4,987 milliseconds. As in the other WorkShop tools, you can make corrections in Source View, recompile and try out your changes.FILE="ch028.gif" POSITION="INLINE" SCALE="FALSE"LBL="2-8"Figure 2-8 ID="17009"Source View with Performance Metricsname='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'arraysum ExperimentNoteAt this point, we have uncovered one performance problem, that the sum1 algorithm is inefficient. As a side exercise, you may wish to take a look at the performance metrics at the assembly level. To do this, return to the main Performance Analyzer window, select sum1 from the function list, and click Disassembled Source. Disassembly View displays, with the performance metrics in the annotation column.Close any windows that are still open.This concludes the tutorial.LBL="3"ID="27257"Setting Up Performance Analysis ExperimentsIn performance analysis, you set up the experiment, run the executable, and analyze the results. To make setup easier, the Performance Analyzer provides predefined tasks that help you establish an objective and ensure that the appropriate performance data will be collected. This chapter tells you how to conduct performance tasks and what to look for. It covers these topics:IDREF="18334" TYPE="TITLE""Experiment Setup Overview"IDREF="12833" TYPE="TITLE""Selecting a Performance Task"IDREF="68861" TYPE="TITLE""Setting Sample Traps"IDREF="15373" TYPE="TITLE""Understanding Predefined Tasks"LBL="" HELPID=""ID="18334"Experiment Setup OverviewPerformance tuning typically consists of examining machine resource usage, breaking down the process into phases, identifying the resource bottleneck within each phase, and correcting the cause. Generally, you run the first experiment to break your program down into phases and run subsequent experiments to examine each phase individually. After you have solved a problem in a phase, you should then reexamine machine resource usage to see if there is further opportunity for performance improvement. ID="ch031"Each experiment has these steps:Specify the performance task.The Performance Analyzer provides predefined tasks for conducting experiments. When you select a task, the Performance Analyzer automatically enables the appropriate performance data items for collection.You should have an objective in mind when you start an experiment. The predefined tasks ensure that only the appropriate data collection is enabled. Selecting too much data can bog down the experiment and skew the data for collection. If you need a mix of performance data not available in the predefined tasks, you can select "Custom Task" from the "Select Task..." submenu, which lets you enable any combination of the data collection options.Specify where to capture the data.If you have selected the "Identify bottleneck resources & phases" task, which automatically polls for performance data, this step is not needed. If you want data at specific points in the process, you need to set sample traps. See IDREF="68861" TYPE="TITLE""Setting Sample Traps" for a brief description of traps or IDREF="25908" BOOK="Debugger_UG" FILE="4Traps.doc" HDG=""Chapter 4, "Setting Traps," in ProDev WorkShop Debugger User's Guide for an in-depth discussion.Performance Analyzer sets sample traps at the beginning and end of the process automatically. If you want to analyze data within phases, then you should set sample traps at the beginning of each phase and at intermediate points, if desired.Specify the experiment configuration parameters.This is an optional step if you use the defaults; otherwise you need to select "Configs..." from the Perf menu. This displays the dialog box shown in IDREF="73230" TYPE="GRAPHIC"Figure 3-1.FILE="f20confg.gif" POSITION="INLINE" SCALE="FALSE"LBL="3-1"Figure 3-1 ID="73230"Performance Experiment Configuration Dialog BoxThe dialog box lets you specifythe experiment directory where the data is to be storedthe instrument directory where the instrumented executable is to be storedtracking exec'd processestracking forked processeslaunching the Performance Analyzer automatically when the experiment finishesRun the program to collect the data.You run the experiment from the Debugger Main View window. If you are running a small experiment to capture resource usage, you may be able to watch the experiment in real time in Process Meter. Performance Analyzer stores the results in the designated experiment subdirectory.Analyze the results.After the experiment completes, you can look at the results in the Performance Analyzer window and its associated views. Use the calipers to get information for phases separately.LBL="" HELPID=""ID="12833"Selecting a Performance TaskTo set up a Performance Analyzer experiment, you need to choose a task from the Select Task submenu in the Perf menu in the Debugger Main View (see ID="ch032"IDREF="46095" TYPE="GRAPHIC"Figure 3-2). FILE="ch033.gif" POSITION="INLINE" SCALE="FALSE"LBL="3-2"Figure 3-2 ID="46095"Perf Menu with Select Task SubmenuThe Select Task submenu provides these tasks:Determine bottlenecks, identify phasesGet Total Time per function & source lineGet CPU Time per function & source lineGet Ideal Time (pixie) per function & source lineTrace I/O activityTrace system callsTrace page faultsFind memory leaksFind Floating Point ExceptionsCustom taskSelecting a task enables data collection. The mode indicator in the upper right corner of the Main View changes to show that performance analysis is enabled. LBL="" HELPID=""ID="68861"Setting Sample TrapsFor a thorough discussion of setting traps, refer to ID="ch033"IDREF="25908" BOOK="Debugger_UG" FILE="4Traps.doc" HDG=""Chapter 4, "Setting Traps,"in the ProDev WorkShop Debugger User's Guide. Sample traps enable you to record data when a specified condition occurs. You set them from the Debugger Main View, Trap Manager, or Source View. You can define sample traps:at function entry or exit pointsat source linesfor eventsconditionallymanually during an experimentSample traps at function entry and exit points are preferable to source line traps, because they are more likely to be preserved as your program evolves. This better enables you to save a set of traps in the Trap Manager in a file for subsequent reuse.Manual sample traps are triggered when you click the Sample button in the Debugger Main View. They are particularly useful for applications with graphical user interfaces. If you have a suspect operation in an experiment, a good technique is to take a manual sample before and after you perform the operation. You can then examine the data for that operation.LBL="" HELPID=""ID="15373"Understanding Predefined TasksIf you are unfamiliar with performance analysis, it is very easy to request more data collection than you actually needname='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'this can degrade performance of the Performance Analyzer and skew results. To help you record data appropriate to your current objective, WorkShop provides predefined combinations of options (or tasks), which are available in the Selact Task submenu in the Perf menu. When you select a task, the required data collection is automatically enabled.ID="ch034"LBL="" HELPID="Bottlenecks""Determine bottlenecks, identify phases""Determine bottlenecks, identify phases" measures machine resource usage and takes pollpoint samples at 1-second intervals. ID="ch035"Call stack data is captured at each pollpoint sample to compute the total time for each function and source line. In call stack profiling, the time spent at a PC (program counter) is determined by multiplying the number of times the PC appears in any call stack by the average time interval between call stacks. Call stacks are gathered whether the program was running or blocked; hence, the time computed represents the total time, both within and outside of the CPU. If the target process was blocked for a long time as a result of an instruction, that instruction will show up as having a high time. Gathering machine resource usage data lets you observe resource consumption over time. With it, you can break your program down into phases with similar resource consumption. You can analyze individual phases in detail in subsequent experiments. You can view resource usage in Usage View (Graphical), Usage View (Numerical), and in the Usage Chart in the Performance Analyzer main window.IDREF="99632" TYPE="GRAPHIC"Figure 3-3 shows a typical example of the resource usage graph and time line portion of the main Performance Analyzer window for a "Determine bottlenecks, identify phases" task. The resource usage graph shows the user vs. system time. The legend indicates the use of color in the graph. The time line is below the resource graph; it has a time scale so that you can correlate the resource usage with experiment time and with specific events. FILE="ch034.gif" POSITION="INLINE" SCALE="FALSE"LBL="3-3"Figure 3-3 ID="99632"Machine Resource Usage in Performance Analyzer WindowLBL="" HELPID="TotalTime""Get Total Time per function & source line"Use "Get Total Time per function & source line" to tune a phase that has been determined not to be CPU-bound. This task records:ID="ch036"call stacks every 100 ms, whether the target program is running or blockedmachine resource usage data at 1-second pollpoints and at sample pointsThe Total Time values for the PCs are summed up and displayed:by function in the function listby source line in Source Viewby instruction in Disassembly ViewLBL="" HELPID="CPUTime""Get CPU Time per function & source line"Use "Get CPU Time per function & source line" to tune a CPU-bound phase. It enables you to display the time spent in the CPU by function, source line, and instruction. This task records:ID="ch037"PC every 10 msfunction countsmachine resource usage data at 1-second intervals and at sample pointsThe CPU time is calculated by multiplying the number of times a PC appears in the profile by 10 ms. PCs are profiled only when the program is running in the CPU; hence, the time computed is the time spent within the CPU, or the CPU time.If the target process was blocked for a long time as a result of an instruction, that instruction will show up as having a low or zero CPU time. On the other hand, CPU-intensive instructions will show up as having a high CPU time.The CPU time values for the PCs are summed up and displayed:by function in the function listby source line in Source Viewby instruction in Disassembly ViewFunction count data is computed by inserting machine code that increments a counter at the start of the function (this is called instrumentation). This data is used in the function list to show how many times the function was called and also in the call graph to show how many times one function called another function, that is, arc counts. PC profiling is done by the kernel and is only minimally intrusive. Gathering function counts intrudes substantially due to:instrumentation code consuming CPU cycles instrumentation code increasing the target executable sizePC profiling of the instrumented code itself, which distorts the metricsHowever, function counts are useful when combined with PC profiling, because they help in the computation of Inclusive CPU times. (Inclusive CPU time is the total time spent in a function and all the functions it calls; exclusive CPU time is the time spent in the function only.) The arc counts indicate what percentage of a function's CPU time can be attributed to each of its callers.If you only need Exclusive CPU times and are willing to forgo Inclusive CPU times, arc counts, and function count information, you should select "Custom task" and enable PC Profile Counts and set Fine-Grained Usage to 1 second.Also look at the task "Get Ideal Time (pixie) per function & source line".LBL="" HELPID="IdealTime"ID="54900""Get Ideal Time (pixie) per function & source line"Use "Get Ideal Time (pixie) per function & source line" to tune a CPU-bound phase. This task provides exact counts with theoretical times. It is very useful when used in conjunction with the "Get CPU Time per function & source line" task. This approach lets you examine actual versus ideal time. The difference is the time spent as a result of:ID="ch038"load operations, which take a minimum of two cycles if the data is available in the cache and a lot longer if the data has to be accessed from the swap area or second-level cachestore operations, which cause the CPU to stall if the write buffer in the CPU gets filledfloating point operations, which consume more than one cycle time spent with the CPU stalled as a result of data dependenciesThis task records:basic block countsmachine resource usage data at 1-second intervals and at sample pointsThe following results are shown in the function list, Source View, and Disassembly View:execution countsresulting machine instructionsa count of resulting loads, stores, and floating point instructionsan approximation of the time spent with the CPU stalling (caused by data interlocks)the ideal time, that is, the product of the number of the machine instructions executed and the cycle time of the machine (The assumption made in the computation of ideal time is that each instruction takes exactly one cycle to execute.)This task requires instrumentation of the target executable. This involves dividing the code into basic blocks, which are a set of instructions with a single entry point, a single exit point, and no branches within. Counter code is inserted at the beginning of each basic block.After the instrumented executable runs, the Performance Analyzer multiplies the number of times a basic block was executed by the number of instructions in it. This yields the total number of instructions executed as a result of that basic block (and similarly for specific kinds of instructions like loads or stores). Note that the execution of the instrumentation code will skew the behavior of the target executable making it almost entirely CPU-bound; so pay no attention to the User vs Sys Time stripcharts.LBL="" HELPID="I/OActivity""Trace I/O activity"Use "Trace I/O activity" when your program is being slowed down by I/O calls and you want to find the responsible code. This task records call stacks at every ID="ch039"read and write system call, along with file descriptor information, and the number of bytes read or written.The number of bytes read and written is presented:by function in the function listby source line in Source Viewby instruction in Disassembly ViewThe I/O View window displays a graph of the number of bytes read and written for each file descriptor over time, and displays the files involved in the I/O. You can also see the read and write system calls.LBL="" HELPID="SysCalls""Trace system calls"Use "Trace system calls" when you suspect that system calls are slowing down performance and you wish to determine the responsible code. ID="ch0310"The number of system calls made is presented:by function in the function listby source line in Source Viewby instruction in Disassembly ViewTo observe the pattern of system calls over time, look in the syscall event chart of the Usage View (Graphical).LBL="" HELPID="PageFaults""Trace page faults"The "Trace page faults" task indicates areas of high page faulting activity and identifies the code responsible. The task records call stacks at every page fault. ID="ch0311"The number of page faults is presented:by function in the function listby source line in Source Viewby instruction in Disassembly ViewTo observe the pattern of page faulting over time, look in the page fault event chart of the Usage View (Graphical).LBL="" HELPID="MemoryLeaks""Find memory leaks"Use "Find memory leaks" to determine where memory leaks and bad ID="ch0312"frees may occur in a process. The task records the call stacks, address, and number of bytes at every malloc, realloc, and free. The currently malloced bytes (that might represent leaks), and the list of double frees are presented in Malloc Error View and the other memory analysis views. The number of bytes malloced is presented:by function in the function listby source line in Source Viewby instruction in Disassembly ViewLBL="" HELPID="FPExceptions""Find Floating Point Exceptions"Use "Find Floating Point Exceptions" when you suspect that large, unaccountable periods of time are being spent in floating point exception handlers. The task records the call stack at each floating point exception. The number of floating point exceptions is presented:ID="ch0313"by function in the function listby source line in Source Viewby instruction in Disassembly ViewTo observe the pattern of floating point exceptions over time, look in the floating point exceptions event chart in the Usage View (Graphical).LBL="" HELPID="CustomTask""Custom task"Use the "Custom task" selection when you need a combination of performance data collected that is not available through the predefined tasks. Selecting "Custom Task" displays the dialog box shown in ID="ch0314"IDREF="76639" TYPE="GRAPHIC"Figure 3-4.FILE="f20custm.gif" POSITION="INLINE" SCALE="FALSE"LBL="3-4"Figure 3-4 ID="76639"Custom Task Dialog BoxThe Custom Task dialog box lets you specifysampling dataname='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'function counts, basic block counts, and PC profile countstracing dataname='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'malloc/free trace, syscall trace, page fault trace, I/O syscall trace, FP exception race, recording intervalsname='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'the frequency of data recording for pollpoint sampling, fine-grained usage, and call stack profilingRemember the basic warnings in this chapter about collecting data:Too much data can bog down the experiment.Combining PC profiling and basic block counting will cause the instrumented code to be profiled, including the count code.Call stack profiling is not compatible with count operations or PC profiling.If you combine count operations with PC profiling, the results will be skewed due to the amount of instrumented code that will be profiled. LBL="4"ID="90728"Performance Analyzer ReferenceThis chapter provides detailed descriptions of the Performance Analyzer toolset, including:IDREF="24567" TYPE="TITLE""Selecting Performance Tasks"IDREF="47061" TYPE="TITLE""Specifying a Custom Task"IDREF="63286" TYPE="TITLE""Specifying the Experiment Configuration"IDREF="13068" TYPE="TITLE""The Performance Analyzer Main Window"IDREF="21919" TYPE="TITLE""Usage View (Graphs)"IDREF="96732" TYPE="TITLE""Process Meter"IDREF="95321" TYPE="TITLE""Usage View (Numerical)"IDREF="41945" TYPE="TITLE""I/O View"IDREF="53051" TYPE="TITLE""Call Graph View"IDREF="28886" TYPE="TITLE""Analyzing Memory Problems"IDREF="74395" TYPE="TITLE""Call Stack"IDREF="52948" TYPE="TITLE""Analyzing Working Sets"LBL="" HELPID="PerfTasks"ID="24567"Selecting Performance TasksYou choose performance tasks from the Select Task submenu in the Perf menu in Main View (see IDREF="94480" TYPE="GRAPHIC"Figure 4-1). You should have an objective in mind before you start an experiment. The tasks ensure that only the appropriate data collection is enabled. Selecting too much data can bog down the experiment and skew the data for collection. LBL="" HELPID=""Task SummaryThe tasks are summarized in IDREF="32155" TYPE="TABLE"Table 4-1. The Task column identifies the task as it appears in the Performance Task menu in the Performance Panel window. The Clues column provides an indication of symptoms and situations appropriate for the task. The Data Collected column indicates performance data set by the task. Note that call stacks are collected automatically at sample points, pollpoints, and process events. The Description column describes the technique used.FILE="ch0415.gif" POSITION="INLINE" SCALE="FALSE"LBL="4-1"Figure 4-1 ID="94480"Performance Panel Window with Task MenuFILE="" POSITION="INLINE" SCALE="FALSE"COLUMNS="4"LBL="4-1"Table 4-1 ID="32155"Summary of Performance Analyzer TasksLEFT="0" WIDTH="81"TaskLEFT="90" WIDTH="72"CluesLEFT="170" WIDTH="126"Data CollectedLEFT="305" WIDTH="225"DescriptionLEFT="0" WIDTH="81"Determine 
bottlenecks, 
identify phasesLEFT="90" WIDTH="72"Slow program, 
nothing else 
knownLEFT="170" WIDTH="126" Pollpoint Sampling (1 sec.) Call Stack Profiling (10 
msec.) call stacks at sample pointsLEFT="305" WIDTH="225"Captures resource usage at the pollpoint sample and 
displays it in resource usage graphs. Minimal intrusion. 
Tracks the total time spent by function, source code line, 
and instruction. LEFT="0" WIDTH="81"Get Total Time per 
function & source 
lineLEFT="90" WIDTH="72"Not CPU-boundLEFT="170" WIDTH="126" Fine-Grained Usage (1 sec.) Call Stack Profiling (10 
msec.) call stacks at sample pointsLEFT="305" WIDTH="225"Tracks the total time spent by function, source code line, 
and instruction. Useful for non-CPU-bound conditions. 
Total time metrics are displayed. LEFT="0" WIDTH="81"Get CPU Time per 
function & source 
lineLEFT="90" WIDTH="72"CPU-boundLEFT="170" WIDTH="126" Function Counts PC Profile Counts Fine-Grained Usage (1 sec.) call stacks at sample pointsLEFT="305" WIDTH="225"Tracks CPU time spent in functions, source code lines, 
and instructions. Useful for CPU-bound conditions. 
CPU time metrics help you separate CPU-bound from 
non-CPU-bound instructions.LEFT="0" WIDTH="81"Get Ideal Time 
(pixie) per 
function & source 
lineLEFT="90" WIDTH="72"CPU-boundLEFT="170" WIDTH="126" Basic Block Counts Fine-Grained Usage (1 sec.) call stacks at sample pointsLEFT="305" WIDTH="225"Calculates the ideal time, that is, the time spent in each 
basic block with the assumption of one instruction per 
machine cycle. Useful for CPU-bound conditions. Ideal 
time metrics also give counts, total machine 
instructions, and loads/stores/floating point 
instructions. It is useful to compare ideal time with the 
CPU time in an "Identify high CPU time functions" 
experiment.LEFT="0" WIDTH="81"Trace I/O activityLEFT="90" WIDTH="72"Process blocking 
due to I/O.LEFT="170" WIDTH="126" I/O System call Trace Fine-Grained Usage (1 sec.) call stacks at sample pointsLEFT="305" WIDTH="225"Captures call stacks at every read and write. The file 
description and number of bytes are available in I/O 
View.LEFT="0" WIDTH="81"Trace system callsLEFT="90" WIDTH="72"Resource usage 
chart shows 
high system 
calls.LEFT="170" WIDTH="126" System call Trace FP Exception Trace Fine-Grained Usage (1 sec.) call stacks at sample pointsLEFT="305" WIDTH="225"Records all system calls and corresponding call stacks. 
Gives system call counts for the functions, source code 
lines, and instructions making the system call. Also 
provides a stripchart showing the chronological 
sequence of system calls.LEFT="0" WIDTH="81"Trace page faultsLEFT="90" WIDTH="72""Noisy disk" 
due to accessesLEFT="170" WIDTH="126" Page Fault Trace Fine-Grained Usage (1 sec.) call stacks at sample pointsLEFT="305" WIDTH="225"Captures all page faults and corresponding call stacks. 
Produces event chart showing the page fault pattern. 
Lists page faults caused by function, source code line, 
and instruction.LEFT="0" WIDTH="81"Find memory 
leaksLEFT="90" WIDTH="72"Swelling in 
process sizeLEFT="170" WIDTH="126" Malloc/Free Trace Fine-Grained Usage (1 sec.) call stacks at sample pointsLEFT="305" WIDTH="225"Determines memory leaks by capturing the call stack, 
address, and size at all mallocs, reallocs, and frees and 
displays them in a memory map. Also indicates double 
frees.LEFT="0" WIDTH="81"Find Floating 
Point ExceptionsLEFT="90" WIDTH="72"High sys time in 
usage charts; 
presence of 
floating point 
operations; 
NaNsLEFT="170" WIDTH="126" FPE Exception Trace Fine-Grained Usage (1 sec.) call stacks at sample pointsLEFT="305" WIDTH="225"Useful when you suspect that time is being wasted in 
floating point exception handlers. Captures the call 
stack at each floating point exception. Lists floating 
point exceptions by function, source code line, and 
instruction.LEFT="0" WIDTH="81"Custom taskLEFT="90" WIDTH="72"LEFT="170" WIDTH="126" call stacks at sample points user's choiceLEFT="305" WIDTH="225"Lets you select the performance data to be collected. 
Remember that too much data can skew results.LBL="" HELPID="CustTask"ID="47061"Specifying a Custom TaskWhen you choose "Custom Task" from the Select Task submenu in the Perf menu in Main View, the dialog box shown in IDREF="26167" TYPE="GRAPHIC"Figure 4-2 appears. This section provides an explanation of the performance data.FILE="f21custm.gif" POSITION="INLINE" SCALE="FALSE"LBL="4-2"Figure 4-2 ID="26167"Custom Task Dialog BoxLBL="" HELPID=""ID="12716"Specifying Sampling DataSampling data is collected and recorded at every sample point. The collection of sampling data requires instrumentation, that is, adding special code to the target executable. You can request four kinds of sampling data: call stackfunction countsbasic block countsPC profile counts LBL="" HELPID=""Call Stack ProfilingThe Performance Analyzer performs call stack data collection automatically, capturing data at every sample point, pollpoint, and process event. There is no instrumentation involved.ID="ch041"LBL="" HELPID=""Function Count CollectionFunction count collection provides this information:ID="ch042"execution count of each function execution count of each call site This data is a subset of the information provided by basic block counts. However, gathering function count data does not slow down the instrumented executable as much as gathering basic block data.NoteIt is not possible to collect function count data simultaneously with call stack profiling data.LBL="" HELPID=""Basic Block Count SamplingIn addition to the data provided by function counts, basic block counting provides you with the execution count of each line of machine code.Basic block counts are translated to ideal CPU time displayed at the function, source line and machine line levels. The assumption made in calculating ideal CPU time is that each instruction takes exactly one cycle, and ignores potential floating point interlocks and memory latency time (cache misses and memory bus contention). Each system call is also assumed to take one cycle. The end result might be better described as ideal user CPU time.The data is gathered by first instrumenting the target executable. This involves dividing the executable into basic blocks consisting of sets of machine instructions that do not contain branches into or out of them. A few lines of code are inserted for every basic block to increment a counter every time that basic block is executed. The basic block data is actually generated, and when the instrumented target executable is run, the data is written out to disk whenever a sample trap fires. Instrumenting an executable increases its size by a factor of three, and greatly modifies its behavior.CautionRunning the instrumented executable causes it to run slower. By instrumenting, you might be changing the crucial resources; during analysis, the instrumented executable might appear to be CPU-bound, whereas the original executable was I/O-bound.NoteIt is not possible to collect basic block count data simultaneously with call stack profiling data.LBL="" HELPID=""PC Profile CountsEnabling PC profile counts causes the Program Counter (PC) of the target executable to be sampled every 10 ms when it is in the CPU. PC profiling is a lightweight, high-speed operation done with kernel support. Every 10 ms, the kernel stops the process if it is in the CPU, increments a counter for the current value of the PC, and resumes the process.PC Profile Counts is translated to the Actual CPU Time displayed at the function, source line and machine line levels. The actual CPU time is calculated by multiplying the PC hit count by 10 ms.A major discrepancy between actual CPU time and ideal CPU Time indicates:cache misses and floating point interlocks in a single process applicationsecondary cache invalidations in a multiprocess application run on a multiprocessorNoteThis comparison is inaccurate over a single run if you collect both basic block and PC profile counts simultaneously. In this situation, the Ideal CPU Time will factor out the interference caused by instrumenting; the Actual CPU Time will not. A rough approximation is to divide the Actual CPU Time by three.A comparison between basic block counts and PC profile counts is shown in IDREF="11720" TYPE="TABLE"Table 4-2.COLUMNS="2"LBL="4-2"Table 4-2 ID="11720"Basic Block Counts and PC Profile Counts ComparedLEFT="0" WIDTH="153"Basic Block CountsLEFT="160" WIDTH="153"PC Profile CountsLEFT="0" WIDTH="153"Used to compute ideal CPU timeLEFT="160" WIDTH="153"Used to estimate actual CPU timeLEFT="0" WIDTH="153"Data collection by instrumentingLEFT="160" WIDTH="153"Data collection done with the kernelLEFT="0" WIDTH="153"Slows program down by factor of 
threeLEFT="160" WIDTH="153"Has minimal impact on program 
speedLEFT="0" WIDTH="153"Generates an exact countLEFT="160" WIDTH="153"Approximates countsLBL="" HELPID=""ID="10209"Specifying Tracing DataTracing data records the time at which an event of the selected type occurred. There are five types of tracing data:ID="ch043"IDREF="36954" TYPE="TITLE""Malloc/Free Tracing"IDREF="32130" TYPE="TITLE""System Call Tracing"IDREF="22671" TYPE="TITLE""Page Fault Tracing"IDREF="38669" TYPE="TITLE""I/O Syscall Tracing"IDREF="34582" TYPE="TITLE""Floating Point Exception Tracing"NoteThese features should be used with care; enabling tracing data adds substantial overhead to the target execution and consumes a great deal of disk space.LBL="" HELPID=""ID="36954"Malloc/Free TracingMallocID="ch044"/free tracing enables you to study your program's use of dynamic storage and to quickly detect memory leaks (mallocs without corresponding frees) and bad frees (freeing a previously freed pointer). For this kind of tracing, you must create the target executable by linking with -lmalloc_cv instead of the usual -lmalloc. This data can be analyzed in Malloc Error View, Leak View, Malloc View, and Heap View (see IDREF="28886" TYPE="TITLE""Analyzing Memory Problems").Note that linking with -lmalloc_cv is not compatible with MP analysis so that using -lmpc -lmalloc_cv will not work.LBL="" HELPID=""ID="32130"System Call TracingEnabling system call tracing causes the call stack to be recorded whenever your program makes a system call. This data can be viewed in the system call event chart in Usage View (Graphs) which indicates where the system calls took place and in the Call Stack window which displays the call stack for a selected system call.ID="ch045"LBL="" HELPID=""ID="22671"Page Fault TracingEnabling page fault tracing causes the call stack and the faulting address to be recorded every time your program makes a memory reference that causes a page fault.ID="ch046"The Page Fault event chart displays where the page faults took place in Process View. The Call Stack Information window displays the call stack for a selected page fault event.ID="ch047"LBL="" HELPID=""ID="38669"I/O Syscall TracingI/O syscall tracing records every I/O-related system call that is made during the experiment. It traces read and write system calls with the call stack at the time, along with the number of bytes read or written. This is useful for I/O-bound processes.LBL="" HELPID=""ID="34582"Floating Point Exception TracingFloating point exception tracing records every instance of a floating point exception. This includes problems like underflow and NaN (not a number) values. If your program has a substantial number of floating point exceptions, you may be able to speed it up by correcting the algorithms.NoteTo use the floating point exception feature, you have to link your program with the library libfpe.a.The floating point exceptions are:overflowunderflowdivide-by-zeroinexact resultinvalid operand, e.g., infinityLBL="" HELPID=""ID="21039"Specifying Polling DataThere are three categories of polling data:IDREF="42269" TYPE="TITLE""Pollpoint Sampling"IDREF="28305" TYPE="TITLE""Fine Grained Usage"IDREF="40399" TYPE="TITLE""Call Stack Profiling"Entering a positive nonzero value in their fields turns them on and sets the time interval at which they will record.LBL="" HELPID=""ID="42269"Pollpoint SamplingSetting pollpoint sampling enables you to specify a regular time interval for capturing performance data, including resource usage and any enabled sampling or tracing functions. Since pollpoint sampling occurs frequently, it is best used with call stack data only rather than other profiling data. Its primary utility is to enable you to identify boundary points for phases. In subsequent runs, you can set sample points to collect the profiling data at the phase boundaries.ID="ch048"LBL="" HELPID=""ID="28305"Fine Grained UsageResource usage data is always collected at each sample point. Setting a time in the Fine Grained Usage field records resource usage data more frequently, at the specified time intervals. Fine grained usage helps you see fluctuations in usage between sample points.You can analyze resource usage trends in the charts in Usage View (Graphs) and can view the numerical values in the Usage View (Numerical).Fine grained usage has little effect on the execution of the target process during data collection. It is of limited use if the program is divided into phases of uniform behavior by the placement of the sample points.LBL="" HELPID=""ID="40399"Call Stack ProfilingEnabling call stack profiling causes the call stack of the target executable to be sampled at the specified time interval (minimum of 10 ms) and saved. The call stack continues to be sampled when the program is not running, while it is internally or externally blocked. Call stack profiling is used in the "Identify high total time functions" task to calculate total times.Call stack profiling is accomplished by the Performance Analyzer views and not by the kernel. As a result, it is less accurate than PC profiling. Collecting call stack profiling data is far more intrusive than collecting PC profile data.CautionCollecting basic block data causes the text of the executable to be modified. Therefore, if call stack profiling data is collected along with basic block counts, the cumulative total time displayed in Usage View (Graphs) is potentially erroneous.IDREF="63946" TYPE="TABLE"Table 4-3 compares call stack profiling and PC profiling.COLUMNS="2"LBL="4-3"Table 4-3 ID="63946"Call Stack Profiling and PC Profiling ComparedLEFT="0" WIDTH="162"PC ProfilingLEFT="170" WIDTH="169"Call Stack ProfilingLEFT="0" WIDTH="162"Done by kernelLEFT="170" WIDTH="169"Done by Performance Analyzer processLEFT="0" WIDTH="162"Accurate, non-intrusiveLEFT="170" WIDTH="169"Less accurate, more intrusiveLEFT="0" WIDTH="162"Used to compute CPU timeLEFT="170" WIDTH="169"Used to compute total timeLBL="" HELPID=""ID="63286"Specifying the Experiment ConfigurationTo specify the experiment configuration, you choose "Configs..." from the Perf menu. This displays the dialog box shown in IDREF="54335" TYPE="GRAPHIC"Figure 4-3.FILE="f21confg.gif" POSITION="INLINE" SCALE="FALSE"LBL="4-3"Figure 4-3 ID="54335"Experiment Configuration Dialog BoxLBL="" HELPID="ExperimentDirectory"ID="39589"Specifying the Experiment DirectoryThe Experiment DirectoryID="ch049" field lets you specify the directory where the data captured during the next experiment is stored. The Performance Analyzer provides a default directory named test0000. If you use the default or any other name that ends in four digits, the four digits are used as a counter and will be incremented automatically for each subsequent experiment. Note that the Performance Analyzer does not remove (or overwrite) experiment directories. You need to remove directories yourself.LBL="" HELPID="InstrumentDirectory"ID="30227"Specifying the Instrument DirectoryThe Instrument Directory lets re-use a previously instrumented executable. This technique avoids the processing necessary for a new instrumentation. Often in a series of experiments, you collect the same type of data while stressing the target executable in different ways. Reusing the instrumented executable lets you do this conveniently. To reuse an executable from a previous experiment, simply enter the old experiment directory.LBL="" HELPID="ConfigOptions"Other OptionsThe Track Exec'd Processes toggle allows you to specify whether or not you want the Performance Analyzer to gather performance data for any programs that are launched by an exec in any of the target processes. If this feature is enabled and there are execs in the course of the experiment, then you can view the performance data for any of these other executables by using the Executable menu in the Performance Analyzer main window. The Track Forked Processes toggle acts analogously for forked processes.The Auto Launch Performance Analyzer toggle provides the convenience of launching the Performance Analyzer automatically when an experiment finishes.LBL="" HELPID="PerfMain"ID="13068"The Performance Analyzer Main WindowThe Performance Analyzer main window is used for analysis after the performance data has been captured (see IDREF="58500" TYPE="GRAPHIC"Figure 4-4). It contains a time line area indicating when events took place over the span of the experiment, a list of functions with their performance data, and a resource usage chart. This section covers these topics:IDREF="19130" TYPE="TITLE""Task Field"IDREF="15285" TYPE="TITLE""Function List Display and Controls"IDREF="19068" TYPE="TITLE""Usage Chart Area"IDREF="24114" TYPE="TITLE""Time Line Area and Controls"IDREF="29539" TYPE="TITLE""Admin Menu"IDREF="54221" TYPE="TITLE""Config Menu"IDREF="20868" TYPE="TITLE""Views Menu"IDREF="29172" TYPE="TITLE""Executable Menu"IDREF="25690" TYPE="TITLE""Thread Menu"The Performance Analyzer main window can be invoked from the "Launch Tool" submenu in the Debugger Admin menu or from the command line, by typing:cvperf -exp experimentdirectorywhere experimentdirectory is the directory containing the performance data from the experiment.FILE="ch0416.gif" POSITION="INLINE" SCALE="FALSE"LBL="4-4"Figure 4-4 ID="58500"Performance Analyzer Main Window with MenusLBL="" HELPID="TaskField"ID="19130"Task FieldThe Task field identifies the task for the current experiment and is read-only. See IDREF="24567" TYPE="TITLE""Selecting Performance Tasks" for a summary of the performance tasks. For an in-depth explanation of each task, refer to IDREF="27257" TYPE="TITLE"Chapter 3, "Setting Up Performance Analysis Experiments."LBL="" HELPID="FunctionList"ID="15285"Function List Display and ControlsThe function list area displays the program's functions with the associated performance metrics. It also provides buttons for displaying function performance data in other views. See ID="ch0410"IDREF="18167" TYPE="GRAPHIC"Figure 4-5. FILE="ch0417.gif" POSITION="INLINE" SCALE="FALSE"LBL="4-5"Figure 4-5 ID="18167"Typical Function List AreaThe main features of the function list are:Function list display areashows all functions in the source code annotated with their associated performance data. The column headings identify the metrics. You select the performance data to display from the "Preferences..." selection in the Config menu. The order of ranking is set by the "Sort..." selection in the Config menu. The default order of sorting (depending on availability) is:inclusive timeexclusive timecountsID="ch0411"Search fieldlets you look for a function in the list and in any active views.ID="ch0412"Hide 0 Functions togglelets you filter functions with 0 counts from the list.ID="ch0413"Show Nodecauses the specified node to display in the call graph.ID="ch0414"Sourcelets you display the Source View window corresponding to the selected function. The Source View window displays, with performance metrics in the annotation column. Source View can also be displayed by double-clicking a function in the Function List or a node or arc in the call graph. This is discussed in the next section.ID="ch0415"Disassembled Sourcelets you display the Disassembly View window corresponding to the selected function. Disassembly View displays, annotated with the performance metrics for total (CPU) time. LBL="" HELPID="UsageChart"ID="19068"Usage Chart AreaThe usage chart area in the Performance Analyzer main window (see IDREF="58500" TYPE="GRAPHIC"Figure 4-4) displays the stripchart most relevant to the current task. The upper subwindow displays the legend for the stripchart and the lower subwindow displays the stripchart itself. This lets you obtain some useful information without having to open the Usage View (Graphs) window. IDREF="42317" TYPE="TABLE"Table 4-4 shows you the data displayed in the usage chart area for each task.COLUMNS="2"LBL="4-4"Table 4-4 ID="42317"Task Display in Usage Chart AreaLEFT="0" WIDTH="153"TaskLEFT="160" WIDTH="153"Data in Usage Chart AreaLEFT="0" WIDTH="153"Determine bottlenecks, identify 
phasesLEFT="160" WIDTH="153"User versus system timeLEFT="0" WIDTH="153"Get total time per function & source 
lineLEFT="160" WIDTH="153"User versus system timeLEFT="0" WIDTH="153"Get CPU time per function & source 
lineLEFT="160" WIDTH="153"User versus system timeLEFT="0" WIDTH="153"Get ideal time per function & source 
lineLEFT="160" WIDTH="153"User versus system timeLEFT="0" WIDTH="153"Trace I/O activityLEFT="160" WIDTH="153"read(), write() system callsLEFT="0" WIDTH="153"Trace system callsLEFT="160" WIDTH="153"System call event chartLEFT="0" WIDTH="153"Trace page faultsLEFT="160" WIDTH="153"Page fault event chartLEFT="0" WIDTH="153"Find memory leaksLEFT="160" WIDTH="153"Process Size stripchartLEFT="0" WIDTH="153"Find floating point exceptionsLEFT="160" WIDTH="153"Floating point exception event chartLEFT="0" WIDTH="153"Custom taskLEFT="160" WIDTH="153"User versus system time unless 
tracing data has been selected (see Trace tasks above)LBL="" HELPID="TimeLine"ID="24114"Time Line Area and ControlsThe time line shows when each sample event in the experiment occurred. IDREF="83968" TYPE="GRAPHIC"Figure 4-6 shows the time line portion of the Performance Analyzer window with typical results. FILE="ch0418.gif" POSITION="INLINE" SCALE="FALSE"LBL="4-6"Figure 4-6 ID="83968"Typical Performance Analyzer Time LineLBL="" HELPID=""The Time Line CalipersThe calipers let you define an interval for performance analysis. You can set the calipers in the time line to any two sample event points, using the caliper controls or by dragging them directly. The calipers appear solid for the current interval. If you drag them with the mouse (left or middle button), they appear dashed to give you visual feedback. When you stop dragging a caliper, it appears in outlined form denoting a tentative and as yet unconfirmed selection. ID="ch0416"Specifying an interval is done as follows: Set the left caliper to the sample event at the beginning of the interval.You can drag the left caliper with the left or middle mouse button or by using the left caliper control buttons in the control area. Note that calipers always snap to sample events. (Note that it actually doesn't matter whether you start with the left or right caliper.)Set the right caliper to the sample event at the end of the interval. This is similar to setting the left caliper. Confirm the change by clicking the OK button in the control area.After you confirm the new position, the solid calipers move to the current position of the outlined calipers and change the data in all views to reflect the new interval.Clicking Cancel or clicking with the right mouse button before the change is confirmed restores the outlined calipers to the solid calipers. LBL="" HELPID=""Current Event SelectionIf you want to get more information on an event in the time line or in the charts in Usage View (Graphs), you can click an event with the left button. The Event field (see IDREF="83968" TYPE="GRAPHIC"Figure 4-6) displaysevent numberdescription of the trap that triggered the eventthe thread in which it was definedwhether the sample was taken in all threads or the indicated thread only, in parenthesesIn addition, the Call Stack View window updates to the appropriate times, stack frames, and event type for the selected event. A black diamond-shaped icon appears in the time line and charts to indicate the selected event. You can also select an event using the event controls below the caliper controls; they work in similar fashion to the caliper controls. LBL="" HELPID=""Time Line Scale MenuThe scale menu lets you change the number of seconds of the experiment displayed in the time line area. The "Full Scale" selection displays the entire experiment on the time line. The other selections are time values; for example, if you select "1 min", the length of the time line displayed will span 1 minute.LBL="" HELPID="PerfAdminMenu"ID="29539"Admin MenuThe Admin menu and its options are shown in IDREF="62144" TYPE="GRAPHIC"Figure 4-7. The Admin menu has selections common to the other WorkShop tools. There are three selections different in the Performance Analyzer:"Experiment..."lets you change the experiment directory and displays the dialog box shown in IDREF="62144" TYPE="GRAPHIC"Figure 4-7."Rerun Experiment"lets you run another experiment with or without the same Performance Panel settings. A dialog box displays requesting confirmation (see IDREF="62144" TYPE="GRAPHIC"Figure 4-7). The Debugger Main View window then displays so that you can start the experiment."Save As Text..."records a text file with preference information selected in the view and displays the dialog box shown in IDREF="62144" TYPE="GRAPHIC"Figure 4-7. You can use the default file name or replace it with another name in the File Selection dialog box that displays. You can specify the number of lines to be saved. The data can be saved as a new file or appended to an existing one.FILE="ch0419.gif" POSITION="INLINE" SCALE="FALSE"LBL="4-7"Figure 4-7 ID="62144"Performance Analyzer Admin Menu OptionsLBL="" HELPID="ConfigMenu"ID="54221"Config MenuThe main purpose of the Config menu in the Performance Analyzer main window is to let you select the performance metrics for display and for ranking the functions in the Function List. The selections in the Config menu are:"Preferences..." lets you select which metrics display and whether they appear as absolute times and counts or percentages. Remember you can only select the types of metrics that were collected in the experiment. You can also specify how C++ file names (if appropriate) are to display:"Demangled" shows the function its argument types."As Is" uses the translator-generated "C" style name."Function" shows the function name only."Class::Function" shows the class and function.See IDREF="44001" TYPE="GRAPHIC"Figure 4-8."Sort..."lets you establish the order in which the functions appear; this helps you find questionable functions. The default order of sorting (depending on availability) is:Inclusive Times or countsExclusive Time or countsCountsSee IDREF="44001" TYPE="GRAPHIC"Figure 4-8.The performance data selections are the same for both the Preferences and Sort dialog boxes. The difference between the inclusive (Incl.) and exclusive (Excl.) metrics is that inclusive data includes a function's calls and exclusive data does not.FILE="ch0420.gif" POSITION="INLINE" SCALE="FALSE"LBL="4-8"Figure 4-8 ID="44001"Performance Analyzer Data Display OptionsFILE="ch0433.gif" POSITION="INLINE" SCALE="FALSE"LBL="4-9"Figure 4-9 ID="44001"Performance Analyzer Sort OptionsThe toggles in the Data Display Options and Sort Options are:Addressis the address of the function.Callsrefers to the number of times a function is called.Incl. Total Time, Excl. Total Timerefers to the time spent inside and outside of the CPU (by a function, source line, or instruction). It is calculated by multiplying the number of times the PC appears in any call stack by the average time interval between call stacks. Incl. CPU Time, Excl. CPU Timerefers to the time spent inside the CPU (by a function, source line, or instruction). It is calculated by multiplying the number of times a PC value appears in the profile by 10 ms.Incl. Ideal Time, Excl. Ideal Timerefers to the theoretical time spent by a function, source line, or instruction under the assumption of one machine cycle per instruction. It is useful to compare ideal time with actual. Incl. Malloc counts, Excl. Malloc countsrefers to the number of malloc, realloc, and free operations.Incl. System calls, Excl. System callsrefers to system calls.Incl. Page faults, Excl. Page faultsrefers to page faults.Incl. FP operations, Excl. FP operations refers to floating point operations.Incl. Load counts, Excl. Load counts refers to the number of load operations.Incl. Store counts, Excl. Store counts refers to the number of store operations.Incl. Bytes Read, Excl. Bytes Readrefers to the number of bytes in a read operation.Incl. Bytes Written, Excl. Bytes Writtenrefers to the number of bytes in a write operation.Incl. FP Exceptions, Excl. FP Exceptionsrefers to the number of floating point exceptions.Incl. Instructions, Excl. Instructionsrefers to the number of instructions.LBL="" HELPID="ViewsMenu"ID="20868"Views MenuFILE="f21vwsmn.gif" POSITION="MARGIN" SCALE="FALSE"LBL="4-10"Figure 4-10 ID="94093"Performance Analyzer Views MenuThe Views menu in Performance Analyzer (see IDREF="94093" TYPE="GRAPHIC"Figure 4-10) provides these selections for viewing the performance data from an experiment. Each view displays the data for the time interval bracketed by the calipers in the time line."Usage View (Graphs)"displays resource usage charts and event charts. Refer to IDREF="21919" TYPE="TITLE""Usage View (Graphs)"."Usage View (Numerical)"displays the aggregate values of resources used. Refer to IDREF="95321" TYPE="TITLE""Usage View (Numerical)"."I/O View"displays I/O events. Refer to IDREF="41945" TYPE="TITLE""I/O View"."Call Graph View"displays a call graph that shows functions and calls and their associated performance metrics. Refer to IDREF="53051" TYPE="TITLE""Call Graph View"."Leak View"displays individual leaks and their associated call stacks."Malloc View"displays individual mallocs and their associated call stacks."Heap View"displays a map of heap memory showing malloc, realloc, free, and bad free operations. Refer to IDREF="53590" TYPE="TITLE""Analyzing the Memory Map with Heap View"."Call Stack"displays the call stack for the selected event and the corresponding event type. Refer to IDREF="74395" TYPE="TITLE""Call Stack".LBL="" HELPID="ExecutableMenu"ID="29172"Executable MenuIf you enabled Track Exec'd Processes (in the Performance Panel) for the current experiment, the Executable menu will be enabled and will contain selections for any exec'd processes. These selections let you see the performance results for the other executables.LBL="" HELPID="ThreadMenu"ID="25690"Thread MenuIf your process forked any processes, the Thread menu is activated and contains selections corresponding to the different threads. Selecting a thread displays its performance results.LBL="" HELPID="UsageViewGraphs"ID="21919"Usage View (Graphs)Usage View (Graphs) displays resource usage and event charts containing the performance data from the experiment. These charts show resource usage over time and indicate where sample events took place. Sample events are shown as vertical lines. IDREF="44449" TYPE="GRAPHIC"Figure 4-11 shows the User vs system time and Page faults graphs; IDREF="37990" TYPE="GRAPHIC"Figure 4-12 shows the other graphs.FILE="ch0421.gif" POSITION="INLINE" SCALE="FALSE"LBL="4-11"Figure 4-11 ID="44449"Usage View (Graphs) Window: Top GraphsFILE="ch04.gif" POSITION="INLINE" SCALE="FALSE"LBL="4-12"Figure 4-12 ID="37990" Usage View (Graphs) Window: Lower GraphsLBL="" HELPID=""Charts in Usage View (Graphs) The available charts are:ID="ch0417"User vs system time shows CPU usage. Whenever the system clock ticks, the process occupying the CPU is charged for the entire ten millisecond interval. The time is charged either as user or system time, depending on whether the process is executing in user mode or system mode. The graph provides these annotations to show how time is spent during an experiment's process: Running (user mode), Running (system mode), Running (graphics mode), Waiting (for block I/O), Waiting (raw I/O, paging), Waiting (for memory), Waiting (in select), Waiting in CPU queue, Sleep (for resource), Sleep (for stream monitor), and Stopped (job control).ID="ch0418"Page faults shows the number of page faults that occur within a process. Major faults are those that require a physical read operation to satisfy; minor faults are those where the necessary page is already in memory but not mapped into the process's address space.Each major fault in a process takes approximately 10-50 ms. A high page fault rate is an indication of a memory-bound situation.ID="ch0419"Context switch shows the number of voluntary and involuntary context switches in the life of the process. Voluntary context switches are attributable to an operation caused by the process itself, such as a disk access or waiting for user input. These occur when the process can no longer use the CPU. A high number of voluntary context switches indicates that the process is spending a lot of time waiting for a resource other than the CPU.Involuntary context switches happen when the system scheduler decides to give the CPU to another process, even if the target process is able to use it. A high number of involuntary context switches indicates a CPU contention problem.ID="ch0420"Read/write: data sizeshows the number of bytes transferred between the process and the operating system buffers, network connections, or physical devices. KBytes read are transferred into the process' address space; KBytes written are transferred out of the process' address space.A high byte transfer rate indicates an I/O-bound process.ID="ch0421"Read/write: counts shows the number of read and write system calls made by the process.ID="ch0422"Poll and I/O calls shows the combined number of poll or select system calls (used in I/O multiplexing) and the number of I/O control system calls made by the process.ID="ch0423"ID="ch0424"Total system calls shows the total number of system calls made by the process. This includes the counts for the calls shown on the other charts.Process signals shows the total number of signals received by the process.ID="ch0425"Process size shows the total size of the process in pages and the number of pages resident in memory at the end of the time interval when the data is read. It is different from the other charts in that it shows the absolute size measured at the end of the interval and not an incremental count for that interval.If you see the process total size increasing over time when your program should be in a steady state, the process most likely has leaks and you should analyze it with Leak View and Malloc View.LBL="" HELPID=""Getting Event Information from Usage View (Graphs)The charts indicate trends; to get detailed data, you click the relevant area on the chart and the data displays in the current event line. The left mouse button displays event data; the right displays interval data.When you click the left mouse button on a sample event in a chart, the following actions take place:The point becomes selected, as indicated by the diamond marker above it. The marker appears in the time line, resource usage chart, and Usage View (Graphs) charts if the window is open.The current event line identifies the event and displays its time.The call stack corresponding to this sample point gets displayed in the Call Stack window (see IDREF="74395" TYPE="TITLE""Call Stack"). IDREF="80592" TYPE="GRAPHIC"Figure 4-13 illustrates the process of selecting a sample event.Clicking a graph with the right button displays the values for the fine-grained interval (if collection was specified) or if not, the interval bracketed by the nearest sample events.FILE="ch0422.gif" POSITION="INLINE" SCALE="FALSE"LBL="4-13"Figure 4-13 ID="80592"Effects of Selecting a Sample EventLBL="" HELPID="ProcessMeter"ID="96732"Process MeterProcess Meter lets you observe resource usage for a running process without conducting an experiment. To call Process Meter, select "Process Meter" from the Views menu in the Debugger Main View. ID="ch0426"A Process Meter window with data and its menus displayed appears in IDREF="32013" TYPE="GRAPHIC"Figure 4-14. Process Meter uses the same Admin menu as the WorkShop Debugger tools.The Charts menu options display the selected stripcharts in the Process Meter. ID="ch0427"The Scale menu adjusts the time scale in the stripchart display area such that the time selected becomes the end value.ID="ch0428"You can select which usage charts and event charts display. You can also display sample point information in the Status field by clicking within the charts. LBL="" HELPID="UsageViewNumerical"ID="95321"Usage View (Numerical)The Usage View (Numerical) window (see IDREF="44163" TYPE="GRAPHIC"Figure 4-15) shows detailed, process-specific resource usage information in a textual format for the interval defined by the calipers in the time line area of the Performance Analyzer main window. To display the Usage View (Numerical) window, select "Usage View (Numerical)" from the Views menu.The top of the window identifies the beginning and ending events for the interval. The middle portion of the window shows resource usage for the target executable. The bottom panel shows resource usage on a system-wide basis. Data is shown both as total values and as per-second rates. FILE="ch0424.gif" POSITION="INLINE" SCALE="FALSE"LBL="4-14"Figure 4-14 ID="32013" The Process Meter with Major Menus DisplayedFILE="ch0423.gif" POSITION="INLINE" SCALE="FALSE"LBL="4-15"Figure 4-15 ID="44163" Usage View (Numerical)LBL="" HELPID="I/OView"ID="41945"I/O ViewI/O View helps you determine the problems in an I/O-bound process. It produces graphs of all I/O system calls for up to 10 files involved in I/O. Clicking an I/O event with the left mouse button displays information about it in the event identification field at the top of the window. See IDREF="52155" TYPE="GRAPHIC"Figure 4-16.FILE="ch0425.gif" POSITION="INLINE" SCALE="FALSE"LBL="4-16"Figure 4-16 ID="52155" I/O ViewLBL="" HELPID="CallGraphView"ID="53051"Call Graph ViewThe Call Graph View window displays a call graph showing the functions as nodes and their calls as connecting arcs, both annotated with performance metrics (see IDREF="26120" TYPE="GRAPHIC"Figure 4-17). You bring up Call Graph View by selecting "Call Graph View" from the Views menu.FILE="ch0426.gif" POSITION="INLINE" SCALE="FALSE"LBL="4-17"Figure 4-17 ID="26120" Call Graph View with Display ControlsSince a call graph can get quite complicated, Performance Analyzer provides various controls for changing the graph display. The "Preferences" selection in the Config menu lets you specify which performance metrics display and also lets you filter out unused functions and arcs. There are two node menus in the display area; these let you filter nodes individually or as a selected group. The top row of display controls is common to all ProDev WorkShop graph displays and let you change scale, alignment, and orientation, or see an overview (see IDREF="95481" BOOK="WrkShp_OV" FILE="appa.doc" HDG=""Appendix A, "Using Graphical Views,") in the ProDev WorkShop Overview. The bottom row of controls lets you define the form of the graph: as a butterfly graph showing the functions that call and are called by a single function or as a chain graph between two functions. LBL="" HELPID=""Special Node IconsAlthough rare, nodes can be annotated with two types of graphic symbols:A right-pointing arrow in a node indicates an indirect call site. It represents a call through a function pointer. In such a case, the called function cannot be determined by the current methods.A circle in a node indicates a call to a shared library with a data-space jump table. The node name is the name of the routine called, but the actual target in the shared library cannot be identified. The table might be switched at run time, directing calls to different routines.LBL="" HELPID=""Annotating Nodes and ArcsYou can specify which performance metrics appear in the call graph as follows.LBL="" HELPID=""Node AnnotationsTo specify the performance metrics that display inside a node, you need the Preferences dialog box in the Config menu from the Performance Analyzer main view (see IDREF="44001" TYPE="GRAPHIC"Figure 4-8). LBL="" HELPID=""Arc AnnotationsArc annotations are specified by selecting "Preferences..." from the Config menu in Call Graph View (see IDREF="44001" TYPE="GRAPHIC"Figure 4-8). You can display the counts on the arcs. You can also display the percentage of calls to a function broken down by incoming arc. For an explanation of the performance metric items, refer to IDREF="54221" TYPE="TITLE""Config Menu".LBL="" HELPID=""Filtering Nodes and ArcsYou can specify which nodes and arcs appear in the call graph as follows.LBL="" HELPID=""Call Graph Preferences Filtering OptionsThe Call Graph Display Options dialog box accessed from the "Preferences" selection in the Call Graph View Config menu also lets you hide functions and arcs that have 0 (zero) calls. See IDREF="44001" TYPE="GRAPHIC"Figure 4-8.LBL="" HELPID=""ID="87672"Node MenuThere are two node menus for filtering nodes in the graph: the Node menu and the Selected Nodes menu. Both menus are shown in IDREF="92761" TYPE="GRAPHIC"Figure 4-18.The Node menu lets you filter a single node. It is displayed by holding the right mouse button down while the cursor is over the node. The name of the selected node appears at the top of the menu.FILE="ch0428.gif" POSITION="INLINE" SCALE="FALSE"LBL="4-18"Figure 4-18 ID="92761" Node MenusThe Node menu selections are:"Hide Node"removes the selected node from the call graph display."Collapse Subgraph"removes the nodes called by the selected node (and subsequently called nodes) from the call graph display."Show Immediate Children"displays the functions called by the selected node."Show Parents"displays all the functions that call the selected node."Show All Children"displays all the functions (descendants) called by the selected node.LBL="" HELPID=""Selected Nodes MenuThe Selected Nodes menu lets you filter multiple nodes. You can select multiple nodes by dragging a selection rectangle around them. You can also Shift-click a node and it will be selected along with all the nodes that it calls. Holding down the right mouse button anywhere in the graph except over a node displays the Selected Nodes menu. The Selected Nodes menu selections are:"Hide"removes the selected nodes from the call graph display."Collapse"removes the nodes called by the selected nodes (and descendant nodes) from the call graph display."Expand"displays all the functions (descendants) called by the selected nodes.LBL="" HELPID="CallGraphControls"Filtering Nodes through the Display ControlsThe lower row of controls in the panel helps you reduce the complexity of a busy call graph (see IDREF="40287" TYPE="GRAPHIC"Figure 4-19).FILE="ch0430.gif" POSITION="INLINE" SCALE="FALSE"LBL="4-19"Figure 4-19 ID="40287" Call Graph View Controls for Content ManipulationYou can perform these display operations:ID="ch0429"ID="ch0430"Butterfly presents the call graph from the perspective of a single node (the target node), showing only those nodes that call it or are called by it. Functions that call it are displayed to the left and functions it calls are on the right. Selecting any node and clicking Butterfly causes the graph to be redrawn with the selected node as the center. The selected node is displayed and highlighted in the function list.ID="ch0431"ID="ch0432"Chainlets you display all paths between a given source node and target node. The Chain dialog box is shown in IDREF="85357" TYPE="GRAPHIC"Figure 4-20. You designate the source function by selecting it or entering it in the Source Node field and clicking the ID="ch0433"Make Source button. Similarly, the target function is selected or entered and then established by clicking the Make TargetID="ch0434" button. If you wish to filter out paths that go through nodes and arcs with 0 counts, click the toggle. After these selections are made, click OK.FILE="f21chain.gif" POSITION="INLINE" SCALE="FALSE"LBL="4-20"Figure 4-20 ID="85357" Chain Dialog BoxPrune Chainsdisplays a dialog box that provides two selections for filtering paths from the call graph (see IDREF="87988" TYPE="GRAPHIC"Figure 4-21). FILE="f21prune.gif" POSITION="INLINE" SCALE="FALSE"LBL="4-21"Figure 4-21 ID="87988" Prune Chains Dialog BoxThe Prune Chains button is only activated when a chain mode operation has been performed. The dialog box selections are:The Hide paths through toggle removes from view all paths that go through the specified node. You must have a current node specified. Note that this operation is irreversible; you will not be able to re-display the hidden paths unless you perform the chain command again.The Hide paths not through toggle removes from view all paths except the ones that go through the specified node. This operation is irreversible.Important Childrenlets you focus on a function and its descendants and set thresholds to filter the descendants. You can filter the descendants either by percentage of the caller's time or by percentage of the total time. The Threshold key field identifies the type of performance time data used as the threshold. See IDREF="60760" TYPE="GRAPHIC"Figure 4-22.FILE="f21child.gif" POSITION="INLINE" SCALE="FALSE"LBL="4-22"Figure 4-22 ID="60760" Show Important Children Dialog BoxImportant Parentslets you focus on the parents of a function, that is, the functions that call it. You can set thresholds to filter only those parents making a significant number of calls, by percentage of the caller's time or by percentage of the total time. The Threshold key field identifies the type of performance time data used as the threshold. See IDREF="37954" TYPE="GRAPHIC"Figure 4-23.FILE="f21parnt.gif" POSITION="INLINE" SCALE="FALSE"LBL="4-23"Figure 4-23 ID="37954"Show Important Parents Dialog BoxClear Graphremoves all nodes and arcs from the call graph.LBL="" HELPID=""Other Manipulation of the Call GraphCall Graph View provides facilities for changing the display of the call graph without changing the data content. ID="ch0435"LBL="" HELPID="CallGraphGeometricControls"Geometric Manipulation through the Control PanelThe controls for changing the display of the call graph are in the upper row of the control panel (see IDREF="36362" TYPE="GRAPHIC"Figure 4-24). FILE="ch0431.gif" POSITION="INLINE" SCALE="FALSE"LBL="4-24"Figure 4-24 ID="36362"Call Graph View Controls for Geometric ManipulationThese facilities are:ID="ch0436"Zoom menu buttonshows the current scale of the graph. If you click this button, a pop-up menu appears displaying other available scales. The scaling range is between 15% and 300% of the normal (100%) size.ID="ch0437"Zoom Out buttonresets the scale of the graph to the next (available) smaller size in the range.ID="ch0438"Zoom In buttonresets the scale of the graph to the next (available) larger size in the range.ID="ch0439"Overview buttoninvokes an overview popup display that shows a scaled down representation of the graph. The nodes appear in the analogous places on the overview popup, and a white outline may be used to position the main graph relative to the popup. Alternatively, the main graph may be repositioned with its scroll bars.ID="ch0440"Realign buttonredraws the graph, restoring the positions of any nodes that were repositioned.ID="ch0441"Rotate buttonflips the orientation of the graph between horizontal (calling nodes at the left) and vertical (calling nodes at the top).For more information on the graphical controls, see IDREF="95481" BOOK="WrkShp_OV" FILE="appa.doc" HDG=""Appendix A, "Using Graphical Views," in the ProDev WorkShop Overview.LBL="" HELPID=""Using the Mouse in Call Graph ViewYou can move an individual node by dragging it using the middle mouse button. This helps reveal obscured arc annotations.You can select multiple nodes by dragging a selection rectangle around them. You can also shift-click a node and it will be selected along with all the nodes that it calls.LBL="" HELPID=""Selecting Nodes from the Function ListYou can also select functions from the function list to be highlighted in the call graph. You select a node from the list and then click the ID="ch0442"Show Node button in the Function List window. The node will be highlighted in the graph.LBL="" HELPID=""ID="28886"Analyzing Memory ProblemsThe Performance Analyzer provides four tools for analyzing memory problems: Malloc Error View, Leak View, Malloc View, and Heap View. Setting up and running a memory analysis experiment is the same for all four tools. After you have conducted the experiment, you can apply any of these tools.LBL="" HELPID=""Conducting Memory Leak ExperimentsTo look for memory leaks or bad frees, or perform other analysis of memory allocation, you need to run a Performance Analyzer experiment with "Find memory leaks" specified as the experiment task. You run a memory corruption experiment like any performance analysis experiment by clicking ID="ch0443"Run in the Debugger Main View. The Performance Analyzer keeps track of each malloc (memory allocation), realloc (reallocation of memory), and free. The general steps in running a memory experiment are:Link your executable with one of the special WorkShop malloc libraries (-lmalloc_cv or -lmalloc_cv_d).NoteFor a detailed discussion of the malloc libraries, see IDREF="20451" BOOK="Debugger_UG" FILE="8Heaps.doc" HDG="""Compiling With the Malloc Library" in the ProDev WorkShop Debugger User's Guide. This tutorial assumes that library -lmalloc_cv is used.Before you even run a memory experiment, you need to relink your executable with the WorkShop malloc library (libmalloc_cv) instead of the malloc library (libmalloc). You can compile it from scratch as follows:cc -g -o targetprogram targetprogram.c -lmalloc_cvor you can relink it by using:ld -o targetprogram targetprogram.o -lmalloc_cvDisplay the Performance Panel.You can bring up the Performance Panel by selecting "Performance Task..." from the Admin menu in the Debugger Main View or by typing cvspeed at the command line.Specify "Find memory leaks" as the experiment task."Find memory leaks" is a selection on the Performance Task menu in the Performance Panel. It ensures that the appropriate performance data is collected. (For more information, see "Find memory leaks" on page 301).Run the memory leak experiment.You run experiments by clicking the Run button in the Debugger Main View window. Display the Performance Analyzer.The Performance Analyzer displays results appropriate to the task selected, in this case, "Find memory leaks". IDREF="92178" TYPE="GRAPHIC"Figure 4-25 shows the Performance Analyzer window after a memory experiment (after resizing). Note the dialog box that appears when memory problems are found.FILE="ch047.gif" POSITION="INLINE" SCALE="FALSE"LBL="4-25"Figure 4-25 ID="92178" Performance Analyzer Displaying Results of a Memory ExperimentNotice that the Function List displays inclusive and exclusive bytes leaked and malloced per function. Double-clicking a function brings up Source View displaying the function's source code annotated with bytes leaked and malloced. (To pinpoint the location of a memory problem more exactly, however, it is better to use Malloc Error View or Leak View and bring up Source View pointing to the exact location of the problem.) You can set other annotations in Source View and the Function List by choosing "Preferences..." from the Config menu in the Performance Analyzer and selecting the desired items. The total process size chart displays in the usage chart portion of the window. Leakage and other memory problems cause a process's size to increase over time.Analyze the results of the experiment in Leak View when doing leak detection and Malloc Error View when performing broader memory allocation analysis. To see all memory operations whether problems or not, use Malloc View. To view memory problems within the memory map, use Heap View. To look at the source code annotated with memory problems, bring up Source View from one of the other three tools.LBL="" HELPID="MallocViews"ID="16414"Using Malloc Error View, Leak View, and Malloc ViewAfter you have run a memory experiment using the Performance Analyzer, you can analyze the results using Malloc Error View (see ID="ch0444"IDREF="53508" TYPE="GRAPHIC"Figure 4-26), Leak View (see IDREF="24108" TYPE="GRAPHIC"Figure 4-27), or Malloc View (see IDREF="85532" TYPE="GRAPHIC"Figure 4-28). Malloc View is the most general showing all memory operations. Malloc Error View shows only those memory operations that caused problems, identifying the cause of the problem and how many times it occurred. Leak View displays each memory leak that occurs in your executable, its size, the number of times the leak occurred at that location during the experiment, and the corresponding call stack (when you select the leak). Each of these views has three major areas:identification areaname='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'This indicates which operation has been selected from the list. Malloc View identifies mallocs, indicating the number of malloc locations and the size of all malloc operations in bytes. Malloc Error View identifies leaks and bad frees, indicating the number of error locations, and how many errors occurred in total. Leak View identifies leaks, indicating the number of leak locations, and the total number of bytes leaked.list areaname='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'This is a list of the appropriate types of memory operations according to the type of view. Clicking an item in the list identifies it at the top of the window and displays its call stack at the bottom of the list. The list displays in order of size.call stack areaname='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]' This displays the contents of the call stack when the selected memory operation occurred. You can double-click a frame in the call stack to see the source code that caused the leak. IDREF="43274" TYPE="GRAPHIC"Figure 4-29 shows a typical Source View window with leak annotations (you can change the annotations through the "Preferences..." selection in the Performance Analyzer Config menu). Notice that high counts display tagged.NoteAs an alternative to viewing leaks in Leak View, you can select one or more memory operations, choose "Save As Text..." from the Admin menu, and view them separately in a text file along with their call stacks. Multiple items are selected by clicking the first and then either dragging the cursor over the others or shift-clicking the last in the group to be selected.FILE="ch048.gif" POSITION="INLINE" SCALE="FALSE"LBL="4-26"Figure 4-26 ID="53508" Malloc Error View Window with Admin MenuFILE="ch049.gif" POSITION="INLINE" SCALE="FALSE"LBL="4-27"Figure 4-27 ID="24108" Leak View Window with Admin MenuFILE="ch0427.gif" POSITION="INLINE" SCALE="FALSE"LBL="4-28"Figure 4-28 ID="85532" Malloc View Window with Admin MenuFILE="ch0410.gif" POSITION="INLINE" SCALE="FALSE"LBL="4-29"Figure 4-29 ID="43274"Source View with Memory Analysis AnnotationsID="ch0445"LBL="" HELPID="HeapView"ID="53590"Analyzing the Memory Map with Heap ViewHeap View lets you analyze data from experiments based on the "Find Memory Leaks" task. The Heap View window provides a memory map that shows memory problems occurring in the time interval defined by the calipers in the Performance Analyzer window. The map indicates these memory block conditions:ID="ch0446"mallocname='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'reserved memory spacefreename='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'open spacereallocname='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'reallocated spacebad free spaceunused spaceIn addition to the Heap View memory map, you can analyze memory leak data using these other tools:If you select a memory problem in the map and bring up the Call Stack window, it will show you where the selected problem took place and the state of the call stack at that time.The function list in the Performance Analyzer main window shows inclusive mallocs and frees with bytes used by function for memory leak experiments.The Source View window shows inclusive mallocs and frees and the number of bytes used by source line.LBL="" HELPID=""Heap View WindowA typical Heap View window with its parts labeled appears in IDREF="38487" TYPE="GRAPHIC"Figure 4-30. FILE="ch0432.gif" POSITION="INLINE" SCALE="FALSE"LBL="4-30"Figure 4-30 ID="38487" Heap View WindowThe major features of a Heap View window are:Map key appears at the top of the heap map area to identify blocks by color. The actual colors depend on your color scheme. Heap View map areashows heap memory as a continuous, wrapping horizontal rectangle. The memory addresses begin at the upper left corner and progress from left to right, row by row. The rectangle is broken up into color-coded segments according to memory use status. Clicking a highlighted area in the heap map identifies the type of problem, the memory address where it occurred, and its size in the event list area and the associated call stack in the call stack display area. Note in IDREF="38487" TYPE="GRAPHIC"Figure 4-30 that there are only a few problems in the memory at the lower addresses and many more at the higher addresses.Memory event indicatorsappear color-coded in the scroll bar. Clicking an indicator with the middle button scrolls the display to the selected problem.Search fieldprovides two functions: If you enter a memory address in the field, the corresponding position will be highlighted in the heap map. If there was a problem at that location, it will be identified in the event list area. If there was no problem, the address at the beginning of the memory block and its size display.If you hold down the left mouse button and position the cursor in the heap map, the corresponding address will display in the Search field.Event list areadisplays the events occurring in the selected block. If only one event was received at the given address, its call stack is shown by default. If more than one event is shown, double-clicking an event will display its corresponding call stack.Call stack areadisplays the call stack corresponding to the event highlighted in the event list area.Malloc Errors buttoncauses a list of malloc errors and their addresses to display in the event list area. You can then enter the address of the malloc error in the Search field, press <Enter> to see the error's malloc information and its associated call stack. Zoom in button(the upward-pointing arrow) redisplays the heap area at twice the current size of the display. If you reach the limit, an error message displays.Zoom out button(the downward-pointing arrow) redisplays the heap area at half the current size (to a limit of one pixel per byte). If you reach the limit, an error message displays.LBL="" HELPID=""Source View malloc AnnotationsLike Malloc View, if you double-click a line in the call stack area of the Heap View window, the Source View window displays the portion of code containing the corresponding line, which is highlighted and indicated by a caret (^) with the number of bytes used by malloc in the annotation column. See IDREF="43274" TYPE="GRAPHIC"Figure 4-29.LBL="" HELPID=""Saving Heap View Data as TextSelecting "Save As Text..." from the Admin menu in Heap View lets you save the heap information or the event list in a text file. When you first select "Save As Text...", a dialog box displays asking you to specify heap information or the event list. After you make your selection, the Save Text dialog box displays (see IDREF="90031" TYPE="GRAPHIC"Figure 4-31). This lets you select the file name for saving the Heap View data. The default file name suggested is <experiment-directory>.out. When you click OK, the data for the current caliper setting and the list of unmatched frees, if any, is appended to the specified file.NoteThe "Save As Text..." selection in the File menu for the Source View from Heap View saves the current file. No filename default is provided, and the file that you name will be overwritten.FILE="ch0434.gif" POSITION="INLINE" SCALE="FALSE"LBL="4-31"Figure 4-31 ID="90031" Heap View Save Text Dialog BoxesLBL="" HELPID=""ID="64332"Memory Experiment TutorialIn this tutorial, you will run an experiment to analyze memory usage. The short program below generates memory problems useful that demonstrate how you can use the Performance Analyzer to detect memory problems. ID="ch0447"Go to the /usr/demos/WorkShop/mallocbug directory. Note the executable mallocbug_cv. This was compiled as follows: cc -g -o mallocbug_cv mallocbug.c -lmalloc_cv -lc Invoke the Debugger by typingcvd mallocbug_cvBring up the Performance Panel by selecting "Performance Task..." from the Admin menu in Main View.Select "Find memory leaks" from the Task menu and click the OK button. Then click Run to begin the experiment. The program runs quickly and terminates.Select "Performance Analyzer" from the "Launch Tool" submenu in the Debugger Admin menu.The Performance Analyzer window appears. A dialog box indicating malloc errors displays also.  Select "Malloc View..." from the Performance Analyzer Views menu.The Malloc View window displays, indicating two malloc locations. Select "Malloc Error View..." from the Performance Analyzer Views menu.The Malloc Error View window displays, showing one problem, a bad free, and its associated call stack. This problem occurred 99 timesSelect "Leak View..." from the Performance Analyzer Views menu.The Leak View window displays, showing one leak and its associated call stack. This leak occurred 99 times at 1,000 bytes each occurrence.Double-click the function foo in the call stack area.Source View displays showing the function's code, annotated by the exclusive and inclusive leaks.Select "Heap View..." from the Performance Analyzer Views menu.The Heap View window displays. The heap size and percentage used is shown at the top. The heap map area of the window shows the heap map as a continuous, wrapping horizontal rectangle. The rectangle is broken up into color-coded segments, according to memory use status.The color key at the top of the heap map area identifies memory usage as malloc, realloc, free, or bad free. Notice also that color-coded indicators showing mallocs, reallocs, and bad frees are displayed in the scroll bar trough. At the bottom of the heap map area are: the Search: field for identifying or finding memory locations; the Malloc Errors button for finding memory problems; a zoom-in control (upwards pointing arrow) and a zoom-out control (downwards arrow).The event display area and the call stack are at the bottom of the window. Clicking any event in the heap area displays the appropriate information in these fields.Click on any memory block in the heap map.The beginning memory address appears in the Search: field. The event information displays in the event field. The call stack information for the last event appears in the call stack area.Select other memory blocks to try out this feature.As you select other blocks, the data at the bottom of the Heap View window changes.Double-click on a frame in the call stack.A Source View window comes up with the corresponding source code displayed.Close the Source View.Click the Malloc Errors button.The data in the Heap View information window changes to display memory problems. Note that a free may be unmatched within the analysis interval, yet it may have a corresponding free outside of the interval.Click Close to leave the Heap View information window.Select "Exit" from the Admin menu in any open window to end the experiment.This ends the tutorial.LBL="" HELPID="CallStack"ID="74395"Call StackThe Call Stack window accessed from the Performance Analyzer Views menu lets you get call stack information for a sample event selected from one of the Performance Analyzer views. See ID="ch0448"IDREF="53508" TYPE="GRAPHIC"Figure 4-26. FILE="ch0435.gif" POSITION="INLINE" SCALE="FALSE"LBL="4-32"Figure 4-32 ID="53508" Performance Analyzer Call StackThere are three main areas in the window:The event identification area displays the number of the event, its time stamp, and the time within the experiment. If you have a multi-process experiment, the thread will be indicated here.The call stack area displays the contents of the call stack when the sample event took place.The event type area highlights the type of event and shows the thread in which it was defined and whether the sample was taken in all threads or the indicated thread only, in parentheses.LBL="" HELPID=""ID="52948"Analyzing Working SetsIf you suspect a problem with high page faulting or instruction cache misses, you should conduct working set analysis to determine if rearranging the order of your functions will improve performance. The term ID="ch0449"working set refers to those executable pages, functions, and instructions that are actually brought into memory during a phase or operation of the executable. If more pages are required than can fit in memory at the same time, then page thrashing, that is, swapping in and out of pages, may result slowing your program down. Strategic selection of which pages functions appear on can dramatically improve performance in such cases. You do this by creating a file containing a list of functions, their sizes, and addresses called a cord mapping file. The functions should be ordered so as to optimize page swapping efficiency. This file is then fed into the cord utility, which rearranges the functions according to the order suggested in the cord mapping file. See the reference (man) page for cord.Working set analysis is appropriate for:any program that runs for a long timeprograms whose operation comes in distinct phasesdistributed shared objects (DSOs) that are shared among several programsLBL="" HELPID=""Working Set Analysis OverviewWorkShop provides two tools to help you conduct working set analysis:Working Set View is part of the Performance Analyzer. It displays the working set of pages for each DSO that you select and indicates the degree to which the pages are used.The Cord Analyzer (cvcord) is separate from the Performance Analyzer and is invoked by typing cvcord at the command line. It displays a list of the working sets that make up a cord mapping file, shows their utilization efficiency, and most importantly, can compute an optimized ordering to reduce working sets. IDREF="33861" TYPE="GRAPHIC"Figure 4-33 presents an overview of the process of conducting working set analysis. FILE="fig21-30snap.gif" POSITION="INLINE" SCALE="FALSE"LBL="4-33"Figure 4-33 ID="33861"Working Set Analysis ProcessFirst you conduct one or more Performance Analyzer experiments using the "Get Ideal Time (pixie) per function & source line" task. You need to set sample traps at the beginning and end of each operation or phase that represents a distinct task. If you want, you can run additional experiments on the same executable to collect data for other situations in which it can be used.After you have collected the data for the experiments, you run the Performance Analyzer and select Working Set View. You need to save the working set for each phase or operation that you wish to improve. Do this by setting the calipers to bracket each phase and selecting "Save Working Set" from the Admin menu. You also must select "Save Cord Map File" to save the cord mapping file (for all runs and caliper settings). This need only be done once.The next step is to create the working set list file, which contains all of the working sets you wish to analyze using the Cord Analyzer. You create the working set list file in a text editor, specifying one line for each working set, in reverse order of priority, that is, the most important comes last. The working set list and the cord mapping file serve as input to the Cord Analyzer. The working set list provides the Cord Analyzer with working sets to be improved. The cord mapping file provides a list of all the functions in the executable. The Cord Analyzer displays the list of working sets and their utilization efficiency. It lets youexamine the page layout and efficiency of each working set with respect to the original ordering of the executableconstruct union and intersection sets as desiredview the efficiency of a different ordering construct a new cord mapping file as input to the cord utilityIf you have a new order that you would like to try out, edit your working set list file in the desired order, submit it to the Cord Analyzer, and save a new cord mapping file for input to cord.LBL="" HELPID="WorkingSetView"ID="51760"Working Set ViewWorking Set View measures the coverage of the dynamic shared objects (DSOs) that make up your executable (see ID="ch0450"IDREF="54751" TYPE="GRAPHIC"Figure 4-34). It indicates instructions, functions, and pages that were not used when the experiment was run. It shows the coverage results for each DSO in the DSO list area. Clicking a DSO in the list displays its pages with color-coding to indicate the coverage of the page.FILE="ch0436.gif" POSITION="INLINE" SCALE="FALSE"LBL="4-34"Figure 4-34 ID="54751"Working Set ViewLBL="" HELPID=""DSO List AreaThe DSO list area displays coverage information for each DSO used by the executable. It has the following columns:Text or DSO Region Nameidentifies the DSO.Ideal Timeis the percentage of ideal time for the caliper setting attributed to the DSO.Counts of: Instrs. is the number of instructions contained in the DSO.Counts of: Funcs. is the number of functions contained in the DSO.Counts of: Pages is the number of pages occupied by the DSO.% Coverage of: Instrs.is the percentage obtained by dividing the number of instructions used by the total number of instructions in the DSO.% Coverage of: Funcs. is the percentage obtained by dividing the number of functions used by the total number of functions in the DSO.% Coverage of: Pages is the coverage obtained by dividing the number of pages touched by the total pages in the DSO.Avg. Covg. of Touched: Pagesis the coverage obtained by dividing the number of instructions executed by the total number of instructions on those pages touched by the DSO.Avg. Covg. of Touched: Funcsis the average percentage use of instructions within used functions.The Search field lets you perform incremental searches to find DSOs in the DSO list. (An incremental search goes to the immediately matching target as you enter each character.)LBL="" HELPID=""DSO Identification AreaThe DSO identification area shows the address, size, and page information for the selected DSO. It also displays the address, number of instructions, and coverage for the page selected in the page display area.LBL="" HELPID=""Page Display AreaThe page display area at the bottom of the window shows all the pages in the DSO and indicates untouched pages, unused functions, executed instructions, unused instructions, and table data (related to rld). It also includes a color legend at the top to indicate how pages are used.Clicking a page displays its address, number of instructions, and coverage data in the identification area. Clicking a function in the function list of the main Performance Analyzer window highlights (using a solid rectangle) the page on which the function begins. Clicking the left mouse button on a page indicates the first function on the page by highlighting it in the function list area of the Performance Analyzer window. Similarly, clicking the middle button on a page highlights the function at the middle of the page and clicking the right button highlights the button at the end of the page. For all three button clicks, the page containing the beginning of the function becomes highlighted. Note that left clicks typically highlight the page before the one clicked since the function containing the first instruction usually starts on the previous page.LBL="" HELPID=""Admin MenuThe Admin menu provides these menu selections:"Save Working Set"saves the working set for the selected DSO. You can incorporate this file into a working set list file to be used as input to the Cord Analyzer."Save Cord Map File"saves all of the functions in the DSOs in a cord mapping file for input to the Cord Analyzer. This file corresponds to the feedback file discussed in the reference page for cord."Save Summary Data as Text"saves a text file containing the coverage statistics in the DSO list area."Save Page Data as Text"saves a text file containing the coverage statistics for each page in the DSO."Save All Data as Text"saves a text file containing the coverage statistics in the DSO list area and for each page in the selected DSO."Close"closes the Working Set View window.LBL="" HELPID="CordAnalyzer"Cord AnalyzerThe Cord Analyzer is not actually part of the Performance Analyzer; it's discussed in this part of the manual because it works in conjunction with Working Set View. The Cord Analyzer lets you explore the working set behavior of an executable or shared library (DSO). With it you can construct a feedback file for input to ID="ch0451"cord to generate an executable with improved working-set behavior. You invoke the Cord Analyzer using this syntax at the command line:cvcord -L executable [-fb feedbackFile] [-wsl workingsetList] [-ws workingsetFile] [-scheme schemeName]where-L executablespecifies a single executable file name as input.-fb feedbackFilespecifies a single text file to use as a feedback file for the executable. It should have been generated either from a Performance Analyzer experiment on the executable or DSO, or from the Cord Analyzer. If no -fb argument is given, the feedback file name will be generated as <executable>.fb.-wsl workingsetList specifies a single text file name as input; the working set list will consist of the working set files whose names appear in the input file. Each file name should be on a single line. -ws workingsetFilespecifies a single working set file name.-scheme schemeName specifies which color scheme should be used for the Cord Analyzer.The Cord Analyzer is shown in IDREF="14449" TYPE="GRAPHIC"Figure 4-35 with its major areas and menus labeled.LBL="" HELPID="WorkingSetDisplay"Working Set Display AreaThe working set display area shows all of the working sets included in the working set list file. It has the following columns:Working-set pgs. (util. %)shows the number of pages in the working set and the percentage of page space that is utilized. cord'd set pgsis the minimum number of pages for this set, that is, the number of pages the working set would occupy if the program or DSO were cord'd optimally for that specific working set.Working-set Nameidentifies the path for the working set.Note also that when the Function List is displayed, double-clicking a function displays a plus sign (+) in the working set display area to the left of any working sets that contain the function. LBL="" HELPID="WorkingSetID"Working Set Identification AreaThe working set identification area shows the name of the selected working set. It all shows the number of pages in the working set list, in the selected working set, in the cord'd working set, and used as tables. It also provides the address for the selected page, its size, and its coverage as a percentage. FILE="ch0429.gif" POSITION="INLINE" SCALE="FALSE"LBL="4-35"Figure 4-35 ID="14449"The Cord AnalyzerLBL="" HELPID="PageDisplay"Page Display AreaThe page display area at the bottom of the window shows the starting address for the DSO, its pages and their use in terms of untouched pages, unused functions, executed instructions, unused instructions, and table data (related to rld). It also includes a color legend at the top to indicate how pages are used.LBL="" HELPID="WkSetFunctionList"Function ListThe Function List displays all the functions in the selected working set. It contains these columns:Useis a count of the working sets containing the function.Addressis the starting address for the function.Insts.shows the number of instructions in the function.Function (File)identifies the function and the file in which it occurs.Note also that when the Function List is displayed, clicking a working set in the working set display area displays a plus sign (+) in the function list to the left of any functions that the working set contains. Similarly, double-clicking a function displays a plus sign in the working set display area to the left of any working sets that contain the function. The Search field lets you do incremental searches for function in the Function List.LBL="" HELPID=""Admin MenuThe Admin menu contains the standard Admin menu commands in WorkShop views (see IDREF="70545" BOOK="Debugger_UG" FILE="AReference.doc" HDG="""Admin Menu" in the ProDev WorkShop Debugger User's Guide). It has one command specific to the Cord Analyzer:"Save Working Set List"lets you save a new working set list with whatever changes you made to it in the session.LBL="" HELPID="WkSetFileMenu"File MenuThe File menu contains these commands:"Delete All Working Sets"removes all the working sets from the working set list. It does not delete any files."Delete Selected Working Set"removes the selected working set from the working set list. It asks you if you want the file deleted as well."Add Working Set"includes a new working set in the working set list."Add Working Set List from File"adds the working sets from the specified list to the current working set file."Construct Cording Feedback"builds a cord mapping file that you can supply as input to the cord utility."Construct Union of Selected Sets"lets you see a new working set built as a union of working sets. This is the same as an OR of the working sets."Construct Intersection of Selected Sets"lets you see a new working set built from the intersection of the specified working sets. This is the same as an AND of the working sets."Read Feedback File"lets you load a new cord mapping file into the Cord Analyzer.LBL="5"ID="37011"Using TesterThis chapter describes the Tester usage model. It shows the general approach of applying Tester for coverage analysis. It contains these sections:IDREF="93914" TYPE="TITLE""Tester Overview"IDREF="23533" TYPE="TITLE""Usage Model"LBL="" HELPID=""ID="93914"Tester OverviewWorkShop Tester is a UNIX‘-based software quality assurance toolset for dynamic test coverage over any set of tests. The term ID="ch051"covered means the test has executed a particular unit of source code. In this product, units are functions, individual source lines, arcs, blocks, or branches. If the unit is a branch, covered means it has been executed under both true and false conditions. This product is intended for software and test engineers and their managers involved in the development, test, and maintenance of long-lived software projects. WorkShop Tester provides these general benefits:Provides visualization of coverage data, which yields immediate insight into quality issues at both engineering and management levels.Provides useful measures of test coverage over a set of tests/experiments.Lets you view the coverage results of a dynamically shared object (DSO) by executables that use it.ID="ch052"ID="ch053"Provides comparison of coverage over different program versions.Provides tracing capabilities for arguments and function arcs that go beyond traditional test coverage tools.Supports programs written in C, C++, and Fortran.Is integrated into the CASEVision family of products.Allows users to build and maintain higher quality software products.There are two versions of Tester:cvcov is the command line version of the test coverage program. cvxcov is the GUI version of the test coverage program. ID="ch054"Most of the functionality is available from either program, although the graphical representations of the data are available only from cvxcov, the GUI tool.LBL="" HELPID=""Test Coverage DataTester provides the following basic coverage:ID="ch055"Basic blockname='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'how many times was this basic block executed?Functionname='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'how many times was this function executed?Branchname='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'did this condition take on both TRUE and FALSE values?You can also request the following coverage information:Arcname='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'was function F called by function A and function B? Which arcs for function F were NOT taken?Source line coveragename='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'how many times has this source line been executed and what percentage of source lines is covered?Argumentname='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'what were the maximum and minimum values for argument X in function F over all tests?When the target program execs, forks, or sprocs another program, only the main target is tested, unless you specify which executables are to be tested, the parent and/or child programs. NoteWhen you compile with theID="ch056" -g flag, you may create assembly blocks and branches that can never be executed, thus preventing "full" coverage from being achieved. These are usually negligible. However, if you compile with the 01 flag (the default), you can increase the number of executable blocks and branches. LBL="" HELPID=""Types of ExperimentsYou can conduct Tester coverage experiments for:Separate tests.A set of tests operating on the same executable.A list of executables related by fork, exec, or sproc commands.A test group of executables sharing a common dynamically shared object (DSO).LBL="" HELPID=""Experiment ResultsTester presents the experiment results in these reports:ID="ch057"Summary of test coverage, including user parameterized dynamic coverage metric.List of functions, which can be sorted by count, file, or function name and filtered by percentage of block, branch, or function covered.Comparison of test coverage between different versions of the same program.Source or assembly code listing annotated with coverage data.Breakdown of coverage according to contribution by tests within a test set or test group.The graphical user interface lets you view test results in different contexts to make them more meaningful. It provides:Annotated function call graph highlighting coverage by counts and percentage (ASCII function call graph supported as well).Annotated Source View showing coverage at the source language level.Annotated Disassembly View showing coverage at the assembly language level.Bar chart summary showing coverage by functions, lines, blocks, branches, and arcs.LBL="" HELPID=""Multiple TestsTester supports multiple tests. You can:ID="ch058"define and run a test set to cover the same program.define and run a test group to cover programs sharing a common DSO. This approach is useful if you want to test different client programs that bind with the same libraries.ID="ch059"automate test execution via command line interface as well as GUI mode.LBL="" HELPID=""Test ComponentsEach test is a named object containing the following:ID="ch0510"instrumentation filename='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'This describes the data to be collected. ID="ch0511"executablename='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'This is the program being instrumented for coverage analysis.executable listname='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'If the program you are testing can fork, exec, or sproc other executables and you want these other executables included in the test, then you can specify a list of executables for this purpose.commandname='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'This defines the program and command line arguments.instrumentation directoryname='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'The instrumentation directory contains directories representing different versions of the instrumented program and related data. Instrumentation directories are named ID="ch0512"ver##<n> where n is the version number. Several tests can share the same instrumentation directory. This is true for tests with the same instrumentation file and program version. The instrumentation directory contains the following files, which are automatically generated:   <program|DSO>.Arg     optional arg trace file
   <program|DSO>.Binmap  basic block & branches bitmap file
   <program|DSO>.Graph   arc data
   <program|DSO>.Log     instrumentation log file (cvinstr)
   <program|DSO>.Map     function map file
   <program|DSO>_Instr   instrumented executable  As part of instrumentation, you can filter the functions to be included or excluded in your test, through the directives INCLUDE, EXCLUDE, and CONSTRAIN.experiment resultsname='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'Test run coverage results are deposited in a results directory. Results directories are named ID="ch0513"exp##<n> where n corresponds to the instrumentation directory used in the experiment. There is one results directory for each version of the program in the instrumentation directory for this test. Note that results are not deposited in the instrumentation directory because the instrumentation directory may be shared by other tests. The results directory is different when you run the test with or without the -keep option.When you run your test without the -keep option the results directory contains the following files:COLUMNS="2"LEFT="0" WIDTH="117"COV_DESCLEFT="125" WIDTH="198"description file of experimentLEFT="0" WIDTH="117"COUNTS_<exe>LEFT="125" WIDTH="198"counts file for each executable; <exe> is an 
executable file nameLEFT="0" WIDTH="117"USER_SELECTIONSLEFT="125" WIDTH="198"instrumentation criteriaWhen you run your test with the -keep option the results directory contains the following files:COLUMNS="2"LEFT="0" WIDTH="117"COV_DESCLEFT="125" WIDTH="198"description file of experimentLEFT="0" WIDTH="117"COUNTS_ <exe>LEFT="125" WIDTH="198"counts file for each executable; <exe> is an 
executable file name.LEFT="0" WIDTH="117"USER_SELECTIONSLEFT="125" WIDTH="198"instrumentation criteriaLEFT="0" WIDTH="117"ARGTRACE_<n>LEFT="125" WIDTH="198"argument trace database; <n> is a unique number 
for each processLEFT="0" WIDTH="117"COUNTS_<n>LEFT="125" WIDTH="198"basic block and branch counts databaseLEFT="0" WIDTH="117"DESCLEFT="125" WIDTH="198"experiment description fileLEFT="0" WIDTH="117"FPTRACE_<n>LEFT="125" WIDTH="198"function pointer tracing databaseLEFT="0" WIDTH="117"LOGLEFT="125" WIDTH="198"experiment log file (cvmon)LEFT="0" WIDTH="117"TRAPLEFT="125" WIDTH="198"N/ALEFT="0" WIDTH="117"USAGE_<n>LEFT="125" WIDTH="198"N/AThere are also soft links of the instrumentation data files in the results directory to the instrumentation directory described above.LBL="" HELPID=""ID="23533"Usage ModelThis section is divided into three parts:ID="ch0514"IDREF="71981" TYPE="TITLE""Single Test Analysis Process" shows the general steps in conducting a test.IDREF="23612" TYPE="TITLE""Automated Testing" discusses using scripts to automate your testing.IDREF="58490" TYPE="TITLE""Additional Coverage Testing" describes strategies using multiple tests.LBL="" HELPID=""ID="71981"Single Test Analysis ProcessIn performing coverage analysis for a single test, you typically go through the following steps:ID="ch0515"Plan your test.Test tools are only as good as the quality and completeness of the tests themselves.ID="21036"Create (or reuse) an instrumentation file.The instrumentation file defines the coverage data you wish to collect in this test. You can define:ID="ch0516"COUNTSname='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'three types of count items perform tracking. ID="ch0517"bbcounts tracks execution of basic blocks. fpcounts counts calls to functions through function pointers. branchcounts tracks branches at the assembly language level.INCLUDE/EXCLUDEname='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'lets you define a subset of functions to be covered. INCLUDE adds the named functions to the current set of functions. EXCLUDE removes the named functions from the set of functions. Simple pattern matching is supported for pathnames and function names. The basic component for inclusion/exclusion is of the form: ID="ch0518"<shared library | program name>:<functionlist>INCLUDE, EXCLUDE, and CONSTRAIN (see below) play a major role in working with DSOs. Tester instruments all DSOs in an executable whether you are testing them or not, so it is necessary to restrict your coverage accordingly. By default, the directory /usr/tmp/cvinstrlib/CacheExclude is used as the excluded DSOs cache and /usr/tmp/cvinstrlib/CacheInclude as the included DSOs cache. If you wish to override these defaults, set the CVINSTRLIB environment variable to the desired cache directory.CONSTRAINname='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'equivalent to EXCLUDE *, INCLUDE <ID="ch0519"subset>. Thus, the only functions in the test will be those named in the CONSTRAIN subset. You can constrain the set of functions in the program to either a list of functions or a file containing the functions to be constrained. The function list file format is:function_1function_2function_3...You can use the -file option to include an ASCII file containing all the functions as follows:CONSTRAIN -file filenameTRACEname='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'lets you monitor argument values in the functions over all experiments. The only restriction is that the arguments must be of the following basic types: ID="ch0520"int, char, long, float, double, or pointer (treated as a 4-byte unsigned int). MAX monitors the maximum value of an argument. MIN monitors the minimum value of an argument. BOUNDS monitors both the minimum and maximum values. RETURN monitors the function return values. ID="ch0521"ID="ch0522"ID="ch0523"ID="ch0524"The default instrumentation file ID="ch0525"/usr/WorkShop/usr/lib/WorkShop/Tester/default_instr_file contains:COUNTS -bbcounts -fpcounts -branchcounts
EXCLUDE libc.so.1:*
EXCLUDE libC.so:*
EXCLUDE libInventor.so:*
EXCLUDE libMrm.so.1:*
EXCLUDE libUil.so.1:*
EXCLUDE libX11.so.1:*
EXCLUDE libXaw.so:*
EXCLUDE libXawI18n.so:*
EXCLUDE libXext.so:*
EXCLUDE libXi.so:*
EXCLUDE libXm.so.1:*
EXCLUDE libXmu.so:*
EXCLUDE libXt.so:*
EXCLUDE libcrypt.so:*
EXCLUDE libcurses.so:*
EXCLUDE libdl.so:*
EXCLUDE libfm.so:*
EXCLUDE libgen.so:*
EXCLUDE libgl.so:*
EXCLUDE libil.so:*
EXCLUDE libks.so:*
EXCLUDE libmf.so:*
EXCLUDE libmls.so:*
EXCLUDE libmutex.so:*
EXCLUDE libnsl.so:*
EXCLUDE librpcsvc.so:*
EXCLUDE libsocket.so:*
EXCLUDE libtbs.so:*
EXCLUDE libtermcap.so:*
EXCLUDE libtermlib.so:*
EXCLUDE libtt.so:*
EXCLUDE libview.so:*
EXCLUDE libw.so:*
EXCLUDE nis.so:*
EXCLUDE resolv.so:*
EXCLUDE straddr.so:*
EXCLUDE tcpip.so:*The excluded items are all dynamically shared objects that might interfere with the testing of your main program.NoteIf you do not use the default_instr_file file, functions in shared libraries will be included by default, unless your instrumentation file excludes them.The minimum instrumentation file contains the line: COUNTS -bbcountsYou create an instrumentation file using your preferred text editor. Comments are allowed only at the beginning of a new line and are designated by the "#" character. Lines can be continued using a back slash (\) for lists separated with commas. White space is ignored. Keywords are case insensitive. Options and user-supplied names are case sensitive. All lines are additive to the overall experiment description.Here is a typical instrument file:COUNTS -bbcounts -fpcounts -branchcounts# defines the counting options, in this case,# basic blocks, function pointers, and branches.CONSTRAIN program:abc, xdr*, functionF, \ classX::methodY, *::methodM, functionG# constrains the set of functions in the # "program" to the list of user specified functionsTRACE BOUNDS functionF(argA)# traces the upper and lower values of argATRACE MAX classX::methodY(argZ)# traces the maximum value of argZEXCLUDE libc.so.1:*
...NoteInstrumentation can increase the size of a program two to five times. Using DSO caching and sharing can alleviate this problem.Apply the instrument file to the target executable(s).This is the instrumentation process. You can specify a single executable or more than one if you are creating other processes through ID="ch0526"fork, exec, or sproc.The command line interface command is runinstr. The graphical user interface equivalent is the "Run Instrumentation" selection in the Test menu.ID="ch0527"The effect of performing a run instrument operation is shown in IDREF="10563" TYPE="GRAPHIC"Figure 5-1. An instrumentation directory is created (.../ver##<n>). It contains the instrumented executable and other files used in instrumentation.FILE="fig22-1snap.gif" POSITION="INLINE" SCALE="FALSE"LBL="5-1"Figure 5-1  ID="10563"Instrumentation ProcessID="55711"Create the test directory.This part of the process creates a test data directory (ID="ch0528"test0000) containing a test description file named TDF. See IDREF="14824" TYPE="GRAPHIC"Figure 5-2.FILE="fig22-2snap.gif" POSITION="INLINE" SCALE="FALSE"LBL="5-2"Figure 5-2  ID="14824"Make Test ProcessID="ch0529"Tester names the test directory test0000 by default and increments it automatically for subsequent make test operations. You can supply your own name for the test directory if you prefer. The TDF file contains information necessary for running the test. A typical ID="ch0530"TDF file contains the test name, type, command-line arguments, instrument directory, description, and list of executables. In addition, for a test set or test group, the TDF file contains a list of subtests.Note that Instrument Directory can be either the instrumentation directory itself (such as ver##0) or a directory containing one or more instrumentation subdirectories.The command line interface command is mktest. The graphical user interface equivalent is the "Make Test" selection in the Test menu.Run the instrumented version of the executable to collect the coverage data.This creates a subdirectory (ID="ch0531"exp##0) under the test directory in which results from the current experiment will be placed. See IDREF="62321" TYPE="GRAPHIC"Figure 5-3. The ID="ch0532"commands to run a test use the most recent instrumentation directory version unless you specify a different directory.FILE="fig22-3snap.gif" POSITION="INLINE" SCALE="FALSE"LBL="5-3"Figure 5-3  ID="62321"Run Test ProcessThe command line interface command is runtest. The graphical user interface equivalent is the "Run Test" selection in the Test menu.Analyze the results.Tester provides a variety of column-based presentations for analyzing the results. The data can be sorted by a number of criteria. In addition, the graphical user interface can display a call graph indicating coverage by function and call.ID="ch0533"The Tester interface provides many kinds of queries for performing analysis on a single test. IDREF="53703" TYPE="TABLE"Table 5-1 shows query commands for a single test that are available either from the command line or the graphical user interface Queries menu. COLUMNS="3"LBL="5-1"Table 5-1 ID="53703"Common Queries for a Single TestLEFT="0" WIDTH="46"Command
 LineLEFT="55" WIDTH="94"Graphical User 
InterfaceLEFT="155" WIDTH="190"DescriptionLEFT="0" WIDTH="46"lsarcID="ch0534"LEFT="55" WIDTH="94"List ArcsLEFT="155" WIDTH="190"Shows the function arc coverage. An arc is a call 
from one function to another.LEFT="0" WIDTH="46"lsblockID="ch0535"LEFT="55" WIDTH="94"List BlocksLEFT="155" WIDTH="190"Shows basic block count information.LEFT="0" WIDTH="46"lsbranchID="ch0536"LEFT="55" WIDTH="94"List BranchesLEFT="155" WIDTH="190"Shows the count information for assembly 
language branches.LEFT="0" WIDTH="46"lsfunID="ch0537"LEFT="55" WIDTH="94"List FunctionsLEFT="155" WIDTH="190"Shows coverage by function.LEFT="0" WIDTH="46"lssumID="ch0538"LEFT="55" WIDTH="94"List SummaryLEFT="155" WIDTH="190"Provides a summary of overall coverage.LEFT="0" WIDTH="46"lstraceID="ch0539"LEFT="55" WIDTH="94"List Argument TracesLEFT="155" WIDTH="190"Shows the results of argument tracing, 
including argument, type, and range.LEFT="0" WIDTH="46"lslineLEFT="55" WIDTH="94"List Line CoverageLEFT="155" WIDTH="190"Shows coverage for native source lines.LEFT="0" WIDTH="46"cattestLEFT="55" WIDTH="94"Describe TestLEFT="155" WIDTH="190"Describes the test details.LEFT="0" WIDTH="46"diffLEFT="55" WIDTH="94"Compare TestLEFT="155" WIDTH="190"Shows the difference in coverage between 
programs.LEFT="0" WIDTH="46"lsinstrLEFT="55" WIDTH="94"List InstrumentationLEFT="155" WIDTH="190"Show instrumentation details for a test.Other queries are accessed differently from either interface. lscallID="ch0540"name='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'shows a function graph indicating caller and callee functions and their counts. From the graphical user interface, function graphs are accessed from a Call Tree View (Views menu selection).lssourceID="ch0541"name='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'displays the source or assembly code annotated with the execution count by line. From the graphical user interface, you access source or assembly code from a Source View (using the Source button) or a Disassembly View (using the Disassembly button), respectively.The queries available in the graphical user interface are shown in ID="ch0542"IDREF="38017" TYPE="GRAPHIC"Figure 5-4. FILE="f22qurmn.gif" POSITION="INLINE" SCALE="FALSE"LBL="5-4"Figure 5-4 ID="38017" The Queries Menu from Main Tester WindowLBL="" HELPID=""ID="23612"Automated TestingTester is best suited to automated testing of command-line programs, where the test behavior can be completely specified at the invocation. Command-line programs let you incorporate contextual information, such as environment variables and current working directory. ID="ch0543"Automated testing of server processes in a client-server application proceeds basically the same as single-program cases except that startup time introduces a new factor. Tester can substantially increase the startup time of your target process so that the instrumented target process will run somewhat slower than the standard, uninstrumented one. Tests which start a server, wait a while for it to be ready, and then start the client will have to wait considerably longer. The additional time depends on the size and complexity of the server process itself and on how much and what kind of data you have asked Tester to collect. You will have to experiment to see how long to wait. Automated testing of interactive or nondeterministic tests is somewhat harder. These tests are not completely determined by their command line; they can produce different results (and display different coverage) from the same command line, depending upon other factors, such as user input or the timing of events. For tests such as these, Tester provides a -sum argument to the runtest command. Normally each test run is treated as an independent event, but when you use "runtest -sum," the coverage from each run is added to the coverage from previous runs of the same test case. Other details of the coverage measurement process are identical to the first case. In each case, you first need to instrument your target program, then run the test, sum the test results if desired, and finally analyze the results. There are two general approaches to applying cvcov in automated testingIf you have not yet created any test scripts or have a small number of tests, you should create a script that makes each test individually and then runs the complete test set. See IDREF="59439" TYPE="TEXT"Example 5-1, a script that automates a test program called target with different arguments:LBL="5-1"Example 5-1 ID="59439"Making Tests and Then Running Them# instrument program
cvcov runinstr -instr_file instrfile mypath/target
# test machinery
# make all tests
cvcov mktest -cmd "target A B C" -testname test0001
cvcov mktest -cmd "target D E F" -testname test0002
...
# define testset to include all tests
cvcov lstest > mytest_list
cvcov mktset -list mytest_list -testname mytestset
# run all tests in testset and sum up results
cvcov runtest mytestsetIf you have existing test scripts of substantial size or an automated test machinery setup, then you may find it straightforward to embed Tester by replacing each test line with a script containing two Tester command lines for making and running the test and then accumulating the results in a testset, such as in IDREF="87341" TYPE="TEXT"Example 5-2. Of course, you can also rewrite the whole test machinery as described in IDREF="59439" TYPE="TEXT"Example 5-1.LBL="5-2"Example 5-2 ID="87341"Applying a Make-and-Run Script# instrument program
cvcov runinstr -instr_file instrfile mypath/target
# test machinery
# make and run all tests
make_and_run "target A B C"
make_and_run "target D E F"
...
# make testset
cvcov lstest > mytestlist
cvcov mktset -list mytestlist -testname mytestset
# accumulate results
cvcov runtest mytestsetwhere the make_and_run script is:#!/bin/sh
testname=`cvcov mktest -instr_dir /usr/tmp -cmd "$*"`
testname=`expr "$testname" : ".*Made test directory: `.*'"`
cvcov runtest $testnameNote that both examples use simple testset structuresname='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'these could have been nested hierarchically if desired.After running your test machinery, you can use cvcov or cvxcov to analyze your results. Make sure that your test machinery does not remove the products of the test run (even if the test succeeds), or it may destroy the test coverage data. LBL="" HELPID=""ID="58490"Additional Coverage TestingAfter you have created and run your first test, you typically need additional testing. Here are some scenarios.ID="ch0544"You can define a test set so that you can vary your coverage using the same instrumentation. You can analyze the new tests singly or you can combine them in a set and look at the cumulative results. If the tests are based on the same executable, they can share the same instrumentation file. You can also have a test set with tests based on different executables but they should have the same instrumentation file.You can change the instrumentation criteria to gather different counts, examine a different set of functions, or perform argument tracing differently.You can create a script to run tests in batch mode (command line interface only).You can run different programs that use a common dynamically shared object (DSO) and accumulate test coverage for a test group containing the DSO.ID="ch0545"You can run the same tests using the same instrumentation criteria for two versions of the same program and compare the coverage differences.You can run a test multiple times and sum the result over the runs. This is typically used for GUI-based applications.As you conduct more tests, you will be creating more directories. A typical coverage testing hierarchy is shown in ID="ch0546"IDREF="53335" TYPE="GRAPHIC"Figure 5-5.There are two different instrumentation directories, ver##0 and ver##1. The test directory test0000 contains results for a single experiment that uses the instrumentation from ver##0. (Note that the number in the name of the experiment results directory corresponds to the number of the instrumentation directory.) Test directory test0001 has results for two experiments corresponding to both instrumentation directories, ver##0 and ver##1.FILE="fig22-5snap.gif" POSITION="INLINE" SCALE="FALSE"LBL="5-5"Figure 5-5 ID="53335" Typical Coverage Testing HierarchyLBL="6"ID="40637"Tester Command Line Interface TutorialThe tutorials in this chapter are based on simple programs written in C. To run them, you need the C compiler. The chapter is broken down into these sections:ID="ch061"IDREF="75972" TYPE="TITLE""Setting Up the Tutorials" shows you how to run the script that creates the files needed for the tutorials.IDREF="89782" TYPE="TITLE""Tutorial #1 - Analyzing a Single Test" takes you through the steps of performing coverage analysis for a single test.IDREF="54062" TYPE="TITLE""Tutorial #2 - Analyzing a Test Set" discusses creating additional tests to achieve full coverage.IDREF="23290" TYPE="TITLE""Tutorial #3 - Optimizing a Test Set" explains how to fine-tune a test set to eliminate redundant tests. IDREF="76469" TYPE="TITLE""Tutorial #4 - Analyzing a Test Group" explains how you would use a test group to analyze the coverage of a dynamically shared object (DSO) in different executables sharing the DSO.Note that if you are going to run these tutorials, you must run them in order; each tutorial builds on the results of previous tutorials.If you'd rather have the test data built automatically, run the script: /usr/demos/WorkShop/Tester/setup_Tester_demoIf at any time a command syntax is not clear, enter:ID="ch062"% cvcov help < commandname > LBL="" HELPID=""ID="75972"Setting Up the TutorialsEnter the following to set up the tutorials:ID="ch063"% cp -r /usr/demos/WorkShop/Tester /usr/tmp/tutorial 
% cd /usr/tmp/tutorial 
% echo ABCDEFGHIJKLMNOPQRSTUVWXYZ > alphabet 
% make -f Makefile.tutorial copyn This moves some scripts and source files used in the tutorial to /usr/tmp/tutorial, creates a test file named alphabet, and makes a simple program, copyn, which copies n bytes from a source file to a target file. To see how the program works, try a simple test by typing:% copyn alphabet targetfile 10 
% cat targetfile 
ABCDEFGHIJ You should see the first 10 bytes of alphabet copied to targetfile.LBL="" HELPID=""ID="89782"Tutorial #1 - Analyzing a Single TestTutorial #1 discusses the following topics:Instrumenting an executableMaking a testRunning a test Analyzing test coverage dataLBL="" HELPID=""Instrumenting an ExecutableThis is the first step in providing test coverage. The user defines the instrumentation criteria in an instrumentation file. ID="ch064"ID="ch065"Enter the following to see the instrumentation directives in the file tut_instr_file used in the tutorials:% cat tut_instr_file 
COUNTS -bbcounts -fpcounts -branchcounts 
CONSTRAIN main, copy_file
TRACE BOUNDS copy_file(size)We will be getting all counting information (blocks, functions, branches, and arcs) for the two functions specified in the CONSTRAIN directive, main and copy_file. We will also be tracing the size argument for the copy_file function.Enter the following command to instrument ID="ch066"copyn:% cvcov runinstr -instr_file tut_instr_file copyn 
cvcov: Instrument "copyn" of version "0" succeeded. Directory ID="ch067"ver##0 has been created by default. This contains the instrumented executable, copyn_Instr, and other instrumentation data.Making a TestA ID="ch068"test defines the program and arguments to be run, instrument directory, executables, and descriptive information about the test. Enter the following to make a test:% cvcov mktest -cmd "copyn alphabet targetfile 20" You will see the message:cvcov: Made test directory: "/usr/var/tmp/tutorial/test0000"Directory ID="ch069"test0000 has been created by default. It contains a single file, TDF, the test description file. NoteThe directory /usr/var/tmp is linked to /usr/tmp.Enter the following to get a textual listing of the test:% cvcov cattest test0000 
Test Info               Settings 
-----------------------------------------------------
Test                    /usr/var/tmp/tutorial/test0000
Type                    single
Description              
Command Line            copyn alphabet targetfile 20
Number of Exes          1
Exe List                copyn
Instrument Directory    /usr/var/tmp/tutorial
Experiment List Running a TestTo run a test, we use technology from the WorkShop Performance Analyzer. The instrumented process is set to run, and a monitor process (ID="ch0610"cvmon) captures test coverage data by interacting with the WorkShop process control server (cvpcs). Enter the following command:% cvcov runtest test0000 You will see the message:cvcov: Running test "/usr/var/tmp/tutorial/test0000" ...Now the directory test0000 contains the directory exp##0, which contains the results of the first test experiment.Analyzing Test Coverage DataYou can analyze test coverage data many ways. In this tutorial, we will illustrate a simple top-down approach. We'll start at the top to get a summary of overall coverage, proceed to the function level, and go finally to the actual source lines. ID="ch0611"Enter the following to get the summary:% cvcov lssum test0000You will see the display shown in IDREF="58186" TYPE="TEXT"Example 6-1.LBL="6-1"Example 6-1 ID="58186"lssum Example% cvcov lssum test0000

Coverages                   Covered     Total       % Coverage    Weight
-------------------------------------------------------------------------
Function                    2           2           100.00%       0.400
Source Line                 17          35          48.57%        0.200  
Branch                      0           10          0.00%         0.200
Arc                         8           18          44.44%        0.200
Block                       19          42          45.24%        0.000
Weighted Sum                                        58.60%        1.000Notice that although both functions have been covered, we have incomplete coverage for source lines, branches, arcs, and blocks.ID="ch0612"NoteItems are highlighted on your screen to emphasize null coverage. As a convention in this manual, we're showing highlighting or user input in boldface.Enter the following to look at the line count information for the ID="ch0613"main function:% cvcov lssource main test0000This produces a source listing annotated with counts, shown in IDREF="20709" TYPE="TEXT"Example 6-2.LBL="6-2"Example 6-2 ID="20709"lssource Example% cvcov lssource main test0000
Counts  Source
--------------------------------------------------------------------
        #include <stdio.h>
        #include <sys/types.h>
        #include <sys/stat.h>
        #include <fcntl.h>
        #define OPEN_ERR         1
        #define NOT_ENOUGH_BYTES 2
        #define SIZE_0           3
 
        int copy_file();
        main (int argc, char *argv[])
1       {
            int bytes, status;
1           if( argc < 4){
0               printf("copyn: Insufficient arguments.\n"); 
0               printf("Usage: copyn f1 f2 bytes\n"); 
0               exit(1); 
            }
1           if( argc > 4 ) {
0               printf("Error: Too many arguments\n"); 
0               printf("Usage: copyn f1 f2 bytes\n"); 
0               exit(1); 
            }
1           bytes = atoi(argv[3]);
1           if(( status = copy_file(argv[1], argv[2], bytes)) >0){
0              switch ( status) { 
                   case SIZE_0:
0                      printf("Nothing to copy\n"); 
0                      break; 
                   case NOT_ENOUGH_BYTES:
0                      printf("Not enough bytes\n"); 
0                      break; 
                   case OPEN_ERR:
0                      printf("File open error\n"); 
0                      break; 
               }
0              exit(1); 
            }
1       }

 int copy_file( source, destn, size)
        char *source, *destn;
        int size;
1       {
            char *buf;
            int fd1, fd2;
            struct stat fstat;
1           if( (fd1 = open( source, O_RDONLY)) <= 0){
0               return OPEN_ERR; 
            }
1           stat( source, &fstat);
1           if( size <= 0){
0               return SIZE_0; 
            }
1           if( fstat.st_size < size){
0               return NOT_ENOUGH_BYTES; 
            }
1           if( (fd2 = creat( destn, 00777)) <= 0){
0               return OPEN_ERR; 
            }
1           buf = (char *)malloc(size);
            
1          read( fd1, buf, size);
1          write( fd2, buf, size);
1          return 0;
0       }Notice that the 0-counted lines appear in a highlight color. In this example, the lines with 0 counts occur where there is an error condition. This is our first good look at branch and block coverage at the source line level. The branch and block coverage in the summary are at the assembly language level.LBL="" HELPID=""ID="54062"Tutorial #2 - Analyzing a Test SetIn the second tutorial, we are going to create additional tests with the objective of achieving 100% overall coverage. From examining the source code in IDREF="20709" TYPE="TEXT"Example 6-2, it seems that the 0-count lines in main and copy_file are due to error-checking code that is not tested by test0000.NoteThis tutorial needs test0000, which was created in the previous tutorial.The script tut_make_testset is supplied to demonstrate how to set up this test set. Enter sh -x tut_make_testset to run the script.IDREF="28748" TYPE="TEXT"Example 6-3 shows the first portion of the script (as it runs), in which the individual tests are created. The tut_make_testset script uses mktest to create eight additional tests. The tests test0001 and test0002 pass too few and too many arguments, respectively. test0003 attempts to copy from a nonexistent file named no_file. test0004 attempts to pass 0 bytes, which is illegal. test0005 attempts to copy 20 bytes from a file called not_enough, which contains only one byte. In test0006, we attempt to write to a directory without proper permission. test0007 tries to copy too many bytes. In test0008, we attempt to copy from a file without read permission. LBL="6-3"Example 6-3 ID="28748"tut_make_testset Script: Making Individual Tests% sh -x tut_make_testset
+ cvcov mktest -cmd copyn alphabet target -des not enough arguments
cvcov: Made test directory: "/usr/var/tmp/tutorial/test0001"
+ cvcov mktest -cmd copyn alphabet target 20 extra_arg \-des too many arguments
cvcov: Made test directory: "/usr/var/tmp/tutorial/test0002"
+ cvcov mktest -cmd copyn no_file target 20 -des cannot access file
cvcov: Made test directory: "/usr/var/tmp/tutorial/test0003"
+ cvcov mktest -cmd copyn alphabet target 0 -des pass bad size arg
cvcov: Made test directory: "/usr/var/tmp/tutorial/test0004"
+ echo a 
+ cvcov mktest -cmd copyn not_enough target 20 -des not enough data \(less bytes than requested) in original file 
cvcov: Made test directory: "/usr/var/tmp/tutorial/test0005"
+ cvcov mktest -cmd copyn alphabet /usr/bin/target 20 \-des cannot create target executable due to permission problems 
cvcov: Made test directory: "/usr/var/tmp/tutorial/test0006"
+ ls -ld /usr/bin 
drwxr-xr-x    3 root     sys         3584 May 12 18:25 /usr/bin
+ cvcov mktest -cmd copyn alphabet targetfile 200 -des size arg too big
cvcov: Made test directory: "/usr/var/tmp/tutorial/test0007"
+ cvcov mktest -cmd copyn /usr/adm/sulog targetfile 20 \-des no read permission on source file
cvcov: Made test directory: "/usr/var/tmp/tutorial/test0008"After the individual tests are created, the script uses mktset to make a new test set and addtest to include the new tests in the set. IDREF="45837" TYPE="TEXT"Example 6-4 shows the portion of the script in which the test set is created and the individual tests are added to the test set.LBL="6-4"Example 6-4 ID="45837"tut_make_testset Script: Making and Adding to the Test Set+ cvcov mktset -des full coverage testset -testname tut_testset 
cvcov: Made test directory: "/usr/var/tmp/tutorial/tut_testset"
+ cvcov addtest test0000 tut_testset 
cvcov: Added "/usr/var/tmp/tutorial/test0000" to "tut_testset"
+ cvcov addtest test0001 tut_testset 
cvcov: Added "/usr/var/tmp/tutorial/test0001" to "tut_testset"
+ cvcov addtest test0002 tut_testset 
cvcov: Added "/usr/var/tmp/tutorial/test0002" to "tut_testset"
+ cvcov addtest test0003 tut_testset 
cvcov: Added "/usr/var/tmp/tutorial/test0003" to "tut_testset"
+ cvcov addtest test0004 tut_testset 
cvcov: Added "/usr/var/tmp/tutorial/test0004" to "tut_testset"
+ cvcov addtest test0005 tut_testset 
cvcov: Added "/usr/var/tmp/tutorial/test0005" to "tut_testset"
+ cvcov addtest test0006 tut_testset 
cvcov: Added "/usr/var/tmp/tutorial/test0006" to "tut_testset"
+ cvcov addtest test0007 tut_testset 
cvcov: Added "/usr/var/tmp/tutorial/test0007" to "tut_testset"
+ cvcov addtest test0008 tut_testset 
cvcov: Added "/usr/var/tmp/tutorial/test0008" to "tut_testset"Enter cvcov cattest tut_testset to check that the new test set was created correctly.This is shown in IDREF="63783" TYPE="TEXT"Example 6-5. The index numbers in brackets in the subtest list are used to identify the individual tests as part of a test set. This index is used to list the contribution of each test.LBL="6-5"Example 6-5 ID="63783"Contents of the New Test Set% cvcov cattest tut_testset
Test Info                Settings
--------------------------------------------------------
Test                    /usr/var/tmp/tutorial/tut_testset
Type                    set
Description             full coverage testset
Number of Exes          1
Exe List                copyn
Number of Subtests      9
Subtest List             
                        [0] /usr/var/tmp/tutorial/test0000
                        [1] /usr/var/tmp/tutorial/test0001
                        [2] /usr/var/tmp/tutorial/test0002
                        [3] /usr/var/tmp/tutorial/test0003
                        [4] /usr/var/tmp/tutorial/test0004
                        [5] /usr/var/tmp/tutorial/test0005
                        [6] /usr/var/tmp/tutorial/test0006
                        [7] /usr/var/tmp/tutorial/test0007
                        [8] /usr/var/tmp/tutorial/test0008
Experiment List   Enter the following to run the tests in the test set:% cvcov runtest tut_testsetBy applying the runtest command to the test set, we can run all the tests together. See IDREF="67789" TYPE="TEXT"Example 6-6. Note that when you run a test set, only tests without results are run; tests that already have results will not be run again. In this case, test0000 has already been run. If you need to rerun a test, you can do so using the -force flag.LBL="6-6"Example 6-6 ID="67789"Running the New Test Set% cvcov runtest tut_testset
cvcov: Running test "/usr/var/tmp/tutorial/test0000" ...
cvcov: Running test "/usr/var/tmp/tutorial/test0001" ...
copyn: Insufficient arguments.
Usage: copyn f1 f2 bytes
cvcov: Running test "/usr/var/tmp/tutorial/test0002" ...
Error: Too many arguments
Usage: copyn f1 f2 bytes
cvcov: Running test "/usr/var/tmp/tutorial/test0003" ...
File open error
cvcov: Running test "/usr/var/tmp/tutorial/test0004" ...
Nothing to copy
cvcov: Running test "/usr/var/tmp/tutorial/test0005" ...
Not enough bytes
cvcov: Running test "/usr/var/tmp/tutorial/test0006" ...
File open error
cvcov: Running test "/usr/var/tmp/tutorial/test0007" ...
Not enough bytes
cvcov: Running test "/usr/var/tmp/tutorial/test0008" ...
File open errorEnter cvcov lssum tut_testset to list the summary for the test set.IDREF="97779" TYPE="TEXT"Example 6-7 shows the results of the tests in the new test set with lssum.LBL="6-7"Example 6-7 ID="97779"Examining the Results of the New Test Set% cvcov lssum tut_testset
Coverages                   Covered     Total       % Coverage    Weight
-------------------------------------------------------------------------
Function                    2           2           100.00%       0.400
Source Line                 35          35          100.00%       0.200
Branch                      9           10          90.00%        0.200
Arc                         18          18          100.00%       0.200
Block                       39          42          92.86%        0.000
Weighted Sum                                        98.00%        1.000Enter cvcov lssource main tut_testset to see the coverage for the individual source lines as shown in IDREF="91499" TYPE="TEXT"Example 6-8.LBL="6-8"Example 6-8 ID="91499"Source with Counts% cvcov lssource main tut_testset
Counts  Source
--------------------------------------------------------------------
        #include <stdio.h>
        #include <sys/types.h>
        #include <sys/stat.h>
        #include <fcntl.h>
        #define OPEN_ERR         1
        #define NOT_ENOUGH_BYTES 2
        #define SIZE_0           3
 
        int copy_file();
 
        main (int argc, char *argv[])
9       {
            int bytes, status;
 
9           if( argc < 4){
1               printf("copyn: Insufficient arguments.\n");
1               printf("Usage: copyn f1 f2 bytes\n");
1               exit(1);
            }
8           if( argc > 4 ) {
1               printf("Error: Too many arguments\n");
1               printf("Usage: copyn f1 f2 bytes\n");
1               exit(1);
            }
7           bytes = atoi(argv[3]);
7           if(( status = copy_file(argv[1], argv[2], bytes)) >0){
6              switch ( status) {
                   case SIZE_0:
1                      printf("Nothing to copy\n");
1                      break;
                   case NOT_ENOUGH_BYTES:
2                      printf("Not enough bytes\n");
2                      break;
                   case OPEN_ERR:
3                      printf("File open error\n");
3                      break;
               }
6              exit(1);
      }
1       }
 
        int copy_file( source, destn, size)
        char *source, *destn;
        int size;
7       {
            char *buf;
            int fd1, fd2;
            struct stat fstat;
7           if( (fd1 = open( source, O_RDONLY)) <= 0){
2               return OPEN_ERR;
            }
5           stat( source, &fstat);
5           if( size <= 0){
1               return SIZE_0;
            }
4           if( fstat.st_size < size){
2               return NOT_ENOUGH_BYTES;
            }
2           if( (fd2 = creat( destn, 00777)) <= 0){
1               return OPEN_ERR;
            }
1           buf = (char *)malloc(size);
            
1          read( fd1, buf, size);
1          write( fd2, buf, size);
1          return 0;
0       }As you look at the source code, notice that all lines are covered.Enter cvcov lssource -asm main tut_testset to see the coverage for the individual assembly lines.When we list the assembly code using lssource -asm, we find that not all blocks and branches are covered at the assembly level. This is due to compilation with the -g flag, which adds debugging code that can never be executed.Enter cvcov lsline tut_testset to see the coverage at the source line level. Notice that 100% of the lines have been covered.LBL="" HELPID=""ID="23290"Tutorial #3 - Optimizing a Test SetTester lets you look at the individual test coverages in a test set. When you put together a set of tests, you may wish to improve the efficiency of your coverage by eliminating redundant tests. The lsfun, lsblock, and lsarc commands all have the -contrib option, which displays coverage result contributions by individual tests. Let's look at the contributions by tests for the test set we just ran, tut_testset. NoteThis tutorial needs tut_testset and all its subtests; these were created in the previous tutorial.Enter cvcov lsfun -contrib -pretty tut_testset to see the function coverage test contribution.IDREF="70804" TYPE="TEXT"Example 6-9 shows how the test set covers functions. Note that the subtests are identified by index numbers; use cattest if you need to map these results back to the test directories.LBL="6-9"Example 6-9 ID="70804"Test Contributions by Function% cvcov lsfun -contrib -pretty tut_testset
Functions     Files         Counts
----------------------------------------
main          copyn.c       9
copy_file     copyn.c       7

Functions    Files       [0]    [1]    [2]    [3]    [4]    [5]
-------------------------------------------------------------------
main         copyn.c     1      1      1      1      1      1
copy_file    copyn.c     1      0      0      1      1      1

Functions    Files       [6]    [7]    [8]
----------------------------------------------
main         copyn.c     1      1      1
copy_file    copyn.c     1      1      1At the function level, each test covers both functions except for Tests [1] and [2]. The information here is not sufficient to tell us if we have optimized the test set. To do this, we must look at contributions at the   arc and block levels. Tester shows arc and block coverage information by test when you apply the -contrib flag to lsarc and lsblock, respectively. Enter the following to see the arc coverage test contribution.% cvcov lsarc -contrib -pretty tut_testsetIDREF="89872" TYPE="TEXT"Example 6-10 shows the individual test contributions. Notice that Tests [5] and [7] have identical coverage to each other; so do Tests [3] and [8].We can get additional information by looking at block coverage, confirming our hypothesis about redundant tests.LBL="6-10"Example 6-10 ID="89872"Arc Coverage Test Contribution Portion of ReportCallers     Callees     Line        Files      [0]    [1]    [2]    [3]    [4]    [5]
------------------------------------------------------------------------------------------
main        copy_file   27          copyn.c     1      0      0      1      1      1
main        printf      17          copyn.c     0      1      0      0      0      0
main        printf      18          copyn.c     0      1      0      0      0      0
main        exit        19          copyn.c     0      1      0      0      0      0
main        printf      22          copyn.c     0      0      1      0      0      0
main        printf      23          copyn.c     0      0      1      0      0      0
main        exit        24          copyn.c     0      0      1      0      0      0
main        atoi        26          copyn.c     1      0      0      1      1      1
main        printf      30          copyn.c     0      0      0      0      1      0
main        printf      33          copyn.c     0      0      0      0      0      1
main        printf      36          copyn.c     0      0      0      1      0      0
main        exit        39          copyn.c     0      0      0      1      1      1
copy_file   _open       50          copyn.c     1      0      0      1      1      1
copy_file   _stat       53          copyn.c     1      0      0      0      1      1
copy_file   _creat      60          copyn.c     1      0      0      0      0      0
copy_file   _malloc     63          copyn.c     1      0      0      0      0      0
copy_file   _read       65          copyn.c     1      0      0      0      0      0
copy_file   _write      66          copyn.c     1      0      0      0      0      0

Callers     Callees     Line        Files       [6]    [7]    [8]
---------------------------------------------------------------------
main        copy_file   27          copyn.c     1      1      1
main        printf      17          copyn.c     0      0      0
main        printf      18          copyn.c     0      0      0
main        exit        19          copyn.c     0      0      0
main        printf      22          copyn.c     0      0      0
main        printf      23          copyn.c     0      0      0
main        exit        24          copyn.c     0      0      0
main        atoi        26          copyn.c     1      1      1
main        printf      30          copyn.c     0      0      0
main        printf      33          copyn.c     0      1      0
main        printf      36          copyn.c     1      0      1
main        exit        39          copyn.c     1      1      1
copy_file   _open       50          copyn.c     1      1      1
copy_file   _stat       53          copyn.c     1      1      0Enter the following to see the test contribution to block coverage:% cvcov lsblock -contrib -pretty tut_testsetIf you examine the results, you'll see that Tests [5] and [7] and Tests [3] and [8] are identical.Now we can try to tune the test set. If we can remove tests with redundant coverage and still achieve the equivalent overall coverage, then we have tuned our test set successfully. Since the arcs and blocks covered by Test [7] are also covered by Test [5], we can remove either one of them without affecting the overall coverage. The same analysis holds true for Tests [3] and [8]. Delete test0007 and test0008 as shown in IDREF="47099" TYPE="TEXT"Example 6-11. Then rerun the test set and look at its summary.Note that the coverage is retabulated without actually rerunning the tests. The test summary shows that overall coverage is unchanged, thus confirming our hypothesis. LBL="6-11"Example 6-11 ID="47099"Test Set Summary after Removing Tests [8] and [7]% cvcov deltest test0008 tut_testset
cvcov: Deleted "/usr/var/tmp/tutorial/test0008" from "tut_testset"
% cvcov deltest test0007 tut_testset
cvcov: Deleted "/usr/var/tmp/tutorial/test0007" from "tut_testset"
% cvcov runtest tut_testset
cvcov: Running test "/usr/var/tmp/tutorial/test0000" ...
cvcov: Running test "/usr/var/tmp/tutorial/test0001" ...
cvcov: Running test "/usr/var/tmp/tutorial/test0002" ...
cvcov: Running test "/usr/var/tmp/tutorial/test0003" ...
cvcov: Running test "/usr/var/tmp/tutorial/test0004" ...
cvcov: Running test "/usr/var/tmp/tutorial/test0005" ...
cvcov: Running test "/usr/var/tmp/tutorial/test0006" ...
% cvcov lssum tut_testset
Coverages                   Covered     Total       % Coverage    Weight
------------------------------------------------------------------------------
Function                    2           2           100.00%       0.400
Source Line                 35          35          100.00%       0.200
Branch                      9           10          90.00%        0.200
Arc                         18          18          100.00%       0.200
Block                       39          42          92.86%        0.000
Weighted Sum                                        98.00%        1.000LBL="" HELPID=""ID="76469"Tutorial #4 - Analyzing a Test GroupTest groups are used when you are conducting tests on executables that use a common dynamically shared object (DSO). The results will be limited to whatever constraints you set on the DSO and thus will not include branches, arcs, and other code that lie outside the executables.NoteThis tutorial may be run independently of the previous tutorials. However, it does use copyn. If you have run the other tutorials previously, the instrumentation directory ver##1 will be created for the new executable; otherwise, ver##0 is created when copyn is compiled.In this tutorial, we will test coverage for a DSO called libc.so.1, which is shared by copyn, the executable from the previous tutorials, and a simple application called printtest. The script tut_make_testgroup is provided to run this tutorial. Run the script by typing tut_make_testgroupThe tut_make_testgroup script creates the test group and its subtests. IDREF="51138" TYPE="TEXT"Example 6-12 shows the results of running the initial preparation part of the script using sh -x. First, the script makes the two applications, printtest and copyn. The next step is to instrument the programs. The script stores the instrumentation data for printtest in a subdirectory called print_instr_dir and the copyn data in copyn_instr_dir.The script then makes test directories for the applications and names them print_test0000 and copyn_test0000, respectively. It makes a test group called tut_testgroup and adds both tests to it. mktgroup is the only command that we haven't used previously in the tutorials. mktgroup creates the test group. As a final part of the preparation, the script performs a cattest to show the contents of the test group.LBL="6-12"Example 6-12 ID="51138"Setting up a Test Group% sh -x tut_make_testgroup
+ make -f Makefile.tutorial all 
         /usr/bin/cc -g -o printtest printtest.c  -lc
+ cvcov runinstr -instr_dir print_instr_dir -instr_file tut_group_instr_file printtest 
cvcov: Instrument "printtest" of version "0" succeeded.
+ cvcov runinstr -instr_dir copyn_instr_dir -instr_file tut_group_instr_file copyn 
cvcov: Instrument "copyn" of version "0" succeeded.
+ cvcov mktest -cmd printtest 10 2 3 -instr_dir print_instr_dir -testname print_test0000 
cvcov: Made test directory: "/usr/var/tmp/tutorial4/print_test0000"
+ cvcov mktest -cmd copyn tut4_instr_file targetfile -instr_dir copyn_instr_dir -testname copyn_test0000 
cvcov: Made test directory: "/usr/var/tmp/tutorial4/copyn_test0000"
+ cvcov mktgroup -des Group sharing libc.so.1 -testname tut_testgroup libc.so.1
cvcov: Made test directory: "/usr/var/tmp/tutorial4/tut_testgroup"
+ cvcov addtest print_test0000 tut_testgroup 
cvcov: Added "/usr/var/tmp/tutorial4/print_test0000" to "tut_testgroup"
+ cvcov addtest copyn_test0000 testgroup 
cvcov: Added "/usr/var/tmp/tutorial4/copyn_test0000" to "tut_testgroup"
+ cvcov cattest tut_testgroup 
Test Info               Settings
---------------------------------------------------------------
Test                    /usr/var/tmp/tutorial4/tut_testgroup
Type                    group
Description             Group sharing libc.so.1
Number of Objects       1
Object List             libc.so.1
Number of Subtests      2
Subtest List             
                        [0] /usr/var/tmp/tutorial4/print_test0000
                        [1] /usr/var/tmp/tutorial4/copyn_test0000
Experiment List         Finally, the script runs the test group and performs the queries shown in IDREF="30881" TYPE="TEXT"Example 6-13.LBL="6-13"Example 6-13 ID="30881"Examining Test Group Results+ cvcov runtest tut_testgroup 
cvcov: Running test "/usr/var/tmp/tutorial4/print_test0000" ...
2
3
10
cvcov: Running test "/usr/var/tmp/tutorial4/copyn_test0000" ...
copyn: Insufficient arguments.
Usage: copyn f1 f2 bytes
+ cvcov lssum tut_testgroup 
Coverages                   Covered     Total       % Coverage    Weight
---------------------------------------------------------------------------
Function                    33          1777        1.86%         0.400
Source Line                 438         25525       1.72          0.200
Branch                      27          10017       0.27%         0.200
Arc                         31          6470        0.48%         0.200
Block                       363         27379       1.33%         0.200
Weighted Sum                                        1.24%         1.000
+ cvcov lsfun -pretty -contrib -pat printf tut_testgroup 
Functions    Files       Counts
-------------------------------------
printf       doprnt.c    5

Functions    Files       [0]    [1]
---------------------------------------
printf       doprnt.c    3      2
+ cvcov lsfun -pretty -contrib -pat sscanf tut_testgroup 
Functions    Files       Counts
-------------------------------------
sscanf       scanf.c     3

Functions    Files       [0]    [1]
---------------------------------------
sscanf       scanf.c     3      0LBL="7"ID="37824"Tester Command Line ReferenceThis chapter describes the cvcov commands. It contains two parts:IDREF="87200" TYPE="TITLE""Common cvcov Options"name='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]' the command arguments that are common to more than one commandIDREF="54663" TYPE="TITLE""cvcov Command Syntax and Description"name='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]' the specifications with descriptions for each command A complete description of the cvcov commands, including individual arguments, is available in the man pages by typing:man cvcovLBL="" HELPID=""ID="87200"Common cvcov OptionsThis section contains descriptions of some cvcov flags and variables that are common to more than one command.[-ver]ID="ch071" displays the version of cvcov. Note that there are no other arguments permitted; you enter: cvcov -ver[-v ID="ch072"versionnumber]allows you to specify a version of the instrumentation or experiment directory other than the most recent, which is the default.[-contrib]ID="ch073" shows the list of tests that contributed to coverage for the particular query.[ID="ch074"-exe exe_name] lets you specify an executable for coverage testing. This is used when there are multiple executables involved, as in testing processes created by the fork, exec, or sproc command.[-instr_dir ID="ch075"instr_dir]allows you to specify an instrumentation directory other than the current working directory, which is the default.[-instr_file ID="ch076"instr_file] specifies the instrumentation file, which is an ASCII description of the instrumentation criteria you have selected. [-list ID="ch077"list_file]specifies a file containing a list of test names to be made part of a test set or group. If no -list option is specified, an empty test set will be created. [-r]ID="ch078" is a mnemonic for recursion. It lets you specify tests in a hierarchy of subdirectories.[-arg]ID="ch079" displays functions with their arguments.[-pretty]ID="ch0710" displays output aligned in columns. Without -pretty, the output is in columns but more condensed.[-sort]ID="ch0711" sorts the output by the specified criteria, as follows:functionname='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'alphabetically by functiondiffname='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'by differences in the counting information for coverage typecallerID="ch0712"name='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'alphabetically by calling functioncalleeID="ch0713"name='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'alphabetically by called functioncountname='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'by counts for current coverage typefilename='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'alphabetically by file nametypename='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'alphabetically by argument type[-functions]ID="ch0714" displays list of constrained functions.[-pat ID="ch0715"func_pattern] lets you enter a pattern instead of a complete function name. The pattern can be of the form func_name, dso_:func_name, or `dso:*'.experiment | test_name lets you specify either the experiment subdirectory or the test directory. The test directory is typically of the form test<nnnn>, where <nnnn> is a number in a sequence counting from 0000. You can specify your own name. The test directory contains all information about a test including the experiment directory. The experiment directory is typically of the form exp##<n>, where <n> is a sequential number, counting from 0.LBL="" HELPID=""ID="54663"cvcov Command Syntax and DescriptionThis section contains the syntax and description for all cvcov commands in the command line interface. If you need information on command arguments that are not described in this section, please refer back to IDREF="87200" TYPE="TITLE""Common cvcov Options".The most general command is the help command.cvcov help command_nameID="ch0716" prints help on the specified command. If the optional command name is not specified, it prints help for all the commands.The rest of the commands are divided up into these categories:general test commandscvcov cattestcvcov lsinstrcvcov lstestcvcov mktestcvcov rmtestcvcov runinstrcvcov runtestcoverage analysis commandscvcov lssumcvcov lsfuncvcov lsblockcvcov lsbranchcvcov lsarccvcov lscallcvcov lslinecvcov lssourcecvcov lstracecvcov difftest set commandscvcov mktsetcvcov addtestcvcov deltestcvcov optimizetest group commandcvcov mktgroupLBL="" HELPID=""General Test CommandsThe following commands support the creation, inspection, modification, and deletion of tests.cvcov cattest [-r] ID="ch0717"test_name describes the test details for a test, test set, or test group. IDREF="30998" TYPE="TEXT"Example 7-1 shows the ASCII display for a single test.LBL="7-1"Example 7-1 ID="30998"cattest ExampleID="ch0718"% cvcov cattest test0000
Test Info                Settings
-------------------------------------------------------------
Test                     /disk2/tutorial/tutorial/test0000
Type                     single
Description              
Command Line             copyn alphabet targetfile 20
Number of Exes           1
Exe List                 copyn
Instrument Directory     /disk2/tutorial/tutorial/
Experiment List          
                         exp##0
                         exp##1IDREF="43714" TYPE="TEXT"Example 7-2 shows the ASCII report for a test set without recursion.LBL="7-2"Example 7-2 ID="43714"cattest Example without -r% cvcov cattest tut_testset
Test Info                Settings
----------------------------------------------------------
Test                    /disk2/tutorial/tutorial/tut_testset
Type                    set
Description             full coverage testset
Number of Exes          1
Exe List                copyn
Number of Subtests      9
Subtest List             
                        [0] /disk2/tutorial/tutorial/test0000
                        [1] /disk2/tutorial/tutorial/test0001
                        [2] /disk2/tutorial/tutorial/test0002
                        [3] /disk2/tutorial/tutorial/test0003
                        [4] /disk2/tutorial/tutorial/test0004
                        [5] /disk2/tutorial/tutorial/test0005
                        [6] /disk2/tutorial/tutorial/test0006
                        [7] /disk2/tutorial/tutorial/test0007
                        [8] /disk2/tutorial/tutorial/test0008
Experiment List          
                        exp##0IDREF="45402" TYPE="TEXT"Example 7-3 shows the ASCII report for a nested test set.LBL="7-3"Example 7-3 ID="45402"cattest Example with -r% cvcov cattest -r tut_testset
Test Info                Settings
---------------------------------------------------------
Test                    /disk2/tutorial/tutorial/tut_testset
Type                    set
Description             full coverage testset
Number of Exes          1
Exe List                copyn
Number of Subtests      9
Subtest List             
                        /disk2/tutorial/tutorial/test0000
                        /disk2/tutorial/tutorial/test0001
                        /disk2/tutorial/tutorial/test0002
                        /disk2/tutorial/tutorial/test0003
                        /disk2/tutorial/tutorial/test0004
                        /disk2/tutorial/tutorial/test0005
                        /disk2/tutorial/tutorial/test0006
                        /disk2/tutorial/tutorial/test0007
                        /disk2/tutorial/tutorial/test0008
Experiment List          
                        exp##0cvcov lsinstr [-exe ID="ch0719"exe_name] [-functions] [-v versionnumber] test_namedisplays the instrumentation information for a particular test. exe_name is the executable targeted for query. The main program is the default if no executable is specified. The -functions parameter shows the functions that are included in the coverage experiment. The versionnumber parameter allows you to specify the version of the program that was instrumented. You can specify the test directory using the test_name parameter. See IDREF="51831" TYPE="TEXT"Example 7-4.LBL="7-4"Example 7-4 ID="51831"lsinstr ExampleID="ch0720"% cvcov lsinstr test0000
Instrumentation         Info
---------------------------------------------------------
Executable              copyn
Version                 0
Instrument Directory    /x/tmp/carol/
Instrument File         tut_instr_file
Criteria                RBPA
Instrumented Objects    copyn_Instr(2.57X)
                        libc.so.1_RBP_Instr(1.07X)
Argument Tracing        copy_file(size[bounds]) [copyn.c]cvcov lstest [-r] [ID="ch0721"test_name...] lists the test directories in the current working directory. Note that the test_name parameter will accept regular expressions for lstest.cvcov mktest -cmdID="ch0722" cmd_line [-des description] [-instr_dir directoryname] [-testname test] [exe1 exe2 ...]creates a test directory. You specify the program and command line options for the program to be tested. This includes any redirection for stdin, stderr, or stdout as run from the Bourne shell. The -cmd qualifier is mandatory, even if it only includes the program name. If no executables are specified, only the main program is tested. IDREF="43403" TYPE="TEXT"Example 7-5 shows an example of mktest, followed by cattest to display the contents of the Test Description File (TDF).LBL="7-5"Example 7-5 ID="43403"Test Description File ExamplesID="ch0723"% cvcov mktest -cmd "copyn tut_instr_file targetfile"
cvcov: Made test directory: /d/Tester/tutorial/test0002

% cvcov cattest test0002
Test Info               Settings
---------------------------------------------------------
Test                    /d/Tester/tutorial/test0002
Type                    single
Description              
Command Line            copyn tut_instr_file targetfile
Number of Exes          1
Exe List                copyn
Instrument Directory    /d/Tester/tutorial
Experiment List cvcov rmtest [-r] ID="ch0724"test_name ... removes tests and test sets. Note that the test_name parameter will accept regular expressions for rmtest. It is recommended to separate the test set directory from its test subdirectories and the instrument directory. In this way, rmtest will not remove instrumentation data or subtests if you choose to remove the test set only.cvcov runinstr [-instr_dir ID="ch0725"instr_dir] [-instr_file instr_file] [-v versionnumber] executableadds code to the target executable to enable you to capture coverage data, according to the criteria you specify. The instrument file is an ASCII description of the instrumentation criteria for the experiment.You can also specify the version of the executable and instrument directory. You can capture basic block counts, function pointer counts, and branch counts (at the assembly language level). You can use INCLUDE, EXCLUDE, or CONSTRAIN to modify the set of functions covered. CONSTRAIN lets you define a set of functions for the test.cvcov runtest [ -bitcount ] [ -compress ] [-force] [-keep]ID="ch0726" [-sum] [-v versionnumber][-noarc] [-rmsub] test_nameruns a test or a set of tests. The -bitcount flag compresses count data file to be 1-bit-per-count. This option can decrease the database size up to 32 times, although branch count information will be lost. The -compress flag compresses the experiment database using standard utility compress. The -force flag forces the test to be run again even if an experiment is present. It uses WorkShop performance tool technology to set up the instrumented process, run the process, and monitor the run, collecting counting information upon exit. The -keep flag retains all performance data collected in the experiment. By default, the performance data is not retained, because it is not required by the coverage tool. The -sum flag accumulates (sum over) the coverage data into the existing experiment results. This allows users to run and rerun the same test and accumulate the results in one place. The -noarc flag prevents arc information from being saved in the test database. With the -noarc flag, all arc-related queries will not work (for example, lsarc and lscall). The -rmsub flag removes results for individual subtests for a test set or test group. There will be no data to query if you are querying a subtest. -noarc and -rmsub save disk space.LBL="" HELPID=""Coverage Analysis CommandsOnce the data has been collected from the test experiments, the user can analyze the data. There are special commands for the various types of coverage available.cvcov lssum [-exe ID="ch0727"exe_name] [-weight func_factor :line_factor: branch_factor : arc_factor : block_factor] experiment | test_nameshows the overall coverage based on the user-defined weighted average over function, line, block, branch, and arc coverage. IDREF="94955" TYPE="TEXT"Example 7-6 shows a typical lssum report.LBL="7-6"Example 7-6 ID="94955"lssum ExampleID="ch0728"% cvcov lssum test0000
Coverages        Covered     Total       % Coverage    Weight
-------------------------------------------------------------
Function         2           2           100.00%       0.400
Source Line      17          35          48.57%        0.200
Branch           0            10           0.00%        0.200   
Arc              8           18          44.44%        0.200
Block            19          42          45.24%        0.000
Weighted Sum                             58.60%        1.000
cvcov lsfun [-arg]ID="ch0729"  [-bf filter_type block_filter_value]  [-blocks] [-branches] [-contrib]  [-exe exe_name] [-ff filter_type func_filter_value]  [-pat func_pattern] [-pretty]  [-rf filter_type branch_filter_value] [-sortcount | file | function] experiment | test_name lists coverage information for the specified functions in the program that was tested. Several sorting, matching, and filtering techniques are available. For example, you can show the list of functions that have 0 counts (were not covered) in alphabetical order. You can display arguments with the -arg flag.IDREF="91392" TYPE="TEXT"Example 7-7 shows a typical lsfun ASCII report.LBL="7-7"Example 7-7 ID="91392"lsfun ExampleID="ch0730"% cvcov lsfun -pretty -sort function test0000
Functions    Files       Counts
-------------------------------------
copy_file    copyn.c     1
main         copyn.c     1NoteC++ in-line functions are not counted as functions.cvcov lsblock [-addr] [-arg] [-contrib] [-exe ID="ch0731"exe_name] [-pat func_pattern] [-pretty] [-sort count | file| function]   experiment | test_namedisplays a list of blocks for one or more functions and the  count information associated with each block. Blocks are  identified by the line numbers in which they occur. If there                      are multiple blocks in a line, blocks subsequent to the first are shown in order with an index number in parentheses. Becareful before listing all blocks in the program, since this canproduce a lot of data. The -addr flag show blocks with the PC range instead of the source line number range. IDREF="23429" TYPE="TEXT"Example 7-8 shows a typical lsblock ASCII report.LBL="7-8"Example 7-8 ID="23429"lsblock ExampleID="ch0732"% cvcov lsblock -pat main -pretty test0000
Blocks      Functions    Files       Counts
-------------------------------------------------
13~16       main         copyn.c     1
17~17       main         copyn.c     0
18~18       main         copyn.c     0
19~19       main         copyn.c     0
21~21       main         copyn.c     1
22~22       main         copyn.c     0
23~23       main         copyn.c     0
24~24       main         copyn.c     0
26~26       main         copyn.c     1
26~27       main         copyn.c     1
27~27       main         copyn.c     1
28~28       main         copyn.c     0   
28~28(2)    main         copyn.c     0   
28~28(3)    main         copyn.c     0   
28~28(4)    main         copyn.c     0   
30~30       main         copyn.c     0   
31~31       main         copyn.c     0   
33~33       main         copyn.c     0   
34~34       main         copyn.c     0   
36~36       main         copyn.c     0   
37~37       main         copyn.c     0   
39~39       main         copyn.c     0   
41~41       main         copyn.c     0   
43~43       main         copyn.c     1
43~43(2)    main         copyn.c     0   
43~43(3)    main         copyn.c     1
ID="ch0733"cvcov lsbranch [-addr] [-arg] [-exe exe_name]                [-pat func_pattern] [-pretty]              [-sort function | file]              experiment | test_name                            lists coverage information for branches in the program,                            including the line number at which the branch occurs.                            Branch coverage counts assembly language branch                            instructions that are both taken and not taken. The                            -addr flag show blocks with the PC range instead of the                            source line number range. IDREF="91805" TYPE="TEXT"Example 7-9 shows a typical branch coverage ASCII report. Note that branches with incomplete or null coverage are highlighted (boldfaced).LBL="7-9"Example 7-9 ID="91805"lsbranch ExampleID="ch0734"% cvcov lsbranch -pretty -sort function test0000
Line        Functions    Files       Taken       Not Taken
-------------------------------------------------------------
50          copy_file    copyn.c     1           0       
54          copy_file    copyn.c     1           0       
57          copy_file    copyn.c     1           0       
60          copy_file    copyn.c     1           0       
16          main         copyn.c     1           0       
21          main         copyn.c     1           0       
27          main         copyn.c     1           0       
28          main         copyn.c     0           0       
28(2)       main         copyn.c     0           0       
28(3)       main         copyn.c     0           0cvcov lsarc [-arg] [-callee ID="ch0735"callee_pattern] [-caller caller_pattern] [-contrib][-exe exe_name] [-pretty][-sort caller | callee | count | file]   experiment | test_nameshows arc coverage, that is, the number of arcs taken out of the total possible arcs. An arc is a function caller-callee pair. Both callee_pattern and caller_pattern can be specified in the same way as func_pattern (used with the -pat option) as shown under IDREF="87200" TYPE="TITLE""Common cvcov Options".IDREF="72715" TYPE="TEXT"Example 7-10 shows a typical lsarc ASCII report.LBL="7-10"Example 7-10 ID="72715"lsarc Example ID="ch0736"% cvcov lsarc -callee printf -pretty test0001
Callers    Callees     Line       Files       Counts
---------------------------------------------------------
main        printf      17          copyn.c     1        
main        printf      18          copyn.c     1        
main        printf      22          copyn.c     0        
main        printf      23          copyn.c     0        
main        printf      30          copyn.c     0        
main        printf      33          copyn.c     0        
main        printf      36          copyn.c     0        cvcov lscall [-arg] [-exe ID="ch0737"exe_name] [-node func_name] [-pretty] [-r] experiment |test_namelists the call graph for the executable with counts for each function. The contribution to this coverage by each test is shown in a separate column. IDREF="85252" TYPE="TEXT"Example 7-11 shows a typical lscall ASCII report. N/A means the node is excluded.LBL="7-11"Example 7-11 ID="85252"lscall Example ID="ch0738"% cvcov lscall -pretty test0000
Graph            Counts
---------------------------------
main             1
   copy_file     1
      _open      N/A
      _stat...   N/A
      _creat     N/A
      _malloc... N/A
      _read      N/A
      _write     N/A
   printf...     N/A
   exit...       N/A
   atoi          N/AA function that has more than one parent and has children is called a subnode. Using -r will display the subnodes. Subnodes are given their own starting point in the textual call graph. They are identified by a trailing ellipsis (...). For example, see printf, exit, and malloc in IDREF="85252" TYPE="TEXT"Example 7-11.cvcov lsline [-arg] [-exe exe_name] [-pat func_pattern][-pretty] [-sort function | file] experiment | test_namelists the coverage for native source lines. Use ID="ch0739"arg to show arguments for functions. If no executable is specified, the main program is the default. Use pretty to provide column-aligned output. See IDREF="28093" TYPE="TEXT"Example 7-12.LBL="7-12"Example 7-12 ID="28093"lsline Example% cvcov lsline -pretty -pat main test0000
Functions   Files      Covered   Total     % Coverage
---------------------------------------------------------
main        copyn.c    6         20        30.00%cvcov lssource [-asm] [-exe ID="ch0740"exe_name] functionexperiment test_namedisplays the source annotated with line counts. The -asm switch displays the assembly level source code annotated with line counts. Lines with 0 counts are highlighted to show the absence of coverage. This is useful for mapping to the source level blocks and branches that were not covered. Lines in functions that were not included in the test appear without count annotations. IDREF="43165" TYPE="TEXT"Example 7-13 shows a segment of a typical lssource ASCII report.Notelssource requires the code to be compiled with the -g option.LBL="7-13"Example 7-13 ID="43165"lssource ExampleID="ch0741"% cvcov lssource main test0000
Counts  Source
-------------------------------------------------------------
        #include <stdio.h>
        #include <sys/stat.h>
        #include <sys/types.h>
        #include <fcntl.h>
 
        #define OPEN_ERR         1
        #define NOT_ENOUGH_BYTES 2
        #define SIZE_0           3
 
        int copy_file();
 
        main (int argc, char *argv[])
1       {
            int bytes, status;
 
1           if( argc < 4){
0               printf("copyn: Insufficient arguments.\n"); 
0               printf("Usage: copyn f1 f2 bytes\n"); 
0               exit(1); 
            }
1           if( argc > 4 ) {
0               printf("Error: Too many arguments\n"); 
0               printf("Usage: copyn f1 f2 bytes\n"); 
0               exit(1); 
            }
1           bytes = atoi(argv[3]);cvcov lstrace [-exe ID="ch0742"exe_name] [-pat func_pattern] [-pretty][-sort function | type]experiment | test_nameshows the argument tracing information. IDREF="92803" TYPE="TEXT"Example 7-14 shows a typical lstrace ASCII report. The Range column shows upper and lower bounds. A dash (-) means that side of the bound has not been counted.LBL="7-14"Example 7-14 ID="92803"lstrace Example ID="ch0743"% cvcov lstrace -pretty testtrace
Arguments       Type    Range
---------------------------------
main(argc)      int     -, 4
copy_file(size) int     1, 20Notelstrace requires the code to be compiled with the -g option.cvcov diff [-arg] [-exe ID="ch0744"exe_name] [-functions] [-pretty][-sort diff | function] experiment1 experiment2shows the difference in coverage for different versions of the same program. IDREF="94010" TYPE="TEXT"Example 7-15 shows an example of the diff command applied to two tests (although you should make sure that the comparison is relevant). IDREF="63778" TYPE="TEXT"Example 7-16 shows diff applied to different instrumentations of the same test.LBL="7-15"Example 7-15 ID="94010"diff between Two TestsID="ch0745"% cvcov diff test0000/exp##0 test0001/exp##0

Experiment 1:    test0000/exp##0
Experiment 2:    test0001/exp##0

Coverages            Exp 1       Exp 2        Differences
----------------------------------------------------------
Function Coverage    2(100.00%)  1(50.00%)    1(50.00%)
Source Line Coverage 17(48.57%)  5(14.29%)    12(34.29%)
Branch Coverage      0(0.00%)    0(0.00%)     0(0.00%)
Arc Coverage         8(44.44%)   3(16.67%)    5(27.78%)
Block Coverage       19(45.24%)  4(9.52%)     15(35.71%) LBL="7-16"Example 7-16 ID="63778"diff ID="ch0746"between Different Instrumentations of the Same Test % cvcov diff test0000/exp##0 test0000/exp##1Experiment 1:    test0000/exp##0
Experiment 2:    test0000/exp##1

Coverages            Exp 1        Exp 2       Differences
----------------------------------------------------------
Function Coverage    2(100.00%)    2(100.00%)    0(0.00%)
Source Line Coverage 17(48.57%)    17(47.22%)    0(1.35%)
Branch Coverage      0(0.00%)      0(0.00%)      0(0.00%)
Arc Coverage         8(44.44%)     8(44.44%)     0(0.00%)
Block Coverage       19(45.24%)    19(44.19%)    0(-1.05%)LBL="" HELPID=""Test Set CommandsA test set is a named collection of tests and other test sets. Test sets can be hierarchical. For example, ID="ch0747"compiler_language_suite might include C++_suite, C_suite, and Fortran_suite, where Fortran_suite is a test set with subdirectories. The following commands support creation, inspection, modification, and deletion of test sets. Both addtest and deltest are also used with test groups, described in the next section.cvcov mktset [-des ID="ch0748"description] [-list list_file] [-testname test]makes a test set. If no test name is specified, the command assigns one automatically.cvcov addtest ID="ch0749"test_nametest_set_name | test_group adds a test or test set to a test set or test group.cvcov deltest ID="ch0750"test_name test_set_name | test_group removes a test or test set from a test set or test group.NoteDon't use UNIX commands ID="ch0751"mv and cp to rename or copy test sets because they are constructed with absolute files paths.ID="ch0752"cvcov optimize [ -blocks ] [ -branches ][ -cbb filter_type bb_filter_value ][ -cbr filter_type br_filter_value ][ -exe exe_name ] [ -pat func_pattern ][ -pretty ] [ -stat ] experiment ...|test_name ...selects the minimum set of tests that give the same coverage or meet the given coverage criteria as the given set. The -blocks flag shows block coverage for all the selected tests. The -branches flag shows branch coverage for all the selected tests. The -cbbfilter_type bb_filter_value gives the basic block coverage criteria for test selection. The rules are the same as the flag -bf of the lsfun command. The -cbr filter_type br_filter_value gives the branch coverage criteria for test selection. The rules are the same as the flag -rf of lsfun command. The -exeexe_name option lets you specify which executable is targeted for test optimization. If no executable is specified, the main program is the default. The -patpattern option lets you specify DSO patterns for calculation of coverage on test selection. The -pretty flag aligns column output. The -stat flag prints out block and branch coverage for all the selected tests. Without this option, cumulative coverages for block and branch are given. The experiment ...|test_name ... option lets you specify names of experiments or tests to be optimized. IDREF="98070" TYPE="TEXT"Example 7-17 demonstrates how test sets are optimized. In this case, optimizing is applied to all tests matching the expression test00*.LBL="7-17"Example 7-17 ID="98070"Optimizing Test Sets% cvcov optimize -pretty -blocks -branches test00*

Test                Block Coverage    Branch Coverage
---------------------------------------------------------
test0000            41.54%            0.00%
test0001            7.69%             10.00%
test0002            7.69%             10.00%
test0003            9.23%             20.00%
test0004            9.23%             20.00%
test0005            6.15%             20.00%
test0006            1.54%             10.00%
Total Coverage      83.08%            90.00%LBL="" HELPID=""Test Group CommandsA test group is a collection of programs to be tested that have a common dynamically shared object (DSO). The coverage testing is limited to activity with the DSO so that the arcs and branches that terminate outside of the DSO will not be included. See descriptions of ID="ch0753"ID="ch0754"addtest and deltest in the previous section as well as the following command. cvcov mktgroup [-des description] [-list list_file] [-testname test] target1 target2 ... creates a test group that can contain other tests or test groups. The targets are either the target libraries or DSOs.ID="ch0755"NoteDon't use UNIX commands ID="ch0756"mv and cp to rename or copy test groups because they are constructed with absolute files paths.ID="ch0757"LBL="8"ID="47213"Tester Graphical User Interface TutorialThis chapter provides a tutorial for the Tester graphical user interface. It covers these topics:IDREF="70032" TYPE="TITLE""Setting Up the Tutorial"IDREF="31114" TYPE="TITLE""Tutorial #1 name='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]' Analyzing a Single Test"IDREF="53762" TYPE="TITLE""Tutorial #2 name='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]' Analyzing a Test Set"IDREF="62202" TYPE="TITLE""Tutorial #3 name='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]' Exploring the Graphical User Interface"LBL="" HELPID=""ID="70032"Setting Up the TutorialIf you have already set up a tutorial directory for the command line interface tutorial, you can continue to use it. If you remove the subdirectories, your directory names will match exactly; if you leave the subdirectories in, you can add new ones as part of this tutorial. ID="ch081"ID="ch082"If you'd like the test data built automatically, run the script: /usr/demos/WorkShop/Tester/setup_Tester_demoTo set up a tutorial directory from scratch, do the following; otherwise you can skip the rest of this section. Enter the following:ID="ch083"% cp -r /usr/demos/WorkShop/Tester /usr/tmp/tutorial 
% cd /usr/tmp/tutorial 
% echo ABCDEFGHIJKLMNOPQRSTUVWXYZ > alphabet 
% make -f Makefile.tutorial copyn This moves some scripts and source files used in the tutorial to /usr/tmp/tutorial, creates a test file named alphabet, and makes a simple program, copyn, which copies n bytes from a source file to a target file. To see how the program works, try a simple test by typing:% copyn alphabet targetfile 10 
% cat targetfile 
ABCDEFGHIJ You should see the first 10 bytes of alphabet copied to targetfile.LBL="" HELPID=""ID="31114"Tutorial #1 name='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]' Analyzing a Single TestTutorial #1 discusses the following topics:IDREF="64660" TYPE="TITLE""Invoking the Graphical User Interface"IDREF="86248" TYPE="TITLE""Invoking the Graphical User Interface"IDREF="22861" TYPE="TITLE""Invoking the Graphical User Interface"IDREF="15785" TYPE="TITLE""Invoking the Graphical User Interface"IDREF="87694" TYPE="TITLE""Invoking the Graphical User Interface"LBL="" HELPID=""ID="64660"Invoking the Graphical User InterfaceYou typically call up the graphical user interface from the directory that will contain your test subdirectories. This section tells you how to invoke the Tester graphical user interface and describes the main window.Enter cvxcov from the tutorial directory to bring up the Tester main window.ID="ch084"ID="ch085"IDREF="13348" TYPE="GRAPHIC"Figure 8-1 shows the main Tester window with all its menus displayed. NoteYou can also access Tester from the Admin menu in other WorkShop tools.Observe the features of the Tester window.The ID="ch086"ID="ch087"Test Name field is used to display the current test. You can switch to different tests through this field.Test results display in the ID="ch088"coverage display area. You display the results by choosing an item from the Queries menu. You also can select the format of the data from the Views menu. The ID="ch089"ID="ch0810"ID="ch0811"ID="ch0812"Source button lets you bring up the standard CASEVision Source View window with Tester annotations. Source View shows the counts for each line included in the test and highlights lines with 0 counts. Lines from excluded functions display but without count annotations. The Disassembly button brings up the CASEVision Disassembly View window for assembly language source. It operates in a similar fashion to the Source button.The Contribution button displays a separate window with the contributions to the coverage made by each test in a test set or test group.ID="ch0813"A sort button lets you sort the test results by such criteria as function, count, file, type, difference, caller, or callee. The criteria available (shown by the name of the button) depend on the current query.ID="ch0814"The status area displays status messages regarding the test.ID="ch0815"The area below the status area will display special query-specific fields when you make queries.You can launch other WorkShop applications from the Launch Tool submenu of the Admin menu. The applications include the Build Analyzer, Debugger, Parallel Analyzer, Performance Analyzer, and Static Analyzer.You'll find an iconized version of Execution View labeled cvxcovExec. It is a shell window for viewing test results as they would appear on the command line.FILE="ch08.gif" POSITION="INLINE" SCALE="FALSE"LBL="8-1"Figure 8-1 ID="13348"Main Tester WindowID="ch0816"ID="86248"Instrumenting an ExecutableThe first step in providing test coverage is to define the instrumentation criteria in an instrumentation file. ID="ch0817"ID="ch0818"On the command line or from Execution View, enter the following to see the instrumentation directives in the file tut_instr_file used in the tutorials:% cat tut_instr_file 
COUNTS -bbcounts -fpcounts -branchcounts
CONSTRAIN main, copy_file
TRACE BOUNDS copy_file(size)We will be getting all counting information (blocks, functions, source lines, branches, and arcs) for the two functions specified in the CONSTRAIN directive, main and copy_file. We will also be tracing the size argument for the copy_file function. Select "Run Instrumentation" from the Test menu.This process inserts code into the target executable that enables coverage data to be captured. The dialog box shown in ID="ch0819"IDREF="25293" TYPE="GRAPHIC"Figure 8-2 displays when "Run Instrumentation" is selected from the Test menu.FILE="ch082.gif" POSITION="INLINE" SCALE="FALSE"LBL="8-2"Figure 8-2 ID="25293"Running InstrumentationEnter copyn in the Executable field.The Executable field is required, as indicated by the red highlight. You enter the executable in this field.Enter tut_instr_file in the Instrument File field. The ID="ch0820"ID="ch0821"Instrument File field lets you specify an instrumentation file containing the criteria for instrumenting the executable. In this tutorial, we use the file tut_instr_file, which was described earlier.Leave the Instrument Dir and Version Number fields as is.The Instrument Dir field indicates the directory in which the instrumented programs are stored. A versioned directory is created (the default is ID="ch0822"ver##n, where n is 0 the first time and is incremented automatically if you subsequently change the instrumentation). The version number n helps you identify the instrumentation version you use in an experiment. The experiment results directory will have a matching version number. The instrument directory is the current working directory; it can be set from the Admin menu.Click OK.This executes the instrumentation process. If there are no problems, the dialog box closes and the message Instrumentation succeeded displays in the status area with the version number created.ID="22861"Making a TestA ID="ch0823"test defines the program and arguments to be run, the instrumentation criteria, and descriptive information about the test. Select "Make Test" from the Test menu.This creates a test directory. ID="ch0824"IDREF="39724" TYPE="GRAPHIC"Figure 8-3 shows the Make Test window.You specify the name of the test directory in the Test Name field, in this case test0000. The field displays a default directory test<nnnn>, where nnnn is 0000 the first time and incremented for subsequent tests. You can edit this field if necessary.FILE="ch083.gif" POSITION="INLINE" SCALE="FALSE"LBL="8-3"Figure 8-3 ID="39724"Selecting "Make Test"Enter a description of the test in the Description field.This is optional, but can help you differentiate between tests you have created.ID="ch0825"Enter the executable to be tested with its arguments in the Command Line field, in this example: copyn alphabet targetfile 20This field is mandatory, as indicated by its highlighting. ID="ch0826"Leave the remaining fields as is. Tester supplies a default instrumentation directory in the Instrument Dir field. The Executable List field lets you specify multiple executables when your main program forks, execs, or sprocs other processes.Click OK to perform the make test operation with your selections.The results of the make test operation display in the status area of the main Tester window. ID="15785"Running a TestTo run a test, we use technology from the WorkShop Performance Analyzer. The instrumented process is set to run, and a monitor process (ID="ch0827"cvmon) captures test coverage data by interacting with the WorkShop process control server (cvpcs). Select "Run Test" from the Test menu.The dialog box shown in ID="ch0828"ID="ch0829"IDREF="75731" TYPE="GRAPHIC"Figure 8-4 displays. You enter the test directory in the Test Name field. You can also specify a version of the executable in the Version Number field if you don't wish to use the latest, which is the default. The ID="ch0830"Force Run toggle forces the test to be run again even if a test result already exists. The Keep Performance Data toggle retains all the performance data collected in the experiment. The ID="ch0831"Accumulate Results toggle sums over the coverage data into the existing experiment results. Both ID="ch0832"No Arc DataID="ch0833" and Remove Subtest ExptID="ch0834" toggles retain less data in the experiments and are designed to save disk space.FILE="ch084.gif" POSITION="INLINE" SCALE="FALSE"LBL="8-4"Figure 8-4 ID="75731""Run Test" Dialog BoxEnter test0000 in the Test Name field.Click OK to run the test with your selections.When the test completes, a status message showing completion displays and you will have data to be analyzed. You can observe the test as it runs in Execution View.ID="ch0835"ID="87694"Analyzing the Results of a Coverage TestYou can analyze test coverage data in many ways. In this tutorial, we will illustrate a simple top-down approach. We will start at the top to get a summary of overall coverage, proceed to the function level, and finally go to the actual source lines. ID="ch0836"Having collected all the coverage data, now you can analyze it. You do this through the Queries menu in the main Tester window.Enter test0000 in the Test Name field in the main window and select "List Summary" from the Queries menu.This loads the test and changes the main window display as shown in ID="ch0837"IDREF="32491" TYPE="GRAPHIC"Figure 8-5. The query type (in this case, "List Summary") is indicated above the display area. Column headings identify the data, which displays in columns in the coverage display area. The status area is shortened. The query-specific fields (in this case, coverage weighting factors) that appear below the control buttons and status area are different for each query type. You can change the numbers and click Apply to weight the factors differently. The Executable List button brings up the Target List dialog box. It displays a list of executables used in the experiment and lets you select different executables for analysis. You can select other experiments from the experiment menu (Expt)."List Summary" shows the coverage data (number of coverage hits, total possible hits, percentage, and weighting factor) for functions, source lines, branches, arcs, and blocks. The last coverage item is the weighted average, obtained by multiplying individual coverage averages by the weighting factors and summing the products.FILE="ch085.gif" POSITION="INLINE" SCALE="FALSE"LBL="8-5"Figure 8-5 ID="32491""List Summary" Query WindowSelect "List Functions" from the Queries menu.This query lists the coverage data for functions specified for inclusion in this test. The default version is shown in ID="ch0838"IDREF="44459" TYPE="GRAPHIC"Figure 8-6, with the available options. FILE="ch086.gif" POSITION="INLINE" SCALE="FALSE"LBL="8-6"Figure 8-6 ID="44459""List Functions" Query with OptionsIf there are functions with 0 counts, they will be highlighted. The default column headings are ID="ch0839"Functions, Files, and Counts. Click the Blocks and Branches toggles.The Blocks and Branches toggle buttons let you display these items in the function list. IDREF="23235" TYPE="GRAPHIC"Figure 8-7 shows the display area with Blocks and Branches enabled.FILE="f25func.gif" POSITION="INLINE" SCALE="FALSE"LBL="8-7"Figure 8-7 ID="23235""List Functions" Display Area with Blocks and BranchesThe Blocks column shows three values. The number of blocks executed within the function is shown first. The number of blocks covered out of the total possible for that function is shown inside the parentheses. If you divide these numbers, you'll arrive at the percentage of coverage.Similarly, the Branches column shows the number of branches covered, followed by the number covered out of the total possible branches. The term covered means that the branch has been executed under both true and false conditions.Select the function main in the display area and click Source.The Source View window displays with count annotations as shown in IDREF="86479" TYPE="GRAPHIC"Figure 8-8. Lines with 0 counts are highlighted in the display area and in the vertical scroll bar area. Lines in excluded functions display with no count annotations.Click the Disassembly button in the main window.The Disassembly View window displays with count annotations as shown in ID="ch0840"IDREF="57044" TYPE="GRAPHIC"Figure 8-9. Lines with 0 counts are highlighted in the display area and in the vertical scroll bar area.FILE="ch088.gif" POSITION="INLINE" SCALE="FALSE"LBL="8-8"Figure 8-8 ID="86479"Source View with Count AnnotationsFILE="ch089.gif" POSITION="INLINE" SCALE="FALSE"LBL="8-9"Figure 8-9 ID="57044"Disassembly View with Count AnnotationsLBL="" HELPID=""ID="53762"Tutorial #2 name='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]' Analyzing a Test SetIn the second tutorial, we are going to create additional tests with the objective of achieving 100% overall coverage. From examining the source code, it seems that the 0-count lines in ID="ch0841"main and copy_file are due to error-checking code that is not tested by test0000.NoteThis tutorial needs test0000, which was created in the previous tutorial.Select "Make Test" from the Test menu.This displays the Make Test dialog box. It is easy to enter a series of tests. As is standard in CASEVision, using the Apply button in the dialog box instead of the OK button completes the task without closing the dialog box. The Test Name field supplies an incremented default test name after each test is created. We are going to create a test set named tut_testset and add to it 8 tests in addition to test0000 from the previous tutorial. The tests test0001 and test0002 pass too few and too many arguments, respectively. test0003 attempts to copy from a file named no_file that doesn't exist. test0004 attempts to pass 0 bytes, which is illegal. test0005 attempts to copy 20 bytes from a file called not_enough, which contains only one byte. In test0006, we attempt to write to a directory without proper permission. test0007 tries to pass too many bytes. In test0008, we attempt to copy from a file without read permission. The following steps show the command line target and arguments and description for the tests in the tutorial. The descriptions are helpful but optional. IDREF="68383" TYPE="GRAPHIC"Figure 8-10 shows the features of the dialog box you'll need for creating these tests.Enter copyn alphabet target in the CommandLine field, not enough arguments in the Description field, and click Apply (or simply press <return>) to make test0001.Enter copyn alphabet target 20 extra_arg in the CommandLine field, too many arguments in the Description field, and click Apply to make test0002.FILE="ch0810.gif" POSITION="INLINE" SCALE="FALSE"LBL="8-10"Figure 8-10 ID="68383""Make Test" Dialog Box with Features Used in TutorialEnter copyn no_file target 20 in the CommandLine field, cannot access file in the Description field, and click Apply to make test0003.Enter copyn alphabet target 0 in the Command Line field, pass bad size arg in the Description field, and click Apply to make test0004.Enter copyn not_enough target 20 in the Command Line field, not enough data in the Description field, and click Apply to make test0005.Enter copyn alphabet /usr/bin/target 20 in the Command Line field, cannot create target executable due to permission problems in the Description field, and click Apply to make test0006.Enter copyn alphabet targetfile 200 in the Command Line field, size arg too big in the Description field, and click Apply to make test0007.Enter copyn /usr/etc/snmpd.auth targetfile 20 in the Command Line field, no read permission on source file in the Description field, and click Apply to make test0008.We now need to create the test set that will contain these tests.Click the Test Set toggle in the Test Type field.This changes the dialog box as shown in IDREF="71444" TYPE="GRAPHIC"Figure 8-11. FILE="ch0811.gif" POSITION="INLINE" SCALE="FALSE"LBL="8-11"Figure 8-11 ID="71444""Make Test" Dialog Box for Test Set TypeChange the default in the Test Name field to tut_testset.This is the name of the new test set. Now we have to add the tests to the test set.Select the first test in the Test List field and click Add.This displays the selected test in the Test Include List field, indicating that it will be part of the test set after you click OK (or Apply and Close). Repeat the process of selecting a test and clicking Add for each test in the Test List field. When all tests have been added to the test set, click OK.This saves the test set as specified and closes the "Make Test" dialog box.Enter tut_testset in the Test Name field and select "Describe Test" from the Queries menu.This displays the test set information in the display area of the main window.Select "Run Test" from the Test menu, enter tut_testset in the Test Name field in the "Run Test" dialog box.This runs all the tests in the test set.Enter tut_testset in the Test Name field in the main Tester window and select "List Summary" from the Queries menu.This displays a summary of the results for the entire test set.Select "List Functions" from the Queries menu.This step serves two purposes. It enables the Source button so that we can look at counts by source line. It displays the list of functions included in the test, from which we can select functions to analyze.Click the main function, which is displayed in the function list, and click the Source button.This displays the source code, with the counts for each line shown in the annotations column. Note that the counts are higher now and full coverage has been achieved at the source level (although not necessarily at the assembly level).LBL="" HELPID=""ID="62202"Tutorial #3 name='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]' Exploring the Graphical User InterfaceThe rest of this chapter shows you how to use the graphical user interface (GUI) to analyze test data. The GUI has all the functionality of the command line interface and in addition shows the function calls, blocks, branches, and arcs graphically.For a discussion of applying Tester to test set optimization, refer to IDREF="23290" TYPE="TITLE""Tutorial #3 name='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]' Optimizing a Test Set". To learn more about test groups, see IDREF="76469" TYPE="TITLE""Tutorial #4 name='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]' Analyzing a Test Group". Although these are written for the command line interface, you can use the graphical interface to follow both tutorials.Enter test0000 in the Test Name field of the main window and press <return>.Since test0000 has incomplete coverage, it is more useful for illustrating how uncovered items appear.Select "List Functions" from the Queries menu.The list of functions displays in the text view format.Select "Call Tree View" from the Views menu.The Tester main window changes to call graph format. ID="ch0842"IDREF="62364" TYPE="GRAPHIC"Figure 8-12 shows a typical call graph. Initially, the call graph displays the main function and its immediate callees.FILE="ch0812.gif" POSITION="INLINE" SCALE="FALSE"LBL="8-12"Figure 8-12 ID="62364" Call Graph for "List Functions" QueryThe call graph displays functions as nodes and calls as connecting arrows. The nodes are annotated by call count information. Functions with 0 counts are highlighted. Excluded functions when visible appear in the background color.The controls for changing the display of the call graph are just below the display area (see IDREF="32053" TYPE="GRAPHIC"Figure 8-13). FILE="ch0813.gif" POSITION="INLINE" SCALE="FALSE"LBL="8-13"Figure 8-13 ID="32053" Call Graph Display ControlsThese facilities are:ID="ch0843"ID="ch0844"Zoom menu iconshows the current scale of the graph. If clicked on, a pop-up menu appears displaying other available scales. The scaling range is between 15% and 300% of the nominal (100%) size.ID="ch0845"Zoom Out iconresets the scale of the graph to the next (available) smaller size in the range.ID="ch0846"Zoom In iconresets the scale of the graph to the next (available) larger size in the range.ID="ch0847"Overview iconinvokes an overview pop-up display that shows a scaled-down representation of the graph. The nodes appear in the analogous places on the overview pop-up, and a white outline may be used to position the main graph relative to the pop-up. Alternatively, the main graph may be repositioned with its scroll bars.ID="ch0848"Multiple Arcs icontoggles between single and multiple arc mode. Multiple arc mode is extremely useful for the "List Arcs" query, because it indicates graphically how many of the paths between two functions were actually used. ID="ch0849"Realign iconredraws the graph, restoring the positions of any nodes that were repositioned.ID="ch0850"Rotate iconflips the orientation of the graph between horizontal (calling nodes at the left) and vertical (calling nodes at the top).Entering a function in the Search Node field scrolls the display to the portion of the graph in which the function is located.There are two buttons controlling the type of graph. Entering a node in the Func Name field and clicking Butterfly displays the calling and called functions for that node only (Butterfly mode is the default). Selecting Full displays the entire call graph (although not all portions may be visible in the display area).Select "List Arcs" from the Queries menu.The "List Arcs" query displays coverage data for calls made in the test. Because we were just in call graph mode for the "List Functions" query, "List Arcs" comes up in call graph rather than text mode. ID="ch0851"See IDREF="52979" TYPE="GRAPHIC"Figure 8-14. To improve legibility, this figure has been scaled up to 150% and the nodes moved by middle-click-dragging the outlines. Arcs with 0 counts are highlighted in color. Notice that in "List Arcs", the arcs rather than the nodes are annotated.FILE="ch0814.gif" POSITION="INLINE" SCALE="FALSE"LBL="8-14"Figure 8-14 ID="52979" Call Graph for "List Arcs" QueryClick the Multiple Arcs button (the third button from the right in the row of display controls).This displays each of the potential arcs between the nodes. See ID="ch0852"IDREF="38631" TYPE="GRAPHIC"Figure 8-15. Arcs labeled N/A connect excluded functions and do not have call counts.FILE="ch0815.gif" POSITION="INLINE" SCALE="FALSE"LBL="8-15"Figure 8-15 ID="38631" Call Graph for "List Arcs" Query name='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]' Multiple ArcsSelect "Text View" from the Views menu.This returns the display area to text mode from call graph mode. See ID="ch0853"IDREF="91746" TYPE="GRAPHIC"Figure 8-16.The ID="ch0854"ID="ch0855"Callers column lists the calling functions. The Callees column lists the functions called. Line provides the line number where the call occurred; this is particularly useful if there are multiple arcs between the caller and callee. The Files column identifies the source code file. Counts shows the number of times the call was made.You can sort the data in the "List Arcs" query by count, file, caller, or callee. FILE="ch0816.gif" POSITION="INLINE" SCALE="FALSE"LBL="8-16"Figure 8-16 ID="91746" Test Analyzer Queries: "List Arcs"Select "List Blocks" from the Queries menu.The window should be similar to ID="ch0856"IDREF="87799" TYPE="GRAPHIC"Figure 8-17. The data displays in order of blocks, with the starting and ending line numbers of the block indicated. Blocks that span multiple lines are labeled sequentially in parentheses. The count for each block is shown with 0-count blocks highlighted. CautionListing all blocks in a program may be very slow for large programs. To avoid this problem, limit your "List Blocks" operation to a single function.FILE="ch0817.gif" POSITION="INLINE" SCALE="FALSE"LBL="8-17"Figure 8-17 ID="87799" Test Analyzer Queries: "List Blocks"You can sort the data for "List Blocks" by count, file, or function.Select "List Branches" from the Queries menu.The "List Branches" query displays a window similar to ID="ch0857"IDREF="58527" TYPE="GRAPHIC"Figure 8-18. FILE="ch0818.gif" POSITION="INLINE" SCALE="FALSE"LBL="8-18"Figure 8-18 ID="58527" Test Analyzer Queries: "List Branches"The first column shows the line number in which the branch occurs. If there are multiple branches in a line, they are labeled by order of appearance within trailing parentheses. The next two columns indicate the function containing the branch and the file. A branch is considered covered if it has been executed under both true and false conditions. The ID="ch0858"Taken column indicates the number of branches that were executed only under the true condition. The Not Taken column indicates the number of branches that were executed only under the false condition. The "List Branches" query permits sorting by function or file.LBL="9"ID="24089"Tester Graphical User Interface ReferenceThis chapter describes the Tester graphical user interface. It contains these sections:ID="ch091"IDREF="51330" TYPE="TITLE""Accessing the Tester Graphical Interface"IDREF="60992" TYPE="TITLE""Main Window and Menus"IDREF="36683" TYPE="TITLE""Test Menu Operations"IDREF="97295" TYPE="TITLE""Views Menu Operations"IDREF="90460" TYPE="TITLE""Queries Menu Operations"IDREF="52359" TYPE="TITLE""Admin Menu Operations"When you run cvxcov, the main Tester window opens and an iconized version of the Execution View appears on your screen. It displays the output and status of a running program and accepts input. To open a closed Execution View, see "Clone Execution View" in ID="ch092"IDREF="52359" TYPE="TITLE""Admin Menu Operations".LBL="" HELPID=""ID="51330"Accessing the Tester Graphical InterfaceThere are two methods of accessing the Tester graphical user interface:Type cvxcov at the command line with these optional arguments: -testname test to load the test; -ver to show the Tester release version; and -scheme schemename to set a predefined CASEVision color scheme.Select "Tester" from the "Launch Tool" submenu in a WorkShop Admin menu (see IDREF="10511" TYPE="GRAPHIC"Figure 9-1). The major WorkShop tools, the Debugger, Static Analyzer, and Build Manager provide Admin menus from which you can access Tester.FILE="ch09.gif" POSITION="INLINE" SCALE="FALSE"LBL="9-1"Figure 9-1 ID="10511"Accessing Tester from the WorkShop DebuggerLBL="" HELPID=""ID="60992"Main Window and MenusThe main window and its menus are shown in ID="ch093"IDREF="13348" TYPE="GRAPHIC"Figure 9-2.FILE="ch092.gif" POSITION="INLINE" SCALE="FALSE"LBL="9-2"Figure 9-2 ID="13348"Main Test Analyzer WindowLBL="" HELPID=""Test Name Input FieldThe current test is entered (and displayed) in the ID="ch094"Test Name field. You can switch to a different test, test set, or test group through this field. To the right, the Type field indicates whether it is a Single Test, Test Set, or Test Group. You can select a test (test set or test group) from the "List Tests" dialog box under the Test menu, to appear in the ID="ch095"Test Name field in the main window.LBL="" HELPID=""Coverage Display AreaTest results display in the ID="ch096"coverage display area. You select the results by choosing an item from the Queries menu. You can select the format of the dataname='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'text, call tree, or bar chartname='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]' from the Views menu. (Note that the Text View format is available for all queries, whereas the other two views are limited.)The Query Type displays under the ID="ch097"Test Name field, just over the display. It is followed on the far right of the window by the Query Size (number of items in the list). Headings above the display are specific to each query.ID="ch098"LBL="" HELPID=""Search FieldThe Search field lets you look for strings in the coverage data. It uses an incremental search, that is, as you enter characters, the highlight moves to the first matching target. When you press ID="ch099"<return>, the highlight moves to the next occurrence.LBL="" HELPID=""Control Area ButtonsApply is a general-purpose button for terminating data entry in text fields; you can use <ID="ch0910"ID="ch0911"return> equivalently. Both start the query. Source lets you bring up the standard CASEVision Source View window with Tester annotations. Source View shows the counts for each line and highlights lines with 0 counts. By default Source View is shared with other applications. For example, if ID="ch0912"cvstatic performs a search for function A, the results of the query overwrite Tester query results that are in the shared Source View. To stop sharing Source View with other applications, set the following resource:ID="ch0913"ID="ch0914"cvsourceNoShare:  TrueDisassembly brings up the CASEVision Disassembly View window, called "Assembly Source Coverage", which operates at the machine level in a similar fashion to the Source View. This view is not shared with other applications.NoteIf a test has very large counts, there may not be enough space in the Source View and Disassembly View windows to display them. To make more room, increase the ID="ch0915"ID="ch0916"ID="ch0917"canvasWidth resource in the Cvxcov app-defaults file, ID="ch0918"Cvxcov*test*testdata*canvasWidth.Contribution brings up the Test Contribution window with the contributions made by each test so that you can compare the results. It is available for the queries "List Functions", "List Arcs", and "List Blocks". When the tests do not fit on one page, multiple pages are used. Use the Previous Page and Next Page buttons to display all the tests. ID="ch0919"ID="ch0920"ID="ch0921"Sort lets you sort the test results by criteria such as function, count, file, type, difference, caller, or callee. The criteria available depend on the current query.ID="ch0922"LBL="" HELPID=""ID="61023"Status Area and Query-Specific FieldsThe status area displays status messages that confirm commands, issue warnings, and indicate error conditions. When you enter a test name in the ID="ch0923"Test Name field, the Func Name field appears (along with other items) in the status area for use with queries. Entering a function in this field displays the coverage results limited to that function only.ID="ch0924"Additional items display in the area below the status area that change when you select commands from the Queries menu. These items are specific to the query selected. Some of these items can be used as defaults (see IDREF="90460" TYPE="TITLE""Queries Menu Operations").LBL="" HELPID=""Main Window MenusThe Admin menu lets you perform general housekeeping concerning saving files, setting defaults, changing directories, launching other WorkShop applications, and exiting.ID="ch0925"The Test menu lets you create, modify, and run tests, test sets, and test groups.The Views menu lets you choose one of the following modes:text mode, which displays results numerically in columns graphical mode, which displays functions as nodes (rectangles) annotated by results calls as arcs (connecting arrows)bar graph mode, which displays the summary of a test as a bar graph.The Queries menu lets you analyze the results of tests. The Help menu is standard in all CASEVision tools.LBL="" HELPID=""ID="36683"Test Menu OperationsAll operations for running tests are accessed from the Test menu in the main Tester window. IDREF="25293" TYPE="GRAPHIC"Figure 9-3 shows the dialog boxes used to perform test operations. The Test menu provides these selections:ID="ch0926""Run Instrumentation"instruments the target executable. Instrumentation adds code to the executable to collect coverage data. For a more detailed discussion of instrumentation and instrument files, see ID="ch0927"IDREF="71981" TYPE="TITLE""Single Test Analysis Process".FILE="ch094.gif" POSITION="INLINE" SCALE="FALSE"LBL="9-3"Figure 9-3 ID="25293"Test Menu CommandsThe "Run Instrumentation" dialog box (see IDREF="42755" TYPE="GRAPHIC"Figure 9-4) provides these fields:Executable lets you enter the name of the target.Instrumentation File is for entering the instrumentation file, which is an ASCII description of the instrumentation criteria for the experiment.Instrumentation Dir lets you enter the directory in which the instrumentation file is stored (not necessary if you're using the current working directory).Version Number lets you specify the version number of the instrumentation directory (ID="ch0928"ver##<versionnumber>). If this field is left blank, the version number increments automatically. If you are testing multiple executables (that is, testing coverage of an executable that forks, execs, or sprocs other processes), then you need to store these in the same instrumentation directory. You do this by entering the same number in the Version Number field.FILE="f26rnins.gif" POSITION="INLINE" SCALE="FALSE"LBL="9-4"Figure 9-4 ID="42755""Run Instrumentation" Dialog Box"Run Test" invokes the executable with selected arguments and collects the coverage data. The "Run Test" dialog box (see ID="ch0929"IDREF="36600" TYPE="GRAPHIC"Figure 9-5) provides these fields and buttons:Test Name is for entering the test name.Version Number is for entering the version number of the directory (ver## <number>) containing the instrumented executable. If you are using the most current (highest) version number, then you can leave the field blank; otherwise, you need to enter the desired number.Force Run is a toggle that when turned on causes the test to be run even if results already exist.Keep Performance Data is a toggle that when turned on retains all the performance data collected in the experiment.Accumulate Results is a toggle that when turned on accumulates (sums over) the coverage data into the existing experiment results.No Arc Data prevents arc information from being collected in the experiment. It can't be used with "List Arcs" or a Call Tree View. "List Summary" and "Compare Test" will have 0% coverage on arc items. Use it to save space if you don't need arc data.Remove Subtest Expt removes results for individual subtests for test sets or test groups, letting you see the top level and taking less space. There will be no data to query if you are querying a subtest.FILE="f26rntst.gif" POSITION="INLINE" SCALE="FALSE"LBL="9-5"Figure 9-5 ID="36600""Run Test" Dialog Box"Make Test"creates a test directory where the coverage data is to be stored and stores a TDF (test description file). For more information on this process, see ID="ch0930"IDREF="71981" TYPE="TITLE""Single Test Analysis Process".The "Make Test" dialog box (see IDREF="27070" TYPE="GRAPHIC"Figure 9-6) provides these fields for tests, test sets, and test groups:Test Name is for entering the test name.Test Type is a toggle for indicating the type of test: single, test set, or test group (for dynamically shared objects). Description lets you enter a description to document the test.FILE="f26mktst.gif" POSITION="INLINE" SCALE="FALSE"LBL="9-6"Figure 9-6 ID="27070""Make Test" Dialog BoxIf you select Single Test, the following fields are provided:Command Line lets you enter the target and any arguments to be used in the test.ID="ch0931"Instrument Dir is the directory in which the instrumentation file and related data are stored (not necessary if current working directory). Executable List is used if you are testing coverage of an executable that forks, execs, or sprocs other processes and want to include those processes. You must specify these executables in the Executable List field.If you select Test Set, the following fields and buttons are provided:ID="ch0932"Test List contains all the tests in the working directory.ID="ch0933"Test Include List (to the right) displays tests included in the test set or test group.ID="ch0934"Add looks at the selected item in the Test List or Select field and adds it to the Test Include List.Remove looks at the selected item in the ID="ch0935"Test Include List and removes it.Select displays the currently selected test.ID="ch0936"For a test group (see IDREF="81128" TYPE="GRAPHIC"Figure 9-7), the following field is added to the same fields and buttons used for a test set:Targets lets you enter a list of target DSOs or shared libraries, separated by spaces.ID="ch0937"ID="ch0938"FILE="ch097.gif" POSITION="INLINE" SCALE="FALSE"LBL="9-7"Figure 9-7 ID="81128""Make Test" Dialog Box with Test Group Selected"Delete Test" removes the specified test directory and its contents. The "Delete Test" dialog box (see ID="ch0939"IDREF="69134" TYPE="GRAPHIC"Figure 9-8) provides these fields:Test Name is for entering the test name.Recursive List is a toggle that when turned on includes all subtests in the removal of test sets and test groups.ID="ch0940"FILE="f26dltst.gif" POSITION="INLINE" SCALE="FALSE"LBL="9-8"Figure 9-8 ID="69134""Delete Test" Dialog Box"List Tests" shows you the tests in the current working directory. The "List Tests" dialog box (see ID="ch0941"IDREF="41623" TYPE="GRAPHIC"Figure 9-9) provides these fields:Working Dir shows the directory containing the tests.A scrollable list field displays the tests present in the specified directory. The scroll bars let you navigate through the tests if they don't fit completely in the field. Clicking an item places it in the Select field. Double-clicking on a test selects and loads it.Select displays the test name you type in or that you clicked in the list. Click OK to load your selection into the Test Name field of the main Tester window. Close lets you exit without loading a selection.FILE="f26lstst.gif" POSITION="INLINE" SCALE="FALSE"LBL="9-9"Figure 9-9 ID="41623""List Tests" Dialog Box"Modify Test" lets you modify a test set or test group. You enter the test name in the ID="ch0942"Test Name field and press <return> or click the View button to load it. The View button changes to Apply, the Test List field displays tests in the current working directory, and the Test Include List field displays the contents of the test set or test group. You can then add or delete tests, test sets, or test groups in the current test set or test group, respectively. The "Modify Test" dialog box (see IDREF="49923" TYPE="GRAPHIC"Figure 9-10) has these fields:Test Name is for entering the test name.Test List displays the tests in the current directory.Test Include List displays the subtests for the test specified in the Test Name field.Select displays the test currently selected for adding or removing. You can enter the test directly in this field instead of selecting it from the Test List or Test Include List.The Add button lets you add the selected test to the Test Include List.The Remove button lets you delete the selected test from the Test Include List.The Apply button applies the changes you have selected. (The button name is View until you load something.)FILE="f26mdtst.gif" POSITION="INLINE" SCALE="FALSE"LBL="9-10"Figure 9-10 ID="49923" "Modify Test" Dialog Box After Loading TestsLBL="" HELPID=""ID="97295"Views Menu OperationsThe Views menu has three selections that let you view coverage data in different forms. The selections are:ID="ch0943""Text View"displays the coverage data in text form. The information displayed depends on which query you have selected. See ID="ch0944"IDREF="83501" TYPE="GRAPHIC"Figure 9-11.FILE="ch0911.gif" POSITION="INLINE" SCALE="FALSE"LBL="9-11"Figure 9-11 ID="83501""List Functions" Query in "Text View" Format"Call Tree View"displays coverage data graphically, with functions as nodes (rectangles) and calls as arcs (connecting arrows). This view is only valid for "List Functions", "List Blocks", "List Branches", and "List Arcs". See ID="ch0945"IDREF="61621" TYPE="GRAPHIC"Figure 9-12. It is not available if you run a test with No Arc Data on.FILE="ch0912.gif" POSITION="INLINE" SCALE="FALSE"LBL="9-12"Figure 9-12 ID="61621""List Functions" Query in "Call Tree View" Format"Bar Graph View"displays a bar chart showing the percentage covered for functions, lines, blocks, branches, and arcs. See ID="ch0946"IDREF="24190" TYPE="GRAPHIC"Figure 9-13. This view is only valid for "List Summary", which is described in detail in IDREF="90460" TYPE="TITLE""Queries Menu Operations".FILE="ch0913.gif" POSITION="INLINE" SCALE="FALSE"LBL="9-13"Figure 9-13 ID="24190""List Summary" Query in "Bar Graph View" FormatLBL="" HELPID=""ID="90460"Queries Menu OperationsThe Queries menu provides different methods for analyzing the results of coverage tests. Each type of query displays the coverage data in the coverage display area in the main Tester window and displays items that are specific to the query in the area below the status area. When you set these items for a query, the same values are used by default for subsequent queries until you change them. You can set these defaults before the first query or as part of any query. For a single test or test set, all queries except "Describe Test" have the fields shown in ID="ch0947"ID="ch0948"IDREF="74002" TYPE="GRAPHIC"Figure 9-14. FILE="ch0914.gif" POSITION="INLINE" SCALE="FALSE"LBL="9-14"Figure 9-14 ID="74002"Query-Specific Default Fields for a Test or Test SetThe Executable field displays the executable associated with the current coverage data. You can switch to a different executable by entering it directly in this field. You can also switch executables by clicking the ID="ch0949"Executable List button, selecting from the list in the Target List dialog box and clicking ID="ch0950"ID="ch0951"Apply in the dialog box.The experiment menu (Expt) lets you see the results for a different experiment that uses the same test criteria.ID="ch0952"NoteWhen you are performing queries on a test group, the Executable field changes to Object field and the ID="ch0953"Executable List button changes to ID="ch0954"Object List as shown in IDREF="74002" TYPE="GRAPHIC"Figure 9-14. These items act analogously except that they operate on dynamically shared objects (DSOs). Refer to IDREF="76469" TYPE="TITLE""Tutorial #4 name='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]' Analyzing a Test Group" for more information on test groups.FILE="ch0916.gif" POSITION="INLINE" SCALE="FALSE"LBL="9-15"Figure 9-15 ID="74002"Query-Specific Default Fields for a DSO Test GroupThe Queries menu (see IDREF="25753" TYPE="GRAPHIC"Figure 9-16) provides these selections:ID="ch0955"FILE="f26qrymn.gif" POSITION="MARGIN" SCALE="FALSE"LBL="9-16"Figure 9-16 ID="25753"Queries MenuID="79831""List Summary"shows the overall coverage based on the user-defined weighted average over function, source line, branch, arc, and ID="ch0956"block coverage. The coverage data appears in the coverage display area. A typical summary appears in IDREF="32491" TYPE="GRAPHIC"Figure 9-17.FILE="ch0919.gif" POSITION="INLINE" SCALE="FALSE"LBL="9-17"Figure 9-17 ID="32491""List Summary" QueryThe Coverages column indicates the type of coverage. The Covered column shows the number of functions, source lines, branches, arcs, and blocks that were executed in this test (or test set or test group). The Total column indicates the total number of items that could be executed for each type of coverage. The % Coverage column is simply the Covered value divided by the Total value in each category. The Weight column indicates the weighting assigned to each type of coverage. It is used to compute the Weighted Sum, a user-defined factor that can be used to judge the effectiveness of the test. The Weighted Sum is obtained by first multiplying the individual coverage percentages by the weighting factors and then summing the products.The "List Summary" command causes the coverage weighting factor fields to display below the status area. Use these to adjust the factor values as desired. They should add up to 1.0.ID="ch0957"If you select "Bar Graph View" from the Views menu, the summary will be shown in bar graph format as shown in ID="ch0958"IDREF="24190" TYPE="GRAPHIC"Figure 9-13. The percentage covered is shown along the vertical axis; the types of coverage are indicated along the horizontal axis."List Functions"displays the coverage data for functions in the specified test. The ID="ch0959"Functions column heading identifies the function, Files shows the source file containing the function, and Counts displays the number of times the function was executed in the test. "List Functions" enables the sort menu that lets you determine the order in which the functions display. Only the sort criteria appropriate for the current query are enabled, in this case, "Sort By Func", "Sort By Count", and "Sort By File" as shown in ID="ch0960"IDREF="44459" TYPE="GRAPHIC"Figure 9-18.The ID="ch0961"Search field scrolls the list to the string entered. The string may occur in any of the columns. This is an incremental search and is activated as you enter characters, scrolling to the first matching occurrence. Entering a function in the Func Name field displays the coverage results limited to that function only in the display area.The Filters button displays the Filters dialog box, which lets you enter filter criteria to display a subset of the coverage results. There are three types of filters: ID="ch0962"Function Count, Block Count (%), and Branch Count (%). For blocks or branch coverage, use the toggles described below. Following each label is an operator menu to define the relationship to the limit quantity entered. Each filter type has a text field for entering the desired limit. The limits for Block Count and Branch Count are percentages (of coverage) and can also be entered using sliders.Two toggles are available for including branch and block counts. Both appear as actual counts followed by parentheses containing the ratio of counts to total possible.ID="ch0963"ID="ch0964"FILE="ch0920.gif" POSITION="INLINE" SCALE="FALSE"LBL="9-18"Figure 9-18 ID="44459""List Functions" Query with OptionsIf you select "Call Tree View" from the Views menu with a "List Functions" query, a call graph displays (see IDREF="75274" TYPE="GRAPHIC"Figure 9-19). The call graph displays coverage data graphically, with functions as nodes (rectangles) and calls as arcs (connecting arrows). The nodes are color-coded according to whether the function was included and covered in the test, included and not covered, or excluded from the test. Arcs labeled N/A connect excluded functions and do not have call counts. If you hold down the right mouse button over a node, the node menu displays, including the function name, coverage statistics, and standard node manipulation commands. If you have a particularly large graph, you may find it useful to zoom to 15% or 40% and look at the coverage statistics through the node menu.FILE="ch0921.gif" POSITION="INLINE" SCALE="FALSE"LBL="9-19"Figure 9-19 ID="75274""List Functions" Example in "Call Tree View" Format"List Blocks"displays a list of blocks for one or more functions and the count information associated with each block (see ID="ch0965"IDREF="68464" TYPE="GRAPHIC"Figure 9-20). The Blocks column displays the line number in which the block occurs. If there are multiple blocks in a line, blocks subsequent to the first are shown in order with an index number in parentheses. The other three columns show the function and file containing the block and the count, that is, the number of times the block was executed in the test. Uncovered blocks (those containing 0 counts) are highlighted. Block data can be sorted by function, file, or count.Be careful before listing all blocks in the program, since this can produce a lot of data. Entering a function in the Func Name field displays the coverage results limited to that function only in the display area.FILE="ch0922.gif" POSITION="INLINE" SCALE="FALSE"LBL="9-20"Figure 9-20 ID="68464""List Blocks" Example"List Branches"lists coverage information for branches in the program. ID="ch0966"Branch coverage counts assembly language branch instructions that are taken and not taken. See IDREF="11311" TYPE="GRAPHIC"Figure 9-21.The first column shows the line number in which the branch occurs. If there are multiple branches in a line, they are labeled by order of appearance within trailing parentheses. The next two columns indicate the function containing the branch and the file. A branch is considered covered if it has been executed under both true and false conditions. The Taken column indicates the number of branches that were executed only under the true condition. The ID="ch0967"Not Taken column indicates the number of branches that were executed only under the false condition. Branch coverage can be sorted only by function and file. Entering a function in the ID="ch0968"Func Name field displays the coverage results limited to that function only in the display area.FILE="ch0923.gif" POSITION="INLINE" SCALE="FALSE"LBL="9-21"Figure 9-21 ID="11311""List Branches" Example"List Arcs"shows arc coverage, that is, the number of arcs taken out of the total possible arcs. An arc is a call from one function (caller) to another (callee). See ID="ch0969"IDREF="18472" TYPE="GRAPHIC"Figure 9-22. The caller and callee functions are identified in the first two columns. The Line column identifies the line in the caller function where the call occurs. The file and arc execution count display in the last two columns. FILE="ch0924.gif" POSITION="INLINE" SCALE="FALSE"LBL="9-22"Figure 9-22 ID="18472""List Arcs" ExampleEntering a function in the Func Name field displays the coverage results limited to that function only.The Caller and Callee toggles let you view the arcs for a single function either as a caller or callee. You do this by entering the function name in the ID="ch0970"ID="ch0971"Func Name field and then clicking the appropriate toggle, Caller or Callee."List Argument Traces"shows argument tracing information (see ID="ch0972"IDREF="15796" TYPE="GRAPHIC"Figure 9-23). Argument tracing is enabled in the instrumentation file using the TRACE command with the MAX, MIN, BOUNDS, and RETURN options. TRACE lets you monitor argument values in the functions over all experiments. The syntax in the file is:ID="ch0973"ID="ch0974"TRACE [RETURN] MAX|MIN|BOUNDS function(arg)where:MAX monitors the maximum value of an argument. MIN monitors the minimum value of an argument. BOUNDS monitors both the minimum and maximum values. RETURN monitors the function return values. For more information on the instrumentation file, see IDREF="71981" TYPE="TITLE""Single Test Analysis Process".FILE="f26lsarg.gif" POSITION="INLINE" SCALE="FALSE"LBL="9-23"Figure 9-23 ID="15796""List Argument Traces" ExampleThe Arguments column shows the calling function with its argument. Type indicates the type of the argument. Range shows the minimum and maximum values if TRACE bounds was selected; otherwise, it shows the end of the range selected with a short line (-) substituted for the opposite end of the range.Entering a function in the Func Name field displays the coverage results limited to that function only in the display area."List Instrumentation"displays the instrumentation information for a particular test. See ID="ch0975"IDREF="87176" TYPE="GRAPHIC"Figure 9-24.Function List toggle shows the functions that are included in the coverage experiment. Ver allows you to specify the version of the program that was instrumented. The latest version is used by default.Executable displays the executable associated with the current coverage data. You can switch to a different executable by entering it directly in this field. You can also switch executables by clicking the ExecutableList button, selecting from the list in the dialog box, and clicking Apply in the dialog box.FILE="ch0926.gif" POSITION="INLINE" SCALE="FALSE"LBL="9-24"Figure 9-24 ID="87176""List Instrumentation" Example"List Line Coverage"lists the coverage for each function for native source lines. Entering a function in the ID="ch0976"Func Name field displays the coverage results limited to that function only in the display area. See IDREF="89790" TYPE="TEXT"IDREF="89790" TYPE="GRAPHIC"Figure 9-25. FILE="ch0927.gif" POSITION="INLINE" SCALE="FALSE"LBL="9-25"Figure 9-25 ID="89790""List Line Coverage" Example"Describe Test"describes the details of the test, test set, or test group. When working with test sets and test groups, it is useful to select the ID="ch0977"Recursive List toggle, because it describes the details for all subtests. See ID="ch0978"IDREF="74105" TYPE="GRAPHIC"Figure 9-26.FILE="ch0928.gif" POSITION="INLINE" SCALE="FALSE"LBL="9-26"Figure 9-26 ID="74105""Describe Test" Example"Compare Test"shows the difference in coverage for the same test applied to different versions of the same program. To perform a comparison, you need to select "Compare Test" from the Queries menu, enter experiment directories in the experiment fields, and click ID="ch0979"Apply or press <return>. The experiments are entered in the form exp##<n> if in the same test or in the form test<nnnn>/exp##<n> when comparing the results of different tests. See IDREF="66240" TYPE="GRAPHIC"Figure 9-27. FILE="ch0929.gif" POSITION="INLINE" SCALE="FALSE"LBL="9-27"Figure 9-27 ID="66240""Compare Test" Example name='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]' Coverage DifferencesThe comparison data displays in the coverage display area. The basic types of coverage display in the Coverages column. Result 1 and Result 2 display the results of the experiments specified in the ID="ch0980"Expt1 and Expt2 fields, respectively. Results are shown as the counts followed by the coverage percentage in parentheses. The values in the Result 2 column are subtracted from those in Result 1 and the differences are shown in the Differences column. If you want to view the available experiments, click the Expt: menu.You can also compare the differences in function coverage by clicking the ID="ch0981"Diff Functions toggle. IDREF="79816" TYPE="GRAPHIC"Figure 9-28 shows a typical function difference example.FILE="ch0930.gif" POSITION="INLINE" SCALE="FALSE"LBL="9-28"Figure 9-28 ID="79816""Compare Test" Example name='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]' Function DifferencesLBL="" HELPID=""ID="52359"Admin Menu OperationsThe Admin menu is shown in ID="ch0982"IDREF="10560" TYPE="GRAPHIC"Figure 9-29. FILE="f26admmn.gif" POSITION="INLINE" SCALE="FALSE"LBL="9-29"Figure 9-29 ID="10560"Admin MenuThe Admin menu provides these selections:"Save Results"brings up the standard CASEVision File Browser dialog box so that you can specify a file in which to save the results.ID="ch0983""Clone Execution View"displays an Execution View window. Use this if you have closed the initial Execution View window and need a new one. (You need this window to see the results of "Run Test".)ID="ch0984"ID="ch0985""Set Defaults"allows you to change the working directory for work on tests in other directories. Also, you can select whether or not to show function arguments. This is useful when distinguishing functions that have the same name but different arguments (for example, C++ constructors and overloaded functions). See ID="ch0986"ID="ch0987"IDREF="91982" TYPE="GRAPHIC"Figure 9-30.FILE="f26setdf.gif" POSITION="INLINE" SCALE="FALSE"LBL="9-30"Figure 9-30 ID="91982""Set Defaults" Dialog Box"Launch Tool"The Launch Tool submenu contains commands for launching other WorkShop applications (see ID="ch0988"ID="ch0989"IDREF="14584" TYPE="GRAPHIC"Figure 9-31). FILE="f26lnch2.gif" POSITION="INLINE" SCALE="FALSE"LBL="9-31"Figure 9-31 ID="14584""Launch Tool" SubmenuIf any of these tools are not installed on your system, the corresponding menu item will be grayed out."Exit" closes all Tester windows."Custom task"IDREF="ch0314""Custom task""Determine bottlenecks, identify phases"IDREF="ch035""Determine bottlenecks, identify phases""Find Floating Point Exceptions"IDREF="ch0313""Find Floating Point Exceptions""Find memory leaks"IDREF="ch0312""Find memory leaks""Get CPU Time per function & source line"IDREF="ch037""Get CPU Time per function & source line""Get Ideal Time per function & source line"IDREF="ch038""Get Ideal Time (pixie) per function & source line""Get Total Time per function & source line"IDREF="ch036""Get Total Time per function & source line""Graph Call Tree"exampleIDREF="ch0842"Tutorial #3 name='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]' Exploring the Graphical User Interface"List Arcs"column headingsIDREF="ch0854"Tutorial #3 name='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]' Exploring the Graphical User InterfaceexampleIDREF="ch0851"Tutorial #3 name='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]' Exploring the Graphical User Interface"List Blocks"exampleIDREF="ch0856"Tutorial #3 name='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]' Exploring the Graphical User Interface"List Branches"column headingsIDREF="ch0858"Tutorial #3 name='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]' Exploring the Graphical User InterfaceexampleIDREF="ch0857"Tutorial #3 name='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]' Exploring the Graphical User Interface"List Functions"column headingsIDREF="ch0839"Invoking the Graphical User InterfaceexampleIDREF="ch0838"Invoking the Graphical User Interface"List Summary"exampleIDREF="ch0837"Invoking the Graphical User Interface"Make Test"IDREF="ch0529"Single Test Analysis ProcessexampleIDREF="ch0824"Invoking the Graphical User Interface"Run Instrumentation"IDREF="ch0526"Single Test Analysis ProcessexampleIDREF="ch0819"Invoking the Graphical User Interface"Run Test"IDREF="ch0532"Single Test Analysis ProcessexampleIDREF="ch0828"Invoking the Graphical User Interface"Text Call Tree" exampleIDREF="ch0853"Tutorial #3 name='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]' Exploring the Graphical User Interface"Trace I/O activity"IDREF="ch039""Trace I/O activity""Trace page faults"IDREF="ch0311""Trace page faults""Trace system calls"IDREF="ch0310""Trace system calls""Bar Graph View"IDREF="ch0946"Views Menu Operations"Call Tree View"IDREF="ch0945"Views Menu Operations"Clone Execution View"IDREF="ch0984"Admin Menu Operations"Compare Test"IDREF="ch0979"Queries Menu Operations"Delete List" dialog boxIDREF="ch0939"Test Menu Operations"Describe Test"IDREF="ch0977"Queries Menu Operations"List Arcs"IDREF="ch0969"Queries Menu Operations"List Argument Traces"IDREF="ch0972"Queries Menu Operations"List Blocks"IDREF="ch0965"Queries Menu Operations"List Branches"IDREF="ch0966"Queries Menu Operations"List Functions"IDREF="ch0959"Queries Menu Operations"List Instrumentation"IDREF="ch0975"Queries Menu Operations"List Line Coverage"IDREF="ch0976"Queries Menu Operations"List Summary"IDREF="ch0956"Queries Menu Operations"List Tests" dialog boxIDREF="ch0941"Test Menu Operations"Make Test"dialog boxIDREF="ch0930"Test Menu Operations"Modify Test" dialog boxIDREF="ch0942"Test Menu Operations"Run Instrumentation"dialog boxIDREF="ch0927"Test Menu Operations"Run Test"dialog boxIDREF="ch0929"Test Menu Operations"Save Results"IDREF="ch0983"Admin Menu Operations"Set Defaults"IDREF="ch0986"Admin Menu Operations"Text View"IDREF="ch0944"Views Menu OperationsAccumulate Results buttonIDREF="ch0832"Invoking the Graphical User InterfaceAdd buttonIDREF="ch0935"Test Menu OperationsaddtestIDREF="ch0749"Test Set CommandsAdmin menuIDREF="ch0982"Admin Menu Operationsapp-defaults file, Cvxcov resourceIDREF="ch0918"Control Area ButtonsApply buttonIDREF="ch0911"Control Area Buttons-argIDREF="ch079"Common cvcov Optionsautomated testing IDREF="ch0543"Automated Testingbad freesIDREF="ch015"Bad Freesbar graph exampleIDREF="ch0958"Queries Menu Operationsbatch testing IDREF="ch0543"Automated TestingBlocks buttonIDREF="ch0964"Queries Menu OperationsBOUNDSIDREF="ch0523"Single Test Analysis ProcessexampleIDREF="ch0821"Invoking the Graphical User InterfaceIDREF="ch065"Instrumenting an ExecutableIDREF="ch0818"Invoking the Graphical User InterfaceBranches buttonIDREF="ch0963"Queries Menu OperationsbutterflyIDREF="ch0429"Filtering Nodes through the Display ControlsButterfly  buttonIDREF="ch0430"Filtering Nodes through the Display ControlscalipersIDREF="ch0416"The Time Line Caliperscall graphIDREF="ch0435"Other Manipulation of the Call Graphcall graph controlsIDREF="ch0843"Tutorial #3 name='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]' Exploring the Graphical User Interfacecall stack data collectionIDREF="ch041"Call Stack ProfilingCall Stack Information windowIDREF="ch047"Page Fault TracingCall Stack windowIDREF="ch0448"Call StackcalleesIDREF="ch0855"Tutorial #3 name='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]' Exploring the Graphical User InterfacecvcovIDREF="ch0713"Common cvcov Options"List Arcs" andIDREF="ch0971"Queries Menu OperationscallersIDREF="ch0855"Tutorial #3 name='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]' Exploring the Graphical User InterfacecvcovIDREF="ch0712"Common cvcov Optionscallers""List Arcs" andIDREF="ch0970"Queries Menu OperationscanvasWidth resourceIDREF="ch0917"Control Area ButtonscattestIDREF="ch0717"General Test CommandsexampleIDREF="ch0718"General Test CommandsIDREF="ch069"Instrumenting an ExecutableIDREF="ch0723"General Test CommandsChain operationIDREF="ch0431"Filtering Nodes through the Display ControlsIDREF="ch0432"Filtering Nodes through the Display ControlsCharts menuIDREF="ch0427"Process MeterCommand Line fieldIDREF="ch0826"Invoking the Graphical User Interface"Make Test" andIDREF="ch0931"Test Menu Operationscommand line tutorialIDREF="ch061"Tester Command Line Interface Tutorialcompiling, effect on coverageIDREF="ch056"Test Coverage DataCONSTRAINIDREF="ch0519"Single Test Analysis ProcessexampleIDREF="ch065"Instrumenting an ExecutableIDREF="ch0821"Invoking the Graphical User InterfaceIDREF="ch0818"Invoking the Graphical User InterfaceContext Switch stripchartIDREF="ch0419"Charts in Usage View (Graphs) -contribIDREF="ch073"Common cvcov OptionsContribution buttonIDREF="ch0919"Control Area ButtonsIDREF="ch0813"Invoking the Graphical User Interfacecontrol area buttonsIDREF="ch0910"Control Area ButtonsCord AnalyzerIDREF="ch018"Cord AnalyzerIDREF="ch0451"Cord AnalyzerCOUNTSIDREF="ch0517"Single Test Analysis ProcessexampleIDREF="ch0818"Invoking the Graphical User InterfaceIDREF="ch0821"Invoking the Graphical User InterfaceIDREF="ch065"Instrumenting an Executablecoverage definedIDREF="ch051"Tester Overviewdisplay areaIDREF="ch096"Coverage Display Areakinds ofIDREF="ch055"Test Coverage Datacoverage analysisIDREF="ch0533"Single Test Analysis ProcessprocedureIDREF="ch0515"Single Test Analysis Processcoverage display areaIDREF="ch088"Invoking the Graphical User Interfacecoverage testing hierarchyIDREF="ch0546"Additional Coverage Testingcoverage weighting factor fieldsIDREF="ch0957"Queries Menu Operationscp, not using with cvcovIDREF="ch0752"Test Set CommandsIDREF="ch0757"Test Group CommandscvcovaddtestIDREF="ch0749"Test Set CommandscattestIDREF="ch0717"General Test CommandsdeltestIDREF="ch0750"Test Set CommandsdiffIDREF="ch0744"Coverage Analysis CommandshelpIDREF="ch062"Tester Command Line Interface TutorialIDREF="ch0716"cvcov Command Syntax and DescriptionlsarcIDREF="ch0735"Coverage Analysis CommandslsblockIDREF="ch0731"Coverage Analysis CommandslsbranchIDREF="ch0733"Coverage Analysis CommandslscallIDREF="ch0737"Coverage Analysis CommandslsfunIDREF="ch0729"Coverage Analysis CommandslsinstrIDREF="ch0719"General Test CommandslslineIDREF="ch0739"Coverage Analysis CommandslssourceIDREF="ch0740"Coverage Analysis CommandslssumIDREF="ch0727"Coverage Analysis CommandslstestIDREF="ch0721"General Test CommandslstraceIDREF="ch0742"Coverage Analysis CommandsmktgroupIDREF="ch0755"Test Group CommandsmktsetIDREF="ch0748"Test Set CommandsrmtestIDREF="ch0724"General Test CommandsruninstrIDREF="ch0725"General Test CommandsruntestIDREF="ch0726"General Test Commandscvcov mktestIDREF="ch0722"General Test CommandscvsourceNoShareIDREF="ch0914"Control Area ButtonscvspeedIDREF="ch032"Selecting a Performance TaskcvxcovIDREF="ch084"Invoking the Graphical User Interface command-line argumentsIDREF="ch054"Tester Overviewdefault instrumentation fileIDREF="ch0525"Single Test Analysis Processdefault_instr_fileIDREF="ch0525"Single Test Analysis ProcessdeltestIDREF="ch0750"Test Set CommandsDescription fieldIDREF="ch0825"Invoking the Graphical User InterfacediffIDREF="ch0744"Coverage Analysis CommandsexampleIDREF="ch0745"Coverage Analysis CommandsIDREF="ch0746"Coverage Analysis CommandsDiff Functions buttonIDREF="ch0981"Queries Menu OperationsdirectoryinstrumentationIDREF="ch0512"Test ComponentsDisassembled Source buttonIDREF="ch0415"Function List Display and ControlsDisassembly buttonIDREF="ch0840"Invoking the Graphical User InterfaceIDREF="ch0810"Invoking the Graphical User InterfaceDisassembly ViewIDREF="ch0812"Invoking the Graphical User InterfaceexampleIDREF="ch0840"Invoking the Graphical User InterfacewidthIDREF="ch0916"Control Area ButtonsDSOIDREF="ch059"Multiple TestsIDREF="ch0545"Additional Coverage TestingIDREF="ch053"Tester Overview making a test groupIDREF="ch0938"Test Menu Operationstest group commandsIDREF="ch0754"Test Group Commandsdynamically shared object See DSOIDREF="ch052"Tester OverviewEXCLUDEIDREF="ch0518"Single Test Analysis Process-exeIDREF="ch074"Common cvcov OptionsExecutable fieldIDREF="ch0949"Queries Menu OperationsExecutable List buttonIDREF="ch0950"Queries Menu OperationsExecution ViewIDREF="ch0985"Admin Menu OperationsIDREF="ch092"Tester Graphical User Interface ReferenceIDREF="ch0835"Invoking the Graphical User Interfaceexp##0IDREF="ch0531"Single Test Analysis Processexperiment resultsIDREF="ch0531"Single Test Analysis ProcessIDREF="ch057"Experiment ResultsIDREF="ch0513"Test ComponentsexperimentsPerformance AnalyzerIDREF="ch031"Experiment Setup OverviewExpt menuIDREF="ch0952"Queries Menu OperationsExpt1 and Expt2 fieldsIDREF="ch0980"Queries Menu OperationsFilters dialog boxIDREF="ch0962"Queries Menu OperationsForce Run buttonIDREF="ch0830"Invoking the Graphical User InterfaceFunc Name fieldIDREF="ch0924"Status Area and Query-Specific Fieldsfunction counts collectionIDREF="ch042"Function Count Collectionfunction listIDREF="ch0442"Selecting Nodes from the Function Listfunction list IDREF="ch0410"Function List Display and Controls-functionsIDREF="ch0714"Common cvcov Optionsgraphical user interfaceIDREF="ch086"Invoking the Graphical User InterfaceIDREF="ch0816"Invoking the Graphical User InterfacereferenceIDREF="ch091"Tester Graphical User Interface ReferencetutorialIDREF="ch081"Setting Up the TutorialHeap ViewIDREF="ch0446"Analyzing the Memory Map with Heap ViewHeap View tutorialIDREF="ch0447"Memory Experiment TutorialhelpIDREF="ch062"Tester Command Line Interface TutorialIDREF="ch0716"cvcov Command Syntax and DescriptionHide 0 functions toggleIDREF="ch0412"Function List Display and ControlsINCLUDEIDREF="ch0518"Single Test Analysis Process-instr_dirIDREF="ch075"Common cvcov Options-instr_fileIDREF="ch076"Common cvcov OptionsInstrument File fieldIDREF="ch0820"Invoking the Graphical User InterfaceinstrumentationIDREF="ch0511"Test ComponentsdirectoryIDREF="ch0512"Test ComponentslsinstrIDREF="ch0719"General Test CommandsprocessIDREF="ch0526"Single Test Analysis ProcesstutorialIDREF="ch0817"Invoking the Graphical User InterfaceIDREF="ch064"Instrumenting an Executableinstrumentation fileIDREF="ch0516"Single Test Analysis ProcessIDREF="ch0821"Invoking the Graphical User Interface"List Argument Traces" andIDREF="ch0973"Queries Menu OperationsBOUNDSIDREF="ch0523"Single Test Analysis ProcessCONSTRAINIDREF="ch0519"Single Test Analysis ProcessCOUNTSIDREF="ch0517"Single Test Analysis ProcessdefaultIDREF="ch0525"Single Test Analysis ProcessEXCLUDEIDREF="ch0518"Single Test Analysis ProcessINCLUDEIDREF="ch0518"Single Test Analysis ProcessMAXIDREF="ch0521"Single Test Analysis ProcessMINIDREF="ch0522"Single Test Analysis ProcessRETURNIDREF="ch0524"Single Test Analysis ProcessTRACEIDREF="ch0520"Single Test Analysis ProcessKeep Performance Data buttonIDREF="ch0831"Invoking the Graphical User InterfaceLaunch Tool submenuIDREF="ch0988"Admin Menu Operationsleak experimentsIDREF="ch0443"Conducting Memory Leak ExperimentsLeak ViewIDREF="ch0444"Using Malloc Error View, Leak View, and Malloc View-listIDREF="ch077"Common cvcov OptionslsarcIDREF="ch0735"Coverage Analysis CommandsIDREF="ch0534"Single Test Analysis ProcessexampleIDREF="ch0736"Coverage Analysis CommandslsblockIDREF="ch0535"Single Test Analysis ProcessIDREF="ch0731"Coverage Analysis CommandsexampleIDREF="ch0732"Coverage Analysis CommandslsbranchIDREF="ch0733"Coverage Analysis CommandsIDREF="ch0536"Single Test Analysis ProcessexampleIDREF="ch0734"Coverage Analysis CommandslscallIDREF="ch0540"Single Test Analysis ProcessIDREF="ch0737"Coverage Analysis CommandsexampleIDREF="ch0738"Coverage Analysis CommandslsfunIDREF="ch0729"Coverage Analysis CommandsIDREF="ch0537"Single Test Analysis ProcessexampleIDREF="ch0612"Instrumenting an ExecutableIDREF="ch0730"Coverage Analysis CommandslsinstrIDREF="ch0719"General Test CommandsexampleIDREF="ch0720"General Test CommandslslineIDREF="ch0739"Coverage Analysis CommandsexampleIDREF="ch0739"Coverage Analysis CommandslssourceIDREF="ch0740"Coverage Analysis CommandsIDREF="ch0541"Single Test Analysis ProcessexampleIDREF="ch0613"Instrumenting an ExecutableIDREF="ch0741"Coverage Analysis CommandslssumIDREF="ch0727"Coverage Analysis CommandsIDREF="ch0538"Single Test Analysis ProcessexampleIDREF="ch0728"Coverage Analysis CommandsIDREF="ch0611"Instrumenting an ExecutableIDREF="ch0836"Invoking the Graphical User InterfacelstestIDREF="ch0721"General Test CommandslstraceIDREF="ch0539"Single Test Analysis ProcessIDREF="ch0742"Coverage Analysis CommandsexampleIDREF="ch0743"Coverage Analysis Commandsmain Tester windowIDREF="ch0816"Invoking the Graphical User InterfaceIDREF="ch086"Invoking the Graphical User InterfaceIDREF="ch086"Invoking the Graphical User InterfaceIDREF="ch0816"Invoking the Graphical User Interfacegraphical overviewIDREF="ch093"Main Window and Menus menusIDREF="ch0925"Main Window MenusMake SourceIDREF="ch0433"Filtering Nodes through the Display ControlsMake TargetIDREF="ch0434"Filtering Nodes through the Display Controlsmalloc/free tracingIDREF="ch044"Malloc/Free TracingMAXIDREF="ch0521"Single Test Analysis ProcessexampleIDREF="ch0818"Invoking the Graphical User InterfaceIDREF="ch065"Instrumenting an ExecutableIDREF="ch0821"Invoking the Graphical User Interfacememory leak experimentsIDREF="ch0443"Conducting Memory Leak Experimentsmemory leakageIDREF="ch014"Memory Leakagememory problemsIDREF="ch013"Malloc Error View, Leak View, Malloc View, and Heap ViewMINIDREF="ch0522"Single Test Analysis ProcessmktestIDREF="ch0722"General Test CommandsIDREF="ch0529"Single Test Analysis ProcessexampleIDREF="ch068"Instrumenting an ExecutableIDREF="ch0823"Invoking the Graphical User InterfaceIDREF="ch0723"General Test CommandsmktgroupIDREF="ch0755"Test Group CommandsmktsetIDREF="ch0748"Test Set CommandsMultiple ArcsexampleIDREF="ch0852"Tutorial #3 name='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]' Exploring the Graphical User InterfaceiconIDREF="ch0848"Tutorial #3 name='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]' Exploring the Graphical User Interfacemultiple testsIDREF="ch0544"Additional Coverage TestingIDREF="ch058"Multiple Testsmv, not using with cvcovIDREF="ch0756"Test Group CommandsIDREF="ch0751"Test Set CommandsNext Page buttonIDREF="ch0921"Control Area ButtonsNo Arc DataIDREF="ch0833"Invoking the Graphical User InterfaceNot Taken columnIDREF="ch0968"Queries Menu OperationsObject field, test group andIDREF="ch0953"Queries Menu OperationsObject List button, test group andIDREF="ch0954"Queries Menu OperationsOverview buttonIDREF="ch0847"Tutorial #3 name='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]' Exploring the Graphical User InterfaceIDREF="ch0439"Geometric Manipulation through the Control Panelpage fault tracingIDREF="ch046"Page Fault TracingPage faults stripchartIDREF="ch0418"Charts in Usage View (Graphs) -patIDREF="ch0715"Common cvcov Optionsperformance analysis theoryIDREF="ch011"Sources of Performance ProblemsPerformance AnalyzerexperimentsIDREF="ch031"Experiment Setup OverviewPerformance Analyzer tasksIDREF="ch034"Understanding Predefined TasksPerformance Analyzer tutorialIDREF="ch021"Performance Analyzer TutorialPerformance PanelIDREF="ch032"Selecting a Performance TaskPoll and I/O Calls stripchartIDREF="ch0422"Charts in Usage View (Graphs) poll system callsIDREF="ch0423"Charts in Usage View (Graphs) pollpoint samplingIDREF="ch048"Pollpoint Sampling-prettyIDREF="ch0710"Common cvcov OptionsPrevious Page buttonIDREF="ch0920"Control Area ButtonsProcess MeterIDREF="ch0426"Process MeterProcess size stripchartIDREF="ch0425"Charts in Usage View (Graphs) Queries menuIDREF="ch0955"Queries Menu OperationsIDREF="ch0947"Queries Menu Operations introductionIDREF="ch0542"Single Test Analysis ProcessQuery SizeIDREF="ch098"Coverage Display AreaQuery TypeIDREF="ch097"Coverage Display Areaquery-specific fieldsIDREF="ch0948"Queries Menu Operations-rIDREF="ch078"Common cvcov OptionsRead/write data size stripchartIDREF="ch0420"Charts in Usage View (Graphs) Read/Write System Calls stripchartIDREF="ch0421"Charts in Usage View (Graphs) realign buttonIDREF="ch0440"Geometric Manipulation through the Control PanelIDREF="ch0849"Tutorial #3 name='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]' Exploring the Graphical User InterfaceRecursive List button"Delete Test" andIDREF="ch0940"Test Menu Operations"Describe Test" andIDREF="ch0978"Queries Menu OperationsRemove buttonIDREF="ch0935"Test Menu OperationsRemove Subtest ExptIDREF="ch0834"Invoking the Graphical User Interfaceresource usage dataIDREF="ch012"Resource Usage Graphsresource, cvsourceNoShareIDREF="ch0913"Control Area Buttonsresults directoryIDREF="ch0531"Single Test Analysis ProcessRETURNIDREF="ch0524"Single Test Analysis ProcessrmtestIDREF="ch0724"General Test Commandsrotate buttonIDREF="ch0441"Geometric Manipulation through the Control PanelIDREF="ch0850"Tutorial #3 name='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]' Exploring the Graphical User InterfaceruninstrIDREF="ch0725"General Test CommandsIDREF="ch0526"Single Test Analysis ProcessexampleIDREF="ch066"Instrumenting an ExecutableruntestIDREF="ch0726"General Test CommandsIDREF="ch0532"Single Test Analysis ProcessexampleIDREF="ch0610"Instrumenting an ExecutableIDREF="ch0827"Invoking the Graphical User Interfacesample trapsIDREF="ch033"Setting Sample TrapsScale menuIDREF="ch0428"Process MeterSearch fieldIDREF="ch0411"Function List Display and ControlsIDREF="ch099"Search Field"List Functions" andIDREF="ch0961"Queries Menu OperationsSelectIDREF="ch0936"Test Menu Operationsselect system callsIDREF="ch0424"Charts in Usage View (Graphs) setting up the tutorialIDREF="ch083"Setting Up the TutorialIDREF="ch063"Setting Up the Tutorialssharing Source View with applicationsIDREF="ch0912"Control Area ButtonsShow Function Arguments buttonIDREF="ch0987"Admin Menu OperationsShow Node buttonIDREF="ch0413"Function List Display and Controls-sortIDREF="ch0711"Common cvcov Optionssort menuIDREF="ch0922"Control Area ButtonsIDREF="ch0814"Invoking the Graphical User Interface"List Functions" andIDREF="ch0960"Queries Menu OperationsSource buttonIDREF="ch089"Invoking the Graphical User InterfaceIDREF="ch0414"Function List Display and ControlsSource ViewIDREF="ch0811"Invoking the Graphical User InterfacewidthIDREF="ch0915"Control Area ButtonsSource View with leak annotationsIDREF="ch0445"Using Malloc Error View, Leak View, and Malloc Viewstarting Tester main windowIDREF="ch085"Invoking the Graphical User Interfacestatus areaIDREF="ch0815"Invoking the Graphical User InterfaceIDREF="ch0923"Status Area and Query-Specific Fieldssystem call tracingIDREF="ch045"System Call TracingTaken columnIDREF="ch0967"Queries Menu Operationstarget directoryIDREF="ch049"Specifying the Experiment DirectoryTarget List dialog boxIDREF="ch0951"Queries Menu OperationsTargetsIDREF="ch0937"Test Menu OperationsTDFIDREF="ch0530"Single Test Analysis ProcessexampleIDREF="ch069"Instrumenting an Executabletest componentsIDREF="ch0510"Test Componentstest description fileIDREF="ch0530"Single Test Analysis ProcessexampleIDREF="ch069"Instrumenting an Executabletest directoryIDREF="ch0528"Single Test Analysis Processtest groupcommandsIDREF="ch0753"Test Group CommandsTest Include ListIDREF="ch0934"Test Menu OperationsTest ListIDREF="ch0933"Test Menu OperationsTest menuIDREF="ch0926"Test Menu OperationsTest Name fieldIDREF="ch087"Invoking the Graphical User InterfaceIDREF="ch094"Test Name Input Fieldtest setIDREF="ch058"Multiple TestsIDREF="ch0544"Additional Coverage TestingIDREF="ch0841"Tutorial #2 name='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]' Analyzing a Test SetIDREF="ch0747"Test Set Commands makingIDREF="ch0932"Test Menu Operationstest0000IDREF="ch0528"Single Test Analysis Processtesting procedureIDREF="ch0515"Single Test Analysis Processtests, Contribution button andIDREF="ch0813"Invoking the Graphical User InterfaceTRACEIDREF="ch0520"Single Test Analysis Process"List Argument Traces" andIDREF="ch0974"Queries Menu OperationsexampleIDREF="ch0821"Invoking the Graphical User InterfaceIDREF="ch065"Instrumenting an ExecutableIDREF="ch0818"Invoking the Graphical User Interfacetracing dataIDREF="ch043"Specifying Tracing Datatutorialcommand line interfaceIDREF="ch061"Tester Command Line Interface Tutorialgraphical user interfaceIDREF="ch081"Setting Up the Tutorial set upIDREF="ch083"Setting Up the TutorialIDREF="ch063"Setting Up the Tutorialsset upIDREF="ch082"Setting Up the TutorialType fieldIDREF="ch095"Test Name Input Fieldunmatched freesIDREF="ch016"Bad Freesusage modelIDREF="ch0514"Usage ModelUser vs system time stripchartIDREF="ch0417"Charts in Usage View (Graphs) -vIDREF="ch072"Common cvcov Options-verIDREF="ch071"Common cvcov Optionsver##0IDREF="ch0527"Single Test Analysis ProcessexampleIDREF="ch067"Instrumenting an ExecutableVersion Number field"Run Executable" andIDREF="ch0822"Invoking the Graphical User Interface"Run Instrumentation" andIDREF="ch0928"Test Menu Operations"Run Test" andIDREF="ch0829"Invoking the Graphical User InterfaceViews menuIDREF="ch0943"Views Menu Operationsworking set analysisIDREF="ch0449"Analyzing Working SetsWorking Set ViewIDREF="ch017"Working Set ViewIDREF="ch0450"Working Set ViewWorkShopIDREF="ch0989"Admin Menu OperationsZoom InIDREF="ch0846"Tutorial #3 name='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]' Exploring the Graphical User InterfaceIDREF="ch0438"Geometric Manipulation through the Control PanelZoom menuIDREF="ch0436"Geometric Manipulation through the Control PanelIDREF="ch0844"Tutorial #3 name='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]' Exploring the Graphical User InterfaceZoom OutIDREF="ch0845"Tutorial #3 name='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]' Exploring the Graphical User InterfaceIDREF="ch0437"Geometric Manipulation through the Control Panel