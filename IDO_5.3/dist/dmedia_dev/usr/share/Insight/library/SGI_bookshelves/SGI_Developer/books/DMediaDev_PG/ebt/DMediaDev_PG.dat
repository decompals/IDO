#EDIR DATA#
LANG=CIRIS Digital MediaProgramming GuideDocument Number 008-1799-040CONTRIBUTORSWritten by Patricia Creek; Part III written by Carolyn CurtisIllustrated by Patricia Creek, Dany Galgani, Cheri Brown, David Bertrand,and Dan YoungEdited by Nancy Schweiger and Christina CaryProduction by Derrald Vogt and Chris EverettEngineering contributions by John Barco, Brian Beach, Don Bennett, David Bertrand, Mark Callow, Wiltse Carpenter, Andrew Cherenson, Doug Cook, Jonathan Devine, Grant Dorman, Dan Fink, Ron Fischer, Jeff Glover, Brian Hill, Bryan James, Bruce Karsh, Robert Keller, Eva Manolis, Ted Marsh, Spencer Murray, Paul Ning, Candace Obert, Gordon Oliver, Chris Pirazzi, Scott Porter, Mike Portuesi, Scott Pritchett, Amit Shoham, Paul Spencer, Dave Story, Archer Sully, Ann Sydeman, Alex Tang, Mike Travis, I-Ching Wang, Jim Wanslow, and Jim Wiggins.© Copyright 1994, Silicon Graphics, Inc.name='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]' All Rights ReservedThis document contains proprietary and confidential information of Silicon Graphics, Inc. The contents of this document may not be disclosed to third parties, copied, or duplicated in any form, in whole or in part, without the prior written permission of Silicon Graphics, Inc.RESTRICTED RIGHTS LEGENDUse, duplication, or disclosure of the technical data contained in this document by the Government is subject to restrictions as set forth in subdivision (c) (1) (ii) of the Rights in Technical Data and Computer Software clause at DFARS 52.227-7013 and/or in similar or successor clauses in the FAR, or in the DOD or NASA FAR Supplement. Unpublished rights reserved under the Copyright Laws of the United States. Contractor/manufacturer is Silicon Graphics, Inc., 2011 N. Shoreline Blvd., Mountain View, CA 94039-7311.Silicon Graphics, Indigo, IRIS, and the Silicon Graphics logo are registered trademarks and CHALLENGE, Cosmo Compress, Galileo Video, GL, Graphics Library, Image Vision Library, IndigoVideo, Indigo2, Indigo2 Video, Indy, Indy Cam, Indy Video, IRIS GL, IRIS Graphics Library, IRIS Indigo, IRIS InSight, IRIX, OpenGL, Personal IRIS, Sirius Video, Showcase, and VINO are trademarks of Silicon Graphics, Inc. Aware and the Aware logo are registered trademarks and AudioPlayback, AudioProducer, AudioPublisher, AudioSuite, Archiver, Audition, BrowsFX, MultiRate, Psycoder, and Speed-of-Sound are trademarks of Aware, Inc. Betacam and Sony are registered trademarks and Hi-8mm is a trademark of Sony Corporation. Macintosh is a registered trademark and AppleTalk and QuickTime are trademarks of Apple Computer, Inc. MII is a trademark of Panasonic, Inc. Network License System and NetLS are trademarks of Apollo Computer Inc., a subsidiary of Hewlett-Packard Company. Prosonus is a registered trademark of Prosonus. MIPS and R3000 are registered trademarks of MIPS Technologies, Inc. Open Software Foundation is a registered trademark and OSF/Motif is a trademark of the Open Systems Foundation. S-VHS is a trademark of JVC, Inc. UNIX is a trademark of AT&T Bell Labs. X Window System is a trademark of Massachusetts Institute of Technology. About This GuideThe IRIS® Digital Media Programming Guide describes the Silicon Graphics® IRIS Digital Media Development Environment software. The IRIS Digital Media Development Environment (DMdev) provides an application programming interface (API) for working with digital audio, MIDI, video, compression, and movies, using standard and optional Silicon Graphics workstation hardware and peripherals.Silicon Graphics also supplies desktop media tools for end users, which are built on top of DMdev. Media tools are described in the online Media Control Panels User's Guide, which you can view from the IRIS InSightÔ viewer.LBL="" HELPID=""What This Guide ContainsThe IRIS Digital Media Programming Guide is divided into six parts, corresponding to the functions of the libraries:IDREF="58645" TYPE="TITLE"Part I, "Digital Media Programming," has two chapters:IDREF="25929" TYPE="TITLE"Chapter 1, "Programming with the IRIS Digital Media Development Environment," gives an overview of the IRIS Digital Media Development Environment.IDREF="46771" TYPE="TITLE"Chapter 2, "Programming with the Digital Media Library," describes the Digital Media (DM) Library, libdmedia, a library that currently supports parameter setting and ring buffering for applications that use the DMdev. Currently, you can use the DM routines with the Movie Library and the Video Library.IDREF="37303" TYPE="TITLE"Part II, "Digital Audio and MIDI Programming," has eight chapters;IDREF="99576" TYPE="TITLE"Chapter 3, "Introduction to Digital Audio and MIDI Programming," introduces the digital audio and MIDI libraries.IDREF="75628" TYPE="TITLE"Chapter 4, "Digital Audio System Architecture," gives a brief overview of the audio hardware and provides some recommendations for development configurations.IDREF="81209" TYPE="TITLE"Chapter 5, "Digital Audio System Software," describes the audio application programming environment and explains how audio resources are shared.IDREF="70508" TYPE="TITLE"Chapter 6, "Programming with the Audio Library," describes the structure of the Audio Library and explains how to use it to sample audio data from analog or digital input sources. Real-time programming techniques are also discussed.IDREF="45737" TYPE="TITLE"Chapter 7, "Programming with the Audio File Library," describes the structure of the Audio File Library and explains how to use it to read and write audio files.IDREF="41223" TYPE="TITLE"Chapter 8, "Programming with the CD Audio Library," describes the CD Audio Library and explains how to use it to control the CD-ROM drive for playing and sampling audio from audio compact discs.IDREF="80723" TYPE="TITLE"Chapter 9, "Programming with the DAT Audio Library," describes the DAT Audio Library and explains how to use it to control the DAT drive for playing, sampling, and recording audio from digital audio tape.IDREF="99417" TYPE="TITLE"Chapter 10, "Programming with the MIDI Library," describes connecting MIDI equipment and describes the MIDI Library, explaining how to use it for implementing and multiplexing MIDI I/O, and synchronizing MIDI and audio.IDREF="13297" TYPE="TITLE"Part III, "Video Programming," has five chapters:IDREF="17607" TYPE="TITLE"Chapter 11, "Video Basics," explains basic video concepts that apply to both the Video Library and the IndigoVideo Library.IDREF="68158" TYPE="TITLE"Chapter 12, "Getting Started with the Video Library," describes the Video Library and explains how to use it to perform video input and output for workstations equipped with standard and optional Silicon Graphics video hardware.IDREF="61872" TYPE="TITLE"Chapter 13, "Using VL Controls," describes how to use VL controls to set video parameters for data transfer and video effects.IDREF="92735" TYPE="TITLE"Chapter 14, "VL Event Handling," describes how to handle video events using the Video Library.IDREF="24419" TYPE="TITLE"Part IV, "IndigoVideo Programming," has six chapters:IDREF="58977" TYPE="TITLE"Chapter 16, "Introduction to IndigoVideo Programming," introduces the IndigoVideo library and gives an overview of the features of the IndigoVideo board.IDREF="91930" TYPE="TITLE"Chapter 17, "Getting Started with the IndigoVideo Library," describes basic concepts for using the IndigoVideo board, and presents a sample video application that displays live video input in a window.IDREF="11499" TYPE="TITLE"Chapter 18, "Controlling the IndigoVideo Input Window," explains how applications can position and scale the video input. It also explains how to select different video sources, formats, and broadcast standards.IDREF="50847" TYPE="TITLE"Chapter 19, "Producing IndigoVideo Output," explains how to encode a portion of your screen to video in real time. This chapter also covers single-frame output.IDREF="46637" TYPE="TITLE"Chapter 20, "Capturing Video from IndigoVideo," explains how to capture frames of video to memory.IDREF="61431" TYPE="TITLE"Chapter 21, "Handling IndigoVideo Events," explains how to handle video events, such as video parameters being changed by another process.IDREF="59277" TYPE="TITLE"Chapter 22, "Using the IndigoVideo Utilities," explains how to use the IndigoVideo end-user tools.IDREF="22118" TYPE="TITLE"Part V, "Compression Programming," has four chapters:IDREF="36492" TYPE="TITLE"Chapter 23, "Introduction to the Compression Library," introduces the CL and describes its applications and features. It provides basic background information on compression technology and on digital audio and video data formats.IDREF="44980" TYPE="TITLE"Chapter 24, "Getting Started with the Compression Library," describes how to use the three types of interfaces supplied by the CL and how to write programs for Cosmo Compress option.IDREF="95875" TYPE="TITLE"Chapter 25, "Using Compression Library Algorithms and Parameters," explains how to use the CL algorithms and global parameters.IDREF="63275" TYPE="TITLE"Chapter 26, "Customizing the Compression Library," explains how to add your own algorithms and parameters to the CL.IDREF="57019" TYPE="TITLE"Part VI, "Movie Programming," has six chapters:IDREF="36899" TYPE="TITLE"Chapter 27, "Introduction to the Movie Library," introduces the Movie Library and describes its applications and features.IDREF="23635" TYPE="TITLE"Chapter 28, "Getting Started with the Movie Library," explains how to set up, compile, and debug Movie Library applications.IDREF="61830" TYPE="TITLE"Chapter 29, "File I/O and Editing Movies with the Movie Library," explains how to perform movie file I/O and how to edit movies.IDREF="30953" TYPE="TITLE"Chapter 30, "Playing Movies with the Movie Library," describes the Movie Library playback and event-handling facilities.IDREF="12291" TYPE="TITLE"Chapter 31, "Using the Movie Library with QuickTime Movies," describes basic concepts for working with QuickTime movies, and then it explains how to add QuickTime capability to a Movie Library application. It also describes the optional QuickTime compressor Library, which provides access to QuickTime compressors for Movie Library applications.IDREF="98659" TYPE="TITLE"Chapter 32, "Using the Movie Library Sample Programs," describes the Movie Library sample programs.Appendices at the back of this guide provide additional information:IDREF="91848" TYPE="TITLE"Appendix A, "Audio Specifications," lists relevant audio and video hardware specifications.IDREF="63476" TYPE="TITLE"Appendix B, "Aware Scalable Audio Compression Software," explains how to incorporate into your application the built-in licensable compression software by Aware®, Inc.The Glossary at the end of this guide provides definitions for video terms.LBL="" HELPID=""How to Use This GuideThis guide is written for C language programmers. This guide assumes that you are somewhat knowledgable about digital media concepts. The discussion of each library begins by presenting the features, applications, and basic concepts pertaining to that library. Readers unfamiliar with the basic concepts can refer to the recommended references for each topic.LBL="" HELPID=""Where to StartIf you're not sure which library to use for a certain application, read IDREF="25929" TYPE="TITLE"Chapter 1, "Programming with the IRIS Digital Media Development Environment," to get a brief overview of the uses and features of each library.If you want to find some code that does what you want your application to do, browse through the List of Examples to locate a code fragment or a sample program that performs a particular task.LBL="" HELPID=""Style ConventionsThese style conventions are used in this guide:Boldfunctions, routinesItalicsarguments, variables, commands, program and file names, book titles, and emphasisCourierfunction prototypes, sample codeCourier Bolduser input entered from the keyboardLBL="" HELPID=""How to Use the Sample ProgramsCode fragments and complete sample programs are used throughout this guide to demonstrate programming concepts. Source code for the sample programs is provided in the /usr/people/4Dgifts/examples/dmedia directory, which is further organized in directories according to topic. For example, Movie Library programs are in /usr/people/4Dgifts/examples/dmedia/movie.You must log in as 4Dgifts to be able to compile 4Dgifts programs. README files in each 4Dgifts directory provide descriptions of the sample programs and instructions for compiling and running them. You must have the IRIS Development Option, dev, and the C language software, c, loaded before you can compile the sample programs. Use the versions command to find out which software is loaded on your system. See the release notes for each library for additional system software requirements for those libraries.You should copy any 4Dgifts program that you intend to modify to your home directory before making any changes.LBL="" HELPID=""Suggestions for Further ReadingThis section lists references containing information on programming topics beyond the scope of this guide, which you may find helpful for developing your digital media application. Additional reference materials are listed in the introductory chapters for each library.LBL="" HELPID=""References for Using Digital Media with Other LibrariesIf you are planning to integrate your digital media application with calls from the OpenGLID="Media0-5ATB1"Ô, IRIS Graphics LibraryÔ (GL) or X Window SystemÔ application, you may want to consult the following manuals:OpenGL Programming Guide and OpenGL Reference Guide, by Jackie Neider, Tom Davis, and Mason Woo, Addison-Wesley, 1993Graphics Library Programming Guide, by Patricia McLendon Creek, Silicon Graphics, 1992Graphics Library Programming Tools and Techniques, by Patricia McLendon Creek and Ken Jones, Silicon Graphics, 1993IRIS IM Programming Notes, by Patricia McLendon Creek and Ken Jones, Silicon Graphics, 1993The X Window System, Volume 1: Xlib Programming Manual, O'Reilly and Associates, 1990The X Window System, Volume 4: Xt Intrinsics, Motif Edition, O'Reilly and Associates, 1990X Window System: The Complete Reference to Xlib, X Protocol, ICCCM, XLFD, Third Edition, by Robert W. Scheifler and James Gettys, Digital Press, 1992X Window System Toolkit: The Complete Programmer's Guide and Specification, Paul J. Asente and Ralph R. Swick, Digital Press, 1992LBL="" HELPID=""References for Adding a User Interface to Your ProgramThe IRIS Digital Media don't impose any particular user interface (UI), so you can use any graphical UI toolkit, such as IRIS IMÔ to build your interface. IRIS IM is Silicon Graphics' port of the industry-standard OSF/MotifID="Media0-5ATB2"Ô software. Consult these OSF/Motif manuals for more information:OSF/Motif User's Guide, Revision 1.2, Prentice-Hall, 1993OSF/Motif Programmer's Reference, Revision 1.2, Prentice-Hall, 1992OSF/Motif Style Guide, Revision 1.2, Prentice-Hall, 1992LBL="I"ID="58645"Digital Media ProgrammingIDREF="25929" TYPE="TITLE"Chapter 1, "Programming with the IRIS Digital Media Development Environment," gives an overview of the IRIS Media Libraries.IDREF="46771" TYPE="TITLE"Chapter 2, "Programming with the Digital Media Library,"describes the Digital Media (DM) Library, libdmedia, a library that currently supports parameter setting and ring buffering for applications that use the IRIS digitial media libraries. Currently, you can use the DM routines with the Movie Library and the Video Library.LBL="1"ID="25929"Programming with the IRIS Digital Media Development EnvironmentThe IRIS Digital Media Development Environment provides a digital media software development environment that includes audio, video, movie, and compression libraries. ID="Media1-1ML1"This chapter provides an overview of the uses and features of these libraries:Digital Media Library, a base library that provides global type definitions and utility routines for digital media applications; it currently supports parameter setting and ring bufferingDigital Audio and MIDI Libraries, a collection of libraries that provides an API for working with digital audio, audio files, digital compact disc, digital audio tape, and MIDIVideo Library, a device-independent API for programming Silicon Graphics on-board video and video optionsIndigo Video Library, an API for programming the IndigoVideo option for IRIS Indigo workstations equipped with Entry graphicsCompression Library, an extensible, algorithm-independent API for compressing and decompressing audio, video, and imagesMovie Library, a file-format-independent API for reading, writing, playing, and editing moviesYou can use these libraries in conjunction with other Silicon Graphics libraries, such as the ImageVisionÔ Library; see the individual library descriptions to learn which libraries are compatible. LBL="" HELPID=""About the Digital Media LibraryThe Digital Media (DM) Library, ID="Media1-1ML2"ID="Media1-1ML3"libdmedia.so, is a library that currently supports parameter setting and ring buffering for applications that use the IRIS Digital Media software. Currently, you can use the DM routines with the Movie Library and the Video Library.The DM Library features:type definitions for digital mediaroutines for creating and configuring digital media parametersroutines for creating and configuring digital media ring buffersa debugging version of the library that lets you check for proper usageLBL="" HELPID=""About the Digital Audio and MIDI LibrariesSilicon Graphics offers a collection of libraries designed for developers of digital audio and MIDI software, as well as those seeking to integrate audio into their existing applications:ID="Media1-1ML4"Audio Library (libaudio.a)Audio File Library (libaudiofile.so)Audio Utility Library (libaudioutil.so)CD Audio Library (libcdaudio.a)DAT Audio Library (libdataudio.a)MIDI Library (libmd.so)The digital audio libraries can be used separately or in combination. Each library is tailored to a particular set of tasks, as follows:COLUMNS="2"LEFT="0" WIDTH="97"Audio LibraryID="Media1-1ML5"LEFT="105" WIDTH="235"Provides an API for configuring the audio system, 
managing audio I/O between the application 
program and the audio hardware, specifying 
attributes of digital audio data, and facilitating real-
time programming. See IDREF="70508" TYPE="TITLE"Chapter 6, "Programming 
with the Audio Library."LEFT="0" WIDTH="97"Audio File LibraryID="Media1-1ML6"LEFT="105" WIDTH="235"Provides an API for reading and writing two 
standard digital audio file formats, AIFF and 
AIFF­C. See IDREF="45737" TYPE="TITLE"Chapter 7, "Programming with the 
Audio File Library."LEFT="0" WIDTH="97"Audio Utility 
LibraryID="Media1-1ML7"LEFT="105" WIDTH="235"Provides convenience routines for creating and 
configuring Audio File Library data structures. LEFT="0" WIDTH="97"CD Audio LibraryID="Media1-1ML8"LEFT="105" WIDTH="235"Provides an API for optional Silicon Graphics SCSI 
CD-ROM drives. The drive features a special mode 
that allows it to read audio CD format as well as 
CD­ROM format. See IDREF="41223" TYPE="TITLE"Chapter 8, "Programming 
with the CD Audio Library."LEFT="0" WIDTH="97"DAT Audio LibraryID="Media1-1ML9"LEFT="105" WIDTH="235"Provides an API for optional Silicon Graphics SCSI 
DAT drives. See IDREF="80723" TYPE="TITLE"Chapter 9, "Programming with the 
DAT Audio Library."LBL="" HELPID=""About the Video LibraryThe Video Library (VL) is a collection of device-independent C language calls for Silicon Graphics workstations equipped with video options, such as Sirius VideoÔ, IndigoID="Media1-1ML10"ID="Media1-1ML11"ID="Media1-1ML12"ID="Media1-1ML13"2 VideoÔ, Indy VideoÔ, or Galileo Video Ô, or workstations equipped with on-board video, such as IndyÔ. ID="Media1-1ML14"ID="Media1-1ML15"The VL provides generic video tools, including simple tools for importing and exporting digital data to and from current and future Silicon Graphics video hardware, as well as to and from third-party video devices that adhere to the Silicon Graphics architectural model for video devices. Video tools are described in the Media Control Panels User's Guide, which you can view using the IRIS InSight viewer; similar applications are supplied in source-code form as examples in the 4Dgifts directory (/usr/people/4Dgifts/examples/dmedia/video/vl). The VL provides an API that enables applications to:ID="Media1-1ML16"perform video teleconferencing on platforms that support itID="Media1-1ML17"blend computer graphics with frames from videotape or any video sourcepresent video in a window on the workstation screendigitize video dataNoteThe range of VL capabilities you can use depends on the capabilities of your workstation and the video options installed in it.LBL="" HELPID=""About the IndigoVideo LibraryThe IndigoVideo board provides video input and output for IRIS Indigo workstations equipped with Entry graphics. The IndigoVideo Library provides a software interface to the IndigoVideo board, enabling applications to:ID="Media1-1ML18"display live video in a windowcapture live video to system memoryencode graphics to video in real timeproduce high-quality single-frame video outputLBL="" HELPID=""About the Compression LibraryThe Compression LibraryID="Media1-1ML19"ID="Media1-1ML20", libcl.so, provides a flexible, extensible, and algorithm-independent software interface for compressing and decompressing audio, video, and image data.The Compression Library features:algorithm independencehardware independencesupport of industry standard algorithmssupport of Silicon Graphics proprietary algorithmsbinary compatibility across Silicon Graphics platformsThe Compression Library provides facilities for working with audio, still images, sequential frames of data (movies), and a buffering mechanism for nonsequential compression and decompression.You can query the Compression Library for the available algorithms, and you can add your own algorithms and parameters. A pass-through capability allows you to pass data through the routines without using an algorithm.The Compression Library can be used with the Audio File Library, and with data used by the IRIS Movie Player and Movie Maker tools.LBL="" HELPID=""About the Movie LibraryThe Movie Library, ID="Media1-1ML21"ID="Media1-1ML22"libmovie, is a collection of routines that provides a C language API for reading, writing, editing, and playing movies on Silicon Graphics workstations. The API provides a uniform interface to movies of various formats and lets you convert movies from one format to another.The Movie Library features:the ability to read, write, and play movie filesa file-format-independent APIfile format conversion capabilitiessupport for Silicon Graphics Movie format, versions 2.0 and 3.0support for Apple® Computer QuickTimeÔ movie formatdata compression and decompressionasynchronous playback supportflexible playback controlsupport for movies embedded in applications software LBL="2"ID="46771"Programming with the Digital Media LibraryThe Digital Media (DM) Library, ID="Media1-2DM1"libdmedia.so, is a library that provides type definitions for digital media and currently supports parameter setting and ring buffering for applications that use the IRIS digital media libraries.This chapter contains basic concepts for working with the Digital Media Library. It describes the digital media data types and explains how to use the digital media parameters.LBL="" HELPID=""Digital Media Library BasicsIt is not likely that you'll use the DM Library by itself. Typically, you call DM Library routines from an application that is written using one or more of the IRIS digital media libraries. Currently, you can use the DM routines with the Movie Library and the Video Library.ID="Media1-2DM2"ID="Media1-2DM3"The DM Library features:ID="Media1-2DM4"type definitions for digital mediaroutines for creating and configuring digital media parametersroutines for creating and configuring digital media ring buffersa debugging version of the library that lets you check for proper usageLBL="" HELPID=""Digital Media Type DefinitionsThe DM Library provides type definitions for digital media. Data types and constant names have an uppercase DM prefix; routines have a lowercase dm prefix.The ID="Media1-2DM5"dmedia/dmedia.h header file provides these type definitions:ID="Media1-2DM6"ID="Media1-2DM7"DMbooleaninteger for conditionals; DM_FALSE is 0 and DM_TRUE is 1DMfractioninteger numerator divided by integer denominatorDMstatusenumerated type consisting of DM_SUCCESS and DM_FAILUREIDREF="10264" TYPE="TABLE"Table 2-1 lists the digital media parameter type definitions that are defined in ID="Media1-2DM8"ID="Media1-2DM9"dmedia/dm_params.h.COLUMNS="2"LBL="2-1"Table 2-1 ID="10264"Digital Media Parameter TypesLEFT="0" WIDTH="166"Parameter TypeLEFT="175" WIDTH="166"MeaningLEFT="0" WIDTH="166"DM_TYPE_ENUMLEFT="175" WIDTH="166"Enumerated typeLEFT="0" WIDTH="166"DM_TYPE_ENUM_ARRAYLEFT="175" WIDTH="166"Array of enumerated typesLEFT="0" WIDTH="166"DM_TYPE_INTLEFT="175" WIDTH="166"Integer valueLEFT="0" WIDTH="166"DM_TYPE_INT_ARRAYLEFT="175" WIDTH="166"Array of integersLEFT="0" WIDTH="166"DM_TYPE_INT_RANGELEFT="175" WIDTH="166"Range of integersLEFT="0" WIDTH="166"DM_TYPE_STRINGLEFT="175" WIDTH="166"StringLEFT="0" WIDTH="166"DM_TYPE_STRING_ARRAYLEFT="175" WIDTH="166"Array of stringsLEFT="0" WIDTH="166"DM_TYPE_FLOATLEFT="175" WIDTH="166"Floating point valueLEFT="0" WIDTH="166"DM_TYPE_FLOAT_ARRAYLEFT="175" WIDTH="166"Array of floatsLEFT="0" WIDTH="166"DM_TYPE_FLOAT_RANGELEFT="175" WIDTH="166"Range of floatsLEFT="0" WIDTH="166"DM_TYPE_FRACTIONLEFT="175" WIDTH="166"RatioLEFT="0" WIDTH="166"DM_TYPE_FRACTION_ARRAYLEFT="175" WIDTH="166"Array of fractionsLEFT="0" WIDTH="166"DM_TYPE_FRACTION_RANGELEFT="175" WIDTH="166"Range of fractionsLEFT="0" WIDTH="166"DM_TYPE_PARAMSLEFT="175" WIDTH="166"Parameter-value listLEFT="0" WIDTH="166"DM_TYPE_TOC_ENTRYLEFT="175" WIDTH="166"Table-of-contents entry for ring buffersLBL="" HELPID=""Digital Media ParametersParameter-value lists are used to store configuration information for movies, movie tracks, ring buffers, and video paths. A parameter-value list is a list of pairs, where each pair contains the name of a parameter and the corresponding value for that parameter.ID="Media1-2DM10"Typical ways in which you might use a parameter-value list include:passing a parameter-value list to a routine that configures a structure passing a parameter-value list that contains new parameter settings to a routine that changes the settingsusing convenience routines provided by one of the IRIS digital media libraries to set and get parameter values that apply to that libraryEvery parameter-value list that describes a format includes the parameter DM_MEDIUM to indicate what kind of data it describes. DM_MEDIUM is an enumerated type consisting of DM_IMAGE and DM_AUDIO.ID="Media1-2DM11"ID="Media1-2DM12"The routines described in this chapter follow the general rule that ownership of data is not passed during procedure calls, except in the routines that create and destroy parameter-value lists. Functions that take strings copy the strings if they want to keep them. Functions that return strings or other structures retain ownership and the caller must not free them.LBL="" HELPID=""Compiling and Linking a Digital Media Library ApplicationApplications that call DM Library routines must include the ID="Media1-2DM13"libdmedia header files to obtain definitions for the library; however, these files are usually included in the header file of the library you are using.This code fragment includes all the ID="Media1-2DM14"libdmedia header files:#include <dmedia/dmedia.h>
#include <dmedia/dm_audio.h>
#include <dmedia/dm_image.h>
#include <dmedia/dm_params.h>
ID="Media1-2DM15"ID="Media1-2DM16"ID="Media1-2DM17"ID="Media1-2DM18"Link with the DM Library when compiling an application that makes DM Library calls by including ID="Media1-2DM19"ID="Media1-2DM20"-ldmedia on the link line. It's likely that you'll be linking with other libraries as well, and because the linking order is usually specific, follow the linking instructions for the library you are using.LBL="" HELPID=""Debugging a Digital Media Library ApplicationThe debugging version of the DM Library checks for library usage violations by setting assertions that state the requirements for a parameter or value.ID="Media1-2DM21"To debug your DM application, link with the debugging version of the DM Library, libdmedia_d.so, by using ID="Media1-2DM22"-ldmedia_d instead of -ldmedia, and then run your program. Your application will abort with an error message if it fails an assertion. The message explains the situation that caused the error and, by implication or by explicit description, suggests a corrective action.When you have finished debugging your application, you should relink with the nondebugging library, libdmedia.a, because the runtime checks imposed by the debugging library cause an undesirable size and performance overhead for a packaged application.LBL="" HELPID=""Initializing a Digital Media ApplicationThis section explains how to use the DM Library routines for:ID="Media1-2DM23"ID="Media1-2DM24"creating and destroying parameter-value listscreating default audio and image configurationssetting and getting values in parameter-value listsmanipulating parameter-value listsIn the initialization section of your application, you create and use parameter-value lists to configure data structures for your application as described in the following steps:Create an empty parameter-value list by calling dmParamsCreate().Set the parameter values by one of the methods listed below:Use a function that sets up a standard configuration for a particular type of data: dmSetImageDefaults() for images, dmSetAudioDefaults() for audio. See IDREF="58937" TYPE="TITLE""Creating Default Audio and Image Configurations" for a description of this method.Use a generic function such as dmParamsSetInt() to set the values of individual parameters within an empty parameter-value list or one that has already been initialized with the standard audio or image configuration. See IDREF="36529" TYPE="TITLE""Setting and Getting Individual Parameter Values" for a description of this method.Use a library function such as mvSetMovieDefaults() to set a group of parameters specific to that library. See IDREF="93494" TYPE="TITLE""Creating a Default Movie Configuration" in Chapter 28 for a discussion of this method.Free the parameter-value list and its contents by calling dmParamsDestroy().These steps are described in detail in the sections that follow.LBL="" HELPID=""Creating and Destroying Parameter-value ListsSome libraries require you to allocate memory for parameter-value lists, but with the DM library, you need not allocate memory for parameter-value lists, because memory management is provided for you by theID="Media1-2DM25"ID="Media1-2DM26" dmParamsCreate() and dmParamsDestroy() routines. These routines work together as a self-contained block within which you create the parameter-value list, set the parameter value(s) and use them, and then destroy the structure, freeing its associated memory.dmParamsCreate() is the only function that can create a parameter-value list, and dmParamsDestroy() is the only function that can free one. This means that parameter-value lists are managed correctly when every call to create one is balanced by a call to destroy one.The creation function can fail because of lack of memory, so it returns an error code. The destructor can never fail.To create an empty parameter-value list, call ID="Media1-2DM27"ID="Media1-2DM28"dmParamsCreate(). Its function prototype is:DMstatus dmParamsCreate ( DMparams** returnNewList )where:returnNewListis a pointer to a handle that is returned by the DM LibraryIf there is sufficient memory to allocate the structure, a pointer to the newly created structure is put into *returnNewList and DM_SUCCESS is returned; otherwise, DM_FAILURE is returned.When you have finished using the parameter-value list, you must destroy it to free the associated memory. To free both the parameter-value list structure and its contents, call ID="Media1-2DM29"dmParamsDestroy(). Its function prototype is:void dmParamsDestroy ( DMparams* params )where:paramsis a pointer to the parameter-value list you want to destroyID="Media1-2DM30"IDREF="85899" TYPE="TEXT"Example 2-1 is a code fragment that creates a parameter-value list called ID="Media1-2DM31"params, then calls a Movie Library routine, mvSetDefaults(), to initialize the default movie parameters, and then destroys the list, freeing both the structure and its contents.LBL="2-1"Example 2-1 ID="85899"Creating and Destroying a Parameter-value List DMparams* params;
if ( dmParamsCreate( &params ) != DM_SUCCESS ) {
    printf( "Out of memory.\n" );
    exit( 1 );
}
if ( mvSetMovieDefaults(params, MV_FORMAT_SGI_3) != DM_SUCCESS ) {
    printf( "Out of memory.\n" );
    exit( 1 );
}
dmParamsDestroy ( params );LBL="" HELPID=""ID="58937"Creating Default Audio and Image ConfigurationsThere are standard parameters that apply to images (for video and movies) and standard parameters that apply to audio (for movies). This section explains how to use the DM Library convenience routines that initialize parameter-value lists for standard audio and image configurations.ID="Media1-2DM32"ID="Media1-2DM33"LBL="" HELPID=""Audio ParametersAudio uses these parameters:audio channelsaudio compression schemeaudio sample format (e.g., twos-complement binary, floating point)audio sample rateaudio sample width (number of bits per sample)IDREF="26491" TYPE="TABLE"Table 2-2 lists the audio parameters and the valid values for each (not all values are supported by all libraries).ID="Media1-2DM34"COLUMNS="3"LBL="2-2"Table 2-2 ID="26491"Audio ParametersLEFT="0" WIDTH="127"ParameterLEFT="135" WIDTH="71"TypeLEFT="215" WIDTH="197"ValuesLEFT="0" WIDTH="127"DM_AUDIO_CHANNELSLEFT="135" WIDTH="71"IntegerLEFT="215" WIDTH="197"1, 2, or 4LEFT="0" WIDTH="127"DM_AUDIO_COMPRESSIONLEFT="135" WIDTH="71"StringLEFT="215" WIDTH="197"DM_AUDIO_UNCOMPRESSED (default)DM_AUDIO_G711_U_LAWDM_AUDIO_G711_A_LAWDM_AUDIO_MPEGDM_AUDIO_MPEG1DM_AUDIO_MULTIRATEDM_AUDIO_G722LEFT="0" WIDTH="127"DM_AUDIO_FORMATLEFT="135" WIDTH="71"DMaudioformatLEFT="215" WIDTH="197"DM_AUDIO_TWOS_COMPLEMENT(default)DM_AUDIO_UNSIGNEDDM_AUDIO_FLOATDM_AUDIO_DOUBLELEFT="0" WIDTH="127"DM_AUDIO_RATELEFT="135" WIDTH="71"DoubleLEFT="215" WIDTH="197"Native rates are 8000, 11025, 16000, 22050, 32000, 
44100, and 48000 HzLEFT="0" WIDTH="127"DM_AUDIO_WIDTHLEFT="135" WIDTH="71"IntegerLEFT="215" WIDTH="197"8, 16, or 24See IDREF="37303" TYPE="TITLE"Part II, "Digital Audio and MIDI Programming," for complete definitions of the audio parameters.See IDREF="94562" TYPE="TITLE""Setting and Getting Audio Track Properties" in Chapter 28 for a description of audio parameters that apply to Movie Library programs.LBL="" HELPID=""Setting Audio DefaultsTo initialize a parameter-value list with the default audio configuration, call ID="Media1-2DM35"ID="Media1-2DM36"ID="Media1-2DM37"ID="Media1-2DM38"dmSetAudioDefaults(), passing in the desired sample width, sample rate, and number of channels. Its function prototype is:DMstatus dmSetAudioDefaults ( DMparams* params, int width,
                              double rate, int channels )where:paramsis a pointer to a parameter-value list that was returned from dmParamsCreate()widthis the number of bits per audio sample: 8, 16, or 24rateis the audio sample rate; the native audio sample rates are 8000, 11025, 16000, 22050, 32000, 44100, and 48000 Hzchannelsis the number of audio channels: 1, 2, or 4dmSetAudioDefaults() returns DM_SUCCESS if there was enough memory available to set up the parameters; otherwise, it returns DM_FAILURE.IDREF="67914" TYPE="TABLE"Table 2-3 lists the parameters and values set by dmSetAudioDefaults().COLUMNS="2"LBL="2-3"Table 2-3 ID="67914"Audio DefaultsLEFT="0" WIDTH="166"ParameterLEFT="175" WIDTH="166"DefaultLEFT="0" WIDTH="166"DM_MEDIUMLEFT="175" WIDTH="166"DM_AUDIOLEFT="0" WIDTH="166"DM_AUDIO_WIDTHLEFT="175" WIDTH="166"widthLEFT="0" WIDTH="166"DM_AUDIO_FORMATLEFT="175" WIDTH="166"DM_AUDIO_TWOS_COMPLEMENTLEFT="0" WIDTH="166"DM_AUDIO_RATELEFT="175" WIDTH="166"rateLEFT="0" WIDTH="166"DM_AUDIO_CHANNELSLEFT="175" WIDTH="166"channelsLEFT="0" WIDTH="166"DM_AUDIO_COMPRESSIONLEFT="175" WIDTH="166"DM_AUDIO_UNCOMPRESSEDLBL="" HELPID=""Determining the Buffer Size Needed to Store an Audio FrameTo determine the audio frame size for a given parameter-value list, call ID="Media1-2DM39"dmAudioFrameSize(). dmAudioFrameSize() returns the number of bytes needed to store one audio frame (one sample from each channel). Its function prototype is:size_t dmAudioFrameSize ( DMparams* params )IDREF="72833" TYPE="TEXT"Example 2-2 is a code fragment that creates a parameter-value list, fills in the audio defaults, and then frees the structure and its contents.LBL="2-2"Example 2-2 ID="72833"Setting Audio Defaults ID="Media1-2DM40"DMparams* audioParams;
if ( dmParamsCreate( &audioParams ) != DM_SUCCESS ) {
    printf( "Out of memory.\n" );
    exit( 1 );
}
if ( dmSetAudioDefaults ( audioParams,
                          16,    /* width (in bits/sample) */
                          22050, /* sampling rate */
                          2      /* # channels (stereo) */
                          ) != DM_SUCCESS ) {
    printf( "Out of memory.\n" );
    exit( 1 );
}
printf( "%d bytes per audio frame.\n",
         dmAudioFrameSize( audioParams ) );
dmParamsDestroy( audioParams );LBL="" HELPID=""Image ParametersImages use these parameters:image compression schemeimage dimensions (width and height)image interlacing image orientation (top-to-bottom vs. bottom-to-top)image packing formatimage rate (number of frames per second)IDREF="86010" TYPE="TABLE"Table 2-4 lists the image parameters and the valid values for each (not all values are supported by all libraries).ID="Media1-2DM41"COLUMNS="2"LBL="2-4"Table 2-4 ID="86010"Image ParametersLEFT="0" WIDTH="134"ParameterLEFT="140" WIDTH="198"ValuesLEFT="0" WIDTH="134"DM_IMAGE_HEIGHTLEFT="140" WIDTH="198"Integer valueLEFT="0" WIDTH="134"DM_IMAGE_WIDTHLEFT="140" WIDTH="198"Integer valueLEFT="0" WIDTH="134"DM_IMAGE_RATELEFT="140" WIDTH="198"Floating point valueLEFT="0" WIDTH="134"DM_IMAGE_COMPRESSIONLEFT="140" WIDTH="198"DM_IMAGE_UNCOMPRESSEDDM_IMAGE_RLEDM_IMAGE_RLE24DM_IMAGE_JPEGDM_IMAGE_MPEG1DM_IMAGE_MVC1DM_IMAGE_MVC2DM_IMAGE_RTRDM_IMAGE_HDCCDM_IMAGE_QT_VIDEODM_IMAGE_QT_ANIMLEFT="0" WIDTH="134"DM_IMAGE_INTERLACINGLEFT="140" WIDTH="198"DM_IMAGE_NONINTERLACED (full frame)DM_IMAGE_INTERLACED_EVEN (two fields, 
even field first)DM_IMAGE_INTERLACED_ODD (two fields, 
odd field first)DM_IMAGE_NONINTERLEAVED (obsolete, 
use DM_IMAGE_NONINTERLACED instead)DM_IMAGE_INTERLEAVED(obsolete, use 
DM_IMAGE_INTERLACED_ODD instead)LEFT="0" WIDTH="134"DM_IMAGE_ORIENTATIONLEFT="140" WIDTH="198"DM_TOP_TO_BOTTOMDM_BOTTOM_TO_TOPLEFT="0" WIDTH="134"DM_IMAGE_PACKINGLEFT="140" WIDTH="198"DM_PACKING_RGBDM_PACKING_RGBXDM_PACKING_RGBADM_PACKING_RGB332 (Indigo Entry graphics)DM_PACKING_RGB8DM_PACKING_GRAYSCALEDM_PACKING_YUVDM_PACKING_YUV411DM_PACKING_YUV422DM_PACKING_YUV422HCDM_PACKING_APPLE_32DM_PACKING_APPLE_16DM_PACKING_Y (equivalent to          
DM_PACKING_GRAYSCALE)DM_PACKING_YCbCr (equivalent to 
DM_PACKING_YUV)DM_PACKING_YCbCr422 (equivalent to 
DM_PACKING_YUV422)DM_PACKING_YCbCr422HC (equivalent to 
DM_PACKING_YUV422HC)DM_PACKING_YUV422DC (equivalent to 
DM_PACKING_YUV422HC)DM_PACKING_YCbCr422DC (equivalent to 
DM_PACKING_YUV422HC)See IDREF="94575" TYPE="TITLE""Setting and Getting Image Track Properties" in Chapter 28 for a description of image parameters that apply to Movie Library programs. See IDREF="75350" TYPE="TABLE"Table 12-10 in IDREF="75350" TYPE="TITLE"Chapter 12, "Getting Started with the Video Library," for a description of image parameters that apply to Video Library programs.LBL="" HELPID=""Setting Image DefaultsTo initialize a parameter-value list with the default image configuration, call ID="Media1-2DM42"ID="Media1-2DM43"dmSetImageDefaults(), passing in the width and height of the image frame, and the image packing format. Its function prototype is:ID="Media1-2DM44"DMstatus dmSetImageDefaults ( DMparams* params, int width,
                              int height, DMpacking packing )where:paramsis a pointer to a parameter-value list that was returned from ID="Media1-2DM45"dmParamsCreate()widthis the width of the imageheightis the height of the imagepackingis the image packing formatIDREF="43587" TYPE="TABLE"Table 2-5 lists the parameters and values set by dmSetImageDefaults().COLUMNS="2"LBL="2-5"Table 2-5 ID="43587"Image DefaultsLEFT="0" WIDTH="166"ParameterLEFT="175" WIDTH="166"DefaultLEFT="0" WIDTH="166"DM_MEDIUMLEFT="175" WIDTH="166"DM_IMAGELEFT="0" WIDTH="166"DM_IMAGE_WIDTHLEFT="175" WIDTH="166"widthLEFT="0" WIDTH="166"DM_IMAGE_HEIGHTLEFT="175" WIDTH="166"heightLEFT="0" WIDTH="166"DM_IMAGE_RATELEFT="175" WIDTH="166"15.0LEFT="0" WIDTH="166"DM_IMAGE_INTERLACINGLEFT="175" WIDTH="166"DM_IMAGE_NONINTERLACEDLEFT="0" WIDTH="166"DM_IMAGE_PACKINGLEFT="175" WIDTH="166"packingLEFT="0" WIDTH="166"DM_IMAGE_ORIENTATIONLEFT="175" WIDTH="166"DM_BOTTOM_TO_TOPLEFT="0" WIDTH="166"DM_IMAGE_COMPRESSIONLEFT="175" WIDTH="166"DM_IMAGE_UNCOMPRESSEDLBL="" HELPID=""Determining the Buffer Size Needed to Store an Image FrameTo determine the image frame size for a given parameter-value list, call ID="Media1-2DM46"dmImageFrameSize(). dmImageFrameSize() returns the number of bytes needed to store one uncompressed image frame in the given format. Its function prototype is:size_t dmImageFrameSize ( DMparams* params )IDREF="36232" TYPE="TEXT"Example 2-3 is a code fragment that creates a parameter-value list, fills in the image defaults, and then frees the structure and its contents.LBL="2-3"Example 2-3 ID="36232"Setting Image Defaults ID="Media1-2DM47"DMparams* imageParams;
if ( dmParamsCreate( &imageParams ) != DM_SUCCESS ) {
    printf( "Out of memory.\n" );
    exit( 1 );
}
if ( dmSetImageDefaults( imageParams,
       320,  /* width */
       240,  /* height */
       DM_PACKING_RGBX ) != DM_SUCCESS ) {
    printf( "Out of memory.\n" );
    exit( 1 );
}
printf( "%d bytes per image frame.\n",
        dmImageFrameSize( imageParams ) );
dmParamsDestroy( imageParams );LBL="" HELPID=""ID="36529"Setting and Getting Individual Parameter ValuesAfter creating an empty parameter-value list or a default audio or image configuration, you can use the routines described in this section to set and get values for individual elements of a parameter-value list.ID="Media1-2DM48"There is a routine for setting and getting the parameter values for each parameter data type defined in the DM Library, as listed in ID="Media1-2DM49"IDREF="10264" TYPE="TABLE"Table 2-1.All of these functions store and retrieve entries in parameter-value lists. They assume that the named parameter is present and is of the specified type; the debugging version of the library asserts that this is the case. All functions that can possibly fail return an error code indicating success or failure. Insufficient memory is the only reason these routines can fail. IDREF="38942" TYPE="TABLE"Table 2-6 lists the DM Library routines for setting parameter values. These routines require three arguments:paramsa pointer to a parameter-value listparamNamethe name of the parameter whose value you want to setvaluea value of the appropriate type for the given parameterCOLUMNS="2"LBL="2-6"Table 2-6 ID="38942"DM Library Routines for Setting Parameter ValuesLEFT="0" WIDTH="114"RoutineLEFT="120" WIDTH="218"PurposeLEFT="0" WIDTH="114"dmParamsSetInt()LEFT="120" WIDTH="218"Sets the value of a parameter whose type is DMintID="Media1-2DM50"LEFT="0" WIDTH="114"dmParamsSetIntArray()LEFT="120" WIDTH="218"Sets the value of a parameter whose type is 
DMintarrayLEFT="0" WIDTH="114"dmParamsSetIntRange()LEFT="120" WIDTH="218"Sets the value of a parameter whose type is 
DMintrangeLEFT="0" WIDTH="114"dmParamsSetEnum()LEFT="120" WIDTH="218"Sets the value of a parameter whose type is DMenumID="Media1-2DM51"ID="Media1-2DM52"LEFT="0" WIDTH="114"dmParamsSetEnumArray()LEFT="120" WIDTH="218"Sets the value of a parameter whose type is 
DMenumarrayLEFT="0" WIDTH="114"dmParamsSetFloat()LEFT="120" WIDTH="218"Sets the value of a parameter whose type is DMfloatID="Media1-2DM53"LEFT="0" WIDTH="114"dmParamsSetFloatArray()LEFT="120" WIDTH="218"Sets the value of a parameter whose type is 
DMfloatarrayLEFT="0" WIDTH="114"dmParamsSetFloatRange()LEFT="120" WIDTH="218"Sets the value of a parameter whose type is 
DMfloatrangeLEFT="0" WIDTH="114"dmParamsSetFract()LEFT="120" WIDTH="218"Sets the value of a parameter whose type is DMfractID="Media1-2DM54"LEFT="0" WIDTH="114"dmParamsSetFractArray()LEFT="120" WIDTH="218"Sets the value of a parameter whose type is 
DMfractionarrayLEFT="0" WIDTH="114"dmParamsSetFractRange()LEFT="120" WIDTH="218"Sets the value of a parameter whose type is 
DMfractionrangeLEFT="0" WIDTH="114"dmParamsSetParams()LEFT="120" WIDTH="218"Sets the value of a parameter whose type is DMparamID="Media1-2DM55"LEFT="0" WIDTH="114"dmParamsSetString()LEFT="120" WIDTH="218"Sets the value of a parameter whose type is DMstringID="Media1-2DM56"LEFT="0" WIDTH="114"dmParamsSetStringArray()LEFT="120" WIDTH="218"Sets the value of a parameter whose type is 
DMstringarrayLEFT="0" WIDTH="114"dmParamsSetTocEntry()LEFT="120" WIDTH="218"Sets the value of a parameter whose type is 
DMTocEntryID="Media1-2DM57"These routines return either DM_SUCCESS or DM_FAILURE.ID="Media1-2DM58"IDREF="73895" TYPE="TABLE"Table 2-7 lists the DM Library routines for setting parameter values. These routines require two arguments:ID="Media1-2DM59"paramsa pointer to a parameter-value listparamNamethe name of the parameter whose value you want to getRoutines that get values return either a pointer to a value or the value itself. For strings, parameter-value lists, and table-of-contents entries, the pointer that is returned points into the internal data structure of the parameter-value list. This pointer should never be freed and is only guaranteed to remain valid until the next time the list is changed. In general, if you need to keep a string value around after getting it from a parameter-value list, it should be copied.COLUMNS="2"LBL="2-7"Table 2-7 ID="73895"DM Library Routines for Getting Parameter ValuesLEFT="0" WIDTH="116"RoutineLEFT="125" WIDTH="216"PurposeLEFT="0" WIDTH="116" dmParamsGetInt()LEFT="125" WIDTH="216"Returns an integer value for the given parameterID="Media1-2DM60"LEFT="0" WIDTH="116"dmParamsGetIntArray()LEFT="125" WIDTH="216"Returns a pointer to a value of type DMintarray for 
the given parameterLEFT="0" WIDTH="116"dmParamsGetIntRange()LEFT="125" WIDTH="216"Returns a pointer to a value of type DMintrange for 
the given parameterLEFT="0" WIDTH="116" dmParamsGetEnum()LEFT="125" WIDTH="216"Returns an integer value for the given parameterID="Media1-2DM61"LEFT="0" WIDTH="116"dmParamsGetEnumArray()LEFT="125" WIDTH="216"Returns a pointer to a value of type DMenumarray 
for the given parameterLEFT="0" WIDTH="116"dmParamsGetString()LEFT="125" WIDTH="216"Returns a pointer to a value of type const char for the 
given parameterID="Media1-2DM62"LEFT="0" WIDTH="116"dmParamsGetStringArray()LEFT="125" WIDTH="216"Returns a pointer to a value of type DMstringarray 
for the given parameterLEFT="0" WIDTH="116" dmParamsGetFloat()LEFT="125" WIDTH="216"Returns a value of type double for the given 
parameterID="Media1-2DM63"LEFT="0" WIDTH="116"dmParamsGetFloatArray()LEFT="125" WIDTH="216"Returns a pointer to a value of type DMfloatarray for 
the given parameterLEFT="0" WIDTH="116"dmParamsGetFloatRange()LEFT="125" WIDTH="216"Returns a pointer to a value of type DMfloatrange for 
the given parameterLEFT="0" WIDTH="116"dmParamsGetFract()LEFT="125" WIDTH="216"Returns a value of type DMfraction for the given 
parameterID="Media1-2DM64"LEFT="0" WIDTH="116"dmParamsGetFractArray()LEFT="125" WIDTH="216"Returns a pointer to a value of type DMfractionarray 
for the given parameterLEFT="0" WIDTH="116"dmParamsGetFractRange()LEFT="125" WIDTH="216"Returns apointer to a value of type DMfractionrange 
for the given parameterLEFT="0" WIDTH="116"dmParamsGetParams()LEFT="125" WIDTH="216"Returns a pointer to a value of type DMparams for 
the given parameterID="Media1-2DM65"LEFT="0" WIDTH="116"dmParamsGetTocEntry()LEFT="125" WIDTH="216"Returns a value of type DMTocEntry for the given 
parameterID="Media1-2DM66"IDREF="61147" TYPE="TEXT"Example 2-4 shows two equivalent ways of setting up a complete image format description; the first sets the parameter values individually, the second creates a default image configuration with the appropriate values.LBL="2-4"Example 2-4 ID="61147"Setting Individual Parameter ValuesID="Media1-2DM67"DMparams* format;
dmParamsCreate( &format );
dmParamsSetInt ( format, DM_IMAGE_WIDTH, 320 );
dmParamsSetInt ( format, DM_IMAGE_HEIGHT, 240 );
dmParamsSetFloat ( format, DM_IMAGE_RATE, 15.0 );
dmParamsSetString( format, DM_IMAGE_COMPRESSION, DM_IMAGE_UNCOMPRESSED );
dmParamsSetEnum( format, DM_IMAGE_INTERLACING, DM_IMAGE_NONINTERLEAVED );
dmParamsSetEnum ( format, DM_IMAGE_PACKING, DM_PACKING_RGBX );
dmParamsSetEnum ( format, DM_IMAGE_ORIENTATION, DM_BOTTOM_TO_TOP );
dmParamsDestroy ( format );The following is equivalent:DMparams* format;
dmParamsCreate ( &format );
dmSetImageDefaults ( format, 320, 240, DM_PACKING_RGBX );
dmParamsDestroy ( format );LBL="" HELPID=""Manipulating Parameter-value ListsThis section explains how to manipulate parameter-value lists.IDREF="74332" TYPE="TABLE"Table 2-8 lists the routines that perform operations on parameter-value lists and the entries within them.COLUMNS="2"LBL="2-8"Table 2-8 ID="74332"Routines for Manipulating Parameter-value Lists and EntriesLEFT="0" WIDTH="114"RoutineLEFT="120" WIDTH="218"PurposeLEFT="0" WIDTH="114"dmParamsCopyAllElems()LEFT="120" WIDTH="218"Copy the entire contents of one list to anotherLEFT="0" WIDTH="114"dmParamsCopyElem()LEFT="120" WIDTH="218"Copy one parameter-value pair from one list to 
anotherLEFT="0" WIDTH="114"dmParamsGetElem()LEFT="120" WIDTH="218"Get the name of a given parameterLEFT="0" WIDTH="114"dmParamsGetElemType()LEFT="120" WIDTH="218"Get the data type of a given parameterLEFT="0" WIDTH="114"dmParamsGetNumElems()LEFT="120" WIDTH="218"Get the number of parameters in a listLEFT="0" WIDTH="114"dmParamsIsPresent()LEFT="120" WIDTH="218"Determine if a given parameter is in the listLEFT="0" WIDTH="114"dmParamsRemoveElem()LEFT="120" WIDTH="218"Remove a given parameter from the listThe sections that follow explain how to use each routine.LBL="" HELPID=""Determining the Number of Elements in a Parameter-value ListTo perform any task that requires your application to loop through the contents of a parameter-value listname='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'for example, to print out a list of parameters and their valuesname='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'you need to know how many parameters are in the list in order to set up a loop to step through the entries one-by-one.To get the total number of elements present in a parameter-value list, call ID="Media1-2DM68"dmParamsGetNumElems()ID="Media1-2DM69". Its function prototype is:int dmParamsGetNumElems ( DMparams* params )The number of elements and their position in a list is guaranteed to remain stable unless the list is changed by using one of the "set" functions, by copying an element into it, or by removing an element from it.LBL="" HELPID=""Copying the Contents of One Parameter-value List into AnotherTo copy the entire contents of the fromParams list into the toParams list, call dmParamsCopyAllElems()ID="Media1-2DM70". Its function prototype is:DMstatus dmParamsCopyAllElems ( DMparams* fromParams,                                DMparams* toParams )If there are any parameters of the same name in both lists, the corresponding value(s) in the destination list are overwritten. DM_SUCCESS is returned if there is enough memory to hold the copied data; otherwise, DM_FAILURE is returned.ID="Media1-2DM71"LBL="" HELPID=""Copying an Individual Parameter Value from One List into AnotherIf a parameter appears in more than one parameter-value list, it is sometimes more convenient to copy the individual parameter or group of parameters from one list to another, rather than individually setting the parameter value(s) for each list.ID="Media1-2DM72"To copy the parameter-value pair for the parameter named paramName from the fromParams list into the toParams list, call ID="Media1-2DM73"dmParamsCopyElem(). Its function prototype is:DMstatus dmParamsCopyElem ( DMparamsfromParams,
                            const char* paramName,                            DMparams* toParams )If there is a preexisting parameter with the same name in the destination list, that value is overwritten. DM_SUCCESS is returned if there is enough memory to hold the copied element; otherwise, DM_FAILURE is returned.LBL="" HELPID=""Determining the Name of a Given ParameterTo get the name of the entry occupying the position given by ID="Media1-2DM74"index in the params list, call dmParamsGetElem()ID="Media1-2DM75". Its function prototype is:const char* dmParamsGetElem ( DMparams* params, int index )The index must be from 0 to one less than the number of elements in the list.LBL="" HELPID=""Determining the Data Type of a Given ParameterTo get the data type of the value occupying the position given by ID="Media1-2DM76"index in the params list, call ID="Media1-2DM77"dmParamsGetElemType(). Its function prototype is:DMparamtype dmParamsGetElemType ( DMparams* params, int index )See IDREF="10264" TYPE="TABLE"Table 2-1 for a list of valid return values.LBL="" HELPID=""Determining if a Given Parameter ExistsTo determine whether the element named paramName exists in the params list, call ID="Media1-2DM78"dmParamsIsPresent(). Its function prototype is:ID="Media1-2DM79"DMboolean dmParamsIsPresent ( DMparams* params, const char* paramName )DM_TRUE is returned if paramName is in params; otherwise, DM_FALSE is returned.LBL="" HELPID=""Removing an Element from a Parameter-value ListTo remove the ID="Media1-2DM80"paramName entry from the params list, call dmParamsRemoveElem()ID="Media1-2DM81". Its function prototype is:ID="Media1-2DM82"const char* dmParamsRemoveElem( DMparams* params, const char* paramName )The element named paramName must be present.IDREF="77833" TYPE="TEXT"Example 2-5 prints the contents of a parameter-value list.ID="Media1-2DM83"LBL="2-5"Example 2-5 ID="77833"Printing the Contents of a Digital Media Parameter-value Listvoid PrintParams( DMparams* params )
{
    int i;
    int numElems = dmParamsGetNumElems( params );
 
   for ( i = 0;  i < numElems;  i++ ) {
        const char* name = dmParamsGetElem( params, i );
        DMparamtype type = dmParamsGetElemType( params, i );
        printf( "    %20s: ", name );
        switch( type )
            {
            case DM_TYPE_ENUM:
                printf( "%d", dmParamsGetEnum( params, name ) );
                break;
            case DM_TYPE_INT:
                printf( "%d", dmParamsGetInt( params, name ) );
                break;
            case DM_TYPE_STRING:
                printf( "%s", dmParamsGetString( params, name ) );
                break;
            case DM_TYPE_FLOAT:
                printf( "%f", dmParamsGetFloat( params, name ) );
                break;
            case DM_TYPE_FRACTION:
                {
                    DMfraction f = dmParamsGetFract( params, name );
                    printf( "%d/%d", f.numerator, f.denominator );
                }
                break;
            case DM_TYPE_PARAMS:
                printf( "... param list ... " );
                break;
            case DM_TYPE_TOC_ENTRY:
                printf( "... toc entry ..." );
                break;
            default:
                assert( DM_FALSE );
            }
        printf( "\n" );
    }
}LBL="" HELPID=""Synchronizing Digital MediaMost digital media applications use more than one medium in conjunction, for example, audio and video. Handling concurrent media streams requires the ability to process coincident data. This section explains how the data can be related to each other for the various IRIS digital media functions that perform capture and presentation of data. The Digital digital media libraries provide their own temporal reference, called unadjusted system time (UST). The UST is an unsigned 64-bit number that measures the number of nanoseconds since the system was booted. UST values are guaranteed to be monotonically increasing and are readily available for all the IRIS digital media libraries.Typically, the UST is used as a timestamp, that is, it is paired with a specific item or location in a digital media stream. Because each type of media, and similarly each of the libraries, possess unique attributes, the UST information is presented in a different manner in each library. IDREF="61001" TYPE="TABLE"Table 2-9 describes how UST information is provided by each of the libraries.COLUMNS="2"LBL="2-9"Table 2-9 ID="61001"Methods for Obtaining Unadjusted System TimeLEFT="0" WIDTH="126"LibraryLEFT="135" WIDTH="207"UST MethodLEFT="0" WIDTH="126"Digital Media LibraryLEFT="135" WIDTH="207"dmGetUST()LEFT="0" WIDTH="126"Audio LibraryLEFT="135" WIDTH="207"ALgetframenumber() and ALgetframetime()LEFT="0" WIDTH="126"MIDI LibraryLEFT="135" WIDTH="207"mdTell() and mdSetTimestampMode()LEFT="0" WIDTH="126"Video LibraryLEFT="135" WIDTH="207"ustime field in the DMediaInfo structureLEFT="0" WIDTH="126"Compression LibraryLEFT="135" WIDTH="207"ustime field in the CLimageInfo structureThe DM Library routine, dmGetUST(), returns a high-resolution, unsigned 64-bit number to processes using the digital media subsystem. Typically, you use the appropriate routine for the library that handles the type of media being processed, as listed in IDREF="61001" TYPE="TABLE"Table 2-9, rather than dmGetUST(). However, dmGetUST() is useful for correlating UST to system time for events that are not related to a media stream, such as pushing a button or making a network connection.LBL="II"ID="37303"Digital Audio and MIDI ProgrammingIDREF="99576" TYPE="TITLE"Chapter 3, "Introduction to Digital Audio and MIDI Programming,"introduces the digital audio and MIDI libraries.IDREF="75628" TYPE="TITLE"Chapter 4, "Digital Audio System Architecture,"gives a brief overview of the audio hardware and provides some recommendations for development configurations.IDREF="81209" TYPE="TITLE"Chapter 5, "Digital Audio System Software," describes the audio application programming environment and explains how audio resources are shared.IDREF="70508" TYPE="TITLE"Chapter 6, "Programming with the Audio Library,"describes the structure of the Audio Library and explains how to use it to sample audio data from analog or digital input sources. Real-time programming techniques are also discussed.IDREF="45737" TYPE="TITLE"Chapter 7, "Programming with the Audio File Library,"describes the structure of the Audio File Library and explains how to use it to read and write audio files.IDREF="41223" TYPE="TITLE"Chapter 8, "Programming with the CD Audio Library,"describes the CD Audio Library and explains how to use it to control the CD-ROM drive for playing and sampling audio from audio compact discs.IDREF="80723" TYPE="TITLE"Chapter 9, "Programming with the DAT Audio Library,"describes the DAT Audio Library and explains how to use it to control the DAT drive for playing, sampling, and recording audio from digital audio tape.IDREF="99417" TYPE="TITLE"Chapter 10, "Programming with the MIDI Library,"describes connecting MIDI equipment and describes the MIDI Library, explaining how to use it for implementing and multiplexing MIDI I/O, and synchronizing MIDI and audio.LBL="3"ID="99576"Introduction to Digital Audio and MIDI ProgrammingSilicon Graphics offers a collection of libraries designed for developers of digital audio and MIDI software, as well as those seeking to integrate audio into their existing applications. IDREF="37303" TYPE="TITLE"Part II, "Digital Audio and MIDI Programming," describes in detail the application programming interfaces (APIs) for these libraries, which are included in the IRIS Digital Media Development Environment:ID="Media2-1GS1"Audio Library (libaudio.a)Audio File Library (libaudiofile.so)Audio Utility Library (libaudioutil.so)CD Audio Library (libcdaudio.a)DAT Audio Library (libdataudio.a)MIDI Library (libmd.so)Each chapter presents the digital audio and MIDI libraries from a task-oriented perspective. Chapters are organized to cover topics in roughly the order you are concerned about them as you write audio or MIDI programs. To illustrate the use of the various component libraries, sample code fragments and demonstration programs are used throughout.Digital audio programs typically access analog or digitally recorded sound data that is either input directly to the workstation audio hardware or stored on disk, digital audio tape, or CD. The application then manipulates the data and outputs the result to analog or digital line-out jacks, to disk, or to tape. MIDI programs read, process, and produce MIDI data streams, which are in turn interpreted by MIDI devices such as synthesizers and drum machines that are distributed across a MIDI network. The libraries described in this part of this guide provide all the necessary features to create audio and MIDI applications for Silicon Graphics workstations that support audio.Reference documentation on the digital audio and MIDI routines is contained in online reference pages. These provide a concise, thorough description of each library function and are available through the use of the man or Xman command.This guide assumes that you're somewhat familiar with principles of digital audio and MIDI. This section lists additional references that cover background material and topics beyond the scope of this part.ID="Media2-1GS2"ID="Media2-1GS3"ID="Media2-1GS4"Although some background material is provided in the chapters on digital audio and MIDI, you may want to do some more in-depth reading. The following texts may provide useful supplementary information:AES, Journal of the Audio Engineering Society, edited by Daniel R. von Recklinghausen, Audio Engineering Society.The Art of Digital Audio, by John Watkinson, Focal Press, 1988.Computer Music Journal, edited by Steven Travis Pope, MIT Press.Elements of Computer Music, F. Richard Moore, Prentice-Hall, 1990.MIDI Sequencing for Musicians, compiled by the staff of Keyboard Magazine, H. Leonard Publishing Corp., 1989.MIDI Sequencing in C, by Jim Conger, M & T Books, 1989.MIDI 1.0 Detailed Specification and Standard MIDI Files 1.0, International MIDI Association, 5316 W. 57th St., Los Angeles, CA 90056.Music Through MIDI, by Michael Boom, Microsoft Press, 1991.Musical Applications of Microprocessors, by Hal Chamberlin, Hayden Books, 1985.If you plan on using the MIDI C++ classes, you may want to use the following books as references:ID="Media2-1GS5"ID="Media2-1GS6"The Annotated C++ Reference Manual, by Margaret Ellis and Bjarne Stroustrup, AT&T Bell Laboratories, 1990name='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'the official C++ language reference manual.C++ Primer, by Stanley Lippman, AT&T Bell Laboratories, 1989name='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'An introductory-level, tutorial-style presentation of C++.The C++ Programming Guidename='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'an online manual provided with the Silicon Graphics C++ library.LBL="4"ID="75628"Digital Audio System ArchitectureBefore you start to program, you should familiarize yourself with your workstation's audio hardware and the peripherals with which you will be working. This chapter describes the audio capabilities and the audio I/O interfaces available on IRIS Indigo, Indigo2, and Indy workstations. This chapter also provides recommendations for minimal and optimal configurations of memory, hard disk, and other peripherals useful for audio development and testing.See IDREF="91848" TYPE="TITLE"Appendix A, "Audio Specifications," and your workstation owner's guide for complete details on audio hardware features. See the online release notes for audiodev, the audio development environment of the IRIS Media Libraries, for information about system software requirements.LBL="" HELPID=""ID="89200"Indigo Audio System ArchitectureThe standard audio hardware supplied with Indigo workstations supports 24-bit digital stereo and 16-bit analog stereo sound. A dedicated real-time processor works in tandem with the CPU to ensure that audio timing isn't degraded by other system demands.ID="Media2-2HW1"LBL="" HELPID=""Indigo Audio FeaturesIndigo audio features include:ID="Media2-2HW2"built-in speakerstereo line-level analog input and outputstereo headphone outputmicrophone input with phantom powerAES/EBU digital audio input and outputsampling rates include 8000, 11025, 16000, 22050, 32000, 44100, and 48000 Hz.ID="Media2-2HW3"independent input and output ratesoutput rate can be synchronized to the digital input ratelow-latency operation for highly interactive applicationsLBL="" HELPID=""Indigo Audio I/O InterfaceThe audio hardware interface on the back panel of Indigo workstations includes these 3.5-mm audio input and output jacks, which are labeled with icons (see ID="Media2-2HW4"ID="Media2-2HW5"IDREF="28600" TYPE="GRAPHIC"Figure 4-1):monaural microphone input jack for mic-level audio inputFILE="audioicons.ai" POSITION="MARGIN" SCALE="FALSE"LBL="4-1"Figure 4-1 ID="28600"Audio Iconsstereo line-level input (line in) jack for analog audio input from a tape deck, CD player, or similar sourcestereo line-level output (line out) jack for analog audio output, for example, to a tape deck or amplifierstereo headphone output jackstereo digital I/O jack for digital audio input and outputAn internal switching mechanism selects one active input source from the three available inputs. All three outputs are always enabled; each transmits a copy of the same output signal, but the volume is adjusted on the headphone/speaker output. Using the headphone jack preempts output to the internal speaker, which normally outputs the sum of the left and right signals. Digital input and output signals are simultaneously transmitted over a stereo cable. The Audio Engineering Society (AES) standard supports mono and stereo streams of 20-bit or 24-bit samples. Each of the digital input and output streams contains two interleaved channels (left and right) of audio samples.ID="Media2-2HW6"IDREF="93977" TYPE="GRAPHIC"Figure 4-2 shows the location of the audio jacks on the back panel of the Indigo workstation.FILE="indigoaudioports.ai" POSITION="INLINE" SCALE="FALSE"LBL="4-2"Figure 4-2 ID="93977"Audio Jacks on the Back Panel of the Indigo WorkstationLBL="" HELPID=""ID="83458"Indigo2 and Indy Audio System ArchitectureThe audio hardware supplied standard with the IndigoID="Media2-2HW7"ID="Media2-2HW8"2 and Indy workstations provides the same basic audio capabilities as that of the Indigo workstation, plus:ID="Media2-2HW9"stereo microphone inputID="Media2-2HW10"4-channel mode that supports full-speed, simultaneous 4-channel analog input and 4-channel analog outputID="Media2-2HW11"LBL="" HELPID=""Indigo2 and Indy Audio I/O InterfaceThe audio hardware interface on Indigo2 and Indy workstations includes these 3.5-mm audio input and output jacks (see IDREF="45129" TYPE="GRAPHIC"Figure 4-3 for the Indigo2 back panel layout, and IDREF="42813" TYPE="GRAPHIC"Figure 4-5 for the Indy back panel layout):microphone/line-in2 jack for mono and stereo mic-level audio inputstereo line-in jack for analog audio input from a tape deck, CD player, or similar sourcestereo line-out jack for analog audio output, for example, to a tape deck or amplifierstereo headphone/line-out2 output jackstereo digital in/out jack for digital audio input and outputAs in the Indigo workstation, all three outputs are enabled, and an internal switching mechanism selects one active input source from the three available inputs. In addition, a software-controllable internal switching mechanism permits 4-channel audio I/O through the standard I/O interface. See ID="Media2-2HW12"IDREF="42805" TYPE="TITLE""4-channel Audio I/O Interface" for details on4­channel audio.ID="Media2-2HW13"IDREF="45129" TYPE="GRAPHIC"Figure 4-3 shows the location of the audio jacks on the back panel of the Indigo2 workstation.ID="Media2-2HW14"FILE="ind2audioports.ai" POSITION="INLINE" SCALE="FALSE"LBL="4-3"Figure 4-3 ID="45129"Audio Jacks on the Back Panel of the Indigo2 WorkstationLBL="" HELPID=""Indy Workstation LayoutThe Indy workstation features a slightly different layout for its audio I/O interface. Two triangular pushbuttons on the front of the Indy workstation let you adjust the volume of the internal speaker/headphone output up or down, as desired. Pressing both buttons at the same time mutes the speaker/headphone output.ID="Media2-2HW15"IDREF="67796" TYPE="GRAPHIC"Figure 4-4 shows the volume control buttons on the front of the Indy workstation.ID="Media2-2HW16"FILE="indyvolume.ai" POSITION="INLINE" SCALE="FALSE"LBL="4-4"Figure 4-4 ID="67796"Volume Control Buttons on the Front of the Indy WorkstationIDREF="42813" TYPE="GRAPHIC"Figure 4-5 shows the location of the audio jacks on the back panel of the Indy workstation.ID="Media2-2HW17"FILE="indyaudioports.ai" POSITION="INLINE" SCALE="FALSE"LBL="4-5"Figure 4-5 ID="42813"Audio Jacks on the Back Panel of the Indy WorkstationLBL="" HELPID=""ID="42805"4-channel Audio I/O InterfaceA software-controllable internal switching mechanism permits 4-channel audio I/O through the standard I/O interface. When a system is operating in 4-channel mode, the electrical properties of the microphone jack can be configured to accept either line-level or mic-level input, and the electrical properties of the headphone jack can be configured to produce line-level output.ID="Media2-2HW18"IDREF="35419" TYPE="GRAPHIC"Figure 4-6 shows an Indy workstation cabling setup for 4-channel audio.FILE="4channel.ai" POSITION="INLINE" SCALE="FALSE"LBL="4-6"Figure 4-6 ID="35419"Cabling Setup for 4-channel Audio on the Indy WorkstationCables like the ones shown in IDREF="35419" TYPE="GRAPHIC"Figure 4-6 can be purchased from audio accessory dealers. One end of the cable has 3.5-mm audio plugs that plug into the Indigo2 or Indy workstation jacks; the other end independently terminates each of the two independent signals with RCA phono plugs.NoteDo not confuse these cables with "Y" connectors that route the same signal to multiple connections.When the system is configured (either from apanel or from the Audio Library) to use 4-channel mode, (L1, R1) samples are input to the line-in jack and (L2, R2) samples are input to the microphone/line-in2 jack. Similarly, in 4-channel mode, (L1, R1) samples are output from the line-out jack, and (L2, R2) samples are output from the headphone/line-out2 jack.LBL="" HELPID=""ID="78427"Recommendations for Audio Development System ConfigurationsThe primary considerations in setting up your system for digital audio software development are memory and disk space. Because of the large sizes of audio sample files, disk space in particular is crucial.ID="Media2-2HW19"LBL="" HELPID=""MemoryA minimum of 32 MB is recommended for digital audio development. The more memory installed, the more responsive your workstation will be when handling large amounts of sample data, as well as during compilation.ID="Media2-2HW20"LBL="" HELPID=""Disk SpaceBe sure to allow an adequate amount of disk space. These statistics should help give you an idea of the kind of disk space required for your application:ID="Media2-2HW21"mono 8-bit, CCITT/TSB G.711 name='mgr' font=symbol charset=fontspecific code=109
	TeX='\mu '      descr='[mgr]'-law encoded 8 kHz (speech quality) audio = 8 kBytes/secmono 16-bit (15-bit range, 14-bit resolution), CCITT/TSB G.722 compressed 16 kHz (high-quality speech with more computationally expensive compression) audio = 8 kBytes/secstereo 16-bit 44.1 kHz (CD-quality digitized analog input) audio = 176 kBytes/secstereo 24-bit 48 kHz (highest-quality digital, 4-byte word) audio = 384 kBytes/sec4-channel 16-bit 44.1 kHz (CD-quality digitized analog input) audio = 352 kBytes/sec4-channel 24-bit 48 kHz (highest-quality digital, 4-byte word) audio = 768 kBytes/secA minimum of 600 MB is suggested; 800 MB or more is recommended, especially if your development work involves storing large amounts of high-quality sample data on disk.LBL="" HELPID=""PeripheralsIf you do not already have a CD-ROM drive, you may want to purchase one. ProsonusÒ, AwareÒ, Inc., and other companies supply CD-ROM libraries of audio sample data (see ID="Media2-2HW22"ID="Media2-2HW23"IDREF="55935" TYPE="TITLE""Third-party Audio Software and Sound Libraries" in Chapter 5 for information on ordering these CD-ROM libraries). You can also use the drive for sampling from audio CDs (obtain permission before using copyrighted material).A DAT drive is recommended both for general data archiving and for transferring audio from hard disk.LBL="5"ID="81209"Digital Audio System SoftwareThis chapter describes the components of the digital audio system software: digital audio libraries, device drivers, and system-wide resources, and explains how these components interact. This chapter also describes other resources available to application developers, such as end-user audio tools, third-party audio software and sound libraries, and sample programs.ID="Media2-3SW1"LBL="" HELPID=""Digital Audio System Software OverviewIDREF="41013" TYPE="GRAPHIC"Figure 5-1 diagrams the interaction between an audio application and the audio libraries, the device drivers, the IRIX file system, the audio hardware, and the optional SCSI devices.ID="Media2-3SW2"ID="Media2-3SW3"FILE="Media2-3SW.cgm" POSITION="INLINE" SCALE="FALSE"LBL="5-1"Figure 5-1 ID="41013"Interaction of Digital Audio System ComponentsLBL="" HELPID=""About the Digital Audio LibrariesThe digital audio libraries can be used separately or in combination. Each library is tailored to a particular set of tasks, as follows:COLUMNS="2"LEFT="0" WIDTH="90"Audio LibraryLEFT="95" WIDTH="248"provides an API for configuring the audio system, 
managing audio I/O between the application program 
and the audio hardware, specifying attributes of digital 
audio data, and facilitating real-time programming. See 
IDREF="70508" TYPE="TITLE"Chapter 6, "Programming with the Audio Library."LEFT="0" WIDTH="90"Audio File LibraryLEFT="95" WIDTH="248"provides an API for reading and writing two standard 
digital audio file formats, AIFF and AIFF-C. See 
IDREF="45737" TYPE="TITLE"Chapter 7, "Programming with the Audio File Library."LEFT="0" WIDTH="90"CD Audio LibraryLEFT="95" WIDTH="248"provides an API for optional Silicon Graphics SCSI CD-
ROM drives. The drive features a special mode that 
allows it to read audio CD format as well as CD-ROM 
format. See IDREF="41223" TYPE="TITLE"Chapter 8, "Programming with the CD 
Audio Library."LEFT="0" WIDTH="90"DAT Audio 
LibraryLEFT="95" WIDTH="248"provides an API for optional Silicon Graphics SCSI 
DAT drives. See IDREF="80723" TYPE="TITLE"Chapter 9, "Programming with the 
DAT Audio Library."LBL="" HELPID=""About Shared System-Wide ResourcesAudio applications share CPU resources with other processes, and they share audio resources with other audio applications running concurrently.ID="Media2-3SW4"LBL="" HELPID=""How Audio Applications Share CPU ResourcesCPU resources are managed by the IRIX kernel, which gives some resources higher priority than others. Programming style can affect CPU usage, so to get the best performance from your application, use native data formats whenever possible (to avoid internal conversion), and free system resources as soon as they are no longer needed (see the individual chapters on each library for details). You can also request exclusive resources or upgrade the priority of your application by using the special IRIX real-time programming techniques described in ID="Media2-3SW5"ID="Media2-3SW6"ID="Media2-3SW7"IDREF="63440" TYPE="TITLE""Real-time Programming Techniques for Audio" in Chapter 6.LBL="" HELPID=""How Audio Applications Share Audio System ResourcesIDREF="59349" TYPE="GRAPHIC"Figure 5-2 shows how the IRIS audio utilities ID="Media2-3SW8"apanel, ID="Media2-3SW9"soundeditor, and soundfilerID="Media2-3SW10" share the system's audio resources. Similarly, your audio application must share the audio resources with other audio applications running concurrently.ID="Media2-3SW11"ID="Media2-3SW12"LBL="" HELPID=""How Outputs from Multiple Audio Applications Are CombinedIn IDREF="59349" TYPE="GRAPHIC"Figure 5-2, three audio applications are running simultaneously. A recording engineer is using ID="Media2-3SW13"soundeditor to combine live input from a microphone with a prerecorded sound file stored on the disk. She is using apanel to monitor the input level and ID="Media2-3SW14"soundfiler to audition sound files through her headphones.Note that while the input is selected from among three possible inputs, all of the outputs are added together and clipped to generate the final output, which is presented to all three outputs. This means that an audio application is responsible for determining if other audio applications are running concurrently, and limiting its output signal accordingly to avoid unnecessary clipping.ID="Media2-3SW15"ID="Media2-3SW16"IDREF="59349" TYPE="GRAPHIC"Figure 5-2 shows how the IRIS audio utilities ID="Media2-3SW17"apanel, ID="Media2-3SW18"soundeditor, and soundfilerID="Media2-3SW19" share the system's audio resources.FILE="5-2.online.bw" POSITION="INLINE" SCALE="FALSE"LBL="5-2"Figure 5-2 ID="59349"Audio Data FlowLBL="" HELPID=""How Global Audio Settings Are Established and MaintainedThe system-wide digital audio hardware and software settings are initialized to reasonable defaults when the system is powered on and whenever it is rebooted.In IDREF="59349" TYPE="GRAPHIC"Figure 5-2, the input rate and output rate are set at 44.1 kHz and remain fixed unless changed from ID="Media2-3SW20"ID="Media2-3SW21"apanel. soundfiler and soundeditor both allow the user to control the volume from apanel. soundfiler changes the input and output rates when needed, and soundeditor has the ability to change the rates but asks for confirmation before making any changes.The values of the global audio settings are known collectively as the audio system ID="Media2-3SW22"state. Certain audio settings can be initialized and modified in software. The AL has routines for querying which elements of state can be controlled by software, and for getting and setting the values of the global state parameters. It is good programming practice to query for the existence of other audio processes before changing global settings.LBL="" HELPID=""Programming Guidelines for Managing System-Wide ResourcesKeep these guidelines in mind when writing audio applications:ID="Media2-3SW23"Determine the availability of special features before attempting to use them.Monitor the existence of concurrent audio applications and process output accordingly.ID="Media2-3SW24"Manage system-wide settings that rely on personal preference, such as volume, through a global audio control program such as apanel; otherwise, query for the existence of other audio processes before changing settings such as data rates that can affect other applications.Manage memory allocation for efficient use of system-wide resources.LBL="" HELPID=""Tools Available for the Audio Application DeveloperThis section describes additional tools that you may find helpful for developing audio applications.LBL="" HELPID=""ID="74207"Graphical User Interface Audio ToolsEnd-user audio tools are provided for playing, recording, and manipulating digital audio signals. These audio tools were created using the digital audio libraries and therefore support AIFF and AIFF-C file formats. These tools are provided as part of the standard system software and feature online help.ID="Media2-3SW25"See the Media Control Panels User's Guide for a complete description of these tools:apanelID="Media2-3SW26"the audio control panel for selecting inputs, input and output levels, and sampling ratescdmanID="Media2-3SW27"for playing audio CDs on a CD-ROM through your workstation's audio outputs, and for recording CD audio tracks to diskID="Media2-3SW28"datmanID="Media2-3SW29"for playing and recording digital audio tapes using the optional internal DAT drive, and for recording DAT audio tracks to diskID="Media2-3SW30"soundeditorID="Media2-3SW31"a simple editor for viewing, manipulating, and combining multiple tracks of recorded samples, as live input or from a sound fileID="Media2-3SW32"soundfilerID="Media2-3SW33"an audio file librarian for organizing and previewing sample sound files and converting between different sound file formatsID="Media2-3SW34"In addition, the system Toolchest contains a tool for performing confidence tests on system components, including the audio system, and the CD-ROM and DAT drives. See the owner's guide for your workstation for more information about confidence tests.ID="Media2-3SW35"LBL="" HELPID=""ID="78872"Online Source Code ExamplesSource code examples are located online in ID="Media2-3SW36"/usr/people/4Dgifts/examples/dmedia, in directories labeled audio, cd+dat, dmplay, dmrecord, and midi. README files in these directories explain how to use and compile these programs. When a program from one of these directories is included in this guide, it is referred to as the ID="Media2-3SW37"4Dgiftsprogramname.c program. Because the online source for these programs can get updated more frequently than the printed version of this guide, you should consider the online source code as the most recent version if there is a discrepancy between them.LBL="" HELPID=""ID="55935"Third-party Audio Software and Sound LibrariesThis section describes third-party audio software and libraries that are made available to the developer as part of the IRIS digital media libraries. Contact the companies directly for licensing and use rights.ID="Media2-3SW38"LBL="" HELPID=""Aware Audio Compression Software and Audio ProductsAware, Inc. scalable audio compression software is provided with the and can be accessed from Audio File Library routines or Compression Library routines. Two Aware codecs (compressor-decompressors) that provide ISO/MPEG and Aware ID="Media2-3SW39"ID="Media2-3SW40"MultiRateID="Media2-3SW41"Ô lossless and near-lossless compression are built into the Audio File Library as compression parameters, and additional Aware audio compression software can be accessed through other parameters in the Audio File and Compression Libraries. Aware also offers other licensable audio products and a CD-ROM library; see IDREF="63476" TYPE="TITLE"Appendix B, "Aware Scalable Audio Compression Software," for details.For more information about Aware products, contact Aware at:Aware, Inc.One Memorial DriveCambridge, MA 02142Phone: (617) 577-1700Fax: (617) 577-1710Email: sales@aware.comID="Media2-3SW42"LBL="" HELPID=""The Prosonus Sound LibraryThe Prosonus Sound Library, which is included with the end user media tools, contains more than 10 MB of professional quality music and sound samples. These files are located in ID="Media2-3SW43"/usr/lib/sounds/prosonus, and they represent a small subset of the music, sound effects, and instrument samples created by Prosonus. All included files are sampled at 44.1 kHz and stored in AIFF format. The complete Prosonus Sound Library is separately available on CD-ROM from Prosonus. ID="Media2-3SW44"ID="Media2-3SW45"ID="Media2-3SW46"For more information about Prosonus products, contact Prosonus at:Prosonus11126 Weddington StreetNorth Hollywood, CA 91601Phone: (800) 999-6191 or (818) 766-5221Fax: (818) 766-6098ID="Media2-3SW47"Prosonus files are license-free when used in private presentations. They may be shared via NFS with other Silicon Graphics computers but may not be copied to other systems. If you intend to ship the Prosonus files with a product intended for resale or broadcast, copyrights and royalties may apply. Please contact Prosonus for questions concerning licensing and resale of Prosonus files.ID="Media2-3SW48"LBL="" HELPID=""ID="31484"Compiling and Linking an Audio ApplicationThis section lists compiling and linking commands for digital audio and MIDI programs.To compile an Audio Library program, enter:ID="Media2-3SW49"cc ­gALsample.c-oALsampleID="Media2-3SW50"­laudioTo compile an Audio File Library program, enter:ID="Media2-3SW51"cc ­gAFLsample.c-oAFLsampleID="Media2-3SW52"­laudiofile -laudioutil -lmID="Media2-3SW53"ID="Media2-3SW54"The Audio File Library also requires linking with ID="Media2-3SW55"libm.a, the math library, and with ID="Media2-3SW56"ID="Media2-3SW57"libaudioutil.so, the Audio Utility Library.To compile a CD Audio Library program, enter:ID="Media2-3SW58"cc ­gCDsample.c-oCDsampleID="Media2-3SW59"­lcdaudio -lds -libmediadID="Media2-3SW60"The CD Audio Library also requires linking with libds.aID="Media2-3SW61", the SCSI device library and ID="Media2-3SW62"libmediad, the media library daemon.ID="Media2-3SW63"To compile a DAT Audio Library program, enter:ID="Media2-3SW64"cc ­gDATsample.c-oDATsampleID="Media2-3SW65"­ldataudioPrograms making use of more than one of these libraries must link to all of the ones they use (the linking order is often specific):cc ­gprog.c-oprog­laudio-laudiofile-lcdaudio-lds-libmediadDepending on the application you are writing, you may also have to link with other libraries such as the GL shared library, the math library, and the C shared library. You can use fast ID="Media2-3SW66"ID="Media2-3SW67"malloc() routines by including malloc.h and linking with libmalloc.aID="Media2-3SW68".The audio and MIDI libraries are compatible with both ANSI C and the standard C. To compile code that is not ANSI-compliant, add ID="Media2-3SW69"ID="Media2-3SW70"ID="Media2-3SW71"­cckr to the command line.LBL="6"ID="70508"Programming with the Audio LibraryThe Audio Library (AL) provides a uniform application programming interface (API) for audio input to and output from Silicon Graphics workstations that feature high-quality digital audio systems.ID="Media2-4AL1"The AL comprises routines that provide these basic capabilities:creating digital audio input and output connectionsreading and writing digital audio dataquerying and controlling digital audio data attributesquerying and controlling the configuration of the audio systemhandling errorsIn this chapter:IDREF="62279" TYPE="TITLE""Audio Library Basics" discusses fundamental audio concepts and explains the features, programming model, error handler, and audio sampling methods of the Audio Library.IDREF="25005" TYPE="TITLE""Initializing an Audio Library Application" explains how to create and configure audio ports.IDREF="43276" TYPE="TITLE""Reading and Writing Audio Data" explains how to read and write audio samples.IDREF="81909" TYPE="TITLE""Querying and Controlling the Global Audio Device State" explains how to query and set global audio parameters.IDREF="63249" TYPE="TITLE""Audio Library Synchronization Facilities" explains how to synchronize audio ports with one another and with other media.IDREF="63440" TYPE="TITLE""Real-time Programming Techniques for Audio" explains how to use IRIX real-time programming facilities in conjunction with AL routines for providing optimal audio performance.LBL="" HELPID=""ID="62279"Audio Library BasicsThis section discusses the basic concepts and data structures used by the ALname='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]' with particular attention devoted to the programming model, sample data formats, error handling, and programming concepts.LBL="" HELPID=""Audio Library FeaturesID="Media2-4AL2"Features of the AL include:Binary compatibilityname='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'AL programs written on one Silicon Graphics workstation equipped with an audio system are executable on other audio-equipped workstations across the product line.Shared audio resourcesname='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'more than one audio application can be active at a time, and multiple programs can have input and output streams open concurrently.Real-time performancename='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'a special group of AL functions useful specifically for writing real-time code.LBL="" HELPID=""Audio Library Programming ModelID="Media2-4AL3"The AL programming model has two basic objects:Audio deviceThe audio hardware used by the AL, which is shared among audio applications. The audio device contains settings pertaining to the configuration of both the internal audio system and the external electrical connections.ID="Media2-4AL4"ALportID="Media2-4AL5"A one-way (input or output) audio data connection between an application program and the host audio system. An ALport contains:ID="Media2-4AL6"ID="Media2-4AL7"an audio sample queue, which stores audio samples awaiting input or outputsettings pertaining to the attributes of the digital audio data it transportsSome of the settings of an ALport are static; they cannot be changed once the ALport has been opened. Other settings are dynamic; they can be changed while an ALport is open.ALconfigID="Media2-4AL8"An opaque data structure for configuring these settings of an ALport:audio device (static setting)size of the audio sample queue (static setting)number of channels (static setting)format of the sample data (dynamic setting)width of the sample data (dynamic setting)range of floating point sample data (dynamic setting)LBL="" HELPID=""Digital Audio Data RepresentationThe digital representation of an audio signal is generated by periodically sampling the amplitude (voltage) of the audio signal. The samples represent periodic "snapshots" of the signal amplitude. The Nyquist Theorem provides a way of determining the minimum sampling frequency required to accurately represent the information (in a given bandwidth) contained in an analog signal. Typically, digital audio information is sampled at a frequency that is at least double the highest interesting analog audio frequency. See ID="Media2-4AL9"ID="Media2-4AL10"The Art of Digital Audio or a similar reference on digital audio for more information.LBL="" HELPID=""Digital Audio Sample RatesThe ID="Media2-4AL11"sample rate is the frequency at which samples are taken from the analog signal. Sample rates are measured in hertz (Hz). A sample rate of 1 Hz is equal to one sample per second. For example, when a mono analog audio signal is digitized at a 48 kilohertz (kHz) sample rate, 48,000 digital samples are generated for every second of the signal.ID="Media2-4AL12"To understand how the sample rate relates to sound quality, consider the fact that a telephone transmits voice-quality audio in a frequency range of about 320 Hz to 3.2 kHz. This frequency range can be represented accurately with a sample rate of 6.4 kHz. The range of human hearing, however, extends up to approximately 18­20 kHz, requiring a sample rate of at least 40 kHz.ID="Media2-4AL13"The sample rate used for music-quality audio, such as the digital data stored on audio CDs is 44.1 kHz. A 44.1 kHz digital signal can theoretically represent audio frequencies from 0 kHz to 22.05 kHz, which adequately represents sounds within the range of normal human hearing. The most common sample rates used for DATs are 44.1 kHz and 48 kHz. Higher sample rates result in higher-quality digital signals; however, the higher the sample rate, the greater the signal storage requirement.ID="Media2-4AL14"LBL="" HELPID=""Digital Audio Sample FramesA ID="Media2-4AL15"sample frame is a set of audio samples that are coincident in time. A sample frame for mono data is a single sample. A sample frame for stereo data consists of a left-right sample pair. A sample frame for 4-channel data has two left-right sample pairs (L1, R1, L2, R2).Stereo samples are interleaved; left-channel samples alternate with right-channel samples. 4-channel samples are also interleaved, but each frame has two left-right sample pairs.ID="Media2-4AL16"ID="Media2-4AL17"IDREF="70857" TYPE="GRAPHIC"Figure 6-1 shows the relationship between the number of channels and the frame size of audio sample data.ID="Media2-4AL18"FILE="Media2-4AL.cgm" POSITION="INLINE" SCALE="FALSE"LBL="6-1"Figure 6-1 ID="70857"Audio Samples and FramesLBL="" HELPID=""Digital Audio Sample FormatsThe AL uses a digital data format called linear ID="Media2-4AL19"pulse code modulation (PCM) (see the audio references for a definition of this term) to represent digital audio samples.The formats supported by the AL and the audio system are:ID="Media2-4AL20"8-bit and 16-bit signed integer24-bit signed, right-justified within a 32-bit integer32-bit and 64-bit floating pointNoteThe audio hardware supports 16-bit I/O for analog data and 24-bitI/O for AES/EBU digital data.ID="Media2-4AL21"For floating point data, the application program specifies the desired range of values for the samples; for example, from -1.0 to 1.0.LBL="" HELPID=""Digital Audio Input and Output Sample ResolutionsThe native data format used by the audio hardware is 24-bit two's complement integers. The audio hardware sign-extends each 24-bit quantity into a 32-bit word before delivering the samples to the Audio Library.ID="Media2-4AL22"Audio input samples delivered to the Audio Library from the Indigo, Indigo2, and Indy audio hardware have different levels of resolution, depending on the input source that is currently active; the AL provides samples to the application at the desired resolution. You can also write your own conversion routine if desired.ID="Media2-4AL23"Microphone/line-level input samples come from analog-to-digital (A/D) converters, which have 16-bit resolution. These samples are treated as 24-bit samples with 0's in the low 8 bits.ID="Media2-4AL24"AES/EBU digital input samples have either 20-bit or 24-bit resolution, depending on the device that is connected to the digital input; for the 20-bit case (the most common), samples are treated as 24-bit samples, with 0's in the least significant 4 bits. The AL passes these samples through to the application if 24-bit two's complement is specified. If two's complement with 8-bit or 16-bit resolution is specified, the AL right-shifts the samples so that they will fit into a smaller word size. For floating point data, the AL converts from the 24-bit format to floating point, using a scale factor specified by the application to map the peak integer values to peak float values.ID="Media2-4AL25"For audio output, the AL delivers samples to the audio hardware as 24-bit quantities sign-extended to fill 32-bit words. The actual resolution of the samples from a given output port depends on the application program connected to the port. For example, an application may open a 16-bit output port, in which case the 24-bit samples arriving at the audio processor will contain 0's in their least significant 8 bits.The Audio Library is responsible for converting between the output sample format specified by an application and the 24-bit native format of the audio hardware. For 8-bit or 16-bit integer samples, this conversion is accomplished by left-shifting each sample written to the output port by 16 bits and 8 bits, respectively. For 32-bit or 64-bit floating point samples, this con version is accomplished by rescaling each sample from the range of floating point values that is specified by the application to the full 24-bit range and then rounding the sample to the nearest integer value. ID="Media2-4AL26"LBL="" HELPID=""Handling Audio Library ErrorsThis section describes techniques for error handling in AL applications.ID="Media2-4AL27"When the AL encounters an error, it:Checks to see whether an error handler is set, and if so, calls the specified routine.Sets an error code, and returns a failure from the function call.The default error handler prints a message to ID="Media2-4AL28"stderr. Although these error messages may be helpful for debugging during the development phase, you should turn off the default error handler in order to provide more effective error handling by using the IRIX ID="Media2-4AL29"oserror(3C) system call to retrieve function return codes.To turn off the default error handler, call ID="Media2-4AL30"ID="Media2-4AL31"ALseterrorhandler(). Its function prototype is:ALerrfunc ALseterrorhandler ( ALerrfunc efunc )where:efuncis a pointer to an alternate error-handling routine of type ALerrfunc that is declared as:ID="Media2-4AL32"void errorfunc ( long arg1, const char* arg2, [args] )Substituting zero for efunc turns off the error handler.Most AL routines set error codes when they fail. Throughout this guide, the return values and relevant error codes are listed along with the description of each routine. You can retrieve these error codes by calling oserror(3C). Based on these return codes, programs can adapt or recover, and/or alert the user by displaying a dialog box type of notifier or by printing information to the shell window from which the application was launched.LBL="" HELPID=""Audio Library Application Programming ConceptsID="Media2-4AL33"Typically, your AL program must:initialize data structuresset up buffers for passing data between your application and the CPUquery for available featuresconfigure and open audio connectionspass data to and from the ALport and operate on that dataprocess errorsclose audio connectionsfree system resourcesThe sections that follow explain these concepts in detail.LBL="" HELPID=""ID="25005"Initializing an Audio Library ApplicationID="Media2-4AL34"To enable audio input and output, your application must create and configure the required audio I/O connections. This section describes how to set up and use the AL data structures that provide audio I/O capability.ID="Media2-4AL35"LBL="" HELPID=""About ALportsThe AL provides an opaque data structure called an ALport for audio I/O connections. An ALport provides a one-way (input or output) mono, stereo, or 4-channel audio data connection between an application program and the host audio system. More than one ALport can be opened by the same application; the number of ALports that can be active at the same time depends on the hardware and software configurations you are using.ID="Media2-4AL36"An ALport consists of a sample queue and static and dynamic state information. For audio input, the hardware places audio samples in an input port's queue at a constant rate, and your application program reads the samples from the queue. Similarly, for audio output, your application writes audio samples to an output port's queue, and the audio hardware removes the samples from the queue. A minimum of two ALports are necessary to provide input and output capability for an audio application.ID="Media2-4AL37"LBL="" HELPID=""Using ALconfig Structures to Configure ALportsYou can open an ALport with the default configuration or you can customize an ALconfig for configuring an ALport suited to your application needs.ID="Media2-4AL38"ID="Media2-4AL39"The default ALconfig has:ID="Media2-4AL40"a buffer size of 100,000 samplesID="Media2-4AL41"stereo dataID="Media2-4AL42"a two's complement sample formatID="Media2-4AL43"a 16-bit sample widthID="Media2-4AL44"These settings provide an ALport that is compatible with CD- and DAT-quality data, but if your application requires different settings, you must create an ALconfig with the proper settings before opening a port. The device, channel, and queue-size settings for an ALport are staticname='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'they cannot be changed after the port has been opened.ID="Media2-4AL45"The steps involved in configuring and opening an ALport are listed below, followed by a sample code fragment that illustrates each of these steps. The sample program is followed by subsections that describe these concepts more fully and explain the use of each routine listed here.Turn off the default error handler by passing a 0 to ALseterrorhandler().If the default ALconfig settings are satisfactory, you can simply open a default ALport by using 0 for the configuration in the ALopenport() routine; otherwise, create a new ALconfig by calling ALnewconfig().If nondefault values are needed for any of the ALconfig settings, set the desired values as follows:Call ALsetchannels() to change the number of channels (IDREF="45077" TYPE="TEXT"page 77).Call ALsetqueuesize() to change the sample queue size (IDREF="83611" TYPE="TEXT"page 79).Call ALsetsampfmt() to change the sample data format (IDREF="47495" TYPE="TEXT"page 80).Call ALsetwidth() to change the sample data width (IDREF="34470" TYPE="TEXT"page 82).Call ALsetfloatmax() to set the maximum amplitude of floating point data (not necessary for integer data formats) (IDREF="61930" TYPE="TEXT"page 84).Open an ALport by passing the ALconfig to the ALopenport() routine.Create additional ALports with the same settings by using the same ALconfig to open as many ports as are needed.IDREF="18635" TYPE="TEXT"Example 6-1 demonstrates how to configure and open an output ALport that accepts floating point mono samples.ID="Media2-4AL46"LBL="6-1"Example 6-1 ID="18635"Configuring and Opening an ALportALconfig audioconfig;
ALport audioport;
int err;

void audioinit /* Configure an audio port */ 
{
ALseterrorhandler(0); 
audioconfig = ALnewconfig(); 

ALsetsampfmt(audioconfig, AL_SAMPFMT_FLOAT); 
ALsetfloatmax(audioconfig, 10.0); 
ALsetqueuesize(audioconfig, 44100); 
ALsetchannels(audioconfig,AL_MONO); 

audioport = ALopenport("surreal","w",audioconfig); 
if (audioport == (ALport) 0) { 
    err = oserror(); 
    if (err == AL_BAD_NO_PORTS) { 
        fprintf(stderr, " System is out of audio ports\n"); 
    } else if (err == AL_BAD_DEVICE_ACCESS) { 
        fprintf(stderr, " Couldn't access audio device\n"); 
    } else if (err == AL_BAD_OUT_OF_MEM) { 
        fprintf(stderr, " Out of memory\n"); 
    } 
    exit(1); 
} The sections that follow explain how to use ALconfigs in greater detail.LBL="" HELPID=""Creating a New ALconfigTo create a new ALconfig structure that is initialized to the default settings, call ID="Media2-4AL47"ID="Media2-4AL48"ALnewconfig(). Its function prototype is:ALconfig ALnewconfig ( void )The ALconfig that is returned can be used to open a default ALport, or you can modify its settings to create the configuration you need. In IDREF="18635" TYPE="TEXT"Example 6-1, the channel, queue size, sample format, and floating point data range settings of an ALconfig named audioconfig are changed.ALnewconfig() returns an ALconfig structure upon successful completion; otherwise, it returns 0 and sets an error code that you can retrieve by calling oserror(3C). Possible errors include:ID="Media2-4AL49"COLUMNS="2"LEFT="0" WIDTH="135"AL_BAD_OUT_OF_MEMLEFT="140" WIDTH="189"insufficient memory available to allocate 
the ALconfig structureLBL="" HELPID=""ID="45077"Setting and Getting the Number of Channels for an ALconfigAn ALport can be configured for one, two, or four audio channels. The channel setting remains in effect as long as the port is open.ID="Media2-4AL50"NoteConfiguring an ALport to use four channels does not depend on the hardware configuration of the system on which the application is running. See IDREF="81909" TYPE="TITLE""Querying and Controlling the Global Audio Device State" for information on configuring the hardware for 4-channel mode.   To set the number of channels for an ALconfig structure, call ALsetchannels()ID="Media2-4AL51". Its function prototype is:int ALsetchannels ( ALconfig config, long channels )where:configis the ALconfig for which you want to set the channels channelsis the number of channels to configure: 1, 2, or 4Any ALport that you open with this config will have the number of channels that you set in channels.ALsetchannels()ID="Media2-4AL52" returns 0 upon successful completion; otherwise, it returns -1 and sets an error code that you can retrieve by calling ID="Media2-4AL53"oserror(3C). Possible errors include:COLUMNS="2"LEFT="0" WIDTH="117"AL_BAD_CONFIGLEFT="125" WIDTH="180"config is either invalid or nullLEFT="0" WIDTH="117"AL_BAD_CHANNELSLEFT="125" WIDTH="180"channels is not 1, 2, or 4To retrieve the channel setting of a given ALconfig structure, call ALgetchannels()ID="Media2-4AL54". Its function prototype is:long ALgetchannels ( ALconfig config )where:configis the ALconfig structure being queriedALgetchannels() returns the channel setting of config, upon successful completion; otherwise, it returns -1 and sets an error code that you can retrieve by calling oserror(3C). Possible errors include:COLUMNS="2"LEFT="0" WIDTH="117"AL_BAD_CONFIGLEFT="125" WIDTH="180"config is either invalid or nullLBL="" HELPID=""ID="34028"Setting and Getting the Sample Queue Size for an ALconfigSelecting the proper size for the sample queue is very important, because continuous sound output depends on the ability of the application to fill the queue at least as fast as the hardware empties it. For example, if the queue is too small, the application may take too long supply new samples, resulting in audible breaks that sound like pops or clicks. The size of the queue determines the maximum delay that can be tolerated while waiting for the application to get more samples at the given sample rate. To determine how much space to allocate for the sample queue, consider the data type and rate. For example, the default queue size of 100,000 samples provides buffer space for slightly more than one second of 48 kHz stereo audio data, and a little more than three seconds of 32 kHz mono data. To better understand these phenomena, see ID="Media2-4AL55"ID="Media2-4AL56"IDREF="74750" TYPE="GRAPHIC"Figure 6-2 on IDREF="74750" TYPE="TEXT"page 91 for an illustration of a sample queue and read the associated discussion.ID="Media2-4AL57"TipThe main point to be concerned about is how full to keep the queue, regardless of its size. If the queue is full, more time passes before samples are played. The ideal situation is to keep enough samples in the queue to allow for the longest possible delay that will be experienced in retrieving the next batch of samples. See IDREF="63440" TYPE="TITLE""Real-time Programming Techniques for Audio" for an explanation of how to set the fill threshold for a queue. The noninclusive values for minimum and maximum allowable queue sizes for ALports on Indigo, Indigo2, and Indy workstations are listed in IDREF="52464" TYPE="TABLE"Table 6-1COLUMNS="3"LBL="6-1"Table 6-1 ID="52464"Minimum and Maximum Allowable Queue Sizes for ALportsID="Media2-4AL58"LEFT="0" WIDTH="126"ALport Type LEFT="135" WIDTH="100"Minimum SizeLEFT="240" WIDTH="100"Maximum SizeLEFT="0" WIDTH="126"MonoLEFT="135" WIDTH="100"510LEFT="240" WIDTH="100"131,069LEFT="0" WIDTH="126"StereoLEFT="135" WIDTH="100"1019LEFT="240" WIDTH="100"262,139LEFT="0" WIDTH="126"4-channel on IndigoID="Media2-4AL59"LEFT="135" WIDTH="100"2038LEFT="240" WIDTH="100"524, 278LEFT="0" WIDTH="126"4-channel on Indigo2 or IndyLEFT="135" WIDTH="100"1019LEFT="240" WIDTH="100"262,139To specify an ALconfig with a sample queue size other than the default for an ALport, call ID="Media2-4AL60"ID="Media2-4AL61"ALsetqueuesize(). Its function prototype is:int ALsetqueuesize ( ALconfig config, const long size )where:configis the ALconfig structure for which you want to change the sample queue sizesizeis the number of sample locations to allocate for the queueAny ALport that you open with this config will have a queue size of size.ID="83611"ALsetqueuesize() returns 0 upon successful completion; otherwise, it returns -1 and sets an error code that you can retrieve by calling oserror(3C). Possible errors include:COLUMNS="2"LEFT="0" WIDTH="117"AL_BAD_CONFIGLEFT="125" WIDTH="180"config is either invalid or nullLEFT="0" WIDTH="117"AL_BAD_QSIZELEFT="125" WIDTH="180"size is either negative or larger than the 
maximum allowable queue sizeTo retrieve the size of the sample queue in a given ALconfig structure, call ALgetqueuesize()ID="Media2-4AL62". Its function prototype is:long ALgetqueuesize ( ALconfig config )where:configis the ALconfig structure being queriedALgetqueuesize() returns the queuesize of config upon successful completion; otherwise, it returns -1 and sets an error code that you can retrieve by calling oserror(3C). Possible errors include:COLUMNS="2"LEFT="0" WIDTH="117"AL_BAD_CONFIGLEFT="125" WIDTH="180"config is either invalid or nullLBL="" HELPID=""ID="47495"Setting and Getting the Sample Data Format for an ALconfigThe AL allows you to choose between three sample formats: ID="Media2-4AL63"two's complement (default)ID="Media2-4AL64"floating pointdouble-precision floating pointTo set the sample format type of a given ALconfig structure, call ALsetsampfmt()ID="Media2-4AL65". Its function prototype is:int ALsetsampfmt ( Alconfig config, long sampleformat )where:configis the ALconfig structure for which you want to change the sample formatsampleformatmust be one of three symbolic constants:COLUMNS="2"LEFT="0" WIDTH="144"AL_SAMPFMT_TWOSCOMPLEFT="150" WIDTH="117"two's complement linear 
PCM format, for which 
the width is specified by 
ALsetwidth()LEFT="0" WIDTH="144"AL_SAMPFMT_FLOATLEFT="150" WIDTH="117"32-bit IEEE double-
precision floating point 
scaled linear PCM formatID="Media2-4AL66"LEFT="0" WIDTH="144"AL_SAMPFMT_DOUBLE LEFT="150" WIDTH="117"64-bit IEEE double-
precision floating point 
scaled linear PCM formatAny ALport that you open with this config will have a sample format of sampleformat.ALsetsampfmt() returns 0 upon successful completion; otherwise, it returns -1 and sets an error code that you can retrieve by calling oserror(3C). Possible errors include:COLUMNS="2"LEFT="0" WIDTH="117"AL_BAD_CONFIGLEFT="125" WIDTH="180"config is either invalid or nullLEFT="0" WIDTH="117"AL_BAD_SAMPFMTLEFT="125" WIDTH="180"sampleformat is not one of 
AL_SAMPFMT_TWOSCOMP, 
AL_SAMPFMT_FLOAT, or 
AL_SAMPFMT_DOUBLETo retrieve the sample format of a given ALconfig structure, call ALgetsampfmt()ID="Media2-4AL67". Its function prototype is:long ALgetsampfmt ( ALconfig config )where:configis the ALconfig structure being queriedALgetsampfmt() returns the sampleformat setting of config upon successful completion; otherwise, it returns -1 and sets an error code that you can retrieve by calling oserror(3C). Possible errors include:COLUMNS="2"LEFT="0" WIDTH="117"AL_BAD_CONFIGLEFT="125" WIDTH="180"config is either invalid or nullLBL="" HELPID=""ID="28367"Setting and Getting the Integer Sample Width for an ALconfigThe sample width represents the degree of precision to which the full-scale range of an audio signal can be sampled. You can specify the width of two's complement integer sample data, but you can't specify the width of floating point samples. Thus, setting the sample width has no effect when the sample format is AL_SAMPFMT_FLOAT or AL_SAMPFMT_DOUBLE; however, the width setting does have an effect if the sample format is subsequently changed to AL_SAMPFMT_TWOSCOMP.ID="Media2-4AL68"ID="Media2-4AL69"ID="Media2-4AL70"ID="Media2-4AL71"The sample width also determines which data type the AL uses when reading and writing samples. The sample widths available for two's complement data, and their associated resolutions and data types, are:ID="Media2-4AL72"ID="Media2-4AL73"8-bit samplesrepresenting a total of 28 quantized signal values. The AL treats 8-bit samples as packed, signed characters (chars).16-bit samplesrepresenting a total of 216 quantized signal values. The AL treats 16-bit samples as packed, signed short integers (shorts).24-bit samplesrepresenting a total of 224 quantized signal values. The AL treats 24-bit samples as right-justified, sign-extended, signed 32-bit integers (longs).For all sample widths, sample values map linearly to intermediate signal amplitudes.ID="92658"To specify the sample width setting of two's complement data for an ALconfig structure, call ID="Media2-4AL74"ALsetwidth(). Its function prototype is:int ALsetwidth ( ALconfig config, long samplesize )where:configis the ALconfig structure for which you want to change the sample widthsamplesizeis a symbolic constant denoting the sample widthCOLUMNS="2"LEFT="0" WIDTH="81"AL_SAMPLE_8LEFT="90" WIDTH="181"1-byte sample width of range -128 to 
127LEFT="0" WIDTH="81"AL_SAMPLE_16LEFT="90" WIDTH="181"2-byte sample width of range -32768 to 
32767LEFT="0" WIDTH="81"AL_SAMPLE_24LEFT="90" WIDTH="181"4-byte sample width of range -8388608 
to 8388607ID="34470"Any ALport that you open with this config will have a sample width of samplesize.ID="76044"ALsetwidth() returns 0 upon successful completion; otherwise it returns -1 and sets an error code that you can retrieve by calling oserror(3C). Possible errors include:COLUMNS="2"LEFT="0" WIDTH="117"AL_BAD_CONFIGLEFT="125" WIDTH="180"config is either invalid or nullLEFT="0" WIDTH="117"AL_BAD_WIDTHLEFT="125" WIDTH="180"samplesize is not one of AL_SAMPLE_8, 
AL_SAMPLE_16, or AL_SAMPLE_24To retrieve the current sample width setting of an ALconfig structure, call ALgetwidth()ID="Media2-4AL75". Its function prototype is:long ALgetwidth ( ALconfig config )where:configis the ALconfig structure being queriedALgetwidth() returns the samplesize of config upon successful completion; otherwise, it returns -1 and sets an error code that you can retrieve by calling oserror(3C). Possible errors include:COLUMNS="2"LEFT="0" WIDTH="117"AL_BAD_CONFIGLEFT="125" WIDTH="180"config is either invalid or nullLBL="" HELPID=""ID="67707"Getting and Setting the Floating Point Data RangeIf you configure an ALport to use floating point data (a sample format of either AL_SAMPFMT_FLOAT or AL_SAMPFMT_DOUBLE), you need to define a maximum value in order to set the upper and lower bounds of the samples that pass through that port. Setting the floating point maximum value specifies a symmetrical range that is centered about zero. ID="Media2-4AL76"TipTo have more control over the scaling, you can program your application to perform its own floating point-to-integer conversion before passing samples through the ALport. ID="Media2-4AL77"To set the maximum value of floating point data, call ALsetfloatmax(). Its function prototype is:int ALsetfloatmax ( ALconfig config, double maximum_value )where:COLUMNS="2"LEFT="0" WIDTH="90"configLEFT="95" WIDTH="243"is the ALconfig structure for which you want to set the 
floating point maximum valueLEFT="0" WIDTH="90"maximum_valueLEFT="95" WIDTH="243"is an IEEE double-precision floating point value, 
which defines the range of floating point data for the 
ALreadsamps() or ALwritesamps() functionsSamples read into any ALport that you open with this config are scaled to the range [-maximum_value, maximum_value]. Samples output from this ALport should be in the range [-maximum_value, maximum_value] to avoid limiting. The default maximum value is 1.0.ID="Media2-4AL78"NoteThe number of quantization steps that can be represented by floating point samples is a function of the value of ID="Media2-4AL79"maximum_value. If maximum_value is too small, you will not be able to represent 216 evenly spaced amplitude levels.   ID="61930"ALsetfloatmax() has no function when the sample format is AL_SAMPFMT_TWOSCOMP; however, the maximum_value setting takes effect if the sample format is subsequently changed to AL_SAMPFMT_FLOAT or AL_SAMPFMT_DOUBLE.ALsetfloatmax() returns 0 upon successful completion; otherwise, it returns -1 and sets an error code that you can retrieve by calling oserror(3C). Possible errors include:COLUMNS="2"LEFT="0" WIDTH="117"AL_BAD_CONFIGLEFT="125" WIDTH="180"config is either invalid or nullLEFT="0" WIDTH="117"AL_BAD_FLOATMAXLEFT="125" WIDTH="180"maximum_value is zeroTo retrieve the floating point maximum value, call ID="Media2-4AL80"ALgetfloatmax(). Its function prototype is:double ALgetfloatmax ( Alconfig config )where:configis the ALconfig structure being queriedALgetfloatmax() returns the maximum_value of config upon successful completion; otherwise, it returns 0 and sets an error code that you can retrieve by calling oserror(3C). Possible errors include:COLUMNS="2"LEFT="0" WIDTH="117"AL_BAD_CONFIGLEFT="125" WIDTH="180"config is either invalid or nullLBL="" HELPID=""Retrieving the Setup of an Existing ALportYou can retrieve an ALconfig whose settings match those of an existing ALport. This is an easy way to create an ALconfig to use for changing the dynamic settings of an ALport, as described next in ID="Media2-4AL81"IDREF="88638" TYPE="TITLE""Modifying the Audio Data Attributes of an Open ALport".To retrieve a new ALconfig structure that is a clone of an existing ALconfig structure already in use by an existing audio port, call ID="Media2-4AL82"ALgetconfig(). Its function prototype is:ALconfig ALgetconfig ( ALport port )where:portis the audio port whose ALconfig structure is being clonedYou should call ID="Media2-4AL83"ALfreeconfig() to deallocate the ALconfig when it is no longer needed.ID="Media2-4AL84"ALgetconfig() returns an ALconfig structure upon successful completion; otherwise, it returns 0 and sets an error code that you can retrieve by calling oserror(3C). Possible errors include:COLUMNS="2"LEFT="0" WIDTH="135"AL_BAD_PORTLEFT="140" WIDTH="198"port is either invalid or nullLEFT="0" WIDTH="135"AL_BAD_OUT_OF_MEMLEFT="140" WIDTH="198"insufficient memory available to allocate 
ALconfig structureLBL="" HELPID=""ID="88638"Modifying the Audio Data Attributes of an Open ALportIn general, you don't change the settings for an ALport while it is open, but sometimes you might need to modify the audio data attributes of an ALport while it is open. For example, to create continuous output from multiple sound files that have different sample widths, such as 8-bit and 16-bit data, an application might need to change the sample width of the output port to match the output data, without closing and reopening the port, in order to prevent interruptions in the output.To change the data attributes of an ALport instantaneously, use ALsetsampfmt(), ALsetfloatmax(), and ALsetwidth() as needed to define the settings of an ALconfig, which you then pass to the ALsetconfig() routine. The only settings that can be modified with this method are the sample format, the sample width, and the maximum floating point value. You can't use this method to change the audio device, the queue size, or the number of channels in an ALport.ALsetconfig() changes an audio port's ALconfig structure to match that of a given ALconfig. Its function prototype is:int ALsetconfig ( ALport port, ALconfig config )where:portis the audio port for which you want to change the ALconfig settingsconfigis the ALconfig from which the new settings are copiedALsetconfig() returns 0 upon successful completion; otherwise, it returns -1 and sets an error code that you can retrieve by calling oserror(3C). Possible errors include:COLUMNS="2"LEFT="0" WIDTH="117"AL_BAD_PORTLEFT="125" WIDTH="180"port is either invalid or nullLEFT="0" WIDTH="117"AL_BAD_CONFIGLEFT="125" WIDTH="180"config is either invalid or nullLEFT="0" WIDTH="117"AL_BAD_DEVICELEFT="125" WIDTH="180"port and config have conflicting device 
settingsLEFT="0" WIDTH="117"AL_BAD_QSIZELEFT="125" WIDTH="180"port and config have conflicting values 
for queuesizeLEFT="0" WIDTH="117"AL_BAD_CHANNELSLEFT="125" WIDTH="180"port and config have conflicting values 
for channels settingLBL="" HELPID=""Freeing Resources Associated with an ALconfigTo minimize memory consumption, you should free the memory associated with an ALconfig that is no longer needed. An ALconfig is no longer needed if the application is not going to open any more ports with it.To deallocate an ALconfig structure, call ALfreeconfig(). Its function prototype is:int ALfreeconfig ( ALconfig config )where:configis the ALconfig to deallocate. Freeing an ALconfig structure does not affect any port(s) that have already been opened using that ALconfigALfreeconfig() returns 0 on successful completion; otherwise, it returns -1 and sets an error code that you can retrieve by calling oserror(3C). Possible errors include:COLUMNS="2"LEFT="0" WIDTH="117"AL_BAD_CONFIGLEFT="125" WIDTH="180"config is either invalid or nullLBL="" HELPID=""Opening and Closing Audio PortsAn ALport provides a one-way (input or output) mono, stereo, or 4-channel audio data connection between an application program and the host audio system. More than one ALport can be opened by the same application; the number of ALports that can be active at the same time depends on the hardware and software configurations you are using. Open ALports use CPU resources, so be sure to close an ALport when I/O is completed and free the ALconfig when it is no longer needed.ID="Media2-4AL85"Audio ports are opened and closed by using ALopenport() and ALcloseport(), respectively. Unless you plan to use the default port configuration, you should set up an ALconfig structure by using ALnewconfig() and then use the routines for setting ALconfig fields, such as ALsetchannels(), ALsetqueuesize(), and ALsetwidth() before calling ALopenport().To allocate and initialize an ALport structure, call ID="Media2-4AL86"ID="Media2-4AL87"ALopenport(). Its function prototype is:ALport ALopenport ( char *name, char *direction,                    ALconfig config )where:nameis an ASCII string used to identify the port for humans (much like a window title in a graphics program). The name is limited to 20 characters and should be both descriptive and unique, such as an acronym for your company name or the application name, followed by the purpose of the portID="Media2-4AL88"directionspecifies whether the port is for input or output:COLUMNS="2"LEFT="0" WIDTH="32""r"LEFT="40" WIDTH="228"configures the port for reading (input)LEFT="0" WIDTH="32""w"LEFT="40" WIDTH="228"configures the port for writing (output)configis an ALconfig that you have previously defined or is null (0) for the default configuration.Upon successful completion, ALopenport() returns an ALport structure for the named port; otherwise, it returns a null-valued ALport, and sets an error code that you can retrieve by calling oserror(3C). Possible errors include:COLUMNS="2"LEFT="0" WIDTH="144"AL_BAD_CONFIGLEFT="150" WIDTH="189"config is either invalid or nullLEFT="0" WIDTH="144"AL_BAD_DIRECTIONLEFT="150" WIDTH="189"direction is invalidLEFT="0" WIDTH="144"AL_BAD_OUT_OF_MEMLEFT="150" WIDTH="189"insufficient memory available to allocate 
the ALport structureLEFT="0" WIDTH="144"AL_BAD_DEVICE_ACCESSLEFT="150" WIDTH="189"audio hardware is inaccessibleLEFT="0" WIDTH="144"AL_BAD_NO_PORTSLEFT="150" WIDTH="189"no audio ports currently availableALcloseport() ID="Media2-4AL89"closes and deallocates an audio portname='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'any samples remaining in the port will not be output. Its function prototype is:ID="Media2-4AL90"int ALcloseport ( ALport port )where:portis the ALport you want to closeIDREF="64071" TYPE="TEXT"Example 6-2 opens an input port and an output port and then closes them.LBL="6-2"Example 6-2 ID="64071"Opening Input and Output ALports ID="Media2-4AL91"input_port = ALopenport("waycoolinput", "r", 0);
if (input_port == (ALport) 0 {
err = oserror();
if (err == AL_BAD_NO_PORTS) {
fprintf(stderr, " System is out of audio ports\n");
} else if (err == AL_BAD_DEVICE_ACCESS) {
fprintf(stderr, " Couldn't access audio device\n");
} else if (err == AL_BAD_OUT_OF_MEM) {
fprintf(stderr, " Out of memory: port open failed\n");
}
exit(1);
}
...
output_port = ALopenport("killeroutput", "w", 0);
if (input_port == (ALport) 0 {
err = oserror();
if (err == AL_BAD_NO_PORTS) {
fprintf(stderr, " System is out of audio ports\n");
} else if (err == AL_BAD_DEVICE_ACCESS) {
fprintf(stderr, " Couldn't access audio device\n");
} else if (err == AL_BAD_OUT_OF_MEM) {
fprintf(stderr, " Out of memory: port open failed\n");
}
exit(1);
...
ALcloseport(input_port);
ALcloseport(output_port);LBL="" HELPID=""ID="43276"Reading and Writing Audio DataThis section explains how an audio application reads and writes audio samples to and from ALports. LBL="" HELPID=""Using Audio Sample QueuesAudio samples are placed in the sample queue of an ALport to await input or output (see ID="Media2-4AL92"IDREF="74750" TYPE="GRAPHIC"Figure 6-2). The audio system uses one end of the sample queue; the audio application uses the other end.During audio input (left side of IDREF="74750" TYPE="GRAPHIC"Figure 6-2), the audio hardware continuously writes audio samples to the tail of the input queue at the selected input rate, for example, 44,100 sample pairs per second for 44.1 kHz stereo data. If the application can't read the samples from the head of the input queue at least as fast as the hardware writes them, the queue fills up and some incoming sample data is irretrievably lost.During audio output (right side of IDREF="74750" TYPE="GRAPHIC"Figure 6-2), the application writes audio samples to the tail of the queue. The audio hardware continuously reads samples from the head of the output queue at the selected output rate, for example, 44,100 sample pairs per second for 44.1 kHz stereo data, and sends them to the outputs. If the application can't put samples in the queue as fast as the hardware removes them, the queue empties, causing the hardware to send 0-valued samples to the outputs (until more data is available), which are perceived as pops or breaks in the sound.For example, if an application opens a stereo output port with a queue size of 100,000, and the output sample rate is set to 48 kHz, the application needs to supply (2 name='times' font=symbol charset=fontspecific code=180
	TeX='\times ' descr='[times]' 48,000 = 96,000) samples to the output port at the rate of at least 1 set of samples per second, because the port contains enough space for about one second of stereo data at that rate. If the application fails to supply data at this rate, an audible break occurs in the audio output.On the other hand, if an application tries to put 40,000 samples into a queue that already contains 70,000 samples, there isn't enough space in the queue to store all the new samples, and the program will block (wait) until enough of the existing samples have been removed to allow for all 40,000 new samples to be put in the queue. The AL routines for reading and writing block; they do not return until the input or output is complete.IDREF="74750" TYPE="GRAPHIC"Figure 6-2 shows how input and output ports use audio sample queues.ID="Media2-4AL93"ID="Media2-4AL94"FILE="6-2.online.bw" POSITION="INLINE" SCALE="FALSE"LBL="6-2"Figure 6-2 ID="74750"Audio Sample Queues LBL="" HELPID=""Monitoring the Audio Sample Queue Status to Provide Nonblocking I/OThis section explains how to use the AL routines for monitoring the status of an ALport's sample queue.ID="Media2-4AL95"The AL maintains the following status information about the queue:filledID="Media2-4AL96"the number of queue locations that contain valid datafillableID="Media2-4AL97"the number of empty locations in the queueThe sum of the empty locations and the full locations is the total size of the queue:ID="Media2-4AL98"filled + fillable = queuesizeChecking the filled and fillable statuses before reading and writing prevents blocking and helps prevent overflow and underflow errors.ALgetfillable() ID="Media2-4AL99"and ID="Media2-4AL100"ALgetfilled() provide instantaneous information on the state of an audio port's queue.To prevent blocking during output, you can determine how many samples will fit into the queue by calling ALgetfillable() before writing any samples, and then write only that many samples to the queue.To get the number of empty queue locations in a given ALport, call ALgetfillable(). Its function prototype is:long ALgetfillable ( ALport port )where:portis the audio port whose queue is being examinedThe value returned indicates how many samples can still be written without blocking.To prevent blocking during input, you can determine how many samples are in the queue by calling ID="Media2-4AL101"ALgetfilled() before reading any samples, then read only that many samples from the queue. You can also periodically check ALgetfilled() to find out whether all of your output data has drained before you shut down a port by calling ALcloseport().To find out how many queue locations in a given audio port currently have valid samples in them at a given instant, call ID="Media2-4AL102"ALgetfilled(). Its function prototype is:long ALgetfilled ( ALport port )where:portis the audio port whose queue is being examinedThe value returned indicates how many samples can still be read without blocking if port is an input port or how many samples have yet to be played if it is an output port.LBL="" HELPID=""More Methods for Working with QueuesBesides using these routines, you can use ID="Media2-4AL103"ALgetstatus() to check for underflow and overflow errors, as described in ID="Media2-4AL104"ID="Media2-4AL105"IDREF="95308" TYPE="TITLE""Detecting Errors in the Audio Stream".IDREF="63440" TYPE="TITLE""Real-time Programming Techniques for Audio" discusses how to use several other routines that allow an application to view and modify the dynamic state of an audio port. These routines are most useful in developing real-time audio applications.LBL="" HELPID=""Reading and Writing SamplesAudio input is accomplished by reading audio data samples from an input ALport's sample queue. Similarly, audio output is accomplished by writing audio data samples to an output ALport's sample queue.ID="Media2-4AL106"ALreadsamps() and ALwritesamps() provide mechanisms for transferring audio samples to and from sample queues. They are blocking routines, which means that a program will halt execution within the ALreadsamps() or ALwritesamps() call until the request to read or write samples can be completed.LBL="" HELPID=""Reading Samples from an Input ALportALreadsamps()ID="Media2-4AL107" reads a specified number of samples from an input port to a sample data buffer, blocking until the requested number of samples have been read from the port. Its function prototype is:ID="Media2-4AL108"int ALreadsamps( const ALport port, void *samples,
                 const long samplecount )where:portis an audio port configured for inputsamplesis a pointer to a buffer into which you want to transfer the samples read from input. samples is treated as one of the following types, depending on the configuration of the ALport:COLUMNS="2"LEFT="0" WIDTH="45"char *LEFT="50" WIDTH="216"for integer samples of width AL_SAMPLE_8LEFT="0" WIDTH="45"short *LEFT="50" WIDTH="216"for integer samples of width AL_SAMPLE_16LEFT="0" WIDTH="45"long *LEFT="50" WIDTH="216"for integer samples of width AL_SAMPLE_24LEFT="0" WIDTH="45"float *LEFT="50" WIDTH="216"for floating point samplesLEFT="0" WIDTH="45"double *LEFT="50" WIDTH="216"for double-precision floating point samplessamplecountis the number of samples to readTo prevent blocking, ID="Media2-4AL109"samplecount must be less than the return value of ALgetfilled().NoteWhen the application is reading samples into an ALport that has channels set to 4, samplecount must be an integer multiple of the frame size, or an error will be returned and no samples will be transferred. When 4-channel data is input on systems that do not support 4 line-level electrical connections, that is, when setting AL_CHANNEL_MODE to AL_4CHANNEL is not possible, ALreadsamps() will provide 4 samples per frame, but the second pair of samples will be set to 0.IDREF="21765" TYPE="TABLE"Table 6-2 shows the input conversions that are applied when reading mono, stereo, and 4-channel input in stereo mode (default) and in 4-channel mode hardware configurations. Each entry in the table represents a sample frame.ID="Media2-4AL110"COLUMNS="3"LBL="6-2"Table 6-2 ID="21765" Input Conversions for ALreadsamps()ID="Media2-4AL111" LEFT="0" WIDTH="73"LEFT="80" WIDTH="123"Hardware ConfigurationLEFT="210" WIDTH="135"LEFT="0" WIDTH="73"InputLEFT="80" WIDTH="123"Indigo, and Indigo2 or Indy in 
Stereo ModeLEFT="210" WIDTH="135"Indigo2 or Indy in 4-channel ModeLEFT="0" WIDTH="73"Frame at 
physical inputsLEFT="80" WIDTH="123"(L1, R1)LEFT="210" WIDTH="135"(L1, R1, L2, R2)LEFT="0" WIDTH="73"Frame as read by 
a mono port LEFT="80" WIDTH="123"(L1 + R1) /2LEFT="210" WIDTH="135"(Clip (L1 + L2), Clip (R1 + R2)) /2LEFT="0" WIDTH="73"Frame as read by 
a stereo portLEFT="80" WIDTH="123"(L1, R1)LEFT="210" WIDTH="135"(Clip (L1 + L2), Clip (R1 + R2))LEFT="0" WIDTH="73"Frame as read by 
a 4-channel portLEFT="80" WIDTH="123"(L1, R1, 0, 0)LEFT="210" WIDTH="135"(L1, R1, L2, R2)NoteIf the summed signal is greater than the maximum allowed by the audio system, it is clipped (limited) to that maximum, as indicated by the Clip function. LBL="" HELPID=""Writing Samples to an Output ALportSamples placed in an output queue are played by the audio hardware after a specific amount of time, which is equal to the number of samples that were present in the queue before the new samples were written, divided by the (sample rate name='times' font=symbol charset=fontspecific code=180
	TeX='\times ' descr='[times]' number of channels) settings of the ALport.ID="Media2-4AL112"ID="Media2-4AL113"ALwritesamps() ID="Media2-4AL114"writes a specified number of samples to an output port from a sample data buffer, blocking until the requested number of samples have been written to the port. Its function prototype is:int ALwritesamps ( ALport port, void *samples,                   long samplecount )where:portis an audio port configured for inputsamplesis a pointer to a buffer from which you want to transfer the samples to the audio portsamplecountis the number of samples you want to readNoteWhen the application is writing samples from an ALport that has ID="Media2-4AL115"channels set to 4, samplecount must be an integer multiple of the frame size, or an error will be returned and no samples will be transferred. IDREF="70336" TYPE="TABLE"Table 6-3 shows the output conversions that are applied when writing mono, stereo, and 4-channel data to stereo mode (default) and 4-channel mode hardware configurations. ID="Media2-4AL116"COLUMNS="4"LBL="6-3"Table 6-3 ID="70336"Output Conversions for ALwritesamps() LEFT="0" WIDTH="54"LEFT="60" WIDTH="72"LEFT="140" WIDTH="119"Hardware ConfigurationLEFT="265" WIDTH="87"LEFT="0" WIDTH="54"OutputLEFT="60" WIDTH="72"Frame as 
Written into PortLEFT="140" WIDTH="119"Indigo, and Indigo2 or Indy in 
Stereo ModeLEFT="265" WIDTH="87"Indigo2 or Indy in4-channel ModeLEFT="0" WIDTH="54"Mono Port LEFT="60" WIDTH="72"(L1)LEFT="140" WIDTH="119"(L1, L1)LEFT="265" WIDTH="87"(L1, L1, 0, 0)LEFT="0" WIDTH="54"Stereo PortLEFT="60" WIDTH="72"(L1, R1)LEFT="140" WIDTH="119"(L1, R1)LEFT="265" WIDTH="87"(L1, R1, 0, 0)LEFT="0" WIDTH="54"4-channel 
PortLEFT="60" WIDTH="72"(L1, R1, L2, R2)LEFT="140" WIDTH="119"(Clip (L1 + L2), Clip (R1 + R2))LEFT="265" WIDTH="87"(L1, R1, L2, R2)LBL="" HELPID=""ID="95308"Detecting Errors in the Audio StreamErrors in an input or output audio stream may occur if an application is unable to read samples from or write samples to a queue fast enough to satisfy the demands of the real-time hardware.ID="Media2-4AL117"This section explains how to use two AL routines that let you identify errors and define custom error-reporting functions.If a program cannot provide samples to an output port fast enough to keep up with the hardware, an audible break in the output may be heard. Likewise, if an application does not read input samples as fast as the hardware puts them in the queue, some samples will be lost.ID="Media2-4AL118"The audio system detects such discontinuities in audio sample streams, and information concerning these breaks can be gathered by the application. This information can be used to dynamically tune the application execution, to increase the priority of a process, or merely to alert the user to errors.ID="Media2-4AL119"ID="Media2-4AL120"ALgetstatus() ID="Media2-4AL121"provides access to information regarding the most recent error in the audio stream associated with a port. Its function prototype is:int ALgetstatus ( Alport port, long *PVbuffer,
                  long bufferlength )where:portis the audio port being queriedPVbufferis an array of longs, the even elements of which should contain the error parameters you want to readbufferlengthis the number of elements in the PVbuffer arrayThe odd element directly following each parameter will then be written with the current values associated with each corresponding parameter.ALgetstatus() lets you determine the number of errors associated with the stream, the type of the last error, the length of the last error, and the location of the error relative to the total lifetime of the port. The location of the error marks the point in the port's lifetime, that is, the time since the port was opened, at which the error was detected. This value is a 48-bit number representing the number of sample frames sent through the port. The value is generated by concatenating the least significant 24 bits of the values associated with AL_ERROR_LOCATION_LSP and AL_ERROR_LOCATION_MSP.IDREF="60703" TYPE="TABLE"Table 6-4 lists and describes the error parametersID="Media2-4AL122"COLUMNS="2"LBL="6-4"Table 6-4 ID="60703"Error Parameters for ALgetstatus()LEFT="0" WIDTH="129"Error ParameterLEFT="135" WIDTH="203"DescriptionLEFT="0" WIDTH="129"AL_ERROR_LENGTHLEFT="135" WIDTH="203"Current length in sample frames of the current 
error. Consecutive values of this variable may 
differ if the current error has not completed. Only 
the least significant 24 bits of this variable are 
valid.LEFT="0" WIDTH="129"AL_ERROR_LOCATION_LSPLEFT="135" WIDTH="203"Least significant portion (LSP) of the location of 
the beginning of the current error. Only the least 
significant 24 bits of this variable are valid.LEFT="0" WIDTH="129"AL_ERROR_LOCATION_MSPLEFT="135" WIDTH="203"Most significant portion of the location of the 
beginning of the current error (in sample frames). 
Only the least significant 24 bits of this variable are 
valid.LEFT="0" WIDTH="129"AL_ERROR_NUMBERLEFT="135" WIDTH="203"Number of errors associated with the port since it 
was opened.LEFT="0" WIDTH="129"AL_ERROR_TYPELEFT="135" WIDTH="203"Type of error that has most recently occurred on 
the port. Supported types are 
AL_ERROR_INPUT_OVERFLOW and 
AL_ERROR_OUTPUT_UNDERFLOW.ID="Media2-4AL123"LBL="" HELPID=""ID="81909"Querying and Controlling the Global Audio Device StateThis section explains how to use the AL routines for querying and modifying the global audio device state. Your application should query for the availability of special audio features because different workstations have different capabilities, and because programming in this way makes it easy to update your application when new features are added.ID="Media2-4AL124"Because the audio device is a shared resource, it is especially important to query whether other audio applications are running, so that your application does not inadvertently change a setting upon which another application relies. If no other audio applications are running, your program can use the AL routines described in this section to modify the settings of the state variables, but an application should always verify that it is the only audio application in use before changing any system-wide settings.There is a core set of parameters that exists on every system and special parameters for capabilities such as 4-channel mode and stereo mic mode that don't exist on all configurations. To query for the availability of a noncore parameter, you have to query for both its existence and whether it supports the settings that you require. It is not necessary to query for the existence of core parameters.ID="Media2-4AL125"IDREF="49733" TYPE="TABLE"Table 6-5 lists the core set of global parameters, describes their roles, and provides valid values.ID="Media2-4AL126"COLUMNS="2"LBL="6-5"Table 6-5 ID="49733"Core Global Parameters for AL_DEFAULT_DEVICELEFT="0" WIDTH="125"Global ParameterLEFT="130" WIDTH="378"Description and Valid ValuesLEFT="0" WIDTH="125"AL_INPUT_SOURCELEFT="130" WIDTH="378"Selects the active input source:AL_INPUT_LINEname='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'line-level input jackAL_INPUT_MICname='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'microphone input jackAL_INPUT_DIGITALname='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'serial digital input jackID="Media2-4AL127"LEFT="0" WIDTH="125"AL_LEFT_INPUT_ATTENLEFT="130" WIDTH="378"Controls the left input attenuation level for both the line-in level and the microphone level. 
Range = 0-255, 0 = no attenuation, 255 = maximum attenuation.ID="Media2-4AL128"LEFT="0" WIDTH="125"AL_RIGHT_INPUT_ATTENLEFT="130" WIDTH="378"Controls the right input attenuation level for both the line-in level and the microphone level. 
Range = 0-255, 0 = no attenuation, 255 = maximum attenuation.LEFT="0" WIDTH="125"AL_INPUT_RATELEFT="130" WIDTH="378"Indicates the sample rate at the analog (line or microphone) inputs. A positive value indicates 
a specific sampling rate in Hz. The AL rounds unsupported values to the nearest supported 
value.ID="Media2-4AL129"A negative value indicates a logical value, including AL_RATE_AES_1, meaning to match the 
analog sampling rate to the rate at which data is arriving at the digital input.Note that AL_INPUT_RATE does not apply when the digital input jack is in use. The digital 
input data stream has its own sample rate, which is determined strictly by the device 
generating the digital data.LEFT="0" WIDTH="125"AL_OUTPUT_RATELEFT="130" WIDTH="378"Indicates the sample rate at the analog and digital outputs. A positive value indicates a specific 
sampling rate in Hz. The AL rounds unsupported values to the nearest supported value. A negative value indicates a logical value, such as AL_RATE_INPUT_RATE, meaning to match 
the output sample rate to the rate used by the currently active input, or AL_RATE_AES_1, 
meaning to match the output sample rate to the rate at which samples are arriving at the digital 
input.LEFT="0" WIDTH="125"AL_LEFT_SPEAKER_GAINLEFT="130" WIDTH="378"Controls the left speaker and headphone volume levels; does not affect line-level and digital 
outputs. Range = 0-255, 0 = no gain, 255 = maximum gain. Zero gain does not necessarily mean 
zero volume.ID="Media2-4AL130"ID="Media2-4AL131"LEFT="0" WIDTH="125"AL_RIGHT_SPEAKER_GAINLEFT="130" WIDTH="378"Controls the right speaker and headphone volume levels; does not affect line-level and digital 
outputs. Range = 0-255, 0 = no gain, 255 = maximum gain. Zero gain does not necessarily mean 
zero volume.LEFT="0" WIDTH="125"AL_INPUT_COUNTLEFT="130" WIDTH="378"Read-only value that indicates the number of system-wide open input ALports.ID="Media2-4AL132"LEFT="0" WIDTH="125"AL_OUTPUT_COUNTLEFT="130" WIDTH="378"Read-only value that indicates the number of system-wide open output ALports.LEFT="0" WIDTH="125"AL_UNUSED_COUNTLEFT="130" WIDTH="378"Read-only value that indicates the number of system-wide unopened ALports.LEFT="0" WIDTH="125"AL_MONITOR_CTLLEFT="130" WIDTH="378"Controls monitoring. When monitoring is enabled, audio input is passed through to the output. 
Input and output sample rates must be precisely matched to prevent distortion. 
AL_MONITOR_ON enables monitoring; AL_MONITOR_OFF disables monitoring.ID="Media2-4AL133"LEFT="0" WIDTH="125"AL_SPEAKER_MUTE_CTLLEFT="130" WIDTH="378"AL_SPEAKER_MUTE_ON mutes speaker and headphones; AL_SPEAKER_MUTE_OFF 
unmutes speaker and headphones. Any change to AL_LEFT_SPEAKER_GAIN or 
AL_RIGHT_SPEAKER_GAIN shuts off speaker muting.ID="Media2-4AL134"IDREF="30574" TYPE="TABLE"Table 6-6 lists and describes special parameters that are available on some systems. You should query for the existence of these parameters ID="Media2-4AL135"and whether they support the required values before using them.COLUMNS="2"LBL="6-6"Table 6-6 ID="30574"Special Global Parameters for System-Dependent Audio CapabilitiesLEFT="0" WIDTH="143"Global ParameterLEFT="150" WIDTH="215"Description and Valid ValuesLEFT="0" WIDTH="143"AL_CHANNEL_MODELEFT="150" WIDTH="215"Configures the audio hardware. AL_STEREO 
configures the hardware for stereo audio; 
AL_4CHANNEL configures the hardware for 4-
channel audio on systems that support it.ID="Media2-4AL136"LEFT="0" WIDTH="143"AL_MIC_MODELEFT="150" WIDTH="215"Selects the microphone mode. AL_MONO selects the 
mono microphone; AL_STEREO selects stereo mic 
input on systems that support it.ID="Media2-4AL137"LEFT="0" WIDTH="143"AL_LEFT2_INPUT_ATTENLEFT="150" WIDTH="215"Controls the attenuation for the L2 line-level or mic-
level input.ID="Media2-4AL138"LEFT="0" WIDTH="143"AL_RIGHT2_INPUT_ATTENLEFT="150" WIDTH="215"Controls the attenuation for the R2 line-level or mic-
level input.LEFT="0" WIDTH="143"AL_LEFT_MONITOR_ATTENLEFT="150" WIDTH="215"Controls the attenuation for the left half of the 
monitor signal. Range = 0-255, 0 = no attenuation, 
255 = maximum attenuation.LEFT="0" WIDTH="143"AL_RIGHT_MONITOR_ATTENLEFT="150" WIDTH="215"Controls the attenuation for the right half of the 
monitor signal. Range = 0-255, 0 = no attenuation, 
255 = maximum attenuation.LEFT="0" WIDTH="143"AL_DIGITAL_INPUT_RATELEFT="150" WIDTH="215"Read-only value; sample rate at which data is 
arriving at the digital input. The rate is that signified 
by the nonaudio bits of the incoming digital signal; it 
is not actually measured. A positive value indicates a 
specific sampling rate in Hz.ID="Media2-4AL139"A negative value indicates a logical value, including 
AL_RATE_UNDEFINED, meaning that the audio 
system could not determine the digital input data 
rate, or the device generating the digital data has 
marked the data as having an indeterminate rate.Note that the digital input data stream contains its 
own clock signal; thus, its notion of a given rate will 
differ slightly from an internally generated version of 
the same rate. LBL="" HELPID=""Techniques for Working with Global ParametersThe AL routines for working with parameters are:COLUMNS="2"LEFT="0" WIDTH="93"ALqueryparams()LEFT="100" WIDTH="210"determines possible hardware parametersLEFT="0" WIDTH="93"ALgetparams()LEFT="100" WIDTH="210"gets current settings of hardware parametersLEFT="0" WIDTH="93"ALsetparams()LEFT="100" WIDTH="210"sets hardware parametersLEFT="0" WIDTH="93"ALgetminmax()LEFT="100" WIDTH="210"gets bounds of hardware parametersLEFT="0" WIDTH="93"ALgetdefault()LEFT="100" WIDTH="210"gets default values of hardware parametersLEFT="0" WIDTH="93"ALgetname()LEFT="100" WIDTH="210"returns name for an audio device state variableAll of these routines expect a device argument of type long, representing the particular audio device whose state you want to know or change. The only currently supported device is AL_DEFAULT_DEVICE.Several of these routines expect parameter-value buffer (ID="Media2-4AL140"PVbuffer) arguments. A PVbuffer is simply an array of long integers, where the integers are logically organized as pairs of elements. The first element of each pair is a parameter constant defined in the include file audio.h. The second element of each pair stores a value associated with the parameter. The second location can be used to pass a value for a parameter into a routine or to return a value for a given parameter from a routine.TipYou don't have to pass an array containing all of the possible parameters; create an array that contains only the values of interest. Some methods for using these routines are:If you need a complete list of all available parameters, call ID="Media2-4AL141"ALqueryparams(). To be certain that you have a large enough buffer to contain the parameter-value pairs, you can pass a zero in place of the buffer, then call malloc() to allocate a buffer the size of the returned value.If you are interested only in certain values, create an array that is twice the size of the number of parameters you are querying, and fill the even locations with the parameters of interest, then: call ALgetparams() to determine the current settings of the state variables.fill in the even entries with the values that you want to change, and then call ALsetparams() to change the values.Some parameters might exist but might not allow the needed settings, so call ALgetminmax() to get the parameter bounds and check to be sure that the values you want to use exist.LBL="" HELPID=""Getting a List of Available ParametersALqueryparams() ID="Media2-4AL142"asks the audio device to supply a list of descriptors and corresponding descriptions for all the currently available global state variables. Its function prototype is:long ALqueryparams ( long device, long *PVbuffer,
                     long bufferlength )where:deviceis the audio device (AL_DEFAULT_DEVICE)PVbufferis an array of longs, into which ALqueryparams() writes a descriptor/description pair for each state variable associated with device. The even (0, 2, 4, name='hellip' font=symbol charset=fontspecific code=188) entries receive the descriptors. The odd entries (1, 3, 5, name='hellip' font=symbol charset=fontspecific code=188) receive one of two description values (negative values indicate read-only parameters):COLUMNS="2"LEFT="0" WIDTH="115"name='plusmn' font=symbol charset=fontspecific code=177 
	TeX='\pm ' descr='[plusmn]' AL_RANGE_VALUELEFT="120" WIDTH="145"means that the associated 
device state variable can 
assume a range of values in 
which the relative magnitude of 
a value has a meaning; that is, 
larger values mean an increase 
in whatever the parameter 
controlsLEFT="0" WIDTH="115"name='plusmn' font=symbol charset=fontspecific code=177 
	TeX='\pm ' descr='[plusmn]' AL_ENUM_VALUELEFT="120" WIDTH="145"means that the associated 
device state variable assumes 
values from an enumerated 
typename='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'the range is limited, but 
there is no inherent relationship 
between valuesbufferlengthis the number of elements in the PVbuffer arrayALqueryparams() returns a long value representing the buffer size necessary to hold all parameters and their values. If your PVbuffer is of smaller dimensions than this value, you have not received a complete set of descriptor/description pairs for device. See IDREF="49733" TYPE="TABLE"Table 6-5 for a list of currently supported core global parameters. See IDREF="30574" TYPE="TABLE"Table 6-6 for a list of special global parameters that are not supported on all systems.ALsetparams()ID="Media2-4AL143" lets you modify the values of many of these global parameters, though you should take care in using these functions. See the description of ALsetparams() at the end of this section for details.LBL="" HELPID=""Getting the Bounds of Global ParametersALgetminmax() ID="Media2-4AL144"obtains maximum and minimum values for a given global parameter. Its function prototype is:ID="Media2-4AL145"int ALgetminmax( long device, long param, long *minparam,
                 long *maxparam )where:deviceis the audio device (AL_DEFAULT_DEVICE)paramis the parameter whose range you want to knowminparamis a pointer to a variable into which the minimum value will be writtenmaxparamis a pointer to a variable into which the maximum value will be writtenLBL="" HELPID=""Getting the Defaults of Global ParametersALgetdefault() ID="Media2-4AL146"returns the default value for a given audio hardware device state parameter. Its function prototype is:ID="Media2-4AL147"long ALgetdefault ( long device, long parameter )where:deviceis the audio device (AL_DEFAULT_DEVICE)parameteris the parameter whose default value you want to obtainLBL="" HELPID=""Getting the Names Corresponding to the Global ParametersALgetname() ID="Media2-4AL148"ID="Media2-4AL149"returns a pointer to a null-terminated string that can be used to label an audio hardware device state parameter. Treat this string as a read-only string. Its function prototype is:char* ALgetname ( long device, long parameter )deviceis the audio device (AL_DEFAULT_DEVICE) parameteris the parameter whose name you want to knowIDREF="14855" TYPE="TABLE"Table 6-7 lists the global parameter name strings.COLUMNS="2"LBL="6-7"Table 6-7 ID="14855"Global Parameter Name StringsLEFT="0" WIDTH="144"Global Parameter   LEFT="150" WIDTH="119"Name StringLEFT="0" WIDTH="144"AL_INPUT_SOURCELEFT="150" WIDTH="119""Line/MIC/AES"LEFT="0" WIDTH="144"AL_LEFT_INPUT_ATTENLEFT="150" WIDTH="119""Left Input Atten"LEFT="0" WIDTH="144"AL_RIGHT_INPUT_ATTENLEFT="150" WIDTH="119""Right Input Atten"LEFT="0" WIDTH="144"AL_INPUT_RATELEFT="150" WIDTH="119""Input Rate"LEFT="0" WIDTH="144"AL_OUTPUT_RATELEFT="150" WIDTH="119""Output Rate"LEFT="0" WIDTH="144"AL_LEFT_SPEAKER_GAINLEFT="150" WIDTH="119""Left Output Gain"LEFT="0" WIDTH="144"AL_RIGHT_SPEAKER_GAINLEFT="150" WIDTH="119""Right Output Gain"LEFT="0" WIDTH="144"AL_INPUT_COUNTLEFT="150" WIDTH="119""Input Count"LEFT="0" WIDTH="144"AL_OUTPUT_COUNTLEFT="150" WIDTH="119""Output Count"LEFT="0" WIDTH="144"AL_UNUSED_COUNTLEFT="150" WIDTH="119""Unused Count"LEFT="0" WIDTH="144"AL_MONITOR_CTLLEFT="150" WIDTH="119""Monitor Control"LEFT="0" WIDTH="144"AL_LEFT_MONITOR_ATTENLEFT="150" WIDTH="119""Left Monitor Atten"LEFT="0" WIDTH="144"AL_RIGHT_MONITOR_ATTENLEFT="150" WIDTH="119""Right Monitor Atten"LEFT="0" WIDTH="144"AL_SPEAKER_MUTE_CTLLEFT="150" WIDTH="119""Speaker Mute Control"LEFT="0" WIDTH="144"AL_MIC_MODELEFT="150" WIDTH="119""Microphone Mode"LEFT="0" WIDTH="144"AL_CHANNEL_MODELEFT="150" WIDTH="119""System Channel Mode"LEFT="0" WIDTH="144"AL_DIGITAL_INPUT_RATELEFT="150" WIDTH="119""Digital Input Rate"LBL="" HELPID=""Getting Current Parameter SettingsALgetparams() ID="Media2-4AL150"gets the current value(s) of the device parameters referenced in the ID="Media2-4AL151"PVbuffer. Its function prototype is:int ALgetparams ( long device, long *PVbuffer,
                  long bufferlength )where:deviceis the audio device (AL_DEFAULT_DEVICE)PVbufferis an array of pairs of longs, the even (0, 2, 4, name='hellip' font=symbol charset=fontspecific code=188) entries of which should contain the global parameters whose values you want to obtainbufferlengthis the number of elements in the PVbuffer arrayALgetparams() fills the odd (1, 3, 5, name='hellip' font=symbol charset=fontspecific code=188) entries in the PVbuffer array with the current values associated with each corresponding parameter.See IDREF="49733" TYPE="TABLE"Table 6-5 for a description of the currently supported core global parameters. See IDREF="30574" TYPE="TABLE"Table 6-6 for a list of special global parameters that are not supported on all systems.LBL="" HELPID=""Modifying the Values of the Global ParametersALsetparams() ID="Media2-4AL152"sets the current value(s) of one or more audio hardware device parameters. Its function prototype is:ID="Media2-4AL153"int ALsetparams ( long device, long *PVbuffer,
                  long bufferlength )where:deviceis the audio device (AL_DEFAULT_DEVICE)PVbufferis an array of pairs of longs, the even (0, 2, 4, name='hellip' font=symbol charset=fontspecific code=188) entries of which should contain the global parameters whose values you want to change to the corresponding values listed in the odd (1, 3, 5, name='hellip' font=symbol charset=fontspecific code=188) entries. bufferlengthis the number of elements in the PVbuffer arraySee IDREF="49733" TYPE="TABLE"Table 6-5 for a description of the currently supported core global parameters. See IDREF="30574" TYPE="TABLE"Table 6-6 for a list of special global parameters that are not supported on all systems.When an application program modifies a global state parameter such as the output sample rate, it may affect other processes on the system that are also engaged in audio processing. For example, if one application is playing a 44.1 kHz recording through an output port, and a second application changes the global output sample rate from 44.1 kHz to 16 kHz, the output of the original application will be distorted.ID="Media2-4AL154"ID="Media2-4AL155"LBL="" HELPID=""Sample Code for Querying Features and ValuesID="Media2-4AL156"This section provides sample code fragments that demonstrate the proper methods to use when querying for certain attributes.LBL="" HELPID=""Determining Whether Other Audio Applications Are RunningTo determine whether other audio applications are running, query the system for open input or output ports. To determine the total number of ports available on your system, add the values returned for AL_INPUT_COUNT, AL_OUTPUT_COUNT, and AL_UNUSED_COUNT.ID="Media2-4AL157"IDREF="16405" TYPE="TEXT"Example 6-3 demonstrates querying for other active audio output.LBL="6-3"Example 6-3 ID="16405"Querying for the Existence of Other Audio Processes/*
 * 'Nonrude' behavior is defined as follows: before modifying global values, first check
 * to see whether any other output ports are currently active; if any other processes have
 * open output ports, don't modify anything.
 */
...
rude = 0;
...
/*
 * Need to determine whether audio is in use. If not, then we
 * can just go ahead and be "rude."
 */
pvbuf[0] = AL_OUTPUT_COUNT;
pvbuf[2] = AL_MONITOR_CTL;
if (ALgetparams(AL_DEFAULT_DEVICE, pvbuf, 4) < 0) {
    if (oserror() == AL_BAD_DEVICE_ACCESS) {
    fprintf(stderr,"%s: Can't play -- could not access audio hardware.\n");
    return -1;
    }
}
if ((pvbuf[1] == 0) && (pvbuf[3] == AL_MONITOR_OFF)) {
    rude = 1;
    }LBL="" HELPID=""Determining the Input and Output RatesQuerying the system for an input or output rate must be done carefully in order to obtain a valid result. ID="Media2-4AL158"IDREF="18344" TYPE="TEXT"Example 6-4IDREF="18344" TYPE="TEXT"contains two routines, get_input_rate() and get_output_rate(), each of which returns a rate either in Hz or AL_RATE_UNDEFINED if the rate cannot be determined. A minimal main() program calls the routines. See 
ID="Media2-4AL159"ratequery.c
 in /usr/people/4Dgifts/examples/dmedia/audio for another example of rate querying.LBL="6-4"Example 6-4 ID="18344"Querying for Input and Output Rates #include <audio.h>
...
/*
 * These routines expect to be run with the AL error handler shut off.
 * (call ALseterrorhandler(0)).
 */
...
int
get_input_rate()
{
    long buf[6];
...
    buf[0] = AL_INPUT_RATE;
    buf[2] = AL_INPUT_SOURCE;
    buf[4] = AL_DIGITAL_INPUT_RATE;
    ALgetparams(AL_DEFAULT_DEVICE,buf,6);
...
    if (buf[1] == AL_RATE_AES_1 || buf[3] == AL_INPUT_DIGITAL) {
        /*
         * We are clocked off of the digital input. Find the
         * real input rate, if we can. 
         */
        if (ALgetdefault(AL_DEFAULT_DEVICE,AL_DIGITAL_INPUT_RATE) >= 0) {
            return buf[5];
        }
    }
    else if (buf[1] > 0) {
        /*
         * Input rate is in Hz and we're using an analog input -- return rate.
         */
        return buf[1];
    }
    return AL_RATE_UNDEFINED;
}
...
int
get_output_rate()
{
    long buf[4];

    buf[0] = AL_OUTPUT_RATE;
    buf[2] = AL_DIGITAL_INPUT_RATE;
    ALgetparams(AL_DEFAULT_DEVICE,buf,4);
    if (buf[1] > 0) {
        /*
         * Output rate is in Hz -- return it.
         */
        return buf[1];
    }
    else {
        /*
         * Output rate is a logical rate -- track down what it means.
         */
        if (buf[1] == AL_RATE_AES_1) {
            /*
             * We are clocked off of the digital input. Find the
             * real input rate, if we can. If we can't, return AL_RATE_UNDEFINED
             */
            if (ALgetdefault(AL_DEFAULT_DEVICE,AL_DIGITAL_INPUT_RATE) >= 0) {
                return buf[3];
            }
        }
        else if (buf[1] == AL_RATE_INPUTRATE) {
            return get_input_rate();
        }
    return AL_RATE_UNDEFINED;
    }
}
...
main()
{
    int x;
    ALseterrorhandler(0);
    x = get_output_rate();
    if (x == AL_RATE_UNDEFINED) {
        printf("can't get output rate\n");
    }
    else {
        printf("output rate = %d\n",x);
    }
    x = get_input_rate();
    if (x == AL_RATE_UNDEFINED) {
        printf("can't get input rate\n");
    }
    else {
        printf("input rate = %d\n",x);
    }
}LBL="" HELPID=""Determining Whether 4-channel Capability ExistsAlthough you can open a 4-channel ALport on any system, you cannot change the system's electrical configurations if it does not support 4-channel mode.To determine whether a system has 4-channel capability, use ID="Media2-4AL160"ALgetminmax(), then verify that the maximum value is 4.IDREF="68696" TYPE="TEXT"Example 6-5 demonstrates how to query for 4-channel hardware capability.LBL="6-5"Example 6-5 ID="68696"Querying for 4-channel Capability /*
 * Query to see if we are on a machine with 4-channel
 * HW capability. If so,switch into 4-channel mode.
 * If AL_CHANNEL_MODE both exists (ALgetminmax doesn't
 * fail) AND has a maximum of 4,then we're OK.
 * 
 * If we wanted to be really nice,  we could check, 
 * by querying AL_INPUT_COUNT and AL_OUTPUT_COUNT, to
 * see if any other apps were doing audio. If so,  we
 * might not want to switch to 4-channel mode,  lest
 * we introduce artifacts into their audio streams.
 */
if (ALgetminmax(AL_DEFAULT_DEVICE, AL_CHANNEL_MODE, 
    &min, &max) >= 0 && max == 4) {
    long buf[2];
    buf[0] = AL_CHANNEL_MODE;
    buf[1] = 4;
    ALsetparams(AL_DEFAULT_DEVICE, buf, 2);
}
/*
 * Even if we don't have 4-channel HW capability, 
 * the AL will let us use a 4-channel buffer,  so 
 * we can continue at this point without regard to
 * HW type.
 */LBL="" HELPID=""ID="63249"Audio Library Synchronization FacilitiesThe AL provides two different facilities for synchronization:The AL allows for multiple audio ports (ALports) to be synchronized in a sample accurate manner, by using the absolute sample frame count. The AL allows audio data to be related to other media based on common time line, by using the unadjusted system time (UST).The AL provides a method of determining the absolute sample count of the current sample frame under program control (that is, the sample frame which can be read/written with a call to the Audio Library) and a method of relating UST values to the count of samples which have entered or exited the audio device.As mentioned in IDREF="46771" TYPE="TITLE"Chapter 2, "Programming with the Digital Media Library,"the digital media libraries provide a single time line, UST, through which media may be related. This time value is the number of nanoseconds since the operating system was started. As an absolute time value, UST is not particularly useful. However, it is extremely useful for relating different media types and for evaluating the relative timing of events.LBL="" HELPID=""Audio Sample Frame CountAbsolute sample frame count is the basis for timing within the AL. Whenever audio is input or output on a device, a count is kept of the sample frames elapsed. This sample frame count is the absolute number of sampling periods elapsed since input or output started. If the audio sample rate is set to 44100 kHz, the sample frame count advances at the nominal rate of 44100 counts per second, regardless of the channel setting for the port (see ALsetchannels() for more details on setting the number of channels for a port).The sample frame count increases regardless of whether an application is reading or writing audio samples using the ALreadsamps() or ALwritesamps() function calls, respectively. As long as an audio port (ALport) is open, the sample frame count advances.The AL function ALgetframenumber() provides a way for an application to query the absolute sample frame count associated with the current sample frame to be written (in the case of an output port) or read (in the case of an input port). The function prototype for ALgetframenumber() is:int ALgetframenumber(const ALport port,                     unsigned long long *framenum);where:portis the audio port of interestframenumis a pointer to a 64-bit number in which to hold the resultant frame count valueIf ALgetframenumber() succeeds, 0 is returned; otherwise a -1 is returned.Since the sample frame count is an absolute value of sample frames entering or exiting an audio device, two audio ports (ALports) can be synchronized by reading/writing samples at the identical sample frame count. This "port-to-port" synchronization is guaranteed to be sample accurate.In general, ALgetframenumber() does not return equal values for the sample frame count for different ports. In order to synchronize two audio ports, you will need to make the sample frame count of the two ports match by reading/writing samples from/to one of the sample queues. IDREF="51378" TYPE="TEXT"Example 6-7 demonstrates synchronizing two audio ports.NoteThe absolute sample frame count is valid only if the port in question does not overflow (in the case of input) or underflow (in the case of output). When your port underflows or overflows, the value of the sample frame count continuously changes, and you cannot reliably place samples in the queue at a desired location. In order to reestablish a valid value for sample frame count (and hence synchronization) your application must recover from the underflow or overflow (read or write samples as appropriate) and then query for the value of sample frame count again. IDREF="98922" TYPE="GRAPHIC"Figure 6-3 shows the relationship of the sample frame count returned by ALgetframenumber() to sample frames in the queue associated with an input or output audio port (ALport).FILE="6-3.online.bw" POSITION="INLINE" SCALE="FALSE"LBL="6-3"Figure 6-3 ID="98922"Sample Frame Count as Returned by ALgetframenumber() In IDREF="14252" TYPE="TEXT"Example 6-6, the first two ALwritesamps() calls are used to bring the audio ports out of an underflow condition. This ensures that subsequent calls to ALgetframenumber() will result in valid sample frame counts.LBL="6-6"Example 6-6 ID="14252"Synchronizing Audio Between Two Output Ports: align.c /* align.c - synchronize audio of two output audio ports */

#include <stdio.h>
#include <dmedia/audio.h>

main(void)
{
    ALport port_1, port_2;
    short buf_1[10000], buf_2[10000];
    short zilch[10000];
    unsigned long long count_1, count_2, delta_count;
    int i;

    /* get two output ports with default configurations */
    port_1 = ALopenport("port_1", "w", NULL);
    port_2 = ALopenport("port_2", "w", NULL);

    if (port_1 == NULL || port_2 == NULL) {
            printf("oops...no audio ports\n");
            exit(-1);
    }

    /* set up the output sample buffers */
    for (i = 0; i < 10000; i++) {
            buf_1[i] = i;
            buf_2[i] = -i;
            zilch[i] = 0;
    } 

    /* bring the output ports out of underflow state */
    ALwritesamps(port_1, zilch, 10000);
    ALwritesamps(port_2, zilch, 5000);

    ALgetframenumber(port_1, &count_1);
    ALgetframenumber(port_2, &count_2);

    /* count_1 should be > count_2 at this point */
    delta_count = count_1 - count_2;
    printf("frame count difference = %lld\n", delta_count); 
    /* write delta_count frames of zeroes to port_2 */
    ALwritesamps(port_2, zilch, delta_count*2); 

    ALgetframenumber(port_1, &count_1);
    ALgetframenumber(port_2, &count_2);
    delta_count = count_1 - count_2;
    printf("frame count difference = %lld\n", delta_count); 

    while (1) {
        ALwritesamps(port_1, buf_1, 10000);
        ALwritesamps(port_2, buf_2, 10000);
        ALgetframenumber(port_1, &count_1);
        ALgetframenumber(port_2, &count_2);
        if (count_1 != count_2) {
            printf("lost synchronization of audio port.\n");
        }
    }
    ALcloseport(port_1);
    ALcloseport(port_2);
}LBL="" HELPID=""Relating Audio Sample Frame Count to USTThe IRIS digital media libraries provide a time line called unadjusted system time (UST) for relating media to one another. The UST is a 64-bit count of the number of nanoseconds elapsed since the workstation operating system was started.The AL provides a way for application programs to relate the number of audio sample frames input to or output from a device to UST values, by providing a pair of values (UST, sample frame count) simultaneously. The UST value is the time when the samples in the frame entered the audio device (in the case of input) or exited the audio device (in the case of output). That is, the UST is the time at which the samples physically "hit the jacks." The audio system software accounts for any latency in hardware and intermediate buffering.The AL function ALgetframetime() provides both UST and sample frame count for an audio port (ALport) to an application. The function prototype for ALgetframetime() is:int ALgetframetime(const ALport port,                   unsigned long long *fnum,                   unsigned long long *ustime);where:portis the audio port of interestustimeis a pointer to a 64-bit number to hold the value of USTfnumis a pointer to a 64-bit number to hold the value of sample frame countIf ALgetframetime() succeeds, it returns 0 to the application; otherwise, it returns a -1 and sets an error number which can be retrieved with oserror(3C).When an application program calls the ALgetframetime() function, the AL provides the most recent pair of (UST, sample frame count) that it has calculated. In general, the value of sample frame count returned by ALgetframetime() is not the same as the sample frame count value returned by ALgetframenumber(). However, a UST value corresponding to the sample frame count returned by ALgetframenumber() can be calculated from (UST, sample frame count) pairs.IDREF="51378" TYPE="TEXT"Example 6-7 demonstrates calculating the UST value for the next sample to be read from an input port.LBL="6-7"Example 6-7 ID="51378"Calculating UST/* getust.c - get ustime for first sample in input port */

#include <stdio.h>
#include <audio.h>

main(void)
{
    ALport port;
    long long count_1, count_2, ustime_1, ustime_2;
    double nrate;

    nrate = 1e+9/44100.0; /* nanosecs per sample @ 44.1 kHz*/

    port = ALopenport("my_input", "r", NULL);
    if (port == NULL) exit(-1);

    ALgetframenumber(port, (unsigned long long*)&count_2);
    ALgetframetime(port, (unsigned long long*)&count_1,
        (unsigned long long*)&ustime_1);
    ustime_2 = ustime_1 - (count_1 - count_2)*nrate;

 /* ustime_2 corresponds to the first sample frame in port */

    printf("ust(1) = %lld msc(1) = %lld\n",
           ustime_1, count_1);
    printf("ust(2) = %lld msc(2) = %lld\n",
           ustime_2, count_2);

    ALcloseport(port);
}This example code could have calculated the sample frame rate from multiple (UST, sample frame count) pairs and used that value instead of calculating it from the nominal audio frame rate.NoteThe sample frame value returned by ALgetframenumber() is valid only if the port does not overflow/underflow. In the case of underflow or overflow, the (UST, sample frame count) pair will continue to be valid (though you may wish to request a new, more recent, pair). Note, however, that two back-to-back invocations of ALgetframetime() are not guaranteed to result in unique (UST, sample frame count) pairs. For a more involved use of UST and sample frame count, see APP="/usr/sbin/jot /usr/people/4Dgifts/examples/dmedia/midi/syncrecord/recordmidi.c++" PARMS=""recordmidi.c++
 in /usr/people/4Dgifts/examples/dmedia/midi/syncrecord. This code demonstrates synchronization of audio and MIDI using the UST to relate the two streams of data and is discussed further in  IDREF="62110" TYPE="TITLE""Hands-On MIDI and Audio Synchronization Experience" in Chapter 10.LBL="" HELPID=""ID="63440"Real-time Programming Techniques for AudioThe Audio Library provides several routines that modify or return information about the dynamic state of an audio port. These routines, together with the ID="Media2-4AL161"select() or ID="Media2-4AL162"poll() IRIX system calls, make it possible to write applications that can multiplex audio processing tasks with other processing such as user interfaces, audio signal processing, or graphics. Other IRIX system calls, such as ID="Media2-4AL163"prctl(), schedctl(), and sproc(), also help audio applications to achieve efficient real-time performance. This section discusses these routines and techniques for using them effectively. See the online book, Topics in IRIX Programming, for a description of the IRIX real-time programming facilities. ID="Media2-4AL164"LBL="" HELPID=""ID="89705"Multiplexing Synchronous I/OThe ID="Media2-4AL165"select() system call makes it possible for an application to multiplex synchronous I/O tasks. An application passes ID="Media2-4AL166"select() three (optionally null) lists of file descriptors, along with an optional timeout parameter. ID="Media2-4AL167"select() blocks until one or more of the following conditions occur:one or more of the file descriptors in the "read list" are ready for readingone or more of the file descriptors in the "write list" are ready for writingan exceptional condition is pending for one of the file descriptors in the "exception list"a timeout occurs (if specified)When select() returns, it replaces the original file descriptor lists with subsets containing the file descriptors for which requested events have occurred. See the select(2) man page for details.The AL provides a mechanism to control the behavior of select() such that you can wake a process before an output queue runs out of samples or before an input sample queue overflows. The functions described in this section control the behavior of select().LBL="" HELPID=""Getting a File Descriptor for an ALportALgetfd() ID="Media2-4AL168"returns an IRIX file descriptor for a port that may be used with the ID="Media2-4AL169"select() call. Its function prototype is:int ALgetfd ( ALport port )where:portis the audio port whose file descriptor you want. This descriptor can then be used to construct the arguments for a call to select() or poll()When using select(), an input port's file descriptor is used in a read fdset and an output port's file descriptor is used in a write fdset.When using ID="Media2-4AL170"poll(), an input port's file descriptor is used with the POLLIN event flag and an output port's file descriptor is used with the POLLOUT event flag.ID="Media2-4AL171"ID="Media2-4AL172"These select() and poll() system calls are used to give up application control of the CPU until the audio port is emptied or filled past a previously set fill point (see the description of ALsetfillpoint() below).LBL="" HELPID=""Setting and Getting the Fill Point for a QueueALsetfillpoint() ID="Media2-4AL173"allows an application to set a threshold level for an input or output port that controls the behavior of the ID="Media2-4AL174"select() function. Its function prototype is:int ALsetfillpoint ( ALport port, long fillpoint )where:portis the audio port whose fill point you want to setfillpointis the fill point value, in number of samplesFor an input port, the fill point is the number of locations in the sample queue that must be filled in order to trigger a return from ID="Media2-4AL175"select(). For an output port, the fill point is the number of locations that must be free in order to wake up from select().When used in conjunction with ALgetfd() and select() or poll(), ALsetfillpoint() lets you programmatically relinquish control from an audio application to other processes.NoteALreadsamps()ID="Media2-4AL176" and ID="Media2-4AL177"ALwritesamps() may alter the fill point, so you should (re)set it just before you call select() or poll(). ALgetfillpoint() ID="Media2-4AL178"returns the current fill point of a port. Its function prototype is:long ALgetfillpoint ( ALport port )where:portis the audio port being queriedIDREF="78141" TYPE="GRAPHIC"Figure 6-4 shows how the relationship between the number of samples and the fill point affects the behavior of the ID="Media2-4AL179"ID="Media2-4AL180"select() call during input and output.FILE="6-4.online.bw" POSITION="INLINE" SCALE="FALSE"LBL="6-4"Figure 6-4 ID="78141"Using Fill PointsLBL="" HELPID=""ID="19644"Using Scheduling Control to Give Audio High PriorityIRIX provides control of process scheduling through the use of the ID="Media2-4AL181"ID="Media2-4AL182"schedctl() function. This function allows the program to change its execution priority. To maintain real-time audio processing, an application may need to be placed at a high priority relative to other jobs in the system. See the schedctl(2) manual page and for more information on usage. See IDREF="81687" TYPE="TITLE""Using Shared Arenas and Semaphores" for an  example  program that demonstrates how to use schedctl().LBL="" HELPID=""Preventing Memory Swapoutprctl() ID="Media2-4AL183"is an IRIX function that gives you control of certain attributes of a process. By using the ID="Media2-4AL184"PR_RESIDENT argument, you can make your audio process immune to kernel memory swapout, thus helping to ensure uninterrupted audio input and output. See the prctl(2) man page for more details.You can also use mpin() or plock() to lock user pages into memory. See the man pages for those functions for more information.LBL="" HELPID=""Creating Multiple Process ThreadsThe sproc() system call lets you split a process into two threads. sproc() is an IRIX system call similar to fork(), except that it makes use of shared memory. The shared memory features of sproc() allow sharing of data, file descriptors, and address space between the two process threads. When using sproc() in an application with audio, you can create one thread that services audio and another thread that handles the user interface. Using sproc() permits the use of graphical user interfaces without interrupting the audio data stream.  See IDREF="81687" TYPE="TITLE""Using Shared Arenas and Semaphores" for an  example  program that demonstrates how to use sproc() in conjunction with an IRIS IM menu (IRIS IM is Silicon Graphics' port of the industry-standard OSF/Motif).LBL="" HELPID=""ID="81687"Using Shared Arenas and SemaphoresAnother real-time programming technique is to use an IRIX ID="Media2-4AL185"shared arena. In essence, a shared arena is a memory-mapped file that you can access just like regular memory.This section provides some hints for working with shared arenas; more information is available in Topics in IRIX Programming.Shared arenas allow:shared memory between unrelated processesID="Media2-4AL186"shared synchronization tools: ID="Media2-4AL187"locks for controlling access, and semaphoresID="Media2-4AL188" for process communicationCreate a shared arena by calling ID="Media2-4AL189"usinit(). (The "us" prefix stands for user space.) The first process that calls usinit() creates an arena with the given file name; subsequent calls to usinit() invoking the same file name attach to the existing arena.Using shared memory can create data dependency situations such as different process writing to the same memory location at the same time, or one process trying to read from a memory location before another has finished writing to that location. Areas where a potential data dependency exists are called ID="Media2-4AL190"ID="Media2-4AL191"critical regions.Critical regions can be protected with locks, which keep trying until access is gained, or semaphores, which sleep until access is available. Semaphores can be used to allow multiple processes into a critical region at the same time. Processes waiting on a semaphore are queued on a first-come, first-served basis. To acquire (decrement) a semaphore, call ID="Media2-4AL192"ID="Media2-4AL193"uspsema(); to release (increment) call ID="Media2-4AL194"usvsema(). When uspsema causes the semaphore count to go negative, the process will block until some other process calls usvsema().The APP="/usr/sbin/jot /usr/people/4Dgifts/examples/dmedia/audio/motifexample.c" PARMS=""motifexample.c
 program in /usr/people/4Dgifts/examples/dmedia/audio demonstrates the Audio Library programming concepts presented in this chapter and some Audio File programming concepts that are discussed in ID="Media2-4AL195"IDREF="45737" TYPE="TITLE"Chapter 7, "Programming with the Audio File Library."Several real-time programming techniques are used in ID="Media2-4AL196"motifexample.c:The ID="Media2-4AL197"sproc() system call creates two separate threads: a user interface thread, and an audio thread. The PR_SALL argument specifies the sharing of all data. Everything that pertains to handling audio is kept in the separate audio process.Scheduling control gives the audio process high-priority, nondegrading scheduling.Memory swapout is prevented by using mpin() to lock samples in memory.A shared memory arena is used to share data.ID="Media2-4AL198"Semaphores provide interprocess communication for handling commands from the application.ID="Media2-4AL199"ID="Media2-4AL200"Polling is used to monitor two kinds of events: commands from the application and the need for more samples in the queue.ID="Media2-4AL201"LBL="7"ID="45737"Programming with the Audio File LibraryThe Audio File (AF) Library, ID="Media2-5AF1"ID="Media2-5AF2"libaudiofile.so, provides a uniform programming interface for reading and writing audio files. Currently, the AF Library supports the Apple Computer Inc. Audio Interchange File Format (AIFF) and the Audio Interchange File Format with Compression (AIFF-C).The AF Library currently supports read-only and write-only file access (but not both simultaneously). Therefore, to alter an existing file, you must create a new file and copy data from the original file. Sample code that demonstrates how to copy the logical components of a file, and other concepts, is available online in ID="Media2-5AF3"ID="Media2-5AF4"/usr/people/4Dgifts/examples/dmedia/soundfile.The Audio File Library comprises routines that handle four basic tasks:ID="Media2-5AF5"creating and configuring new audio filesreading and writing track information to and from audio filesreading and writing instrument configurations to and from audio filesreading and writing miscellaneous data to and from audio filesIn this chapter:IDREF="63966" TYPE="TITLE""Audio File Library Basics" discusses the basics of programming with the AF Library.IDREF="10191" TYPE="TITLE""Creating and Configuring Audio Files" explains how to initialize AF Library data structures.IDREF="78081" TYPE="TITLE""Opening, Closing, and Updating Audio Files" explains how to create and use audio files.IDREF="20612" TYPE="TITLE""Reading and Writing Audio Track Information" explains how to work with audio file tracks.IDREF="90519" TYPE="TITLE""Audio File Library Programming Tips" contains important programming tips for making AF Library programs format independent and multithread/multiprocessor safe.LBL="" HELPID=""ID="63966"Audio File Library BasicsThis section explains fundamental AF Library concepts.LBL="" HELPID=""Audio File Library Programming ModelThe AF Library has two basic data structures:AFfilesetup, an audio file setup that stores initialization parameters used when creating a new audio file handleAFfilehandle, an audio file handle that provides access to the audio fileThe basic steps required for setting up an audio file for writing are:ID="Media2-5AF6"Initialize an AFfilesetup, by calling AFnewfilesetup().Configure the AFfilesetup for your data, as described in IDREF="92961" TYPE="TITLE""Creating an Audio File Setup".Open an audio file for reading or writing, as described in IDREF="92961" TYPE="TEXT"IDREF="92961" TYPE="TITLE""Creating an Audio File Setup" by calling either AFopenfile() or AFopenfd(). These routines return an AFfilehandle whose data configuration matches the settings in the AFfilesetup.LBL="" HELPID=""Handling Audio File Library ErrorsThe AF Library provides an error handling mechanism that directs error messages to ID="Media2-5AF7"stderr. You can replace the default AF Library error handler with one of your own.AFseterrorhandler() lets you replace the default error handler function with one of your own. Its function prototype is:AFerrfunc AFseterrorhandler(AFerrfunc errfunc)where errfunc is a pointer to an alternate error handling routine of type AFerrfunc that is declared as:void errfunc(long arg1, const char* arg2)LBL="" HELPID=""About Audio FilesThis section explains basic concepts for working with audio files. It describes data structures used by the Audio File Library and in particular, the structure of AIFF-C files and the higher-level abstraction that the AF Library API uses to read and write AIFF-C (and AIFF) files.ID="Media2-5AF8"The AF Library breaks audio files into the following four functional components:ID="Media2-5AF9"COLUMNS="2"LEFT="0" WIDTH="82"Audio file formatLEFT="90" WIDTH="250"Allows applications to identify audio file formats and 
format versions.ID="Media2-5AF10"LEFT="0" WIDTH="82"Audio tracksID="Media2-5AF11"LEFT="90" WIDTH="250"Contain audio sample data, parameters that 
characterize the data format (such as sample rate, 
channel configuration, and compression type), and 
ID="Media2-5AF12"marker structures that store sample frame locations in 
the track for looping and other purposes.LEFT="0" WIDTH="82"Instrument 
configurationsID="Media2-5AF13"LEFT="90" WIDTH="250"Contain instrument parameters for configuring digital 
samples when playing back audio track data, and loop 
markers for repeating tracks or portions of a track.LEFT="0" WIDTH="82"Miscellaneous 
dataLEFT="90" WIDTH="250"Include text strings (author, copyright, name, 
annotation, and so on) and other non-audio information 
(such as MIDI data and application-specific data).The two portions of an audio file you will make most use of are audio tracks and instrument configurations.LBL="" HELPID=""Audio File FormatsAudio file format is typically indicated by header information preceding the actual data that describes the nature of the data in that file. The file format of an audio file constrains the data format of each of its tracks to one of a set of track formats supported by that file format, but you do not necessarily know which one. You must therefore set and query the track format for each of an audio file's tracks independently of its file format. It is often possible and desirable to write your application so that it queries only the data format(s) of the track(s) (instead of querying the file format) of the audio files it opens.LBL="" HELPID=""Audio Tracks, Sample Frames, and Track MarkersAudio tracks contain the recorded samples that produce sound when sent to the audio hardware. These samples are stored linearly for mono recordings and as interleaved left-right pairs (left channel in even locations, right channel in odd locations) for stereo recordings. These pairs are called ID="Media2-5AF14"ID="Media2-5AF15"sample frames (this term is also used for mono tracks, but a sample frame is the same thing as a sample when mono data is used). Audio tracks also contain trackID="Media2-5AF16"markers, which can be set to point to arbitrary locations in the audio track. These markers, which are identified by a long integer ID number and (optionally) a name string, point to locations between sample frames. LBL="" HELPID=""Audio Track Format ParametersData format information, including sample rate, sample format, sample width, and sample compression type is stored as part of the audio track. Several kinds of compression are supported (you can also choose not to use compression). The AF Library automatically compresses samples being written to a file and decompresses samples read from a file. The ability of the AF Library to perform compression/decompression of audio data in real time is dependent on system overhead. To guarantee real-time performance, you should make use of scheduling control as described in IDREF="19644" TYPE="TITLE""Using Scheduling Control to Give Audio High Priority" in Chapter 6.LBL="" HELPID=""Instrument Configurations and LoopsInstrument configurations contain a set of parameters that define the aspects of a sampler, including detuning, key velocity, and gain. They also contain ID="Media2-5AF17"ID="Media2-5AF18"ID="Media2-5AF19"ID="Media2-5AF20"ID="Media2-5AF21"loopID="Media2-5AF22"markers, which identify the beginning and ending points of loops that allow all or part of the audio track to be repeated. These loop markers point to previously created audio track markers, which in turn refer to locations in the audio track that comprise the beginning and ending of the loop. AIFF and AIFF-C files support two kinds of loops, ID="Media2-5AF23"sustain and releaseID="Media2-5AF24", each with a beginning and ending marker, which can be used in audio tracks and track markers.LBL="" HELPID=""ID="42513"ID="91555"AIFF-C and the AF Library APISilicon Graphics has adopted AIFF-C as its standard digital audio file format. AIFF-C is based on Apple Computer's Audio Interchange File Format (AIFF), which conforms to the ID="Media2-5AF25"ID="Media2-5AF26"ID="Media2-5AF27"EA IFF 85 Standard for Interchange Format Files developed by Electronic Arts. Unlike the older AIFF standard, AIFF-C files can store compressed sample data as well as two's complement linear PCM sample data. ID="Media2-5AF28"ID="Media2-5AF29"AIFF-C provides a standard file format for storing sampled sounds on magnetic media. The format can store any number of channels of sampled sound at a variety of sample rates and sample widths. The format is extensible, allowing future support of new compression types and application-specific data, while maintaining backward compatibility.ID="Media2-5AF30"An AIFF-C file is composed of a series of different kinds of data ID="Media2-5AF31"chunks. For the most part, the AF Library API handles low-level chunk manipulation. For complete information on the types of chunks supported by AIFF-C, see the ID="Media2-5AF32"Audio Interchange File Format with Compression (AIFF-C) Specification.Both AIFF and AIFF-C files consist of similar component structures. The chunks in an AIFF-C file are grouped together inside a special container chunk. The ID="Media2-5AF33"EA IFF 85 specification defines several types of container chunks, but the kind used by AIFF-C is of type 'FORM'.IDREF="71823" TYPE="TABLE"Table 7-1 shows the mapping between the Audio File Library API functional components and the low-level AIFF-C/AIFF data chunks.COLUMNS="2"LBL="7-1"Table 7-1 ID="71823"Mapping of AF Library Components to AIFF-C/AIFF File ChunksLEFT="0" WIDTH="144"AF Library Functional ComponentLEFT="150" WIDTH="188"AIFF-C/AIFF ChunksLEFT="0" WIDTH="144"File format informationLEFT="150" WIDTH="188"'FVER', 'FORM'LEFT="0" WIDTH="144"Audio tracksLEFT="150" WIDTH="188"'SSND', 'COMM', 'MARK', 'AESD', 'COMT'IDREF="Media2-5AFTF1a"aLEFT="0" WIDTH="144"Instrument configurationsLEFT="150" WIDTH="188"'INST'LEFT="0" WIDTH="144"Miscellaneous dataLEFT="150" WIDTH="188"'AUTH', 'NAME', '(c) ', 'ANNO',' MIDI ', 
'APPL'LBL="a" ID="Media2-5AFTF1a"'COMT' chunks are not currently supported by the AF Library.LBL="" HELPID=""ID="10191"Creating and Configuring Audio FilesThis section explains how to initialize an AF Library application, including how to create, configure, and free AF Library data structures for working with audio files.LBL="" HELPID=""ID="92961"Creating an Audio File SetupThe AFfilesetup structure stores initialization parameters used when creating a new audio file. When you open an audio file for reading or writing the AF Library returns another structure, an AFfilehandle, which provides access to the audio file and is used as an argument by all AF Library routines.ID="Media2-5AF34"ID="Media2-5AF35"AFnewfilesetup() ID="Media2-5AF36"creates and initializes an AFfilesetup structure that you configure for your data, and then use to open an audio file:AFfilesetup AFnewfilesetup(void)AFnewfilesetup() returns a default AFfilesetup structure.IDREF="67145" TYPE="TABLE"Table 7-2 lists the AFfilesetup configuration parameters and their defaults.ID="Media2-5AF37"ID="Media2-5AF38"COLUMNS="2"LBL="7-2"Table 7-2 ID="67145"AFfilesetup Parameters and DefaultsLEFT="0" WIDTH="172"Parameter   LEFT="180" WIDTH="160"DefaultLEFT="0" WIDTH="172"File formatID="Media2-5AF39"LEFT="180" WIDTH="160"AF_FILE_AIFFCLEFT="0" WIDTH="172"Audio trackID="Media2-5AF40"LEFT="180" WIDTH="160"AF_DEFAULT_TRACKLEFT="0" WIDTH="172"Audio track sample format, sample widthLEFT="180" WIDTH="160"AF_SAMPFMT_TWOSCOMP, 16-bitLEFT="0" WIDTH="172"Audio track channels (interleaved)LEFT="180" WIDTH="160"2 (stereo)LEFT="0" WIDTH="172"Audio track compression ID="Media2-5AF41"LEFT="180" WIDTH="160"AF_COMPRESSION_NONELEFT="0" WIDTH="172"Audio track markers ID="Media2-5AF42"LEFT="180" WIDTH="160"Four markers with IDs: 1,2,3,4LEFT="0" WIDTH="172"InstrumentID="Media2-5AF43"LEFT="180" WIDTH="160"AF_DEFAULT_INSTLEFT="0" WIDTH="172"Instrument ParametersLEFT="180" WIDTH="160"(See IDREF="94362" TYPE="TABLE"Table 7-3)LEFT="0" WIDTH="172"LoopsID="Media2-5AF44"LEFT="180" WIDTH="160"Two loops with IDs: 1,2; default mode 
is AF_LOOP_MODE_NOLOOPIDREF="94362" TYPE="TABLE"Table 7-3 lists the AFfilesetup instrument parameters and their defaults.ID="Media2-5AF45"COLUMNS="2"LBL="7-3"Table 7-3 ID="94362"AFfilesetup Instrument Parameter Constants and DefaultsLEFT="0" WIDTH="162"Instrument Parameter ConstantLEFT="170" WIDTH="171"DefaultLEFT="0" WIDTH="162"AF_INST_MIDI_BASENOTELEFT="170" WIDTH="171"60LEFT="0" WIDTH="162"AF_INST_MIDI_HINOTELEFT="170" WIDTH="171"127LEFT="0" WIDTH="162"AF_INST_MIDI_HIVELOCITYLEFT="170" WIDTH="171"127LEFT="0" WIDTH="162"AF_INST_MIDI_LONOTELEFT="170" WIDTH="171"0LEFT="0" WIDTH="162"AF_INST_MIDI_LOVELOCITYLEFT="170" WIDTH="171"1LEFT="0" WIDTH="162"AF_INST_NUMCENTS_DETUNELEFT="170" WIDTH="171"0LEFT="0" WIDTH="162"AF_INST_NUMDBS_GAINLEFT="170" WIDTH="171"0LEFT="0" WIDTH="162"AF_INST_SUSLOOPIDLEFT="170" WIDTH="171"1 (loop ID for sustain loop)LEFT="0" WIDTH="162"AF_INST_RELLOOPIDLEFT="170" WIDTH="171"2 (loop ID for release loop)Your application should free an AFfilesetup that is no longer needed. ID="Media2-5AF46"AFfreefilesetup() deallocates an AFfilesetup structure. Its function prototype is:ID="Media2-5AF47"void AFfreefilesetup(AFfilesetup setup)where setup is an AFfilesetup previously created by a call to AFnewfilesetup(). This does not affect any file previously opened using the same AFfilesetup structure.Before using the new AFfilesetup to open an audio file, you might need to modify the default AFfilesetup in order to create the configuration you want. The sections that follow explain how to change the default AFfilesetup configuration.LBL="" HELPID=""Initializing Audio File FormatYou need to set the file format in an AFfilesetup structure before passing the structure to AFopenfile().AFinitfilefmt() ID="Media2-5AF48"configures the file format parameter in an AFfilesetup structure. Its function prototype is:ID="Media2-5AF49"ID="Media2-5AF50"void AFinitfilefmt(AFfilesetup setup, long fmt)where setup is the AFfilesetup structure, and fmt is an integer constant which specifies an audio format supported by the AF Library. Two valid format types are currently available:ID="Media2-5AF51"AF_FILE_AIFFC (AIFF-C format)ID="Media2-5AF52"AF_FILE_AIFF (AIFF format)ID="Media2-5AF53"A new audio file that is opened by calling AFopenfile() with this AFfilesetup as an argument will then be formatted accordingly.LBL="" HELPID=""Initializing Audio Track DataThis section explains how to change the default settings for audio track parameters in an AFfilesetup structure before passing the structure to ID="Media2-5AF54"ID="Media2-5AF55"AFopenfile().NoteEach of the functions in this section contains a trackid argument, which identifies an audio track in the AFfilesetup structure being initialized. In the current release of the AF Library, the value of trackid must always be AF_DEFAULT_TRACK.LBL="" HELPID=""Initializing Audio Track Sample RateThe AF Library requires that you specify the sample rate for a new file before you pass the AFfilesetup structure to AFopenfile(). AFinitrate() ID="Media2-5AF56"configures the sample rate in Hz for an audio track in an AFfilesetup structure. Its function prototype is:void AFinitrate(AFfilesetup setup, long trackid, double rate)where setup is the AFfilesetup structure, trackid is a long integer that identifies an audio track in setup, and rate is a positive double-precision integer that specifies the sample rate in Hz. For example, to configure setup for a CD-quality AIFF-C file, initialize the rate for AF_DEFAULT_TRACK to 44100.0.LBL="" HELPID=""Initializing Audio Track Sample Format and Sample WidthID="Media2-5AF57"AFinitsampfmt() initializes the sample format and width parameters for an audio track in an AFfilesetup structure. Its function prototype is:void AFinitsampfmt(AFfilesetup setup, long trackid, long fmt,                   long width)where setup is the AFfilesetup structure, trackid is a long integer that identifies an audio track in setup, and fmt is a long integer constant that denotes a sample format. Currently, only one format is supported: AF_SAMPFMT_TWOSCOMP. width is a positive long integer value from 1 to 32 that specifies the width (in bits) of the sample data. See IDREF="77680" TYPE="TITLE""Getting Audio Track Sample Format and Sample Width" for more details about sample format and sample width.NoteIf the audio track in an AIFF-C file is configured for compression, fmt and width should match the data format specified by the compression algorithm. See IDREF="61459" TYPE="TABLE"Table 7-4 for a list of compression algorithms.LBL="" HELPID=""Initializing Audio Track ChannelsAFinitchannels() ID="Media2-5AF58"configures the number of interleaved audio channels for an audio track within an AFfilesetup structure. This information is then used by ID="Media2-5AF59"AFopenfile() when it is called with the AFfilesetup structure as an argument. Its function prototype is:void AFinitchannels(AFfilesetup setup, long trackid,                    long channels)where setup is the AFfilesetup structure, trackid is a long integer that identifies an audio track in setup, and channels is a long integer representing the number of interleaved audio channels. Valid values for channels are 1 (mono) or 2 (stereo); the default value is 2.LBL="" HELPID=""Initializing AES DataAES channel status bytes are embedded in AES audio samples to provide additional information about that data, such as whether an emphasis has been added to a sample. For example, on early CD recordings, high frequencies were sometimes emphasized to compensate for the nature of CD players. You might want to reverse compensate for that emphasis if you are loading AES stream data directly from a CD player through the AES serial input of your workstation for playback on a different source, such as DAT. See the ID="Media2-5AF60"ID="Media2-5AF61"ID="Media2-5AF62"ID="Media2-5AF63"ID="Media2-5AF64"AES3-1985 (ANSI S4.40-1985) document for more information about AES channel status bytes.AFinitaeschanneldata() ID="Media2-5AF65"sets a flag, which is off by default, in an AFfilesetup structure to indicate that space should be reserved for the 24 AES channel status bytes that are embedded in all AES data. Its function prototype is:ID="Media2-5AF66"void AFinitaeschanneldata(AFfilesetup setup, long trackid)where setup is the AFfilesetup structure, and trackid is a long integer that identifies an audio track in setup. ID="Media2-5AF67"AFsetaeschanneldata() sets the values of the AES channel status bytes. Its function prototype is:void AFsetaeschanneldata(AFfilehandle file, long trackid,                         unsigned char buf[24])where file is the AFfilehandle structure previously created by a call to AFopenfile() or AFopenfd(), trackid is the ID for the audio track (for AIFF and AIFF-C files, this value should always be AF_DEFAULT_TRACK), and buf is a 24-element array that specifies the AES channel status bytes. If no header space has been reserved in the file (by calling AFinitaeschanneldata() before creating the file), AFsetaeschanneldata() ignores the data and returns without error.LBL="" HELPID=""Initializing Audio Track CompressionID="Media2-5AF68"AFinitcompression() and ID="Media2-5AF69"AFinitcompressionparams() let you configure an audio track in an AFfilesetup structure to store compressed audio data. All compression encoding is handled automatically by ID="Media2-5AF70"AFwriteframes(); therefore your application program need only work with linear PCM data.NoteAIFF files do not support compression. It is an error to try to open an AIFF file using an AFfilesetup whose compression setting is other than AF_COMPRESSION_NONE.ID="Media2-5AF71"AFinitcompression() lets you select from among several built-in default codec (compressor-decompressor) configurations that are preconfigured. If you use AFinitcompression() to select one of the default codecs that are built in to the Audio File Library, you don't have to worry about setting the individual compression parameters, because they are automatically set to the proper values for each default configuration.AFinitcompressionparams() lets you chose the codec configuration and set the associated codec-specific compression parameters yourself, although it does supply the defaults listed in IDREF="61459" TYPE="TABLE"Table 7-4. If you use AFinitcompressionparams(), you have to create and fill in an Audio Utility Library parameter-value list (AUpvlist), as described in IDREF="35981" TYPE="TITLE""Using the Audio Utility Library to Initialize Parameter Lists".You may also select from additional audio codecs from Aware, Inc. that provide ISO/MPEG I and Aware MultiRate I audio compression, which are built in to the Audio File Library and can be accessed under license from Aware, Inc. by using the parameters in IDREF="61459" TYPE="TABLE"Table 7-4. See IDREF="63476" TYPE="TITLE"Appendix B, "Aware Scalable Audio Compression Software," for more information.The function prototypes are:void AFinitcompression( AFfilesetup setup, long track,                        long compression)
long AFinitcompressionparams( AFfilesetup setup, long track,                              long compression,                              AUpvlist pvlist, long numitems)where:setupis the AFfilesetup structure that was previously created by calling AFnewfilesetup().trackis a positive long integer that identifies an audio track in setup. Because AIFF-C files contain only one audio track per file, you should use the constant AF_DEFAULT_TRACK to access the track.compressionis a positive integer symbolic constant that indicates the type of audio compression being used. See IDREF="61459" TYPE="TABLE"Table 7-4 for a list of valid compression values.pvlistis an Audio Utility Library parameter-value list (AUpvlist) structure, filled with parameters and values related to the compression scheme compression. Currently, the only compression schemes that have any parameters are those supplied by Aware, Inc. numitemsis the number of valid entries in the pvlist.IDREF="61459" TYPE="TABLE"Table 7-4 lists the valid compression values that you can set for AIFF-C files; compression must be AF_COMPRESSION_NONE for AIFF files.COLUMNS="2"LBL="7-4"Table 7-4 ID="61459"Settable Compression Parameter Values and TypesLEFT="0" WIDTH="231"Parameter ValueLEFT="240" WIDTH="271"Compression TypeLEFT="0" WIDTH="231"AF_COMPRESSION_NONELEFT="240" WIDTH="271"No compression.LEFT="0" WIDTH="231"AF_COMPRESSION_G722LEFT="240" WIDTH="271"64 Kbps ADPCM for 16 kHz 16-bit.ID="Media2-5AF72"LEFT="0" WIDTH="231"AF_COMPRESSION_G711_ULAWLEFT="240" WIDTH="271"64 Kbps PCM encoding for 8 kHz 16-bit.LEFT="0" WIDTH="231"AF_COMPRESSION_G711_ALAWLEFT="240" WIDTH="271"64 Kbps PCM encoding for 8 kHz 16-bit.LEFT="0" WIDTH="231"AF_COMPRESSION_AWARE_MPEGIDREF="Media2-5AFTF4a"aLEFT="240" WIDTH="271"Aware implementation of ISO/MPEG I-audio, Layers I and II.The default setting for this parameter is 
AF_COMPRESSION_AWARE_DEFAULT_MPEG_II).LEFT="0" WIDTH="231"AF_COMPRESSION_AWARE_MULTIRATEaLEFT="240" WIDTH="271"Aware MultiRate I lossless or near-lossless algorithmThe default setting for this parameter is 
AF_COMPRESSION_AWARE_DEFAULT_MULTIRATE).LEFT="0" WIDTH="231"AF_COMPRESSION_AWARE_DEFAULT_MPEG_IIDREF="Media2-5AFTF4b"bLEFT="240" WIDTH="271"Aware implementation of ISO/MPEG I-audio layer I, joint-stereo, 
fixed rate at 192 Kbps per channel.LEFT="0" WIDTH="231"AF_COMPRESSION_AWARE_DEFAULT_MPEG_IIbLEFT="240" WIDTH="271"Aware implementation of ISO/MPEG I-audio layer II, joint-stereo, 
fixed rate at 128 Kbps per channel.LEFT="0" WIDTH="231"AF_COMPRESSION_AWARE_DEFAULT_MULTIRATEbLEFT="240" WIDTH="271"Aware MultiRate I operating in high-resolution near-lossless (near 
perfect reconstruction) mode.LEFT="0" WIDTH="231"AF_COMPRESSION_AWARE_DEFAULT_LOSSLESSbLEFT="240" WIDTH="271"Aware MultiRate I operating in lossless (perfect reconstruction) 
mode.LBL="a" ID="Media2-5AFTF4a"These values are intended for use with AFinitcompressionparams().LBL="b" ID="Media2-5AFTF4b"These values are intended for use with AFinitcompression().LBL="" HELPID=""Initializing Audio Track MarkersAudio track marker structures store sample frame locations in the track for looping and other purposes. Markers are identified by a long integer ID number and (optionally) a name string. Markers point to a location between two samples in the audio track: position 0 is before the first sample, position 1 is between the first and second sample, and so on. You can assign positions to the markers by calling AFsetmarkpos(). By default, AFnewfilesetup() allocates space for four markers, which is sufficient to store the beginning and end points for both a sustain loop and a release loop.ID="Media2-5AF73"AFinitmarkids() initializes a list of unique marker IDs corresponding to marker structures in a given audio track. Its function prototype is:void AFinitmarkids(AFfilesetup setup, long trackid,                   long markids[], long nmarks)where setup is the AFfilesetup structure, trackid is a long integer that identifies an audio track in setup, markids is an array of unique positive long integers that will be used as handles for the marker structures in the file opened with setup, nmarks is a long integer that specifies the number of marker IDs in the markids array, that is, the total number of marker structures that will be allocated for the audio track. AIFF-C (and AIFF) files can contain up to 65535 markers in a track.AFinitmarkname() ID="Media2-5AF74"specifies a name string for a marker structure. Marker names default to empty strings. Its function prototype is:void AFinitmarkname(AFfilesetup setup, long trackid,                    long markid, char *name)where setup is the AFfilesetup structure, trackid is a long integer that identifies an audio track in setup, markid is a positive long integer that identifies a marker structure configured previously by AFinitmarkids(), name is a string that will be written into the marker structure when an audio file is created by passing setup to AFopenfile().LBL="" HELPID=""Initializing Instrument DataThis section explains how to initialize the instrument parameters in an AFfilesetup structure before passing the structure to AFopenfile().ID="Media2-5AF75"AFinitinstids() initializes a list of unique instrument IDs that are used to reference the instrument configurations in an AFfilesetup. Its function prototype is:void AFinitinstids(AFfilesetup setup, long instids[],                   long ninsts)where setup is the AFfilesetup structure, instids is an array of positive long integers that are used as handles for the instrument configurations in an audio file, and ninsts is the number of entries in instids.NoteCurrently, the AF Library supports only one instrument configuration per file, which is the maximum allowed by both AIFF and AIFF-C formats; therefore, ninsts should be set to either 0 or 1 and instids contains at most one element, whose value must be AF_DEFAULT_INST. If you set ninsts to 0 (meaning that no instrument configuration will be in the audio file you plan to open), AFinitinstids() will ignore the instids argument, and instids can be made a null pointer in this case.ID="Media2-5AF76"AFinitloopids() initializes a list of unique instrument loop IDs that correspond to the loops supplied for a specified instrument in an audio file. Its function prototype is:void AFinitloopids(AFfilesetup setup, long instid,                   long loopids[], long nloops)where setup is the AFfilesetup structure, instid is a long integer that identifies an instrument configuration in an audio track. In the current release of the AF Library, the value of instid should always be AF_DEFAULT_INST. loopids is an array of unique, positive long integers that will identify individual loops within an audio file opened using setup. nloops is a long integer that indicates the number of elements in loopids.The values set in loopids can be used by other AF Library functions to set the start point, end point, and play mode for each loop (see IDREF="44196" TYPE="TITLE""Reading and Writing Instrument Configurations").NoteIn the current release of the AF Library, both AIFF and AIFF-C files must contain exactly 2 loops: a "sustain" loop and a "release" loop. nloops is currently ignored, since its value is always 2.LBL="" HELPID=""Initializing Miscellaneous DataUse these functions to initialize miscellaneous data chunks in an AFfilesetup structure, including file name, author, copyright, and annotation strings, MIDI data, and application-specific data.ID="Media2-5AF77"AFinitmiscids() initializes a list of unique miscellaneous chunk IDs that are then used to reference various file format-dependent data chunks in an audio file. Its function prototype is:void AFinitmiscids(AFfilesetup setup, long miscids[],                   long nmisc)where setup is the AFfilesetup structure, miscids is an array of unique, positive long integers used to reference the miscellaneous data chunks in an audio file opened using setup, nmisc is the number of elements in miscids, that is, the total number of miscellaneous chunks in the file configuration. The default number of miscellaneous IDs in an AFfilesetup structure is 0.ID="Media2-5AF78"AFinitmisctype() initializes a miscellaneous data chunk with a given ID to one of a variety of supported chunk types in AIFF and AIFF-C files. Its function prototype is:void AFinitmisctype(AFfilesetup setup, long miscid,                    long type)where setup is the AFfilesetup structure, miscid is a positive long integer that identifies a miscellaneous chunk in setup, and type is a long integer constant that defines the chunk type. IDREF="75400" TYPE="TABLE"Table 7-5 lists the valid parameters for each chunk type. COLUMNS="2"LBL="7-5"Table 7-5 ID="75400"Miscellaneous Chunk Types and Parameter ValuesLEFT="0" WIDTH="162"Parameter ValueLEFT="170" WIDTH="171"Miscellaneous Chunk TypeLEFT="0" WIDTH="162"AF_MISC_AIFF_ANNOLEFT="170" WIDTH="171"Annotation stringLEFT="0" WIDTH="162"AF_MISC_AIFF_APPLLEFT="170" WIDTH="171"Application-specific dataLEFT="0" WIDTH="162"AF_MISC_AIFF_AUTHLEFT="170" WIDTH="171"Author stringLEFT="0" WIDTH="162"AF_MISC_AIFF_COPYLEFT="170" WIDTH="171"Copyright stringLEFT="0" WIDTH="162"AF_MISC_AIFF_MIDILEFT="170" WIDTH="171"MIDI dataLEFT="0" WIDTH="162"AF_MISC_AIFF_NAMELEFT="170" WIDTH="171"Name stringA single AIFF or AIFF-C file may contain any number of ANNO, APPL, or MIDI chunks, but only one of each of the other (NAME, AUTH, and (c)) miscellaneous chunks.ID="Media2-5AF79"ID="Media2-5AF80"AFinitmiscsize() initializes the amount of space reserved for miscellaneous chunks of data in an AFfilesetup structure. This space is then reserved (written as a zero-filled area) in the header structure of an audio file that is opened using the specified AFfilesetup structure.Use AFwritemisc() to write the data after the file has been opened. The application program is responsible for managing the contents of the header space reserved for each chunk. Its function prototype is:void AFinitmiscsize(AFfilesetup setup, long miscid,                    long size)where setup is the AFfilesetup structure, miscid is a positive long integer that identifies a miscellaneous chunk in setup, and size is a non-negative long integer that specifies the number of bytes to reserve for the chunk data identified by miscid. It is not necessary to add a trailing "zero pad byte" normally required by chunks in AIFF/AIFF-C files with odd numbers of data bytes (see the description for AFreadmisc()); the AF Library handles this transparently.LBL="" HELPID=""ID="35981"Using the Audio Utility Library to Initialize Parameter ListsThe Audio Utility Library, libaudioutil.so, provides routines for getting and setting parameters, parameter types, and parameter values contained in lists. Currently, these routines are used only when initializing and querying parameters for the built-in licensable audio compression software from Aware Inc., which is accessible from AF routines. Licenses can be verified by using the AUchecklicense() routine.These routines use the Audio Utility Library parameter-value list (AUpvlist) data structure, which is an array of structures, each of which contains a list of parameters, parameter types, and parameter values.LBL="" HELPID=""Creating and Configuring an Audio Utility Parameter-value ListUse AUpvnew() to create an empty AUpvlist with the specified number of blank structures. Its function prototype is:AUpvlist AUpvnew(int numitems)where:numitemsis an integer number of list items to use when creating a new AUpvlistname='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'one list item contains the parameter, parameter type, and parameter value entries. AUpvnew() returns an empty AUpvlist structure. If an error occursname='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]' either because numitems is less than or equal to zero, or because of a memory allocation errorname='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'a null pointer, AU_NULL_PVLIST, is returned.LBL="" HELPID=""Freeing an Audio Utility Parameter-value ListWhen an AUpvlist is no longer needed, you should free the memory associated with it by calling AUpvfree(). Its function prototype is:int AUpvfree(AUpvlist pvlist)where pvlist is the structure for which memory should be freed.LBL="" HELPID=""Getting and Setting Parameter ValuesUse the AUpvlist structure when setting and getting a parameter, its type, and its value. The "set" routines fill in the structure entries for the designated list item with the specified information; the "get" routines return the requested information in pointers corresponding to the item being queried.IDREF="22500" TYPE="TABLE"Table 7-6 lists and describes the AU Library get and set routines.COLUMNS="2"LBL="7-6"Table 7-6 ID="22500"Audio Utility Library Set and Get RoutinesLEFT="0" WIDTH="90"RoutineLEFT="95" WIDTH="243"DescriptionLEFT="0" WIDTH="90"AUpvgetmaxitems()LEFT="95" WIDTH="243"Returns the number of list entries allocated for pvlist when 
it was created by AUpvnew() LEFT="0" WIDTH="90"AUgetparam()LEFT="95" WIDTH="243"Gets the parameter of the itemth entry in pvlist and returns 
it in param_ptrLEFT="0" WIDTH="90"AUpvgetval()LEFT="95" WIDTH="243"Gets the value of the itemth entry in pvlist and returns it in 
val_ptrLEFT="0" WIDTH="90"AUpvgetvaltype()LEFT="95" WIDTH="243"Gets the value type of the itemth entry in pvlist and returns 
it in type_ptrLEFT="0" WIDTH="90"AUpvsetparam()LEFT="95" WIDTH="243"Sets the parameter of the itemth entry in pvlist to paramLEFT="0" WIDTH="90"AUpvsetval()LEFT="95" WIDTH="243"Sets the value of the itemth entry in pvlist to the value stored 
in val_ptrLEFT="0" WIDTH="90" AUpvsetvaltype()LEFT="95" WIDTH="243"Sets the type of the value of the itemth entry in pvlist to typeThe function prototypes of the routines in IDREF="22500" TYPE="TABLE"Table 7-6 are:int AUpvgetmaxitems(AUpvlist pvlist)
int AUpvgetparam(AUpvlist pvlist, int item, int *param_ptr)
int AUpvgetval(AUpvlist pvlist, int item, void *val_ptr)
int AUpvgetvaltype(AUpvlist pvlist, int item, int *type_ptr)
int AUpvsetparam(AUpvlist pvlist, int item, int param)
int AUpvsetval(AUpvlist pvlist, int item, void *val_ptr)
int AUpvsetvaltype(AUpvlist pvlist, int item, int type)pvlistis an Audio Utility Library parameter-value list data type created by a previous call to AUpvnew().itemis an integer zero-based index into an AUpvlist. The index should be a non-negative value that is less than numitems-1.typeis a symbolic constant describing the type of parameter. Currently supported types are:AU_PVTYPE_LONGname='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'values are longsAU_PVTYPE_DOUBLEname='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'values are double-precision floating pointsparamis an integer that will become the parameter or the parameter-value pair.valis a pointer to a void type. Data is read from this pointer, interpreted according to the type associated with this entry, and stored in the AUpvlist.param_ptris a pointer to an integer that is filled with the value of the parameter portion of a parameter-value pair.value_ptris a pointer to a void type. Data representing the value portion of a parameter-value pair is copied to this address as interpreted by this entry's type.LBL="" HELPID=""Verifying a LicenseUse AUchecklicense() to verify whether a license for a particular audio product is available. Its function prototype is:int AUchecklicense(int product, int *errorval,
                   char **message)where:productis a constant symbol for the product license that is being queried. Currently defined licenses are:AU_LICENSE_AWARE_MPEG_ENCODERAU_LICENSE_AWARE_MPEG_DECODERAU_LICENSE_AWARE_MULTIRATE_ENCODERAU_LICENSE_AWARE_MULTIRATE_DECODERerrorvalis a pointer to an integer describing a NetLS error, which will be set only if the return value is AU_LICENSE_ERR.messageis a pointer to a character pointer, which is changed to point to an informative string only if the return value is AU_LICENSE_ERR. The string contains the NetLS error that occurred and contact information on how to obtain support or a license.On successful completion, AUchecklicense() returns AU_LICENSE_OK. If product is unknown, then AU_BAD_PRODUCT is returned. If a NetLS error occurs, then AU_LICENSE_ERR is returned and *errorval and *message are set, describing the error. See IDREF="63476" TYPE="TITLE"Appendix B for more information about NetLS.IDREF="73161" TYPE="TEXT"Example 7-1 contains a listing of a portion of code from the APP="/usr/sbin/jot /usr/people/4Dgifts/examples/dmedia/soundfile/aifcinfo.c" PARMS=""aifcinfo.c
 demo program that is provided in /usr/people/4Dgifts/examples/dmedia/soundfile. This portion of code creates an AUpvlist with 3 items, fills those items with the pertinent information, then frees the memory associated with the AUpvlist when it is no longer required.LBL="7-1"Example 7-1 ID="73161"Creating, Filling, Querying and Freeing an AUpvlist{
AUpvnew(&pvlist, 3);

AUpvsetparam(pvlist, 0, AF_AWARE_PARAM_LAYER);
AUpvsetvaltype(pvlist, 0, AU_PVTYPE_LONG);
AUpvsetparam(pvlist, 1, AF_AWARE_PARAM_BITRATE_POLICY);
AUpvsetvaltype(pvlist, 1, AU_PVTYPE_LONG);
AUpvsetparam(pvlist, 2, AF_AWARE_PARAM_BITRATE_TARGET);
AUpvsetvaltype(pvlist, 2, AU_PVTYPE_LONG);

AFgetcompressionparams(file, AF_DEFAULT_TRACK,
&track_desc->compressiontype, pvlist, 3);
AUpvgetval(pvlist, 0, &track_desc->aware_desc.layer);
AUpvgetval(pvlist, 1,
&track_desc->aware_desc.bitratepolicy);
AUpvgetval(pvlist, 2,
&track_desc->aware_desc.bitratetarget);

AUpvfree(pvlist);
}LBL="" HELPID=""ID="78081"Opening, Closing, and Updating Audio FilesBefore opening a new audio file using AFopenfile(), create and configure an appropriate AFfilesetup structure (as described in IDREF="10191" TYPE="TITLE""Creating and Configuring Audio Files"). Audio files can be opened either for reading or writing (but not both simultaneously). In order to change an existing file, you must copy the contents of the file to a new file, writing edits as you go. See the sample source code in /usr/people/4Dgifts/examples/dmedia/soundfile for a demonstration of this process.LBL="" HELPID=""Opening an Audio FileID="Media2-5AF81"AFopenfile() allocates and initializes an AFfilehandle structure for a named file. The audio track logical read/write pointer used by AFreadframes() and AFwriteframes() is initialized to point to the location of the first sample in the audio file. Its function prototype is:AFfilehandle AFopenfile(char *name, char *mode,                        AFfilesetup setup)where name is a character string that names the file to be opened, and mode identifies whether the file is being opened for read or write access. Valid values for mode are:"r" ­ read-only access"w" ­ write-only accesssetup is an AFfilesetup structure previously created using AFnewfilesetup() and configured using various AF Library initialization functions described in previous sections. setup is ignored when mode is set to "r".AFopenfile() returns an AFfilehandle structure for the named file. If an error occurs, AFopenfile() returns the value AF_NULL_FILEHANDLE.LBL="" HELPID=""Getting an IRIX File Descriptor for an Audio FileAnother way of opening a file is to call the IRIX system function open() to open the file, and then get a handle to the file descriptor from the AF Library.ID="Media2-5AF82"AFopenfd() returns an AFfilehandle structure for a file that has already been opened. Its function prototype is:AFfilehandle AFopenfd(int fd, char *mode, AFfilesetup setup)where fd is an IRIX file descriptor previously returned by open(), mode identifies whether the file is being opened for read or write access (see AFopenfile()), and setup is an AFfilesetup structure previously created using AFnewfilesetup() and configured using various AF Library initialization functions described in previous sections. setup is ignored when mode is setto "r".AFopenfd() returns an AFfilehandle structure for the named file. If an error occurs, AFopenfd() returns the value AF_NULL_FILEHANDLE.ID="Media2-5AF83"AFgetfd() returns the IRIX file descriptor associated with the audio file referred to by the given AFfilehandle structure. Its function prototype is:int AFgetfd(AFfilehandle file)where file is the AFfilehandle structure previously created by a call to AFopenfile().The file descriptor returned by AFgetfd() is intended for use in a select() loop. It is not intended to allow reading, writing, and seeking in an audio file without the knowledge of the Audio File Library. Doing so causes unpredictable results unless you save and restore the file position whenever you modify it.The AF does not reposition the file to the correct place before reading from (using AFreadframes()) or writing to (using AFwriteframes()) it. If you modify the file position of the file descriptor given by AFgetfd(), you should save the file position and restore it to its previous position before reading or writing data to the file. Alternately, you can use one of two different file descriptors opened to the same file. The file must be re-opened in order to get a separate file descriptor (dup(2) will not work because it gives you two file descriptors that share the same file offset). In addition, if you attempt to write to the file, no matter how the AFfilehandle was opened, the results are undefined. LBL="" HELPID=""Closing and Updating FilesID="Media2-5AF84"AFclosefile() releases a file's resources back to the system. It also updates the headers of files opened for write access. The AFfilehandle structure deallocated by AFclosefile() should not be used by any subsequent AF Library function calls. Its function prototype is:long AFclosefile(AFfilehandle file)where file is the AFfilehandle structure to be deallocated. This structure was returned by AFopenfile() when the file being closed was created.AFclosefile() returns a negative value if an error occurs while closing a file and updating the header fields. If compression was used to write a file, a negative value indicates that some sample frames were lost due to the filter delay of the compressor. If no error occurs, the return value is 0.ID="Media2-5AF85"AFsyncfile() updates the complete contents of an audio file opened for writing without actually closing the file. This is useful for maintaining consistent header information between writing samples to the file's audio track. Its function prototype is:long AFsyncfile(AFfilehandle file)where file is the AFfilehandle structure to be updated. This structure was returned by AFopenfile() when the file being closed was created.AFsyncfile() returns a negative value if an error occurs while trying to update file. If the update is successful, or if file was opened as read-only, AFsyncfile() returns 0.LBL="" HELPID=""ID="20612"Reading and Writing Audio Track InformationThis section describes functions that read and manipulate audio track data and parameters in an audio file. Your application should query for audio file characteristics before opening a file and reading and writing data.ID="Media2-5AF86"LBL="" HELPID=""Getting Audio File FormatThis section describes functions that query the file format from either a file handle or from an IRIX file descriptor of an opened audio file.ID="Media2-5AF87"AFgetfilefmt() returns an integer value indicating the format of the file and returns a separate version number for AIFF-C files. Its function prototype is:long AFgetfilefmt(AFfilehandle file, long *version)where file is the AFfilehandle structure previously created by a call to AFopenfile() or AFopenfd(), and version is used to return a file format version number in the form of a non-negative long integer. AIFF files do not use version numbers, so a value of 0 is always returned as the AIFF version number.AFgetfilefmt() returns a non-negative long integer indicating the format of the file. Currently supported values include:AF_FILE_AIFFC (AIFF-C format)AF_FILE_AIFF (AIFF format)but your application should allow for the possibility of other (or unknown) file formats being returned.ID="Media2-5AF88"AFidentifyfd() returns the file format of a given IRIX file descriptor. Its function prototype is:long AFidentifyfd(int fd)where fd is an IRIX file descriptor previously returned by open().AFidentifyfd() returns a long integer value representing the audio file format (see AFgetfilefmt() for the return values for supported formats). If AFidentifyfd() does not recognize the format, AF_FILE_UNKNOWN is returned. If the format is not one supported by the AF Library, AF_FILE_UNSUPPORTED is returned.To determine whether a file is a sound file that can be opened by the AF, check for an unrecognizable format rather than a recognizable format. For example, rather than testing whether the file format is either AF_FILE_AIFF or AF_FILE_AIFC, use this code:if (filefmt == AF_FILE_UNSUPPORTED ||
    filefmt == AF_FIILE_UNKNOWN)
        {
        printf("file is not supported by the AF library!");
    exit(0);
}Applications that branch depending on the file format should still check for unrecognized formats:switch (AFidentifyfd(fd))
{
case AF_FILE_AIFF: do_aiff_thing(); break;
case AF_FILE_AIFC: do_aiffc_thing(); break;
case AF_FILE_UNKNOWN:
case AF_FILE_UNSUPPORTED:
        printf("this file is not supported by AF library!!");
        exit(0);
default:
        printf("program cannot handle this file format!");
        exit(0);
}TipSometimes, instead of checking the file format, you should check the sampling format and other track parameters from the audio file track, as described in IDREF="33089" TYPE="TITLE""Getting and Setting Audio Track Parameters". For example, a program that simply reads 16-bit AF_SAMPFMT_TWOSCOMP audio data out of an AIFF file should be able to correctly read that type of data out of a file whose file format is not AIFF, as long as it does not also intend to read AIFF-specific chunks from the data (for example, certain MISC and INST chunks). Such a program has no need to call AFidentifyfd() or AFgetfilefmt() to get the file format.LBL="" HELPID=""ID="33089"Getting and Setting Audio Track ParametersMost audio track parameters (except markers) must be initialized before a new audio file is opened and cannot be modified after that point, but you should query an audio file for its track parameters.LBL="" HELPID=""Getting Audio Track Sample RateID="Media2-5AF89"AFgetrate() returns the sample rate of an audio track in an opened audio file. Its function prototype is:double AFgetrate(AFfilehandle file, long trackid)where file is the AFfilehandle structure previously created by a call to AFopenfile() or AFopenfd(), and trackid is the ID for the audio track (for AIFF and AIFF-C files, this value should always be AF_DEFAULT_TRACK). AFgetrate() returns a double-precision floating point value that describes in Hz the audio sampling rate of the audio track.LBL="" HELPID=""ID="77680"Getting Audio Track Sample Format and Sample WidthID="Media2-5AF90"AFgetsampfmt() retrieves the sample format and sample width for an audio track in an opened audio file. Its function prototype is:void AFgetsampfmt(AFfilehandle file, long trackid,                  long *sampfmt, long *width)where file is the AFfilehandle structure previously created by a call to AFopenfile() or AFopenfd(), trackid is the ID for the audio track (for AIFF and AIFF-C files, this value should always be AF_DEFAULT_TRACK), sampfmt is a pointer to a long integer denoting the format of the sample data (for AIFF and AIFF-C files, this value is always AF_SAMPFMT_TWOSCOMP), and width is a pointer to a long integer that denotes the sample width in bits (for AIFF and AIFF-C files, this value is between 1 and 32).TipDo not assume that AF_SAMPFMT_TWOSCOMP is the only value that can be returned by ID="Media2-5AF91"AFgetsampfmt(). Write your application so that it rejects files with sample formats it does not support.Sample width may or may not have meaning, depending on the value of sampfmt. For AF_SAMPFMT_TWOSCOMP data, you can use the sample width value to determine the data type used to pass samples to AFwriteframes() and from AFreadframes(): 1­8 bit samples are packed into chars, 9­16 bit samples are packed into shorts, and 17­32 bit samples are packed into longs. Data formats whose sample width is not a multiple of eight are augmented by zero-bit-padding on the right (see IDREF="64196" TYPE="GRAPHIC"Figure 7-1).There is a special case for reading 24-bit integer data. The AF automatically converts 3-byte data into 4-byte quantities in a manner that is compatible with the Audio Library (AL) by sign-extending the left-most bits of 17 to 24­bit data.IDREF="64196" TYPE="GRAPHIC"Figure 7-1 shows the data packing for twos complement integer data (AF_SAMPFMT_TWOSCOMP).FILE="compdata.ai" POSITION="INLINE" SCALE="FALSE"LBL="7-1"Figure 7-1 ID="64196"Audio Data Packing FormatsTipDon't assume that the maximum size of integers in files opened by the AF Library is 32 bits or that the number of bits will be a multiple of 8. Even for AIFF files, the sample width is not necessarily a multiple of 8. Generally, this can be ignored, because audio samples that do not take up an integral number of bytes are left-justified inside the next larger integral number of bytes (with the remaining bits set to 0). But you should write your application so that it does not assume the sample width is a multiple of 8, as demonstrated in IDREF="56801" TYPE="TEXT"Example 7-2.IDREF="56801" TYPE="TEXT"Example 7-2 checks for the audio track sample format, and then classifies integer data according to its sample width.LBL="7-2"Example 7-2 ID="56801"Checking Audio Track Sample Format and Sample Width#include <dmedia/audiofile.h>
...
AFfilehandle h = AFopenfile(....);
if (!h) return;

AFgetsampfmt(h, AF_DEFAULT_TRACK, &sampfmt, &sampwidth);

if (sampfmt != AF_SAMPFMT_TWOSCOMP)
    {
    printf("This program can't read audio files of this "
           sample format");
           exit(0);
           }
/* round sampwidth up to nearest number of bytes */
int nbytes = ( (sampwidth-1) / 8 ) + 1;
switch (nbytes)
    {
    case 1:  do_8_thing(); break;
    case 2: do_16_thing(); break;
    case 3: do_24_thing(); break;
    case 4: do_32_thing(); break;
    default:
        printf("This program can't read audio files of this "
               sample width %d\n", sampwidth);
               exit(0);
               }LBL="" HELPID=""Getting Audio Track ChannelsThe number of channels in an audio track is initially set by AFinitchannels() before the file is created.ID="Media2-5AF92"AFgetchannels() returns the number of interleaved audio channels in the audio track of an opened audio file. Its function prototype is:long AFgetchannels(AFfilehandle file, long trackid)where file is the AFfilehandle structure previously created by a call to AFopenfile() or AFopenfd(), and trackid is the ID for the audio track (for AIFF and AIFF-C files, this value should always be AF_DEFAULT_TRACK). AFgetchannels() returns 1 if trackid is monaural, 2 if it is stereo, or any other positive integer (even for AIFF/AIFF-C files). TipYour application should be able to handle audio files containing an arbitrary number of channels. For example, the application could reject a file that has more than the supported number of channels, or it could combine channels selectively or use certain channels while ignoring others.LBL="" HELPID=""Getting AES DataID="Media2-5AF93"AFgetaeschanneldata() retrieves AES channel status information from an opened audio file. Its function prototype is:long AFgetaeschanneldata(AFfilehandle file, long trackid,                         unsigned char buf[24])where file is the AFfilehandle structure previously created by a call to AFopenfile() or AFopenfd(), trackid is the ID for the audio track (for AIFF and AIFF-C files, this value should always be AF_DEFAULT_TRACK), and buf is a 24-element array that receives the AES channel status bytes.AFgetaeschanneldata() returns a 1 if there is AES channel data, or a 0 if there is no data.TipThere is no guarantee whether a given file format will contain AES data, so your application should call AFgetaeschanneldata() to determine whether AES channel bytes are encoded in an audio file.LBL="" HELPID=""Getting Audio Track CompressionThis section describes routines that let you get compression information for an audio track from an AFfilehandle structure.When reading or writing a file (even an AIFF-C file) containing compressed data, first call AFgetsampfmt() to get the native sample format of the codec, and check that it is able to be read/written using that format. The native sample format of a codec is the sample format of the data it produces on decompression or expects on compression.TipYour application should reject compressed files with native sample formats it does not support. Check for an unrecognized format rather than a defined format. The currently defined codecs all convert the compressed data to and from 16-bit AF_SAMPFMT_TWOSCOMP data, but you should not assume that a certain format is guaranteed for future codecs. For example, if you know that the file is AF_COMPRESSION_G711_ULAW, then the native format for that codec is 16-bit AF_SAMPFMT_TWOSCOMP. However, you should call AFgetsampfmt() in any case, to allow for the possibility of future codecs whose native sample format is something other than 16-bit signed integer or which have more than one native sample format (some may be configurable or may vary depending on what kind of data was originally compressed). ID="Media2-5AF94"AFgetcompression() and ID="Media2-5AF95"AFgetcompressionparams() return the compression type used in the audio track of an opened audio file. In addition, AFgetcompressionparams() scans a requested number of items and returns codec-specific parameters for the audio track. AFgetcompression() returns a long integer representing the compression algorithm used for the audio track's data; AFgetcompressionparams() returns this value in the compression pointer.The function prototypes are:long AFgetcompression(AFfilehandlefile file, long trackid)
long AFgetcompressionparams(AFfilehandle file, long trackid,           long *compression, AUpvlist pvlist, long numitems)where:fileis the AFfilehandle structure previously created by a call to AFopenfile() or AFopenfd().trackidis the ID for the audio track (for AIFF and AIFF-C files, this value should always be AF_DEFAULT_TRACK). compressionis a pointer to a positive long integer that will be filled in with the symbolic constant that indicates the type of audio compression being used for the specified audio track. See IDREF="67942" TYPE="TABLE"Table 7-7 for a list of possible return values.pvlistis an AUpvlist structure, to be filled with parameters and values related to the compression scheme compression. Currently, the only compression schemes that have any parameters are those supplied by Aware, Inc. numitemsis the number of valid entries in the pvlist.IDREF="67942" TYPE="TABLE"Table 7-7 lists the valid return values for AIFF-C files. AIFF files always return AF_COMPRESSION_NONE.COLUMNS="2"LBL="7-7"Table 7-7 ID="67942"Valid Return Values for Compression Algorithms and ParametersLEFT="0" WIDTH="185"Parameter ValueLEFT="190" WIDTH="147"Compression TypeLEFT="0" WIDTH="185"AF_COMPRESSION_UNKNOWNLEFT="190" WIDTH="147"Unrecognized compression schemeLEFT="0" WIDTH="185"AF_COMPRESSION_NONELEFT="190" WIDTH="147"No compressionLEFT="0" WIDTH="185"AF_COMPRESSION_G722LEFT="190" WIDTH="147"64 Kbps ADPCM for 16 kHz 16-bitLEFT="0" WIDTH="185"AF_COMPRESSION_G711_ULAWLEFT="190" WIDTH="147"64 Kbps encoding for 8 kHz 16-bitLEFT="0" WIDTH="185"AF_COMPRESSION_G711_ALAWLEFT="190" WIDTH="147"64 Kbps encoding for 8 kHz 16-bitLEFT="0" WIDTH="185"AF_COMPRESSION_AWARE_MPEGLEFT="190" WIDTH="147"Aware implementation of ISO/
MPEG I-audio Layers I and IILEFT="0" WIDTH="185"AF_COMPRESSION_AWARE_MULTIRATELEFT="190" WIDTH="147"Aware MultiRate I lossless or near-
lossless algorithmLEFT="0" WIDTH="185"AF_COMPRESSION_APPLE_ACE3LEFT="190" WIDTH="147"Not currently supportedLEFT="0" WIDTH="185"AF_COMPRESSION_APPLE_ACE8LEFT="190" WIDTH="147"Not currently supportedLEFT="0" WIDTH="185"AF_COMPRESSION_APPLE_MAC3LEFT="190" WIDTH="147"Not currently supportedLEFT="0" WIDTH="185"AF_COMPRESSION_APPLE_MAC6LEFT="190" WIDTH="147"Not currently supportedThe Audio File Library provides built-in codec support for five compression algorithms: CCITT G.722, CCITT G.711 name='mgr' font=symbol charset=fontspecific code=109
	TeX='\mu '      descr='[mgr]'-law and A-law, and the Aware, Inc. ISO/MPEG I-audio and MultiRate I algorithms. To get more specific information about the Aware algorithms, such as MPEG I layers, see IDREF="63476" TYPE="TITLE"Appendix B, "Aware Scalable Audio Compression Software."NoteThe four Apple compression algorithms listed in IDREF="67942" TYPE="TABLE"Table 7-7 are proprietary to Apple Computer Inc., and are not currently supported by the Audio File Library.ID="Media2-5AF96"AFgetcompressionname() returns a null-terminated string containing the name of the compression algorithm used for an audio track in an opened audio file. Its function prototype is:char *AFgetcompressionname(AFfilehandle file, long trackid)where file is the AFfilehandle structure previously created by a call to AFopenfile() or AFopenfd(), and trackid is the ID for the audio track (for AIFF and AIFF-C files, this value should always be AF_DEFAULT_TRACK). If compression is not used, as is the case with AIFF files, AFgetcompressionname() returns a null string.LBL="" HELPID=""Getting Audio Track Sample Frame CountID="Media2-5AF97"AFgetframecnt() returns the total number of sample frames in the audio track of an opened audio file. Its function prototype is:long AFgetframecnt(AFfilehandle file, long trackid)where file is the AFfilehandle structure previously created by a call to AFopenfile() or AFopenfd(). trackid is the ID for the audio track (for AIFF and AIFF-C files, this value should always be AF_DEFAULT_TRACK).AFgetframecnt() returns a long integer value that is the current total of sample frames in the track.LBL="" HELPID=""ID="90262"Getting and Setting Audio Track MarkersThis section describes functions that get information about the markers in a given audio track and explains how to set the position of those markers. Markers point to positions between adjacent sample frames. For a track containing n sample frames, position 0 is before the first sample frame, and position n is after the last sample frame in the track.ID="Media2-5AF98"AFgetmarkids() retrieves an array of marker IDs from a given audio track in an opened audio file. It returns the number of marker structures in the specified audio track. Its function prototype is:long AFgetmarkids(AFfilehandle file, long trackid,                  long markids[])where file is the AFfilehandle structure previously created by a call to AFopenfile() or AFopenfd(), trackid is the ID for the audio track (for AIFF and AIFF-C files, this value should always be AF_DEFAULT_TRACK), and markids is an array of long integers that receives the marker IDs for the marker structures in the audio track.AFgetmarkids() returns a non-negative integer value specifying the number of marker structures in the given audio track.TipCheck for unrecognized mark return values rather than recognized values. Write your application so that it expects any number of marks and any type of mark (not just the currently defined types) and rejects files containing marks it does not support. Typically, you call AFgetmarkids() twice. The first time, you pass markids a null pointer and check the return value of the function. This value tells you how many locations to allocate in the markids array, which you pass back to AFgetmarkids() to obtain the list of marker IDs.ID="Media2-5AF99"AFgetmarkname() returns the name string of a given marker within the audio track of an opened audio file. Its function prototype is:char *AFgetmarkname(AFfilehandle file, long trackid,                    long markid)where file is the AFfilehandle structure previously created by a call to AFopenfile() or AFopenfd(), trackid is the ID for the audio track (for AIFF and AIFF-C files, this value should always be AF_DEFAULT_TRACK), and markid is the ID of the marker whose name you want to retrieve.AFgetmarkname() returns a null-terminated character string that is the name associated with the given markid.ID="Media2-5AF100"AFgetmarkpos() returns the frame location of a given marker in the audio track of an opened audio file. Its function prototype is:long AFgetmarkpos(AFfilehandle file, long trackid, long markid)where file is the AFfilehandle structure previously created by a call to AFopenfile() or AFopenfd(), trackid is the ID for the audio track (for AIFF and AIFF-C files, this value should always be AF_DEFAULT_TRACK), and markid is the ID of the marker whose position you want to discover.AFgetmarkpos() returns a non-negative long integer value indicating the position of the marker in the track.AFsetmarkpos() ID="Media2-5AF101"sets the frame location of a given marker in the audio track of an audio file opened for write access. Its function prototype is:void AFsetmarkpos(AFfilehandle file, long track, long markid,                  long markpos)where file is the AFfilehandle structure previously created by a call to AFopenfile() or AFopenfd(), trackid is the ID for the audio track (for AIFF and AIFF-C files, this value should always be AF_DEFAULT_TRACK), markid is the ID of the marker whose position you want to move, and markpos is a non-negative long integer that describes the position to which you want to move the marker in the track.LBL="" HELPID=""Seeking, Reading, and Writing Audio Track FramesThis section describes functions that position the read pointer in a file's audio track and functions that read and write frames. You can read and seek only from a file opened for reading. Similarly, you can write frames only to a file opened for writing.LBL="" HELPID=""Seeking to a Position in an Audio File TrackWhen a file is opened for read access by AFopenfile() or AFopenfd(), the logical track pointer for the audio track is initialized to point to the first frame in the track. This location can be changed by calling AFseekframe(). Before returning, AFreadframes() moves the logical track pointer so that it points to the frame following the one last copied into frames. CautionThe logical track pointer is not the same thing as the IRIX file pointer which you position by calling the IRIX lseek(2) command.ID="Media2-5AF102"AFseekframe() moves the logical track pointer in the audio track of an audio file opened for read-only access to a specified frame. Its function prototype is:long AFseekframe(AFfilehandle file, long trackid,                 long offset)where file is the AFfilehandle structure previously created by a call to AFopenfile() or AFopenfd(), trackid is the ID for the audio track (for AIFF and AIFF-C files, this value should always be AF_DEFAULT_TRACK), and offset is the number of frames from the beginning of the track that the pointer will be moved to. This value is between 0 and the total number of frames in the track, minus 1. The total number of frames in the track can be determined by calling AFgetframecnt().When AFseekframe() succeeds, it returns the actual offset value; otherwise, it returns a negative value.LBL="" HELPID=""Reading Audio Frames from an Audio TrackID="Media2-5AF103"AFreadframes() copies sample frames from an audio file opened for reading to a buffer. Its function prototype is:long AFreadframes(AFfilehandle file, long trackid,                  void *frames, long count) where file is the AFfilehandle structure previously created by a call to AFopenfile() or AFopenfd(), trackid is the ID for the audio track (for AIFF and AIFF-C files, this value should always be AF_DEFAULT_TRACK), frames is a pointer to a buffer into which you want to transfer copies of sample frames from file, and count is the number of sample frames you want to read from file. AFreadframes() returns a long value indicating the number of frames successfully read from the audio track.The data copied into frames must be interpreted according to the sample format and sample width parameter returned by AFgetsampfmt() and channel count returned by AFgetchannels(), as described in IDREF="77680" TYPE="TITLE""Getting Audio Track Sample Format and Sample Width". For AF_SAMPFMT_TWOSCOMP, AFreadframes() copies the frames to the buffer using the smallest data type (char, short, or long) that capable of holding the data. AFreadframes() automatically decompresses data encoded using any of the supported compression algorithms. (For Aware compression, an Aware license must be installed.)TipQuery for the sample format, sample width, and channels. Don't assume that a particular file format determines the sample format, sample width, or number of channels. Provide a mechanism for detecting and handling unsupported file configurations.LBL="" HELPID=""Writing Audio Frames to an Audio TrackWhen a file is opened for write access by AFopenfile() or AFopenfd(), the logical track pointer for the file's audio track is initialized to point to the first frame in the track. Before returning, AFwriteframes() moves the logical track pointer so that it points to the frame following the one last copied into samples.CautionThe logical track pointer is not the same thing as the IRIX file pointer which you position by calling the IRIX lseek(2) command.ID="Media2-5AF104"AFwriteframes() copies frames from a buffer to an audio file opened for writing. Its function prototype is:long AFwriteframes(const AFfilehandle file, long track,                   void samples, const long count) where file is the AFfilehandle structure previously created by a call to AFopenfile() or AFopenfd(), track is a long integer which identifies the audio track (for AIFF and AIFF-C files, this value should always be AF_DEFAULT_TRACK), samples is a pointer to a buffer containing sample frames that you want to write to file, and count is the number of sample frames you want to write to file. For AF_SAMPFMT_TWOSCOMP data, AFwriteframes() expects the frames to be buffered using the smallest data type (char, short, or long) capable of holding the data. AFwriteframes() automatically compresses data encoded using any of the supported compression algorithms.AFwriteframes() returns a long value indicating the number of frames successfully written to the audio track. The return value is normally greater than or equal to 0; however, when a codec is being used and buffered data cannot be written to disk, that data is lost. In such a case, AFwriteframes() returns a negative value, indicating the number of sample frames lost.LBL="" HELPID=""ID="44196"Reading and Writing Instrument ConfigurationsUse the functions in this section to retrieve and manipulate instrument configuration data and parameters.ID="Media2-5AF105"LBL="" HELPID=""Getting and Setting Instrument ParametersUse the functions described in this section to retrieve and set the instrument configuration parameters of an audio file. The parameters can be read from any opened audio file and written to any audio file opened as write-only.ID="Media2-5AF106"AFgetinstids() retrieves an array of instrument IDs corresponding to the instrument chunks in a given audio file. It returns the number of instrument chunks in the file. Its function prototype is:long AFgetinstids(AFfilehandle file, long instids[])where file is the AFfilehandle structure previously created by a call to AFopenfile() or AFopenfd(), and instids is an array of long integer instrument IDs that reference instrument chunks within the file.Typically, you call AFgetinstids() twice. The first time, you pass instids a null pointer and check the return value of the function. This value tells you how many locations to allocate in the instids array, which you pass back to AFgetinstids() to obtain the list of instrument IDs.NoteThe AF Library currently supports only AIFF and AIFF-C file types, so the number of instrument chunks is always either 0 or 1. If the file does contain an instrument chunk, its ID will always be AF_DEFAULT_INST for AIFF and AIFF-C files. But other instrument configurations could be returned in future releases of the AF Library.TipWrite your application so that it checks for and rejects instrument configurations that you don't want to support.ID="Media2-5AF107"AFgetinstparamlong() retrieves a long instrument configuration parameter value from an instrument configuration in an open audio file. Its function prototype is:long AFgetinstparamlong(AFfilehandle file, long instid,                        long param)where file is the AFfilehandle structure previously created by a call to AFopenfile() or AFopenfd(). instid is the instrument ID for the instrument configuration chunk (for AIFF and AIFF-C files, this value should always be AF_DEFAULT_INST). param is a symbolic constant that identifies an instrument parameter. See IDREF="94362" TYPE="TABLE"Table 7-3 and IDREF="95625" TYPE="TABLE"Table 7-8 for a list of valid parameter constants and values associated with them.AFgetinstparamlong() returns the long integer value associated with the parameter specified in param. If instid or param is not valid, the value returned is 0.IDREF="95625" TYPE="TABLE"Table 7-8 lists the instrument parameter constants and their valid values.COLUMNS="2"LBL="7-8"Table 7-8 ID="95625"Instrument Parameter Constants and Valid ValuesLEFT="0" WIDTH="163"Instrument Parameter ConstantLEFT="170" WIDTH="168"Valid ValuesLEFT="0" WIDTH="163"AF_INST_MIDI_BASENOTELEFT="170" WIDTH="168"0­127LEFT="0" WIDTH="163"AF_INST_NUMCENTS_DETUNELEFT="170" WIDTH="168"-50 to 50LEFT="0" WIDTH="163"AF_INST_MIDI LONOTELEFT="170" WIDTH="168"0­127LEFT="0" WIDTH="163"AF_INST_MIDI_HINOTELEFT="170" WIDTH="168"0­127LEFT="0" WIDTH="163"AF_INST_MIDI_LOVELOCITYLEFT="170" WIDTH="168"1­127LEFT="0" WIDTH="163"AF_INST_MIDI_HIVELOCITYLEFT="170" WIDTH="168"1­127LEFT="0" WIDTH="163"AF_INST_NUMDBS_GAINLEFT="170" WIDTH="168"-32768 to 32767LEFT="0" WIDTH="163"AF_INST_SUSLOOPIDLEFT="170" WIDTH="168"Any positive long integer valueLEFT="0" WIDTH="163"AF_INST_RELLOOPIDLEFT="170" WIDTH="168"Any positive long integer valueTipCheck for unrecognized instrument configuration and parameters rather than recognized types. Write your application so that it expects any type of instrument configuration (not just the currently defined types) and rejects files containing instruments it does not recognize. ID="Media2-5AF108"AFsetinstparamlong() writes a long instrument configuration parameter value to a given instrument configuration chunk in an audio file that has been opened for writing. Its function prototype is:void AFsetinstparamlong(AFfilehandle file, long instid,                        long param, long value)where file is the AFfilehandle structure previously created by a call to AFopenfile() or AFopenfd(), instid is the instrument ID for the instrument configuration chunk (for AIFF and AIFF-C files, this value should always be AF_DEFAULT_INST), param is a symbolic constant that identifies an instrument parameter, and value is the long integer value you want to assign to parameter named by param. See IDREF="94362" TYPE="TABLE"Table 7-3 and IDREF="95625" TYPE="TABLE"Table 7-8 for a list of valid parameter constants and values associated with them.LBL="" HELPID=""Getting and Setting Loop InformationThis section describes functions that retrieve and set the positions of instrument loops within an opened audio file. The loop information may be read from any opened audio file and written to any audio file opened as write-only. To get and set instrument loop IDs, use AFgetinstparamlong() and AFsetinstparamlong(), as described in IDREF="44196" TYPE="TITLE""Reading and Writing Instrument Configurations".ID="Media2-5AF109"AFgetloopmode() returns the loop mode of a given loop in the instrument configuration of an opened audio file. Its function prototype is:long AFgetloopmode(AFfilehandle file, long instid, long loopid)where file is the AFfilehandle structure previously created by a call to AFopenfile() or AFopenfd(), instid is the instrument ID for the instrument configuration chunk (for AIFF and AIFF-C files, this value should always be AF_DEFAULT_INST), and loopid is the ID number associated with the loop whose mode you wish to read.AFgetloopmode() returns a long integer value representing the loop mode. Current valid values for loop mode are:AF_LOOP_MODE_NOLOOP (no loop)AF_LOOP_MODE_FORW (forward loop)AF_LOOP_MODE_FORWBAKW (alternating forward/backward)ID="Media2-5AF110"AFsetloopmode() sets the loop mode of a given loop in the instrument configuration of an audio file opened as write-only. Its function prototype is:void AFsetloopmode(AFfilehandle file, long instid,                   long loopid, long mode)where file is the AFfilehandle structure previously created by a call to AFopenfile() or AFopenfd(), instid is the instrument ID for the instrument configuration chunk (for AIFF and AIFF-C files, this value should always be AF_DEFAULT_INST), loopid is the ID number associated with the loop whose mode you wish to write, and mode is the long integer value you wish to set for the loop mode. See AFgetloopmode() for the list of valid mode values.ID="Media2-5AF111"AFgetloopstart() returns an audio track marker ID associated with the starting point of a given instrument loop. Its function prototype is:long AFgetloopstart(AFfilehandle file, long instid, long loopid)where file is the AFfilehandle structure previously created by a call to AFopenfile() or AFopenfd(), instid is the instrument ID for the instrument configuration chunk (for AIFF and AIFF-C files, this value should always be AF_DEFAULT_INST), and loopid is the ID number associated with the loop whose starting point you wish to read.AFgetloopstart() returns a long integer value, which is a marker ID in the audio track. See IDREF="90262" TYPE="TITLE""Getting and Setting Audio Track Markers" in IDREF="20612" TYPE="TITLE""Reading and Writing Audio Track Information" for information on how to manipulate the position of the markers referred to by the marker IDs.AFsetloopstart() ID="Media2-5AF112"causes an audio track marker ID to be associated with the starting point of a given instrument loop. Its function prototype is:void AFsetloopstart(AFfilehandle file, long instid,                    long loopid, long markid)where file is the AFfilehandle structure previously created by a call to AFopenfile() or AFopenfd(), instid is the instrument ID for the instrument configuration chunk (for AIFF and AIFF-C files, this value should always be AF_DEFAULT_INST), loopid is the ID number associated with the loop whose starting point you wish to write, and markid is the audio track marker that you wish to assign as the starting point of the given loop.ID="Media2-5AF113"AFgetloopend() returns an audio track marker ID associated with the ending point of a given instrument loop. Its function prototype is:long AFgetloopend(AFfilehandle file, long instid, long loopid)where file is the AFfilehandle structure previously created by a call to AFopenfile() or AFopenfd(), instid is the instrument ID for the instrument configuration chunk (for AIFF and AIFF-C files, this value should always be AF_DEFAULT_INST), and loopid is the ID number associated with the loop whose ending point you wish to read.AFgetloopend() returns a long integer value which is a marker ID in the audio track. See IDREF="90262" TYPE="TITLE""Getting and Setting Audio Track Markers" in IDREF="20612" TYPE="TITLE""Reading and Writing Audio Track Information" for information on how to manipulate the position of the markers referred to by the marker IDs.ID="Media2-5AF114"AFsetloopend() causes an audio track marker ID to be associated with the ending point of a given instrument loop. Its function prototype is:void AFsetloopend(AFfilehandle file, long instid,                  long loopid, long markid)where file is the AFfilehandle structure previously created by a call to AFopenfile() or AFopenfd(), instid is the instrument ID for the instrument configuration chunk (for AIFF and AIFF-C files, this value should always be AF_DEFAULT_INST), loopid is the ID number associated with the loop whose ending point you wish to write, and markid is the audio track marker that you wish to assign as the ending point of the given loop.TipLoop queries can return any configuration of loops within an instrument, not just the fixed value of 2 in AIFF/AIFF-C files. Have your application check for and reject loop configurations it does not support.LBL="" HELPID=""Handling Miscellaneous Data ChunksThe following sections describe how to read to, write from, and get information about the miscellaneous data chunks in an audio file.ID="Media2-5AF115"LBL="" HELPID=""Getting Miscellaneous Data ParametersThis section describes functions that get information about the number, size and type of miscellaneous data chunks in an opened audio file.ID="Media2-5AF116"AFgetmiscids() returns the number of miscellaneous data chunks in a file and an array containing the IDs of each miscellaneous chunk. Its function prototype is:long AFgetmiscids(AFfilehandle file, long miscids[])file is the AFfilehandle structure previously created by a call to AFopenfile() or AFopenfd(). miscids is an array of positive long integers that contains the IDs for the miscellaneous data chunks in file.AFgetmiscids() returns a long integer value equal to the number of miscellaneous data chunks in file. To fill the miscids array with the corresponding IDs, you first call AFgetmiscids() with a null miscids pointer, and then allocate a miscids buffer according to the return value. You can then call AFgetmiscids() again, passing the properly dimensioned miscids buffer to obtain the list of IDs.ID="Media2-5AF117"AFgetmisctype() returns the type of a given miscellaneous chunk. Its function prototype is:long AFgetmisctype(AFfilehandle file, long chunkid)where file is the AFfilehandle structure previously created by a call to AFopenfile() or AFopenfd(), and chunkid is a positive long integer miscellaneous chunk ID from the miscids array returned by AFgetmiscids().AFgetmisctype() returns a long integer constant that describes the chunk type. See IDREF="75400" TYPE="TABLE"Table 7-5 for the list of valid chunk types and constants. If the chunk is not of any of the types listed in IDREF="75400" TYPE="TABLE"Table 7-5, AFgetmisctype() will return the value AF_MISC_AIFF_UNRECOGNIZED.TipThe set of chunk types may expand at any time. Check for unrecognized chunk types rather than recognized chunk types. Write your application so that it expects any type of MISC chunk (not just the currently defined types) and rejects miscellaneous chunks it does not recognize. ID="Media2-5AF118"AFgetmiscsize() returns the size of a given miscellaneous data chunk in bytes. Its function prototype is:long AFgetmiscsize(AFfilehandle file, long chunkid)where file is the AFfilehandle structure previously created by a call to AFopenfile() or AFopenfd(), and chunkid is a positive long integer miscellaneous chunk ID from the miscids array returned by AFgetmiscids().AFgetmiscsize() returns a long integer value that describes the size of the data in the chunk in bytes. This number does not take into account null-terminators in strings, so you will need to add one to the value returned when actually reading string data (see AFreadmisc()).LBL="" HELPID=""Reading, Writing, and Seeking Miscellaneous DataThis section describes functions that read and write miscellaneous data and to position the read/write location pointer within the data portion of a miscellaneous chunk. The AFfilehandle structure maintains a logical read/write pointer for each miscellaneous data chunk in the file. Each pointer is initialized to point at the first data byte with the chunk when the AFfilehandle structure is created.TipTo avoid file corruption, don't copy MISC chunks from one file to another unless the content of those chunks is known. A chunk can contain references to other parts of the file that have been modified by the application, in which case attempting to copy it without properly modifying its contents would cause an error.ID="Media2-5AF119"AFreadmisc() reads data from a given miscellaneous chunk into a buffer, and returns the number of bytes read. Its function prototype is:long AFreadmisc(AFfilehandle file, long chunkid,                void *buf, long nbytes)where file is the AFfilehandle structure previously created by a call to AFopenfile() or AFopenfd(), chunkid is a positive long integer miscellaneous chunk ID from the miscids array returned by AFgetmiscids(), buf is a pointer to a buffer that will receive the data from the miscellaneous chunk, and nbytes is the number of bytes you want to read from the audio file into buf, beginning at the current position of file's logical read pointer for the data in miscid. AFreadmisc() will not read past the end of the chunk's data area. After reading the data, AFreadmisc() updates the position of the read/write pointer to point to the data byte following the last one read.ID="Media2-5AF120"AFwritemisc() writes data from a buffer to a given miscellaneous chunk, and returns the number of bytes successfully written. Its function prototype is:long AFwritemisc(AFfilehandle file, long chunkid,                 void *buf, long nbytes)where file is the AFfilehandle structure previously created by a call to AFopenfile() or AFopenfd(), chunkid is a positive long integer miscellaneous chunk ID from the miscids array returned by AFgetmiscids(), buf is a pointer to a buffer that contains the data you want to write to the miscellaneous chunk, and nbytes is the number of bytes you want to write to the audio file from buf, beginning at the current position of file's logical write pointer for the data in miscid. AFwritemisc() will not write past the end of the chunk's data area. After writing the data, AFreadmisc() updates the position of the read/write pointer to point to the data byte following the last one written.It is up to the application to fill the data area of a chunk with consistent information (for example, if you don't use all the bytes you allocated in a MIDI data chunk, you need to fill the remaining bytes with no-ops).ID="Media2-5AF121"AFseekmisc() moves the logical read/write pointer for a miscellaneous chunk to a specified offset from the beginning of the chunk's data area. Its function prototype is:void AFseekmisc(AFfilehandle file, long chunkid, long offset)where file is the AFfilehandle structure previously created by a call to AFopenfile() or AFopenfd(), chunkid is a positive long integer miscellaneous chunk ID from the miscids array returned by AFgetmiscids(), offset is a non-negative long integer specifying the number of bytes past the start of the data area the read/write pointer should be moved, and offset should always be less than the size of the total data area (in bytes).AFseekmisc() returns the new location of the logical read/write pointer, measured as the number of bytes from the beginning of the chunk data area.LBL="" HELPID=""ID="90519"Audio File Library Programming TipsThis section describes important Audio File Library programming tips: IDREF="21228" TYPE="TITLE""Minimizing Data and File Format Dependence" describes how to maximize application compatibility by minimizing format dependence.IDREF="36539" TYPE="TITLE""Preventing Concurrent Access from Multiple Threads" explains how to write a multithreaded AF application in order to prevent simultaneous access to an AFfilehandle from multiple threads.IDREF="69122" TYPE="TITLE""Handling Errors in Multithreaded Applications" explains how to prevent an error handler from reporting simultaneous errors from a multithreaded application.LBL="" HELPID=""ID="21228"Minimizing Data and File Format DependenceCurrently, the Audio File Library supports the AIFF and AIFF-C file formats. As the AF Library evolves to support new file formats (beyond AIFF and AIFF-C) and new data formats (beyond 2's complement integer and compressed data formats), file-format dependent applications will require more modifications to maintain compatibility than file-format independent programs. Making your application file format independent decreases the likelihood of compatibility problems with future releases of the library and minimizes future modifications. Programming tips presented throughout this chapter call attention to methods you can use to make your application format independent.LBL="" HELPID=""ID="36539"Preventing Concurrent Access from Multiple ThreadsThe AF is not multithread/multiprocessor safe. Making multiple, simultaneous, uncoordinated AF calls on different AFfilehandles from different threads is possible and correct. Each AFfilehandle completely encapsulates the state (except for error handling, which is global) needed to perform operations on that AFfilehandle. In contrast, making multiple, simultaneous, uncoordinated AF calls on the same AFfilehandle from different threads is currently possible, but it is not proper programming practice.In the following code, two threads are using one AFfilehandle:COLUMNS="2"LEFT="0" WIDTH="166"Thread 1LEFT="175" WIDTH="166"Thread2LEFT="0" WIDTH="166"          Some amount of timeLEFT="175" WIDTH="166"         Some amount of timeLEFT="0" WIDTH="166"          No semaphore lockingLEFT="175" WIDTH="166"         No semaphore lockingLEFT="0" WIDTH="166"LEFT="175" WIDTH="166"LEFT="0" WIDTH="166"AFseekframe(h,track,place1;LEFT="175" WIDTH="166"AFseekframe(h,track,place2); LEFT="0" WIDTH="166"AFreadframes(h,track,...);LEFT="175" WIDTH="166"AFreadframes(h,track,...);LEFT="0" WIDTH="166"          Some amount of timeLEFT="175" WIDTH="166"         Some amount of timeLEFT="0" WIDTH="166"          No semaphore lockingLEFT="175" WIDTH="166"         No semaphore lockingLEFT="0" WIDTH="166"LEFT="175" WIDTH="166"It is possible that these calls would be executed in the following order, in which case both threads would read the wrong data.:COLUMNS="3"LEFT="0" WIDTH="159"AFseekframe(h,track,place1);LEFT="165" WIDTH="19"||LEFT="190" WIDTH="154"LEFT="0" WIDTH="159"AFreadframes(h,track,...);LEFT="165" WIDTH="19"||LEFT="190" WIDTH="154"AFseekframe(h,track,place2); LEFT="0" WIDTH="159"LEFT="165" WIDTH="19"||LEFT="190" WIDTH="154"AFreadframes(h,track,...);The only way to ensure that concurrent operations take place in the correct order is to use a process coordination facility such as semaphore locking.Proper multithreading looks like this:COLUMNS="2"LEFT="0" WIDTH="166"Thread 1LEFT="175" WIDTH="166"Thread 2LEFT="0" WIDTH="166"LEFT="175" WIDTH="166"LEFT="0" WIDTH="166"           Some amount of timeLEFT="175" WIDTH="166"         Some amount of timeLEFT="0" WIDTH="166"LEFT="175" WIDTH="166"LEFT="0" WIDTH="166"Lock Semaphore that guards hLEFT="175" WIDTH="166"Lock Semaphore that guards hLEFT="0" WIDTH="166"AFseekframe(h,track,place1;LEFT="175" WIDTH="166"AFseekframe(h,track,place2); LEFT="0" WIDTH="166"AFreadframes(h,track,...);LEFT="175" WIDTH="166"AFreadframes(h,track,...);LEFT="0" WIDTH="166"Unlock Semaphore that guards hLEFT="175" WIDTH="166"Unlock Semaphore that guards hLEFT="0" WIDTH="166"LEFT="175" WIDTH="166"LEFT="0" WIDTH="166"          Some amount of timeLEFT="175" WIDTH="166"         Some amount of timeLEFT="0" WIDTH="166"LEFT="175" WIDTH="166"IRIX guarantees that only one of the Lock Semaphore calls will succeed immediately. The thread whose lock does not succeed waits in the Lock Semaphore call (and thus does not proceed to the AFseekframe() call) until the other thread has unlocked the semaphore (after it has finished seeking and reading). When the first thread unlocks the semaphore, the thread that is waiting can now proceed. Follow these steps to add semaphore locking to a multithreaded application:Use usnewsema(3P) to code to create a semaphore whose value is 1.Use uspsema(3P) to lock the semaphore.Use usvsema(3P) to unlock the semaphore.IDREF="67054" TYPE="TEXT"Example 7-3 is a code fragment that demonstrates how to create a semaphore for protecting critical regions.LBL="7-3"Example 7-3 ID="67054"Creating a Semaphore#include <ulocks.h>

AFfilehandle h;      /* global file handle */
usema_t *HSema;      /* global semaphore to protect h */

/* Initialize semaphore support -- do this once. */
    {
    usptr_t *usptr;
    char *arenafile;

    /* Use the fastest type (nondebugging) semaphores. */
    usconfig(CONF_LOCKTYPE, US_NODEBUG);

    /* Create a shared arena to hold the semaphore.   */

    arenafile = tmpnam(NULL);
    usptr = usinit(arenafile);

    /* 
    Create the semaphore with count 1 in that arena.
    There is 1 resource (h) initially available.     */

    HSema = usnewsema(usptr,1);

    /* No need to refer to arena again, so unlink file */

    unlink(arenafile);
    }To use the semaphore created in IDREF="67054" TYPE="TEXT"Example 7-3 do this:COLUMNS="2"LEFT="0" WIDTH="166"Thread 1LEFT="175" WIDTH="166"Thread 2LEFT="0" WIDTH="166"LEFT="175" WIDTH="166"LEFT="0" WIDTH="166"          Some amount of timeLEFT="175" WIDTH="166"         Some amount of timeLEFT="0" WIDTH="166"LEFT="175" WIDTH="166"LEFT="0" WIDTH="166"uspsema(HSema); /* lock */LEFT="175" WIDTH="166"uspsema(HSema); /* lock */LEFT="0" WIDTH="166"AFseekframe(h,track,place1;LEFT="175" WIDTH="166"AFseekframe(h,track,place2); LEFT="0" WIDTH="166"AFreadframes(h,track,...);LEFT="175" WIDTH="166"AFreadframes(h,track,...);LEFT="0" WIDTH="166"usvsema(HSema); /* unlock */LEFT="175" WIDTH="166"usvsema(HSema); /* unlock */LEFT="0" WIDTH="166"LEFT="175" WIDTH="166"LEFT="0" WIDTH="166"          Some amount of timeLEFT="175" WIDTH="166"         Some amount of timeLEFT="0" WIDTH="166"LEFT="175" WIDTH="166"Semaphore locking can prevent a worst-case scenario such as seeking from the second thread before the first thread has finished reading. Currently, an AF application without semaphores might not cause any problems when making simultaneous, uncoordinated AF calls on the same AFfilehandle from different threads. But this is becausename='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'by chancename='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'the CPU scheduler timing has arranged the process timing so that both threads don't use the handle at the same time. Another time, the CPU scheduling might not be favorable, so it's best to protect the critical regions with semaphores.In summary, you cannot make multiple, simultaneous, uncoordinated AF calls on the same AFfilehandle from different threads, even if the order of execution of those calls does not matter. Doing so is likely to cause a core dump, or at least corruption of the AFfilehandle. The application is responsible for implementing any semaphore protection that is needed; such protection is not built in to the AF calls themselves.LBL="" HELPID=""ID="69122"Handling Errors in Multithreaded ApplicationsYou cannot make multiple, simultaneous, uncoordinated AF calls from different threads that affect the library's global statename='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'namely, the error handler function. If two threads simultaneously try to set the error handler (even if it is the same error handler), the behavior is undefined.If you write your own error handler and then make multiple, simultaneous, uncoordinated AF calls on different file handles from different threads (and both AF calls issue an error simultaneously), then two instances of your error handler are called in a simultaneous, uncoordinated manner in both threads. If this situation is possible in your program, you should use semaphores in your error handler (in addition to the semaphores in your main program) to prevent simultaneous error reporting or handling.LBL="" HELPID=""ID="35767"Sample Audio File ProgramIDREF="87144" TYPE="TEXT"Example 7-4 contains a listing of APP="/usr/sbin/jot /usr/people/4Dgifts/examples/dmedia/audio/recordexample.c" PARMS=""recordexample.c
, in /usr/people/4Dgifts/examples/dmedia/audio/ program, which records stereo data from an audio port. If you incorporate this code in a program, use the method of rate querying shown in ID="Media2-5AF122"ID="Media2-5AF123"APP="/usr/sbin/jot /usr/people/4Dgifts/examples/dmedia/audio/ratequery.c" PARMS=""ratequery.c
 instead of the method used in recordexample.c.LBL="7-4"Example 7-4 ID="87144"Recording Stereo from an Audio Port: recordexample.c #include <stdio.h>
#include <signal.h>
#include <dmedia/audio.h>
#include <dmedia/audiofile.h>

/*
 * small example program: "recordexample"
 *
 * record an AIFF-C file from an audio input port
 * stop recording when user sends an interrupt
 *
 * file is configured for 16-bit stereo data at the current
 *     sampling rate of the audio hardware
 *
 * usage: "recordexample <filename>"
 */
int      caught_sigint;

/*
 * catch interrupt signal
 */
static void
catch_sigint()
{
    caught_sigint++;
}

main(int argc, char **argv)
{
    char         *myname;             /* name of this program              */
    char         *portname;           /* audio port name                   */
    ALconfig      portconfig;         /* audio port configuration          */
    ALport        port;               /* audio port                        */
    long          portchannels;       /* audio port channels               */
    long          portrate;           /* audio port sampling rate          */
    long          portsampwidth;      /* audio port sample width           */
    long          portsampfmt;        /* audio port sample format          */
    AFfilesetup   filesetup;          /* audio file setup                  */
    AFfilehandle  file;               /* audio file handle                 */
    char         *filename;           /* audio file name                   */
    long          filechannels;       /* audio file channels               */
    double        filerate;           /* audio file sampling rate          */
    long          filesampwidth;      /* audio file sample width           */
    long          filesampfmt;        /* audio file sample format          */
    long          pvbuf[2];           /* parameter-value buffer            */
    void         *buf;                /* sample transfer buffer            */
    int           numframeswrit;      /* number of frames written          */
    int           done;               /* flag                              */
    int           samplesperbuf;      /* samples transfered per loop       */
    int           framesperbuf;       /* sample frames transfered per loop */
    int           samplespersec;      /* samples transfered per sec        */

    myname   = argv[0];
    portname = myname;

    if (argc != 2)
    {
        fprintf(stderr, "Usage: %s filename\n", myname);
        exit(1);
    }

    sigset(SIGINT, catch_sigint);

    filename = argv[1];
    /*
     * get the global IRIS Audio Processor input rate
     */
    pvbuf[0] = AL_INPUT_RATE;
    ALgetparams(AL_DEFAULT_DEVICE, pvbuf, 2);
    portrate = pvbuf[1];

    /*
     * initialize the audio port and audio file configuration
     */
    portchannels    = AL_STEREO;              /* port channels      */
    portsampwidth   = AL_SAMPLE_16;           /* port sample width  */
    portsampfmt     = AL_SAMPFMT_TWOSCOMP;    /* port sample format */
    filechannels    = 2;                      /* file  channels     */
    filesampwidth   = 16;                     /* file sample width  */
    filesampfmt     = AF_SAMPFMT_TWOSCOMP;    /* file sample format */
    /*
     * configure file sample rate to match IRIS audio processor input rate
     */
    switch (portrate)
    {
       case AL_RATE_48000: filerate = 48000.0; break;
       case AL_RATE_44100: filerate = 44100.0; break;
       case AL_RATE_32000: filerate = 32000.0; break;
       case AL_RATE_22050: filerate = 22050.0; break;
       case AL_RATE_16000: filerate = 16000.0; break;
       case AL_RATE_11025: filerate = 11025.0; break;
       default:
       case AL_RATE_8000: filerate =  8000.0; break;
    }
    /*
     * compute the number of input samples equal to half a 
     * second and allocate a transfer buffer
     */
    samplespersec   = ((long)filerate) * 2; /* stereo             */
    samplesperbuf   = samplespersec / 2;    /* half second buffer */
    framesperbuf    = samplesperbuf / 2;    /* stereo             */
    buf             = (short *)malloc(samplesperbuf * sizeof(short));
    /*
     * open the audio port
     */
    portconfig    = ALnewconfig();
    ALsetchannels(portconfig, portchannels);
    ALsetwidth(portconfig, portsampwidth);
    ALsetqueuesize(portconfig, samplesperbuf);
    port = ALopenport(portname, "r", portconfig);
    /* 
     * configure an audio file 
     */
    filesetup    = AFnewfilesetup();
    AFinitfilefmt(filesetup, AF_FILE_AIFFC); 

    AFinitchannels(filesetup, AF_DEFAULT_TRACK,  filechannels);
    AFinitrate(filesetup, AF_DEFAULT_TRACK, filerate);
    AFinitsampfmt(filesetup, AF_DEFAULT_TRACK, 
                      AF_SAMPFMT_TWOSCOMP, filesampwidth); /*in bits */
    /*
     * open the audio file
     */
    file = AFopenfile(filename, "w", filesetup);
    /*
     * play the buffer
     */
    done = 0;
    caught_sigint = 0;
    while (!done && !caught_sigint)
    {
        ALreadsamps(port, buf, samplesperbuf);
        if ((numframeswrit 
                 = AFwriteframes(file, AF_DEFAULT_TRACK, 
                             buf, framesperbuf)) < framesperbuf)
        {
            done++;
        }
    }

    AFclosefile(file);   /* this is important: it updates the file header */
    ALcloseport(port);
    exit(0);
}LBL="8"ID="41223"Programming with the CD Audio LibraryThe IRIS Media Libraries have two libraries that help you retrieve and process digital audio and related information from two sources. This chapter describes the CD Audio Library, libcdaudio.a, which gives you access to the data on an audio compact disc (CD), including nonaudio information. ID="Media2-6CD1"ID="Media2-6CD2"IDREF="80723" TYPE="TITLE"Chapter 9, "Programming with the DAT Audio Library," describes the DAT Audio Library, libdataudio, which helps you process audio information stored on digital audio tape (DAT). Because these libraries deal with digital audio information, they contain many analogous routines for processing audio data. But the libraries diverge when it comes to writing audio data and controlling their respective devices. libcdaudio includes calls that control the CD-ROM drive; the DAT drive uses the standard IRIX device drivers.In this chapter:IDREF="14198" TYPE="TITLE""CD Audio Library Basics" explains basic concepts for using the CD Audio Library. IDREF="79578" TYPE="TITLE""Navigating through a CD" explains getting locations from and seeking to locations on a CD.IDREF="38354" TYPE="TITLE""Using the CD-ROM Drive" explains how to use the CD-ROM drive for playing audio CDs, reading and parsing CD information, and communicating CD status to the end user.IDREF="28767" TYPE="TITLE""CD Sample Program" presents a CD sample program.LBL="" HELPID=""ID="14198"CD Audio Library BasicsThe CD Audio Library lets you: ID="Media2-6CD3"control the CD-ROM drive (eject CDs, prohibit ejection of CDs)read or play information from that driveparse and process digital information This section describes the basic concepts that underlie libcdaudio. Because both CDs and DATs digitally encode an audio signal as a series of samples, the concepts and terms used when dealing with these media are similar; however, there are differences between the two. LBL="" HELPID=""CD Frames, Samples, and SubcodesPer second of playing time, a CD contains 75 CD frames, each containing 588 stereo audio frames (that is, pairs of left and right channel audio samples). A CD frame has both audio and nonaudio information. The sum of the nonaudio information in a frame composes a single complete chunk of ID="Media2-6CD4"ID="Media2-6CD5"ID="Media2-6CD6"subcode. When in audio mode and reading from a CD, you need complete subcodes. Thus, in audio mode, a CD frame is the smallest parcel of information you can read from a CD.To give you controlled access to either the audio data or the subcode in a CD frame, libcdaudio hands you a CDFRAME structure: ID="Media2-6CD7"ID="Media2-6CD8"typedef struct cdframe {
char audio[CDDA_DATASIZE];
struct subcodeQ ;subcode;
} ;CDFRAME;
ID="Media2-6CD9"ID="Media2-6CD10"An audio sample is linearly encoded in a 16-bit two's-complement format. Because a complete stereo audio sample contains two interleaved channels, it takes four bytes of audio[] to contain a complete stereo audio sample.IDREF="62227" TYPE="GRAPHIC"Figure 8-1 shows the structure of a CD audio sample.ID="Media2-6CD11"FILE="8-1.online.bw" POSITION="INLINE" SCALE="FALSE"LBL="8-1"Figure 8-1 ID="62227"CD Audio Sample StructureThe byte ordering of the samples in audio[] is the raw data from the CD; its byte ordering is reversed from that on the Indigo workstation. The sampling rate at which CD audio data is originally recorded is 44.1 kHz; therefore, CDDA_DATASIZE (the size of ID="Media2-6CD12"ID="Media2-6CD13"audio[]) is defined as 2352. This allows for 588 stereo audio samples per CD frame, which, at 75 frames per second, allows for a sample rate of 44.1 kHz. The subcode member has three information modes: ID="Media2-6CD14"mode1for reporting on the track, index, and timing for the current CD track; or, if the current track is the nonaudio lead-in track (see ID="Media2-6CD15"IDREF="18765" TYPE="TITLE""CD Tracks, Indices, and Time Codes" ), mode1 contains a table of contents for the CDID="Media2-6CD16"mode2for reporting the catalog number for the CD as well as an absolute CD frame countmode3for reporting the International Standard Recording Code (ISRC) identification information: country, owner, year, and serial numberID="Media2-6CD17"ID="Media2-6CD18"Thus, the subcodeQ structure in the CDFRAME structure contains a union of three structures: ID="Media2-6CD19"mode1, mode2, and mode3. Which mode is used depends on the information from the CD. For more information on the CDFRAME structure and the subcodeQ structure, see the ID="Media2-6CD20"CDFRAME(4) man page. LBL="" HELPID=""ID="18765"CD Tracks, Indices, and Time CodesAs many as 99 audio program tracks are allowed on a CD. These tracks are numbered 01 through 99. Two nonaudio tracks of general interest are also available: the lead-in track (numbered 00) and the lead-out track (numbered AA). Track 00, the lead-in track, contains a table of contents in its subcodes.ID="Media2-6CD21"ID="Media2-6CD22"A track can have up to 99 subdivisions containing audio information. These subdivisions use the index numbers 01 through 99. Index number 00 is used for the pause between the tracks. The time code gives the current minute, second, and CD frame for the current track. The ID="Media2-6CD23"subcodeQ structure with mode1 uses a cdtimecodeID="Media2-6CD24" structure to contain time codes.ID="Media2-6CD25"LBL="" HELPID=""CD Seeking, Reading, and PlayingAccessing information from a CD-ROM drive is analogous to accessing information from a standard disk drive. To read a particular piece of information from the CD, you must move to that location. The process of moving to a location on the CD is known as seeking.ID="Media2-6CD26"Reading from a CD-ROM drive is analogous to reading from a disk drivename='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'you copy information from the device to a memory-resident buffer for further processing.Playing the CD is a variation on reading it. But instead of transferring the information to a buffer for processing, the information is dumped out the audio jacks on the back of the CD-ROM drive, with a minimum of buffering and with no real chance to process it. For information on processing audio from a CD through the workstation's audio hardware, see IDREF="65266" TYPE="TITLE""Reading Audio Data from the CD-ROM Drive".LBL="" HELPID=""CD ParserThe parser lets your application change state in response to changes in the subcode data on a CD. This lets you deal with the audio data in a way that is based on its content. To use the parser, you must give it callback routines that can deal the subcode changes that interest you. Then you set up a loop that reads CD frames from the CD and calls the parser for each CD frame. ID="Media2-6CD27"The parser checks the subcode in every submitted CD frame. If the parts of the subcode you care about have changed from the previous CD frame, the parser executes one of your callbacks and hands it the new subcode information. Within your callback, you can examine the subcode information and change the state of your application as needed. LBL="" HELPID=""Opening and Closing the CD-ROM DeviceThe CD-ROM device does not use a standard IRIX device driver. So, a session with the CD-ROM device starts by calling ID="Media2-6CD28"CDclose()ID="Media2-6CD29". For detailed information on these routines, see the man pages CDopen(3) and CDclose(3). LBL="" HELPID=""Controlling the CD-ROM Drive CaddyTo give your application control over the caddy-eject feature on the CD-ROM drive, libcdaudio defines the following routines:COLUMNS="2"LEFT="0" WIDTH="108"ID="Media2-6CD30"CDeject()LEFT="115" WIDTH="225"to eject the caddy from the CD-ROM driveID="Media2-6CD31"LEFT="0" WIDTH="108"ID="Media2-6CD32"CDpreventremoval()ID="Media2-6CD33"LEFT="115" WIDTH="225"to lock the CD-ROM drive eject button to prevent 
end users from ejecting the caddy at an 
inopportune moment LEFT="0" WIDTH="108"ID="Media2-6CD34"CDallowremoval()LEFT="115" WIDTH="225"to unlock the CD-ROM drive eject buttonFor more information on these routines, see the appropriate man pages.LBL="" HELPID=""ID="79578"Navigating through a CDTo move through a CD, you use one of the libcdaudio calls: ID="Media2-6CD35"ID="Media2-6CD36"CDseek(), CDseektrack(), or CDseekblock(). But before you can call these routines, you need to know where you are going. For most applications, locations can come from either of two sources, the end user or calculations internal to your application. ID="Media2-6CD37"Seek destinations can be in any one of three forms:ID="Media2-6CD38"integer CD frame counts<minute, second, CD frame> integer triples"minute:second:CD frame" ASCII stringsThe ASCII format is the one you receive from an end user of your application; the other two formats are used for internal calculation.LBL="" HELPID=""Getting CD Locations from the End UserID="Media2-6CD39"If your application wants to give end users the option of seeking to a CD location defined in terms of time, your application can prompt the user for the time and then call CDatomsf() to convert the ASCII string to a <minute, second, CD frame> triple that you can use for seeking. You can also let the user specify a track number, convert that track number to an integer and seek to that track. ID="Media2-6CD40"LBL="" HELPID=""Getting CD Locations from Calculations Internal to Your ApplicationGenerally, the pure CD frame count is the most convenient format to use when comparing two locations.ID="Media2-6CD41"To convert to pure CD frame counts, call: COLUMNS="2"LEFT="0" WIDTH="117"ID="Media2-6CD42"CDmsftoframe()LEFT="125" WIDTH="216"to convert a <minute, second, CD frame> triple 
into a pure CD frame countLEFT="0" WIDTH="117"ID="Media2-6CD43"CDtctoframe()LEFT="125" WIDTH="216"to generate a pure CD frame count from a 
cdtimecode structureLEFT="0" WIDTH="117"CDatomsf() ID="Media2-6CD44"followed by 
CDmsftoframe()ID="Media2-6CD45"LEFT="125" WIDTH="216"to convert an ASCII "minute:second:CD frame" 
string into a pure frame countYou can then make your calculations and determine the destination to which you want to seek. Despite the convenience of pure CD frame counts for calculation, they are not suitable for seeking. To seek, you must call CDframetomsf() to convert the pure CD frame count to a <minute, second, CD frame> triple.It is also possible to make comparisons between locations expressed in terms of minutes, seconds, and CD frames. In that case, you can convert locations into <minute, second, CD frame> triples by calling:COLUMNS="2"LEFT="0" WIDTH="126"ID="Media2-6CD46"CDatomsf()LEFT="135" WIDTH="207"to convert an ASCII string to a<minute, second, CD frame> tripleLEFT="0" WIDTH="126"ID="Media2-6CD47"CDframetomsf()LEFT="135" WIDTH="207"to convert a pure CD frame count to a 
<minute, second, CD frame> tripleLEFT="0" WIDTH="126"CDtctoframe() ID="Media2-6CD48"followed by 
CDframetomsf()LEFT="135" WIDTH="207"to convert a time code to a<minute, second, CD frame> tripleAfter making these calculations, the location is in terms suitable for seeking. LBL="" HELPID=""Getting the Current CD Location To get your current location within a CD, call ID="Media2-6CD49"ID="Media2-6CD50"ID="Media2-6CD51"CDgetstatus(). This routine takes a CDSTATUS structure and fills it with information on current track, minute, second, CD frame, and additional data. To make it easier to compare your current location to another location, you should express the locations in terms of pure CD frame counts. But depending on how you got a location, it could be expressed as three separate integers giving the minute, second, and CD frame, or as an ASCII string, or as a cdtimecode structure. For more information on this routine, see the appropriate man pages.LBL="" HELPID=""Seeking to a CD LocationSeeking sets up the read pointer to retrieve data from a particular location on the CD. You can define the seek location in terms of: trackTo seek to a track, call ID="Media2-6CD52"ID="Media2-6CD53"CDseektrack().  absolute timeTo seek to a location defined in terms of minute, second, and CD frame, call ID="Media2-6CD54"CDseek(). logical blockTo seek to a location defined in terms of a logical block number, call ID="Media2-6CD55"CDseekblock(). (On a CD-ROM, one logical block contains a single CD frame, which is 588 stereo audio samples plus one complete subcode.)ID="Media2-6CD56"To do a series of consecutive seeks, your first seek can be defined in any of the formats mentioned above. But, because all seek routines return the logical block number of the next logical block, it is often more convenient to define the subsequent seeks in terms of logical blocks. ID="Media2-6CD57"If you want to do all seeks using CDseekblock(), but your first seek is defined in terms of time, call CDmsftoblock() to convert time to logical block number. NoteAlthough logical blocks and CD frames are the same size, you cannot use CD frame counts as if they were logical block counts. The CD frame counts are relative to the start of the CD. A logical block count is offset from the start of the CD. In addition, the size of the offset varies from device to device. LBL="" HELPID=""ID="38354"Using the CD-ROM DriveThis section explains how to use the CD Audio Library routines for:playing an audio CD from the CD-ROM driveID="Media2-6CD58"reading audio data from the CD-ROM driveparsing CD informationcommunicating CD status to the end userLBL="" HELPID=""Playing an Audio CD from the CD-ROM DriveThis section explains how to use these libcdaudio routines to play audio from the CD-ROM as if it were a standard CD player: COLUMNS="2"LEFT="0" WIDTH="99"ID="Media2-6CD59"CDplay()LEFT="105" WIDTH="234"plays an audio CD through CD-ROM audio jacksID="Media2-6CD60"LEFT="0" WIDTH="99"ID="Media2-6CD61"CDtogglepause()LEFT="105" WIDTH="234"toggles a CD-ROM drive between pause and playID="Media2-6CD62"LEFT="0" WIDTH="99"ID="Media2-6CD63"CDstop()LEFT="105" WIDTH="234"stops play of an audio CD in CD-ROMLEFT="0" WIDTH="99"ID="Media2-6CD64"CDplaytrack()LEFT="105" WIDTH="234"plays a single track of an audio CD through CD-
ROM audio jacksLEFT="0" WIDTH="99"ID="Media2-6CD65"CDplayabs()LEFT="105" WIDTH="234"plays an audio CD through CD-ROM audio jacks 
starting at a particular minute, second, and CD 
frameLEFT="0" WIDTH="99"ID="Media2-6CD66"CDplaytrackabs()LEFT="105" WIDTH="234"plays a single track of an audio CD starting at a 
particular minute, second, and CD frameID="Media2-6CD67"When these routines play a CD, they direct the sound to the drive's headphones and to the audio jacks.LBL="" HELPID=""ID="65266"Reading Audio Data from the CD-ROM DriveOnce you have set the read pointer with a call to one of the seek routines, you are ready to read data from the CD. But how much data should you read at a time in order to create a continuous flow of data from the CD? To determine this, call ID="Media2-6CD68"CDbestreadsize(). The returned value of this function is the number of CD frames to request in your read call. To actually read data from the CD, call ID="Media2-6CD69"ID="Media2-6CD70"CDreadda(). Because libcdaudio already includes routines for playing audio data from the CD, you might think that you would never need to read from the CD; however, the libcdaudio play routines allow for only a very simple CD-player applicationname='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'one that cannot even display the current program time while the CD is playing. Thus, if you are writing a real-world application, you probably want to read samples from the CD into the workstation's memory through the CD-ROM's SCSI interface, play the audio samples from the audio hardware using the Audio Library, parse the CD frames for the current program time, and display the program time in a continuously updated field of the control panel for your application. To do this, you can write your own play routine that executes as a cd_audio callback. You should also write a ID="Media2-6CD71"cd_ptime callback to get the current program time and to update your "program time" display. LBL="" HELPID=""Controlling the CD ParserAfter you have read data from the CD into a buffer, you can start to process it. Typically, how you process the audio data depends on what its associated subcodes tell you about the data. To make it possible for your application to avoid dealing with the complex CDFRAME structure directly, libcdaudio includes a parser. ID="Media2-6CD72"If you write a loop that passes all read CD frames through the parser, the parser can examine all CD frames for changes in the subcode. When the parser finds a change (seeing a subcode for the first time counts as a change), it executes the appropriate callback routinename='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'depending on what sort of subcode change occurredname='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'and passes the new subcode data into your callback routine. The CD parser distinguishes among eight categories of subcode information. Thus, if you are interested in subcode changes for only one category of subcode data, the parser does not bother your application with subcode changes that you consider irrelevant. LBL="" HELPID=""Allocating and Initializing the CD ParserTo allocate and initialize the parser data structures, call ID="Media2-6CD73"ID="Media2-6CD74"CDcreateparser(). To reset the parser after the user changes the CD in the CD-ROM drive, call ID="Media2-6CD75"CDresetparser().ID="Media2-6CD76" This clears out any information the parser has about the last CD frame but leaves the callback routines in place. LBL="" HELPID=""Defining Callbacks for the CD ParserWhen you define a callback for the parser, write a function of the form:ID="Media2-6CD77"MyCDSomethingCallBack( void* arg, CDDATATYPES type,
void* data) {
/* your code here */
}The parser uses the third parameter to pass in information it reads from the subcodes. The parser uses the second parameter to pass in the type of callback it thinks it is calling. You can use this to assign the same function to different types of callbacks. Internally, you can switch on the type. This feature is useful if two callbacks are essentially the same, with the exception of a few lines.The parser does not use the first parameter. You can use that to pass in information if your application needs to call the callback directly. LBL="" HELPID=""Adding Callbacks to the CD Parser To add callback routines to the parser, call ID="Media2-6CD78"CDaddcallback(). If you do not specify a callback for a category, the parser assumes that you are not interested in changes of that type. You can add callbacks that respond to changes in any of the following categories of subcode data:ID="Media2-6CD79"ID="Media2-6CD80"cd_audiocallbacks respond to changes in the audio data in a CD frame. You can use this class of callback to notify you when you are beyond the lead-in track and have started to see audio samples. When the parser calls this routine, it passes in the audio sample data. If this callback routine is a play routine for your application, it should write the audio sample to an audio port using the Audio Library. See the ALwritesamps(3) man page and IDREF="70508" TYPE="TITLE"Chapter 6, "Programming with the Audio Library."cd_pnumcallbacks respond to changes in the program number. You can use this callback to notice when you have moved from one program (track) to the next. cd_indexcallbacks respond to changes in the index number. You can use this callback to notice when you have moved from one subsection of a track to the next.cd_ptimecallbacks respond to changes in the program time. You can use this callback to continuously update a "program time display" in a CD-playing application. cd_atimecallbacks respond to changes in the absolute time elapsed since the start of the CD. You can use this callback to continuously update your application's information about total elapsed time.cd_catalogcallbacks respond to changes in the catalog number for the CD. Because this information should not change within the CD, this sort of callback executes only oncename='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'typically during the lead-in track for the CD. cd_identcallbacks respond to changes in the ISRC identification number for the recording on the CD. Because this information should not change within the CD, this sort of callback executes only oncename='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'typically during the lead-in track for the CD.cd_controlcallbacks respond to changes in the control bits. These bits can tell you things such as whether the CD is copy protected and whether preemphasis is off or on. Because this information should not change within the CD, this sort of callback executes only oncename='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'typically during the lead-in track.For more information on each callback type, see the CDaddcallback(3) man page. LBL="" HELPID=""Deleting and Changing a CD Parser CallbackTo delete a callback, call ID="Media2-6CD81"ID="Media2-6CD82"CDremovecallback(). To change a callback, call CDremovecallback() followed by CDaddcallback(). LBL="" HELPID=""Parsing CD Frames To submit a group of CD frames to the parser, your loop should set up a loop that calls CDparseframe()ID="Media2-6CD83"ID="Media2-6CD84" for each frame that you have read into your buffer.LBL="" HELPID=""Freeing the Memory Allocated for the Parser If you are done with the parser and want to free the memory it uses, callID="Media2-6CD85"ID="Media2-6CD86"CDdeleteparser() to delete the parser. LBL="" HELPID=""Communicating CD Status to the End UserIn addition to playing a CD or processing the information read from a CD, your application probably needs to tell the user something about the CD (even if it is only the number of the current track). Also, sometimes your application must take data from the end user and convert it to a form that the CD-ROM device can understand.ID="Media2-6CD87"To get information for the end user, call:COLUMNS="2"LEFT="0" WIDTH="117"ID="Media2-6CD88"CDgettrackinfo()LEFT="125" WIDTH="216"to get information about a particular trackLEFT="0" WIDTH="117"ID="Media2-6CD89"CDgetstatus()LEFT="125" WIDTH="216"to get information about the CD as a wholeThe CD frames, however, sometimes contain information that is not accessible to the routines mentioned above. For example, the subcodes of track 00 on a CD contain a table of contents. To access this information, you can inspect the subcodes in the CDFRAME structures, or, better still, you can submit that track to the parser. If you have added callbacks for the categories of subcode information that you want, the parser passes that information into your callbacks. To help you present the information the parser hands to your callbacks (or that you read directly from a CDFRAME structure), libcdaudio contains the routines: COLUMNS="2"LEFT="0" WIDTH="81"ID="Media2-6CD90"CDsbtoa()LEFT="90" WIDTH="252"for converting the 6-bit ISRC country and owner code to 
an ASCII stringID="Media2-6CD91"ID="Media2-6CD92"LEFT="0" WIDTH="81"ID="Media2-6CD93"CDtimetoa()LEFT="90" WIDTH="252"for expressing the contents of a cdtimecode structure as an 
ASCII stringID="Media2-6CD94"For more information on the CDFRAME structure and the format of its data, set the CDFRAME(4) man page. LBL="" HELPID=""CD Time Code Conversion RoutinesOther libcdaudio routines that you might find useful are: COLUMNS="2"LEFT="0" WIDTH="81"CDframetotc()LEFT="90" WIDTH="252"for converting a CD frame number to a time code LEFT="0" WIDTH="81"CDatotime()LEFT="90" WIDTH="252"for converting an ASCII string to a time codeLBL="" HELPID=""ID="28767"ID="67030"CD Sample ProgramIDREF="65997" TYPE="TEXT"Example 8-1 contains a listing of cdsample.c, a program that lets you copy timed amounts of data from a CD to an audio file.ID="Media2-6CD95"LBL="8-1"Example 8-1 ID="65997"Copying CD Data to an Audio File: ID="Media2-6CD96"cdsample.c/*
 * cdsample--command line tool to read audio data off CD, 
 * record it in an AIFF file. Hacked together from various
 * other sample programs.
 *
 * Compile with 
 * cc -o cdsample cdsample.c -lcdaudio -lds -laudiofile -lm
 */

#include <sys/types.h>
#include <cdaudio.h>
#include <audio.h>
#include <audiofile.h>
#include <stdio.h>
#include <string.h>

AFfilehandle audiofile;
openAudioFile(char *filename)
{
    AFfilesetup filesetup;
 
    filesetup = AFnewfilesetup();
    AFinitfilefmt(filesetup, AF_FILE_AIFFC);
    AFinitchannels(filesetup, AF_DEFAULT_TRACK, 2);
    AFinitrate(filesetup, AF_DEFAULT_TRACK, 44100.0);
    AFinitsampfmt(filesetup, AF_DEFAULT_TRACK, AF_SAMPFMT_TWOSCOMP, 16);  
    AFinitcompression(filesetup, AF_DEFAULT_TRACK, AF_COMPRESSION_G722);
    audiofile = AFopenfile(filename, "w", filesetup);
}
closeAudioFile()
{
    AFclosefile(audiofile);
}
writeAudioFile(void *arg, CDDATATYPES type, short *audio)
{
    AFwritesamps(audiofile, AF_DEFAULT_TRACK, audio, CDDA_NUMSAMPLES);
}

void parseTime(char *timestr, int *min, int *sec)
{
    char *tmp, buf[5];
    int n;
    tmp = strchr(timestr, `:');
    if (tmp == NULL) {
         *sec = atoi(timestr);
    } else {
         *tmp = `\0';
         tmp++;
        *min = atoi(timestr);
         *sec = atoi(tmp);
    }
}

main(int argc, char **argv)
{
    CDPLAYER *cd;
    CDPARSER *cdp;
    CDSTATUS status;
    CDTRACKINFO trackinfo;
    CDFRAME buf[12];
    int i, n;
    int track, numframes, frame;
    char *filename;
    char *tmp, strbuf[12];
    int startmin, startsec, endmin, endsec, totalsec;
    extern int errno;
 
    if (argc != 5) {
         fprintf(stderr, "Usage: cdsample filename track start_time end_time\n");
         exit(1);
    }
 
    filename = argv[1];
    track = atoi(argv[2]);
    /* 
    * Note that we do not check if the arguments are sane ...
    */
    parseTime(argv[3], &startmin, &startsec);
    parseTime(argv[4], &endmin, &endsec);
    if ((cd = CDopen(NULL, "r")) == NULL) {
         fprintf(stderr, "Can't open CD device\n");
         exit(1);
    }
    if ((cdp = CDcreateparser()) == NULL) {
         fprintf(stderr, "Can't create parser\n");
         exit(1);
    }
 
   /*
    * Set up a callback function to process the CD data.
    * In this case, CDparseframe() will feed the data to the
    * writeAudioFile() function (defined above).
    */
    CDsetcallback(cdp,cd_audio,(CDCALLBACKFUNC) writeAudioFile, 0);

    openAudioFile(filename);
 
    /* 
    * Determine the number of frames in the requested 
    * snippet (75 frames/sec)
    */ 
    numframes = ((endmin * 60 + endsec) - (startmin * 60 + startsec)) * 75;
    if (CDgetstatus(cd, &status) == 0) {
         fprintf(stderr, "Couldn't get status\n");
         exit(1);
    } else {
    if (!status.scsi_audio) {
        fprintf(stderr, "This CD-ROM can't do SCSI audio\n");
        exit(1);
     }
     /*
      * Convert relative time (in track) to absolute time
      * (on disk) so we can seek to the proper position.
      */
     CDgettrackinfo(cd, track, &trackinfo);
     totalsec = (trackinfo.start_min + startmin) * 60 + 
       trackinfo.start_sec + startsec;
     startmin = totalsec / 60;
     startsec = totalsec % 60;
     CDseek(cd, startmin, startsec, 0);
     for (frame=0;frame<numframes;frame += 12) {
         n = CDreadda(cd, buf, 12);
         if (n < 0) {
             fprintf(stderr, "Error reading CD data\n");
             exit(1);
         }
         if (n == 0) /* We're at the end of the disc */
             break;
         for (i = 0; i < 12; i++)
             CDparseframe(cdp, &buf[i]);
          }
     CDclose(cd);
     closeAudioFile();
    exit(0);
    }
}
ID="Media2-6CD97"LBL="9"ID="80723"Programming with the DAT Audio LibraryThis chapter describes the DAT Audio Library, libdataudio, which you can use to process audio information stored on digital audio tape (DAT). ID="Media2-7DAT1"ID="Media2-7DAT2"In this chapter:IDREF="82124" TYPE="TITLE""DAT Audio Library Basics" explains basic concepts for using libdataudio.IDREF="98073" TYPE="TITLE""Navigating through a DAT" explains getting locations from and seeking to locations on a DAT.IDREF="37686" TYPE="TITLE""Using the DAT Drive" explains how to use the DAT drive for playing and recording DATs, reading, writing and parsing DAT information, and communicating DAT status to the end user.IDREF="75008" TYPE="TITLE""DAT Sample Program" presents a DAT sample program. LBL="" HELPID=""ID="82124"DAT Audio Library BasicsThe DAT Audio Library (libdataudio) supports processing the data from a digital audio tape (DAT). Because the device driver for the DAT drive is a standard IRIX tape device driver, the libdataudio library does not need the special positioning and status calls. Instead, you can use the standard ID="Media2-7DAT3"open(), close(), read(), write(), and ioctl() system calls. This section describes the basic concepts that underlie libdataudio. Because both CDs and DATs digitally encode an audio signal as a series of samples, the concepts and terms used when dealing with these media are similar; however, there are some differences between them.LBL="" HELPID=""DAT Frames, Samples, and SubcodesA DAT contains 33.33 DAT frames per second of playing time. A DAT frame has both audio and nonaudio information. The sum of the nonaudio information in a DAT frame composes a single complete DAT subcode. When in audio mode and reading from a DAT, you need complete subcodes. Thus, in audio mode, a DAT frame is the smallest parcel of information you should read from a DAT.ID="Media2-7DAT4"ID="Media2-7DAT5"To give you controlled access to either the audio data or the subcode in a DAT frame, libdataudio hands you a ID="Media2-7DAT6"ID="Media2-7DAT7"DTFRAME structure: typedef struct dtframe {
char audio[DTDA_DATASIZE];
struct dtsubcode sc;
} DTFRAME;A DAT audio sample is linearly encoded in a 16-bit two's-complement format. Because a complete stereo audio sample contains two interleaved channels, it takes four bytes of ID="Media2-7DAT8"audio[] to contain a complete stereo audio sample (see IDREF="64805" TYPE="GRAPHIC"Figure 9-1).FILE="9-1.online.bw" POSITION="INLINE" SCALE="FALSE"LBL="9-1"Figure 9-1 ID="64805"DAT Audio Sample StructureThe byte ordering of audio sample frames in ID="Media2-7DAT9"audio[] is based on the raw data from the DAT; its byte ordering is reversed from that on the IRIS workstation. DTDA_DATASIZE (the size of audio[]) is defined as 5760. This allows for 1440 audio sample frames per DAT frame, which, at 33.33 DAT frames per second, is enough to deal with audio sampled at rates of up to 48 kHz. The subcode member uses a dtsubcode structure to contain the subcode read from the DAT. The subcodes contain information on sampling frequency, the number of channels, table of contents, catalog number, and more. For more information on the ID="Media2-7DAT10"ID="Media2-7DAT11"ID="Media2-7DAT12"ID="Media2-7DAT13"ID="Media2-7DAT14"dtsubcode structure, see the DATFRAME(3) man page. LBL="" HELPID=""DAT Audio Program Numbers and IndicesA DAT can have as many as 99 audio programs, each typically corresponding to a single song or musical piece. These programs are numbered 01 through 99. An audio program can have up to 99 subdivisions containing audio information. These subdivisions use the index numbers 01 through 99. Index number 00 is used for the pause between the audio programs. ID="Media2-7DAT15"LBL="" HELPID=""DAT Run Time, Absolute Time, and Program TimeA time code gives the hour, minute, second, and DAT frame offset into a DAT. When dealing with program time, the time code is a measure of the time elapsed since the start of the audio program. When dealing with absolute time, the time code measures the time elapsed since the start of the DAT. When dealing with run time, the time code measures the time elapsed since the beginning of the recording and contains several audio programs. ID="Media2-7DAT16"LBL="" HELPID=""DAT Seeking and ReadingAccessing information from a DAT drive is analogous to reading information from a standard tape drive. To read a particular piece of information from the DAT, you must move to that location. The process of moving to a location on the DAT is known as seeking. Reading from the DAT is analogous to reading from a tape drive. You copy information from the device to a memory-resident buffer for further processing. LBL="" HELPID=""DAT ParserThe parser lets your application change state in response to changes in the subcode data on a DAT. This lets you deal with the audio data in a way that is based on its content. To use the parser, you must give it callback routines that can deal with the subcode changes that interest you. Then you set up a loop that reads DAT frames from the DAT and calls the parser for each DAT frame. ID="Media2-7DAT17"The parser checks the subcode in every submitted DAT frame. If the parts of the subcode you care about have changed from the previous DAT frame, the parser executes one of your callbacks and hands it the new subcode information. Within your callback, you can examine the subcode information and change the state of your application as needed. LBL="" HELPID=""ID="73715"Opening and Closing the DAT Device for AudioThe DAT device driver is a standard IRIX device, so you can use the generic ID="Media2-7DAT18"open()ID="Media2-7DAT19", ID="Media2-7DAT20"close(), and ID="Media2-7DAT21"ioctl() calls that you would use for any other tape device; however, unlike a standard tape drive, the DAT drive has an audio mode in addition to a straight data mode.ID="Media2-7DAT22"To put the DAT drive in audio mode, use ID="Media2-7DAT23"ioctl() with MTIOCTOP and an ID="Media2-7DAT24"mtop type structure, but set the mt_count member of the mtop structure to 1 before submitting that mtop structure to ioctl(). For example:struct mtop mt_com;
mt_com.mt_op = MTAUD;
mt_com.mt_count = 1; /* 1 == audio mode, 0 == data mode */
ioctl(fd, MTIOCTOP, &mt_com);LBL="" HELPID=""ID="98073"Navigating through a DATTo move through a DAT tape, you use ID="Media2-7DAT25"ioctl(), a standard IRIX system call. But before you can call ID="Media2-7DAT26"ioctl(), you need to know where you are going. For most applications, destinations can come from either of two sources, the end user or calculations internal to your application. Destinations from the end user come to your application in the form of ASCII strings. Destinations from internal calculations typically come in the form of a DAT frame count or, sometimes, as four values that specify the location in terms of hours, minutes, seconds, and DAT frames. Unfortunately, these forms are not suitable for seeking, so you must convert them before you can use them. LBL="" HELPID=""Getting DAT Locations from the End UserIf your application wants to give end users the option of seeking to a DAT location defined in terms of time, your application can prompt the user for the time and then call ID="Media2-7DAT27"DTatotime() to convert the string to a time code that you can submit to ID="Media2-7DAT28"ioctl() for seeking.LBL="" HELPID=""Getting DAT Locations from Calculations Internal to Your ApplicationGenerally, the pure DAT frame count is the most convenient format to use when comparing two locations.ID="Media2-7DAT29"To convert to pure DAT frame counts, call: COLUMNS="2"LEFT="0" WIDTH="117"DTtctoframe()ID="Media2-7DAT30"LEFT="125" WIDTH="216"to extract a pure DAT frame count from a 
dttimecode structureLEFT="0" WIDTH="117"DThmsftoframe()ID="Media2-7DAT31"LEFT="125" WIDTH="216"to convert hours, minutes, seconds, and DAT 
frames to a pure DAT frame countLEFT="0" WIDTH="117"DTatohmsf()ID="Media2-7DAT32" followed 
by DThmsftoframe()LEFT="125" WIDTH="216"to convert an ASCII string to a pure DAT frame 
countYou can then make your calculations and call DTframetotc() to convert the DAT frame count to a time code suitable for seeking.It is also possible to make comparisons between locations expressed in terms of hours, minutes, seconds, and DAT frames. In that case, you can convert all locations into hours, minutes, seconds, DAT frames format by calling:COLUMNS="2"LEFT="0" WIDTH="126"DTatohmsf()LEFT="135" WIDTH="207"to convert an ASCII string to hours, minutes, 
seconds, and DAT frames LEFT="0" WIDTH="126"DTframetohmsf(ID="Media2-7DAT33"LEFT="135" WIDTH="207"to convert a pure frame count to hours, 
minutes, seconds, and framesLEFT="0" WIDTH="126"DTtctoframe() ID="Media2-7DAT34"followed by 
DTframetohmsf()LEFT="135" WIDTH="207"to convert a time code to hours, minutes, 
seconds, and framesAfter making your calculations, convert the destination to a time code suitable for seeking by calling DThmsftoframe() followed by DTframetotc().LBL="" HELPID=""Seeking to a DAT LocationTo seek to a location on a DAT, call ID="Media2-7DAT35"ID="Media2-7DAT36"ioctl() with MTSETAUDIO and an mtaudio type structure.To specify the type of seek, set the seektype member of the mtaudio type structure to the appropriate MTAUDPOSN_* constant: COLUMNS="2"LEFT="0" WIDTH="126"MTAUDPOSN_PROGLEFT="135" WIDTH="207"to seek to a program number LEFT="0" WIDTH="126"MTAUDPOSN_ABSLEFT="135" WIDTH="207"to seek to an absolute timeLEFT="0" WIDTH="126"MTAUDPOSN_RUNLEFT="135" WIDTH="207"to seek to a running time LEFT="0" WIDTH="126"MTAUDPOSN_PTIMELEFT="135" WIDTH="207"to seek to a program time (within program)To seek to a particular audio program on the DAT, set seektype to MTAUDPOSN_PROG, and use pno1, pno2, and pno3 members to pass in the three BCD numbers that identify the audio program you want. Program numbers range from 001 to 799. The pno1 member contains the most significant digit and pn3 contains the least significant digit. Thus, to seek to program 578, set the pn* members as follows:struct mtaudio AudioProgNum;

AudioProgNum.pn1 = 5;
AudioProgNum.pn2 = 7;
AudioProgNum.pn3 = 8;To seek to a location on the tape defined in terms of time, set the mtaudioseektype member to MTAUDPOSN_ABS, MTAUDPOSN_RUN, or MTAUDPOSN_PTIME and then specify the time location in the mtaudio members: atimefor MTAUDPOSN_ABSrtimefor MTAUDPOSN_RUNptimefor MTAUDPOSN_PTIMEThese atime, rtime, and ptime members contain structures of type mtaudtimecode:struct mtaudtimecode {
    unchar hhi:4, hlo:4; /* hours */
    unchar mhi:4, mlo:4; /* minutes */
    unchar shi:4, slo:4; /* seconds */
    unchar fhi:4, flo:4; /* DAT frame # */
};The hhi and hlo members expect two digits that specify the hour to which you want to seek. The valid range for these two digits is from 00 to 99. The mhi and mhl expect the two digits that specify the minute to which you want to seek. The valid range for these two digits if from 00 to 59. The shi and slo expect the two digits that specify the second to which you want to seek. The valid range for these two digits is from 00 to 59. The fhi and fhl expect the two digits that specify the DAT frame to which you want to seek. The valid range for these two digits is from 00 to 33.LBL="" HELPID=""ID="37686"Using the DAT DriveThis section explains how to use the DAT Audio Library routines for:ID="Media2-7DAT37"playing a DATrecording a DATreading and writing audio data from a DATparsing DAT informationcommunicating DAT status to the end userLBL="" HELPID=""Playing a Tape in the DAT DrivePlaying audio from a DAT is a little more complicated than playing a CD. For example, the sample rate for all CDs is 44.1 kHz; however, DAT audio may have been recorded at a sampling rate of 48 kHz, 44.1 kHz, or 32 kHz. Fortunately, a DAT records its sampling rate in the subcodes at the start of the tape, so you can read this sampling rate from the DAT before you must write DAT audio samples to the audio port.ID="Media2-7DAT38"In outline, a simple DAT-playing application must:Define a callback routine for dt_sampfreq. When the parser calls this routine, it passes in the frequency just read from the tape. Your callback should set a global variable to the frequency it gets from the parser. (See the DTaddcallback(3) man page.) Define a callback routine for dt_audio. When the parser calls this routine, it passes in the audio data from the DAT frame just parsed. The callback routine should write this data to the audio port using the sampling rate set by the dt_sampfreq callback. Open the audio port.Open the DAT drive. Create a parser. Add your dt_sampfreq and dt_audio callbacks to the parser. Read samples from the DAT. Parse the samples. Write the samples to an audio port using the Audio Library.When the application first starts reading the tape, it sees the frequency, calls your dt_sampfreq callback, and hands it the sampling frequency. As the parser continues to parse DAT frames, it also sees the audio data and executes your dt_audio callback for each new DAT frame containing audio. For an example of a simple program that plays a tape in the DAT drive, see IDREF="75008" TYPE="TITLE""DAT Sample Program". For more information on using the audio port, see IDREF="70508" TYPE="TITLE"Chapter 6, "Programming with the Audio Library."LBL="" HELPID=""Making DAT Recordings for Playback on the DAT DriveWhen making recordings on DAT recorders that you want to play on a Silicon Graphics DAT drive, you must make sure you record at least one of the time codes. Most recorders will let you record audio without any time codes, so be certain you record the time codes. Record in standard mode; the DAT drive does not support long play (LP) mode or 4-channel (4CH) mode tapes.ID="Media2-7DAT39"LBL="" HELPID=""Reading Audio Data from the DAT Drive To read audio data from the DAT drive, you need to open the DAT drive and put it in audio mode. Then you can call the standard IRIX ID="Media2-7DAT40"read() system call as you would for any other tape device. The only complicating factor is that you need to ensure that you read complete DAT frames. This is not particularly difficult if you declare your receiving buffer to be an array of ID="Media2-7DAT41"DTFRAME structures. For example:DTFRAME MyDATbuffer[4];declares a buffer of four DTFRAME structures. If you then do a read such as:n = read( MyDATtapeDevice, MyDATbuffer, sizeof(MyDATbuffer) );you read in complete DAT frames and can easily access those complete DAT frames when you want to parse them. LBL="" HELPID=""Writing Audio Data to the DAT DriveTo write audio data to the DAT drive, you need to open the DAT drive and put it in audio mode. Then you can call the standard IRIX ID="Media2-7DAT42"write() system call as you would for any other tape device. Writing the tape is just a matter of writing DAT frames to the tape. But setting the contents of the DAT frames is not just a matter of gathering together your audio samples. You must write subcode information that specifies things such as the sampling rate at which the audio was recorded. You must also update the DAT time code for each DAT frame that you write to the tape. To help you set the subcode information for the DAT frames you want to write, libdataudio contains these routines:DTsetdate()to set a date pack to the current time (useful for timestamps)ID="Media2-7DAT43"DTinctime()to increment a DAT time codeID="Media2-7DAT44"DTtcvalid()to check that a time code is valid (use it after calling DTinctime())For more information on the time code routines, see the appropriate man pages. For information on what you can write into the DAT frame subcodes, see the DTFRAME(4) man page. For additional information about properly writing DAT subcodes, see the DAT specification.LBL="" HELPID=""Ensuring that your DAT Recording Is Recognized as AudioThe DAT drive determines whether a tape is audio by looking for valid audio DAT frames. These frames must contain at least one valid time code field (absolute time, run time, or program time). When making recordings on DAT recorders that you want to later play on the Silicon Graphics DAT drive, you must make sure you record one of these time codes and that you record in standard mode.LBL="" HELPID=""Recording the DAT Lead-in AreaThe DAT specification requires that a tape begin with a special lead-in area of 100 DAT frames. Recording 100 frames ensures that the real recording will not begin over the plastic leader on the tape.ID="Media2-7DAT45"The following procedure provides the proper lead-in area:create an empty DTFRAMEset the program number contained in the DAT frame to 0x0BB (beginning-of-tape, or BOT, code) set the START bit in the control IDset the subcode packs to 0x0AA (readable, not valid)fill the audio data block with zerosrewind the tape and repeatedly write the DAT frame at least 100 timesLBL="" HELPID=""Recording Digital Audio over Digital Data Storage (DDS) TapesID="Media2-7DAT46"This section explains special precautions that must be taken when recording audio onto a tape that has previously been used as a data (DDS) tape.When you insert a DDS tape into the DAT drive, it is rewound to the logical beginning-of-tape (BOT). On data tapes, the logical BOT differs from the physical BOT by approximately 10 centimeters (30 seconds). If you attempt to write data to the drive in audio mode, writing begins at the logical BOT. When you then rewind and play this tape, there is an initial 30-second gap before playback starts. If the tape is removed and then reinserted into a DAT drive, it is recognized as a data tape because DDS format data exists between the physical BOT and the DDS logical BOT.NoteWith the current DAT drives (firmware revision 2.63), the following procedure is necessary to work around the problem: Check to see if the tape in the drive is DDS media and at BOT. If so, switch the drive to audio mode and write a frame of data to move the tape off logical BOT, and then issue a rewind. This rewinds the tape all the way back to the physical BOT. ID="Media2-7DAT47"LBL="" HELPID=""Example Programs Demonstrating DAT RecordingTwo sample programs are available to help you with DAT recording:ID="Media2-7DAT48"APP="/usr/sbin/jot /usr/people/4Dgifts/examples/dmedia/cd+dat/cdtodat.c" PARMS=""cdtodat.c, in /usr/people/4Dgifts/examples/dmedia/cd+datThis program copies audio from a CD to a DAT. It contains example code for recording to DAT, including handling of the lead-in area and recording over data tapes. APP="/usr/sbin/jot /usr/people/4Dgifts/examples/dmedia/cd+dat/verifydat.c" PARMS=""verifydat.c, in /usr/people/4Dgifts/examples/dmedia/cd+datThis program verifies that a DAT has been recorded correctly and has continuously running absolute time code. LBL="" HELPID=""Controlling the DAT ParserAfter you have read in data from a DAT, you can start to process it. Typically, how you process the audio data depends on what its associated subcodes tell you about the data (for example, the sample rate at which the audio was recorded). If you want, you can directly examine the subcode associated with each DAT frame and respond appropriately. ID="Media2-7DAT49"The DTFRAME structure, however, is large and complicated and subject to change. libdataudio includes a parser so that your application can avoid dealing with the ID="Media2-7DAT50"DTFRAME structure directly. If you write a loop that passes all the read DAT frames through the parser, the parser can examine all the DAT frames for changes in the subcode. When the parser finds a change (seeing a subcode for the first time counts as a change), it executes the appropriate callback routinename='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'depending on what sort of subcode change occurredname='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'and passes the new subcode data into your callback routine. The DAT parser distinguishes among 14 categories of subcode information. Thus, if you are interested in subcode changes for only one category of subcode data, the parser does not bother your application with subcode changes that you consider irrelevant. LBL="" HELPID=""Allocating and Initializing the DAT ParserTo allocate and initialize the data structures for the DAT parser, you must call ID="Media2-7DAT51"ID="Media2-7DAT52"DTcreateparser().To reset the parser after the user changes the tape in the DAT drive, call DTresetparser().ID="Media2-7DAT53" This clears out any information the parser has about the last DAT frame but leaves the callback routines in place. LBL="" HELPID=""Defining Callbacks for the DAT ParserWhen you define a callback for the parser, you must write a function of the form:My_dat_SomethingCallBack( void* arg, DTDATATYPES type,
void* data)
{ 
/* your code here */
}The parser uses the third parameter to pass in information it read from the subcodes. The parser uses the second parameter to pass in the type of callback it thinks it is calling. You can use this to assign the same function to different types of callbacks. Internally, you can switch on the type. This feature is useful if two callbacks are the same except for a few lines. The parser does not use the first parameter. You can use that to pass in information if your application needs to call the callback directly. LBL="" HELPID=""Adding and Removing DAT Parser Callbacks To add callback routines to the parser, call ID="Media2-7DAT54"ID="Media2-7DAT55"DTaddcallback(). If you do not specify a callback for a category, the parser assumes you are not interested in changes of that type.You can add callbacks that respond to changes in any of the following categories of subcode data:dt_audiocallbacks respond to changes in the audio data in a DAT frame. You can use this class of callback to notify you when you have gotten past the lead-in track and have started to see audio samples. When the parser calls this routine, it passes in the audio sample data. If this callback routine is a play routine for your application, it should write the audio sample to an audio port using the Audio Library. See the ALwritesamps(3) man page and IDREF="70508" TYPE="TITLE"Chapter 6, "Programming with the Audio Library."dt_pnumcallbacks respond to changes in the program number. You can use this callback to notice when you have moved from one program (track) to the next. dt_indexcallbacks respond to changes in the index number. You can use this callback to notice when you have moved from one subsection of a track to the next.dt_ptimecallbacks respond to changes in the program time. You can use this callback to continuously update a "program time display" in a DAT-playing application. dt_atimecallbacks respond to changes in the absolute time elapsed since the start of the DAT. You can use this callback to continuously update your application's information about total elapsed time.dt_rtimecallbacks respond to changes in the run time elapsed since the start of a recording on the DAT. You can use this callback to continuously update your application's information about total elapsed time since the start of a recording.dt_prortime callbacks are like dt_rtime callbacks in that they respond to changes in the elapsed run timename='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'however, the parser hands the callback more information than it gives to a dt_rtime callbackname='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'this type of callback is intended for professional usesdt_mainidcallbacks respond to changes in the contents of the ID field dt_sampfreqcallbacks respond to changes in the subcodes that describe the sampling frequency for the recording on the DATdt_toccallbacks respond to changes in the subcode data that describe the table of contents for the tapeNoteWhen parsing the DAT, you should note a separate subcode for each entry in the table of contents.dt_datecallbacks respond to changes in the timestamp for a recordingdt_catalogcallbacks respond to changes in the DAT catalog numberdt_identcallbacks respond to changes in the ISRC identification number for the recording on the DATdt_probinarycallbacks respond to changes in the IEC (SMPTE) or Pro DIO time codesFor more information on each subcode category, see the DTaddcallback(3) man page and the DAT specification.LBL="" HELPID=""Deleting or Changing a DAT Parser Callback To delete a callback, call ID="Media2-7DAT56"DTremovecallback(). To change a callback, call DTremovecallback() followed by DTaddcallback(). LBL="" HELPID=""Parsing DAT Frames To submit a group of DAT frames to the parser, set up a loop that calls ID="Media2-7DAT57"DTparseframe() ID="Media2-7DAT58"for each DAT frame that you have read into your buffer.LBL="" HELPID=""Freeing the Memory Reserved for the DAT Parser If you are done with the parser and want to free the memory it uses, callID="Media2-7DAT59"ID="Media2-7DAT60"DTdeleteparser()  to delete the parser. LBL="" HELPID=""Communicating DAT Status to the End User Whether you get status information for the DAT directly from the DTFRAMEID="Media2-7DAT61" structure or from one of your parser-callback routines, you need to convert that information to an ASCII string.libdataudio includes these conversion routines:ID="Media2-7DAT62"DTsbtoa()ID="Media2-7DAT63"to convert a 6-bit country and owner code to an ASCII stringDTtimetoa()ID="Media2-7DAT64"to convert a time code to an ASCII stringDTpnotodec()ID="Media2-7DAT65"to convert a BDC program number to a decimal, which you can then check and convert to ASCII if appropriateFor more information on these routines, see the relevant man pages.LBL="" HELPID=""ID="75008"DAT Sample ProgramThis section contains ID="Media2-7DAT66"datplay.c, a simple program for reading and processing DAT data.LBL="" HELPID=""Playing a DATIDREF="33580" TYPE="TEXT"Example 9-1 reads samples from the DAT and uses the parser and two callbacks to process the data read. One callback, frequency(), extracts the sampling rate from the subcodes on the DAT. The other callback, playaudio(), extracts audio samples from the frames and writes them to the audio port.LBL="9-1"Example 9-1 ID="33580"Reading DAT Samples/* DAT example from digital audio and MIDI programming guide */

#include <stdio.h>
#include <sys/fcntl.h>
#include <sigfpe.h>     /* Floating point exception error handling package */
                        /* with this you'll need to compile w/ -lfpe */

#include <dataudio.h>   /* DAT audio library. */
                        /* with this you'll need to compile w/ -ldataudio */

#include <audio.h>      /* audio library */
                        /* with this you'll need to compile w/ -laudio */

static int sampsperframe = DTDA_NUMSAMPS48K;    /* Number of samples you'll */
                                                /* get in 1 DAT frame when  */
                                                /* the sampling rate is at  */
                                                /* 48kHz.                   */

ALport audioport;                               /* Audio port to output the */
                                                /* DAT sample data */

/* Our dt_audio callback */
/* It gets called when there's a change in the audio data in a frame. */

playaudio(void *arg, DTDATATYPES type, short *audio)
{
   ALwritesamps(audioport, audio, sampsperframe);
        /* Send the audio samples read out to the audio port */
}

/* Our dt_sampfreq callback. */
/* It gets called when there's a change in the subcodes that describe the */
/* sampling frequency for the recording on the DAT. */
frequency(void *arg, DTDATATYPES type, int *freq)
{
   switch (*freq) 
   {
     case DT_FREQ48000:
       sampsperframe = DTDA_NUMSAMPS48K;        /* Number of samples you'll */
                                                /* get in 1 DAT frame when  */
                                                /* the sampling rate is at  */
                                                /* 48kHz.                   */
       break;
     case DT_FREQ44100:
       sampsperframe = DTDA_NUMSAMPS44K;        /* Number of samples you'll */
                                                /* get in 1 DAT frame when  */
                                                /* the sampling rate is at  */
                                                /* 44kHz.                   */
       break;
     case DT_FREQ32000:
       sampsperframe = DTDA_NUMSAMPS32K;        /* Number of samples you'll */
                                                /* get in 1 DAT frame when  */
                                                /* the sampling rate is at  */
                                                /* 32kHz.                   */
       break;
   }

main()
{
int tape = open("/dev/nrtape", O_RDONLY);
                /* Open the file descriptor for reading data off of  */
                /* DAT Tape assumed to be /dev/nrtape.               */

DTPARSER *dtp = DTcreateparser();       /* Initialize DAT parser.   */

DTFRAME buf[4];                         /* Will describe content of current */
                                        /* DAT frame. This is what gets     */
                                        /* sent to the DAT parser.          */
                                        /* See man pagfe for DATFRAME for   */
                                        /* detailed info.                   */

struct mtop mt_com;                     /* Message structure for magntic    */
                                        /* tape device interface for use in */
                                        /* passing to the ioctl command. De-*/
                                        /* fined in /usr/include/sys/mtio.h */
                                        /* See mtio and dataudio man pages. */
}
int i, n;
   audioport = ALopenport("DAT Test", "w", 0);  /* Open audio port. */
   if (dtp)                                     /* Check for DAT parser. */
   {
      DTsetcallback(dtp, dt_audio, (DTCALLBACKFUNC)playaudio, 0);
                                /* Set up function to be called when there's */
                                /* a change in audio information.            */

      DTsetcallback(dtp, dt_sampfreq, (DTCALLBACKFUNC)frequency, 0);
                                /* Set up function to be called when there's  */
                                /* a change in sampling frequency information.*/
        /* Make sure we get sane underflow exception handling */

        sigfpe_[_UNDERFL].repls = _ZERO;
        handle_sigfpes(_ON, _EN_UNDERFL, NULL, _ABORT_ON_ERROR, NULL);
                                /* See man page for sigfpe */

   } 
   else
      exit(1);                  /* Can't do much without a DAT parser. */

   if (tape >= 0)               /* Check for tape reading OK. */ 
   {

       mt_com.mt_op = MTAUD;    /* Set up MT(io) AUD(io) message to tell */
                                /* DAT drive to turn audio mode on/off.  */
       mt_com.mt_count = 1;     /* 1 == audio mode, 0 == data mode       */

       ioctl(tape, MTIOCTOP, &mt_com);  /* Perform M(agnetic) T(ape) I/O */
                                        /* C(ontrol) T(ape) OP(eration). */
                                        /* This is needed for both       */
                                        /* reading and writing audio to  */
                                        /* or from the DAT drive.        */

       for (;;) 
       {
           n = read(tape, buf, sizeof(buf));    /* Read frame of DAT audio */
                                                /* data from the tape's    */
                                                /* file descriptor.        */
           if (n < 0) 
           {
               printf("Couldn't read DAT tape.\n");       /* Report error */
               exit(2);
           }
           if (n == 0)                    /* We're at the end of the tape */
               break;
           for (i = 0; i < 4; i++)
               DTparseframe(dtp, &buf[i]);      /* Sort out what info was in  */
                                                /* the frame we just read and */
                                                /* invoke the callbacks we    */
                                                /* specified previously.      */
       }
       exit(0);
   }
   exit(3);                     /* Can't do much without a tape to read from */
}
ID="Media2-7DAT67"LBL="10"ID="99417"Programming with the MIDI LibraryThe MIDI Library, ID="Media2-8MI1"ID="Media2-8MI2"libmd.so, provides an API for sending, receiving, and processing musical instrument digital interface (MIDI) messages through the serial interface of Silicon Graphics IRIS Indigo, Indigo2, and Indy workstations. The MIDI Library featurestimed input and output of MIDI databuffered I/O with user-adjustable buffering timeactive sensing and system-exclusive data handlingsimultaneous access to MIDI devices from multiple programsthe ability to have multiple input and output streams open concurrentlya correlation to other media streams with unadjusted system time (UST)sample applications online in /usr/people/4Dgifts/examples/dmedia/midiHands-on experiences are presented throughout this chapter:IDREF="32768" TYPE="TITLE""Hands-On MIDI Output Experience" demonstrates sending MIDI messages.IDREF="39835" TYPE="TITLE""Hands-On Multiplexed MIDI I/O Experience" demonstrates a MIDI thru box.IDREF="60416" TYPE="TITLE""Hands-On MIDI File Player Experience" demonstrates a multithreaded MIDI file player with a graphical user interface.IDREF="62110" TYPE="TITLE""Hands-On MIDI and Audio Synchronization Experience" demonstrates synchronized MIDI playback and recording.In this chapter:IDREF="94034" TYPE="TITLE""MIDI System Architecture" describes the system configurations and I/O interfaces for MIDI.IDREF="15542" TYPE="TITLE""MIDI Library Basics" discusses basic MIDI concepts and the main MIDI Library data structures.IDREF="17828" TYPE="TITLE""Opening and Closing MIDI Ports" explains how to open and close MIDI ports and how to get a file descriptor for a MIDI port.IDREF="39235" TYPE="TITLE""Programming MIDI I/O" explains how to implement basic MIDI I/O functions.IDREF="59758" TYPE="TEXT"IDREF="59758" TYPE="TITLE""Multiplexing MIDI I/O with File Descriptors" explains how to use file descriptors to multiplex synchronous I/O.IDREF="18137" TYPE="TITLE""Controlling MIDI Timing" describes timestamping modes and explains how to specify and scale MIDI tempo.IDREF="42349" TYPE="TITLE""Synchronizing MIDI I/O with Other Media" explains how to use Unadjusted System Time (UST) for synchronizing MIDI timing with other media streams. It contains synchronized MIDI recording and playback applications that use the MIDI and Audio libraries in conjunction.LBL="" HELPID=""ID="94034"MIDI System ArchitectureThis section describes system configurations for MIDI development and the MIDI input and output interfaces.LBL="" HELPID=""Configuring Your System for MIDI DevelopmentThe most essential peripheral for MIDI development is a serial-to-MIDI converter. You can use any Apple MacintoshÒ compatible serial-to-MIDI converter. Many MIDI converters include additional features, including SMPTE-to-MIDI conversion (which is useful for synchronizing MIDI to tape and film), and built-in MIDI patchbays for switching between multiple MIDI inputs and outputs. You should choose your serial-to-MIDI converter based on the functionality you expect your users to require.ID="Media2-8MI3"ID="Media2-8MI4"ID="Media2-8MI5"Once you have selected a MIDI converter, you will need some MIDI devices to attach to it. The kind and number of MIDI devices you choose to create your MIDI network depends largely on the scope of the application you are writing and your budget. A single keyboard synthesizer may be sufficient for your needs if you are writing a very simple sequencer, but for more complex programs, you should have a keyboard, several rack-mount synthesizer modules, an alternate MIDI controller (such as a wind controller), and a mixer with enough channels for all the instruments you have (so you can hear the results).ID="Media2-8MI6"ID="Media2-8MI7"If you are not satisfied with listening through headphones or through your workstation's internal speaker, you should probably invest in an amplifier and/or a pair of powered speakers. A multi-track tape recorder may also be useful for testing your application if it will ultimately be used for recording music. You should plan on testing your application using the kind of equipment that you anticipate the end users of your application to have.IDREF="30028" TYPE="GRAPHIC"Figure 10-1 shows one possible MIDI setup.FILE="MIDInet.ai" POSITION="INLINE" SCALE="FALSE"LBL="10-1"Figure 10-1 ID="30028"MIDI SetupLBL="" HELPID=""ID="42578"Connecting Devices to MIDI I/O InterfacesThe MIDI Library is currently supported on the Indigo, IndigoID="Media2-8MI8"2, and Indy workstations. The two serial ports on your workstation are configured for MIDI from the Port Setup tool, as described in ID="Media2-8MI9"IDREF="89257" TYPE="TITLE""Configuring Serial Ports for MIDI WIth the Port Setup Tool".Any Apple Macintosh-compatible serial-to-MIDI interface operates when connected to either or both of the serial ports. Many of these interfaces offer additional useful features, such as SMPTE time-code conversion and integrated software-configurable MIDI patching.IDREF="56080" TYPE="GRAPHIC"Figure 10-2 shows the serial ports on the back panel of the Indigo workstation.FILE="indigoserialports.ai" POSITION="INLINE" SCALE="FALSE"LBL="10-2"Figure 10-2 ID="56080"Serial Ports on the Back Panel of the Indigo WorkstationNoteDo not use the keyboard port for MIDI.IDREF="90824" TYPE="GRAPHIC"Figure 10-3 shows the serial ports on the back panel of the IndigoID="Media2-8MI10"2 workstation.FILE="ind2serialports.ai" POSITION="INLINE" SCALE="FALSE"LBL="10-3"Figure 10-3 ID="90824"Serial Ports on the Back Panel of the Indigo2 WorkstationIDREF="93443" TYPE="GRAPHIC"Figure 10-4 shows the serial ports on the back panel of the Indy workstation.ID="Media2-8MI11"FILE="indyserialports.ai" POSITION="INLINE" SCALE="FALSE"LBL="10-4"Figure 10-4 ID="93443"Serial Ports on the Back Panel of the Indy WorkstationLBL="" HELPID=""ID="89257"Configuring Serial Ports for MIDI WIth the Port Setup ToolBefore you can run a MIDI application, you must first configure your workstation's serial ports for MIDI by using the Port Setup tool. The Port Setup tool provides a GUI for configuring connections to your system's serial ports. Once set up, a serial port remains configured for MIDI, even if the system reboots, until you reset it from the Port Setup tool.ID="Media2-8MI12"To configure a serial port for MIDI:Open the System Manager and select the System Administration tools from the Tools menu.Click the Port Setup icon, shown in IDREF="68846" TYPE="GRAPHIC"Figure 10-5.  FILE="PortIcon.bw" POSITION="INLINE" SCALE="FALSE"LBL="10-5"Figure 10-5 ID="68846"Port Setup IconIDREF="87930" TYPE="GRAPHIC"Figure 10-6 shows the Port Setup tool.  FILE="PortSetup.bw" POSITION="INLINE" SCALE="FALSE"LBL="10-6"Figure 10-6 ID="87930"Port Setup ToolSelect the available serial port by clicking its icon.Click Connect.  IDREF="38941" TYPE="GRAPHIC"Figure 10-7 shows the device connections available from the Port Setup tool.  FILE="PortConnect.bw" POSITION="INLINE" SCALE="FALSE"LBL="10-7"Figure 10-7 ID="38941"Serial Port ConnectionsSelect the MIDI device by clicking its icon.Click Set Up.  The system displays the MIDI Port Configuration menu shown in IDREF="59012" TYPE="GRAPHIC"Figure 10-8, asking you to confirm whether you want to start MIDI.  FILE="MIDI.bw" POSITION="INLINE" SCALE="FALSE"LBL="10-8"Figure 10-8 ID="59012"MIDI Port ConfigurationClick OK to start MIDI on the selected port.LBL="" HELPID=""ID="15542"MIDI Library BasicsThis section discusses fundamental MIDI concepts and describes the primary data structures used by the MIDI Library.MIDI is a control protocol, as opposed to a data protocol, meaning that a MIDI network does not carry audio signals, but rather instructions that tell MIDI instruments how to behave. MIDI information is transmitted in the form of an event: a control instruction (or message), combined with time information (called a timestamp). Typical messages are ID="Media2-8MI13"note on and note off (describing the beginning and ending time of a certain musical note) and values for continuous controllers such as sustain pedals, modulation wheels, and pitch bend controllers. LBL="" HELPID=""ID="94782"Initializing MIDI Library ProgramsBefore calling any MD Library routines, you must initialize the MIDI Library by calling mdInit(), which returns the number of available MIDI ports. If the MIDI daemon is not running, mdInit() returns -1. See IDREF="89257" TYPE="TITLE""Configuring Serial Ports for MIDI WIth the Port Setup Tool" for instructions on configuring the serial ports and starting and stopping the MIDI daemon.LBL="" HELPID=""Compiling and Linking MIDI Library ProgramsTo compile a MIDI Library program, enter:cc ­g MLsample.c -o MLsample ­lmdYou should also link with any other libraries, such as the Audio Library, that your application uses.LBL="" HELPID=""MIDI Library Error HandlingAll libmd functions return -1 on error, and set oserror(3C) appropriately.LBL="" HELPID=""MIDI Library Programming ModelThe MIDI Library has two basic data structures:MDportAn opaque structure containing information about the state of MIDI data and timing, as well as the state of all options for the port.MDeventA public structure containing fields for regular and system-exclusive MIDI messages, timestamps, and message lengths.LBL="" HELPID=""ID="17828"Opening and Closing MIDI PortsThe MDport, or MIDI port, is the basic MIDI I/O structure in the MIDI Library. An MDport provides a one-way (input-only or output-only) connection to a MIDI device. Each port can transmit or receive on as many as 32 independent MIDI channels, 16 per serial port. The use of separate MIDI channels allows complex orchestration of MIDI instruments, when each MIDI device is "tuned" to a different channel.ID="Media2-8MI14"ID="Media2-8MI15"LBL="" HELPID=""Getting the Name of an Available MIDI PortOnce the MIDI Library has been initialized by calling mdInit(), you can get the name of an available port by calling mdGetName():char *mdGetName(int portno)Then you can build a menu of available MIDI ports for your application. mdGetName() returns the name string associated with portno or NULL, if portno does not refer to an existing port. For example, if MIDI has been initialized with the following command:startmidi -n ttyd2 -d /dev/ttyd2mdInit() returns 1, and mdGetName(0) returns the "ttyd2" string. LBL="" HELPID=""Opening and Closing MIDI Input and Output PortsThe MIDI Library has task-specific calls for opening MDports: mdOpenInPort() opens an input port and mdOpenOutPort() opens an output port. Their function prototypes are:MDport mdOpenInPort(char *name)
MDport mdOpenOutPort(char *name)Each returns a handle to the appropriate type of port.Use the name returned by mdGetName() to indicate a particular MIDI device to which a port is to be connected, as demonstrated in IDREF="53937" TYPE="TEXT"Example 10-1.LBL="10-1"Example 10-1 ID="53937"Opening MIDI Input and Output Ports#include "dmedia/md.h"

main()
{
        int nports, x;
        MDport inport, outport;
name='hellip' font=symbol charset=fontspecific code=188
        nports = mdInit();
        printf("%d devices available\n", nports);

        inport = mdOpenInPort(0);
        if (inport == NULL) 
                printf("open failed\n");

        outport = mdOpenOutPort(0);
        if (outport == NULL) 
                printf("open failed\n");
name='hellip' font=symbol charset=fontspecific code=188
}The initial state of a newly-opened port is undefined, except for the timestamping mode, which is MD_DELTASTAMP.You can open up to 64 MDports, less the number of devices. For example, if you have both serial ports configured for MIDI, you can open 62 MDports. When a port is no longer needed, call mdClosePort() to close the port and free its associated resources.LBL="" HELPID=""ID="39235"Programming MIDI I/OThis section explains how to implement the most basic tasks for MIDI applications: sending and receiving MIDI messages.LBL="" HELPID=""ID="32768"Hands-On MIDI Output ExperienceTo begin, try playing a sound through the MIDI equipment connected to your system. The sample application plays a sound by sending a MIDI message. To test whether you can send MIDI output, try this:If you have not already done so, connect your MIDI equipment to your workstation's serial port, and configure the port for MIDI, as described in IDREF="42578" TYPE="TITLE""Connecting Devices to MIDI I/O Interfaces".Click on APP="/usr/people/4Dgifts/examples/dmedia/midi/simple/scale" PARMS=""scale
 to launch the scale program, which sends a musical scale through the MIDI output.NoteThe application will not launch if you don't have your MIDI equipment connected and set up.See APP="/usr/sbin/jot /usr/people/4Dgifts/examples/dmedia/midi/simple/scale.c" PARMS=""scale.c
, in /usr/people/4Dgifts/examples/dmedia/midi/simple to view the code that plays the musical scale.A tone is played through the MIDI output by specifying the note to play and its duration, and then sending a note on event to sound the tone, followed by another event to end the sound (often, a note on with zero velocity is used to silence a note).IDREF="45511" TYPE="TEXT"Example 10-2 shows the playnote() routine from scale.c, which creates a MIDI event structure (mdEvent) named mdv. The mdv structure contains a 3-byte message, a timestamp, and the message length. The 3-part message consists of the MD_NOTEON event, or'ed to a channel, followed by the note and its velocity. See IDREF="26449" TYPE="TITLE""About MIDI Events" for a description of the mdEvent structure.The message is given an initial timestamp of 0. After the specified time interval has elapsed, the note's velocity is set to zero and then a zero velocity note on message is sent to silence the output.LBL="10-2"Example 10-2 ID="45511"Sending a MIDI Message#include "dmedia/midi.h"
name='hellip' font=symbol charset=fontspecific code=188
playnote ( MDport port, char note, unsigned long long time,
           char channel, char velocity )
{
    mdEvent mdv;
    
    mdv.msg[0] = MD_NOTEON | (channel & 0xf);
    mdv.msg[1] = note;
    mdv.msg[2] = velocity;
    
    mdv.stamp = 0;
    mdv.msglen = 3;

    if (mdSend(port, &mdv, 1) < 0) {
        exit(-1);
    }
    mdv.stamp = time;
    mdv.msg[2] = 0;
    
    if (mdSend(port, &mdv, 1) < 0) {
        exit(-1);
    }
}MIDI event structures are described in IDREF="26449" TYPE="TITLE""About MIDI Events".LBL="" HELPID=""ID="26449"About MIDI EventsMIDI events are contained in the MIDI Library mdEvent data structure:typedef struct __mdevent {
    char msg[4];                 /* channel message data */
    char *sysexmsg;              /* sysex message data */
    unsigned long long stamp;    /* time stamp in nanosecs */
    int msglen;              /* length of data, sysex only */
} mdEvent;msgis an array of characters representing the data of a non-system-exclusive message, which can include status, note, and controller information and is from 1 to 3 bytes in length.sysexmsgis a pointer to a string of characters representing a block of system-exclusive (SYSEX) data, which can include bulk data such as instrument patch configuration parameters and can be of arbitrary length. When SYSEX data is received, msg[0] is set to MD_SysEx (0xf0), and then the actual data storage is allocated with mdMalloc(); similarly, it must be released with mdFree().stampis the timestamp of the event, in nanoseconds or in ticks, if you are using one of the tick modes.msglenis used by system-exclusive messages to indicate the length of the SYSEX packet, or when sending multiple messages in a single event. For single events, msglen should be set to 0.The timestamp for the MIDI event, which is the time at which the event did or should occur, is reckoned from either a fixed time or the previous event's time. The MIDI Library supports two types of timestamping:relativeID="Media2-8MI16"stamping, in which time is reckoned for all events as an interval from an initial specified time. This is useful for sequencers.delta stamping, in which time for each individual event is reckoned as the interval since the last event occurred. This is useful for insertions of events in lists.See IDREF="18137" TYPE="TITLE""Controlling MIDI Timing" for information about setting the timestamping mode and other parameters.LBL="" HELPID=""Sending and Receiving MIDI EventsThis section explains how to send and receive MIDI events.LBL="" HELPID=""Sending MIDI EventsTo send a MIDI event from a MIDI output port, call mdSend(). Its function prototype is:int mdSend(MDport port, MDevent *buf, int count)Depending on the port's timestamping mode, MIDI events have either relative or delta timestamps, or no timestamps at all. If temporal buffering is used, mdSend() blocks, waiting for the output to catch up, until the amount of time represented by the sum of the timestamps in the event buffer exceeds the timeout value set by mdSetTemporalBuffering().If no errors occur, mdSend() returns the number of messages successfully sent; otherwise, it returns either 0, indicating that an error occurred and no messages were sent, or -1 times the number of messages not sent. mdSend() sleeps if the output queue size limit is exceeded.LBL="" HELPID=""Receiving MIDI EventsTo receive a MIDI event into a MIDI input port, call mdReceive(). Its function prototype is:int mdReceive(MDport port, MDevent *buf, int count)mdReceive() allocates storage for messages coming into the designated port, except for system-exclusive messages; these require the application to allocate and free the necessary storage. mdReceive() copies the message(s) and timestamp(s) into a buffer, and returns either the number of messages read from the given port or -1, if an error occurred.LBL="" HELPID=""Handling System-Exclusive MIDI EventsSYSEX messages are received in chunks of up to 1 kilobyte. To check for SYSEX messages, scan for the EOX marker in buf.sysexmsg[buf.msglen-1]. When receiving sysex messages, buf.msg[0] is set to 0xf0 for each chunk received, while the actual data is stored in buf.sysexmsg.LBL="" HELPID=""Printing MIDI EventsTo print the messages in the MIDI event buffer, you must first convert them to a human-readable format by calling mdPrintEvent(). Its function prototype is:int mdPrintEvent(char *buf, mdEvent *evbuf, int count)bufis a pointer to buffer allocated by the application. It should be large enough to contain the formatted representation of all the events in evbuf. Eighty bytes per message is sufficient.evbufis a pointer to the event buffercountis the number of events to formatThe message format is:timestamp : channel : status type (string) : byte 1 : byte 2If the message is a note on or note off, then the note name (for example, A3) is printed in the field occupied by byte 1; otherwise, the numeric value is printed.LBL="" HELPID=""Processing MIDI Event MessagesA MIDI message is an array of 2 or 3 bytes. The first byte contains the status in the high nibble, and the channel in the low nibble. The remaining 1 or 2 bytes contain the values.LBL="" HELPID=""Setting and Getting MIDI Message StatusThe status byte determines the type of message and its length. IDREF="10155" TYPE="TABLE"Table 10-1 lists the name, length, and purpose of each status byte.COLUMNS="4"LBL="10-1"Table 10-1 ID="10155"MIDI Message Status BytesLEFT="0" WIDTH="136"StatusLEFT="145" WIDTH="32"LengthLEFT="185" WIDTH="80"Byte 1LEFT="270" WIDTH="70"Byte 2LEFT="0" WIDTH="136"MD_CHANNELMODESELECT LEFT="145" WIDTH="32"2LEFT="185" WIDTH="80"LEFT="270" WIDTH="70"LEFT="0" WIDTH="136"MD_CHANNELPRESSURE LEFT="145" WIDTH="32"2LEFT="185" WIDTH="80"LEFT="270" WIDTH="70"LEFT="0" WIDTH="136"MD_CONTROLCHANGE LEFT="145" WIDTH="32"2LEFT="185" WIDTH="80"Controller numberLEFT="270" WIDTH="70"Controller valueLEFT="0" WIDTH="136"MD_NOTEOFF LEFT="145" WIDTH="32"3LEFT="185" WIDTH="80"LEFT="270" WIDTH="70"LEFT="0" WIDTH="136"MD_NOTEON LEFT="145" WIDTH="32"3LEFT="185" WIDTH="80"Note numberLEFT="270" WIDTH="70"VelocityLEFT="0" WIDTH="136"MD_PITCHBENDCHANGE LEFT="145" WIDTH="32"3LEFT="185" WIDTH="80"MSBLEFT="270" WIDTH="70"LSBLEFT="0" WIDTH="136"MD_POLYKEYPRESSURE LEFT="145" WIDTH="32"3LEFT="185" WIDTH="80"LEFT="270" WIDTH="70"LEFT="0" WIDTH="136"MD_PROGRAMCHANGE LEFT="145" WIDTH="32"2LEFT="185" WIDTH="80"Program numberLEFT="270" WIDTH="70"UnusedTo set the status, call mdSetStatus(); to get the status, call mdGetStatus().LBL="" HELPID=""Setting and Getting MIDI Message ChannelThe channel is the low nibble of the high byte. It takes a value of 0 through 15, which corresponds to a MIDI channel range of 1 through 16.The functions for setting and getting the channel are:int mdGetChannel(char *msg)
void mdSetChannel(char *msg, int x)There are sixteen channels for each serial port (device 0 and device 1). Channels 0­15 are sent on device 0, and channels 16­31 are sent on device 1.LBL="" HELPID=""Setting and Getting MIDI Message ValueByte1 and Byte2 are the values associated with the message. Message-specific information (for example, note names for note on and note off messages and channel numbers for channel messages), is contained in these bytes. The functions that set and get MIDI message values are:void mdSetByte1(char *msg, int x)
void mdSetByte2(char *msg, int x)
int mdGetByte1(char *msg)
int mdGetByte2(char *msg)See the following references for MIDI message codes:MIDI 1.0 Detailed Specification and Standard MIDI Files 1.0, International MIDI Association, 5316 W. 57th St., Los Angeles, CA 90056.MIDI Sequencing in C, by Jim Conger, ISBN 1-55851-045-1, M & T Books, 1989. Available from:M&T BooksA Division of M&T Publishing, Inc.501 Galveston DriveRedwood City, CA 94063LBL="" HELPID=""ID="59758"Multiplexing MIDI I/O with File DescriptorsYou can multiplex MIDI input and output by using the IRIX select(2) system call to wait on MD file descriptors. Using this technique allows your workstation to function as a MIDI thru box.LBL="" HELPID=""ID="39835"Hands-On Multiplexed MIDI I/O ExperienceTo send and receive MIDI messages through your workstation:If you have not already done so, connect your MIDI equipment to your workstation's serial port, and configure the port for MIDI, as described in IDREF="42578" TYPE="TITLE""Connecting Devices to MIDI I/O Interfaces".Click on APP="/usr/people/4Dgifts/examples/dmedia/midi/examples/simple/thru" PARMS=""thru
 to launch the thru sample program, which receives MIDI events and in turn sends them out through an mdOutport.NoteThe application will not launch if you don't have your MIDI equipment connected and set up.See APP="/usr/sbin/jot /usr/people/4Dgifts/examples/dmedia/midi/simple/thru.c" PARMS=""thru.c
, in /usr/people/4Dgifts/examples/dmedia/midi/simple to view the code that implements MIDI-thru capability.LBL="" HELPID=""Getting a File Descriptor for a MIDI PortFile descriptors can be used with the IRIX select(2) or poll(2) system calls to multiplex input and output of MIDI messages with other I/O devices. To get a file descriptor for an MDport, call mdGetFd(), which returns a file descriptor associated with the port:int mdGetFd(MDport port)IDREF="42254" TYPE="TEXT"Example 10-3 is an excerpt from thru.c that demonstrates putting the file descriptor returned by mdGetFd() into a file descriptor set and using select to wait on the file descriptor set. Using select requires including thesys/select.h header file.LBL="10-3"Example 10-3 ID="42254"Using MIDI File Descriptors#include "dmedia/md.h"
#include "sys/select.h"

main()
{
    int nports, x;
name='hellip' font=symbol charset=fontspecific code=188
    fd_set inports, outports;
    int nfds, highfd;

    nports = mdInit();
name='hellip' font=symbol charset=fontspecific code=188
    FD_SET(mdGetFd(inport),&inports);
    FD_SET(mdGetFd(outport),&outports);
    highfd = mdGetFd(outport) + 1; 

    while(1) {
        nfds = select(highfd,&inports,0,0,0);
        name='hellip' font=symbol charset=fontspecific code=188
    }
}LBL="" HELPID=""ID="18137"Controlling MIDI TimingThe MIDI Library provides for timed input and output of MIDI data. Messages are timestamped on input and are scheduled for output on the basis of their timestamp. Output scheduling can be disabled.You can synchronize I/O for MIDI streams with other media streams by correlating timestamps in terms of unadjusted system time (UST).LBL="" HELPID=""Controlling MIDI Timing ModeYou can choose to time MIDI events using either actual time or musical beats, also called ticks. The default mode of an MDport is delta timestamping, which measures the time elapsed from the previous event, but you can reset it to use any mode. To get a port's timestamping mode, call mdGetStampMode(); to set a port's timestamping mode, call mdSetStampMode().The MIDI Library has three modes in which timing is controlled by actual time:COLUMNS="2"LEFT="0" WIDTH="110"MD_NOSTAMPLEFT="115" WIDTH="225"Causes the output timestamps to be ignored. 
Input timestamps are undefined.LEFT="0" WIDTH="110"MD_DELTASTAMPLEFT="115" WIDTH="225"Causes each input event to be timestamped with 
the number of milliseconds from the previous 
event, and interprets output timestamps in the 
same way.LEFT="0" WIDTH="110"MD_RELATIVESTAMPLEFT="115" WIDTH="225"Causes input timestamps to be marked relative to 
a time specified by mdSetOrigin(). Output 
timestamps are also reckoned against the same 
origin time.The MIDI Library has two modes in which timing is controlled by beats:COLUMNS="2"LEFT="0" WIDTH="108"MD_RELATIVETICKSLEFT="115" WIDTH="225"Allows the ticks to be reckoned from an origin 
time that is set with mdSetOrigin().LEFT="0" WIDTH="108"MD_DELTATICKSLEFT="115" WIDTH="225"Uses simple delta timestamps. Ticks are defined in 
terms compatible with Standard MIDI files; for 
example, pulses per quarter note (PPQ). Ticks are 
controlled by mdSetTempo() and 
mdSetDivision().Time is measured from the time the MDport was opened. You can reset the reference time by calling mdSetOrigin(), which sets the start time to the 64-bit UST that you specify. The result depends on the value used:0Sets the start time to the system's current UST.< 0Sets the start time to the number of nanoseconds before the current UST. This allows streams of files to be restarted in the middle of the data> USTSets the start time to some time in the future.Upon successful completion, mdSetOrigin returns 0; otherwise it returns ­1 and sets an error code that you can retrieve with oserror(3C). To get the start time, call mdGetOrigin().Setting the reference time to match a UST value is useful in MIDI recording, for setting the recording start time to correspond to the arrival of an audio signal at the input jacks. See IDREF="62110" TYPE="TITLE""Hands-On MIDI and Audio Synchronization Experience" for a demonstration of this technique.LBL="" HELPID=""Controlling MIDI TempoYou can vary the tempo for ports whose timestamping mode is in ticks. Tempo is expressed in microseconds per beat, as in Standard MIDI Files (SMF). Tempo and division values specify the conversion from MIDI clock ticks to real time values for the MIDI driver. Divisions represent the subsamples per beat, or pulses per quarter note (PPQ). This is useful for MIDI sequencing. To set the tempo, call mdSetTempo(); to get the tempo, call mdGetTempo().To set the divisions per beat (pulses per quarter note), call mdSetDivision(); to get the divisions per beat, call mdGetDivision().To convert a timestamp from ticks to nanoseconds, taking into account the port's tempo, call mdTicksToNanos(). Similarly, to convert from nanoseconds to ticks, call mdNanosToTicks().Sometimes you need to adjust the tempo when audio is not synchronized or to compensate for a slow tape deck when recording MIDI. You can specify a tempo scale factor by calling mdSettemposcale(). When messages whose timestamps are expressed in ticks are written to a port that has a tempo scale factor, the timestamps are multiplied by the scale factor before being queued for output. This allows for nondestructive tempo matching.LBL="" HELPID=""Controlling MIDI Output BufferingThe MIDI driver buffers data according to time. An application that responds to user interaction must compensate for the fact that events can be sent to the MIDI port faster than they can actually be transmitted. By default, the MIDI library does not allow a process to get more than 2 seconds ahead of the actual output.You can control how much playback can get ahead of data transmission by specifying the amount of temporal buffering. To set the number of milliseconds an application can get ahead of its output, that is, the time to drain an MDport, call mdSetTemporalBuffering(). When the event timestamps exceed the specified timeout value, mdSend() sleeps until the output catches up.To determine the amount of time an application can get ahead of its output, call mdGetTemporalBuffering().You can pause output momentarily or even completely silence output when necessary.To stop pending output on a port and return the UST value or the tick of the last message sent to or from a port, call mdPause(). mdPause() immediately halts output.Sometimes you may need to do more than simply pause the output. You can send a panic event for a given port by calling mdPanic(), which sends an all notes off message and a reset controllers message on each channel.LBL="" HELPID=""ID="60416"Hands-On MIDI File Player ExperienceThe MIDI file player sample application illustrates the use of the timing concepts presented in this sectionname='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'it lets the user change the tempo of a MIDI file, pause playback, or drag a slider to start playback at a random location.To play MIDI files using the MIDI file player:If you have not already done so, connect your MIDI equipment to your workstation's serial port, and configure the port for MIDI, as described in IDREF="42578" TYPE="TITLE""Connecting Devices to MIDI I/O Interfaces".Click on APP="/usr/people/4Dgifts/examples/dmedia/midi/Mfp" PARMS=""Mfp
Enter Mfp to launch the MIDI file player. NoteThe application will not launch if you don't have your MIDI equipment connected and set up.See APP="/usr/sbin/jot /usr/people/4Dgifts/examples/dmedia/midi/mfp/player.c++" PARMS=""player.c++
, in /usr/people/4Dgifts/examples/dmedia/midi/mfp to view the playback source code.The MIDI file player application uses three threads: one to manage the user interface (UI), one to manage the playback, and one to update the song position. These threads use shared memory; semaphores are used to protect critical regions so that both processes don't try to access the same data simultaneously.The playback thread waits on the semaphore. Processes that are waiting on a semaphore are queued on a first-come, first-served basis. When the UI process acquires the semaphore, playback stops; when it releases the semaphore, playback starts. An important point to note is that the playback loop uses uscpsema(), which tests the semaphore and returns immediately if it can't be acquired. This provides an opportunity where it is known to be safe to pause the output. Without doing this, it is possible to send the pause command without having the playback thread acknowledge it, because it is busy sending data.Another interesting point to note is that error checking is performed to determine whether the application is sending more data than can be handled; if so, the playback thread releases the semaphore and polls the MIDI port until enough data has drained to allow more data to be sent.The song position thread loops until either the stop button is hit or the song finishes. If a sequence is paused, the position resets to 0, so you need to save the starting position, then add it back in when resuming playback.The MIDI file player uses non-blocking I/O so that stopping a sequence is possible without flushing currently queued data. If mdSend() is waiting for either room or time to send data, that data is sent as soon as the currently waiting data is flushed.When setting the division and tempo, the division must always be set first, because the constants set in the driver by the tempo change, depending on the division.When setting the origin time, putting the position of the file in a signed   quantity avoids a compiler warning. Multiplying the start time by -1 allows the file to start playing in the middle without waiting for the first timestamp to expire.When pausing playback, keep track of the timestamp of the MIDI message that was most recently sent and add it to the previous pause time, so that playback will resume from the proper location in the file.LBL="" HELPID=""ID="42349"Synchronizing MIDI I/O with Other MediaYou can synchronize I/O for MIDI streams with other media streams by correlating timestamps in terms of unadjusted system time (UST).One technique is to set the MIDI port to use relative timestamping, and then use mdSetOrigin() to set the port's origin time to match the UST of the media stream to which you want to synchronize.Alternatively, you can obtain the UST for a MIDI event and compare it to the UST of another media stream counter. To return the UST or tick of the last event sent out, call mdTell().LBL="" HELPID=""ID="62110"Hands-On MIDI and Audio Synchronization ExperienceTo try a synchronized audio and MIDI application:If you have not already done so, connect your MIDI equipment to your workstation's serial port, and configure the port for MIDI, as described in IDREF="42578" TYPE="TITLE""Connecting Devices to MIDI I/O Interfaces".Click on APP="/usr/people/4Dgifts/examples/dmedia/midi/syncrecord/recordmidi" PARMS=""syncrecord
 to launch a sample application that demonstrates synchronized audio and MIDI recording.NoteThe application will not launch if you don't have your MIDI equipment connected and set up.See APP="/usr/sbin/jot /usr/people/4Dgifts/examples/dmedia/midi/syncrecord/recordmidi.c++" PARMS=""recordmidi.c++
 and APP="/usr/sbin/jot /usr/people/4Dgifts/examples/dmedia/midi/syncrecord/playmidi.c++" PARMS=""playmidi.c++
, in /usr/people/4Dgifts/examples/dmedia/midi/syncrecord to view the code that implements the synchronized MIDI record and play application.ID="98215"LBL="III"ID="13297"Video ProgrammingIDREF="17607" TYPE="TITLE"Chapter 11, "Video Basics,"explains basic video concepts that apply to both the Video Library and the IndigoVideo Library.IDREF="68158" TYPE="TITLE"Chapter 12, "Getting Started with the Video Library,"describes the Video Library and explains how to use it to perform video input and output for workstations equipped with standard and optional Silicon Graphics video hardware.IDREF="61872" TYPE="TITLE"Chapter 13, "Using VL Controls,"describes how to use VL controls to set video parameters for data transfer and video effects.IDREF="92735" TYPE="TITLE"Chapter 14, "VL Event Handling," describes how to handle video events using the Video Library.LBL="11"ID="17607"Video BasicsComputer graphics and video differ in a number of ways; understanding the differences can help you produce better results with the VL and your Silicon Graphics video option. This chapter introduces some of the important terms and concepts used in conjunction with video. For more detail about a particular term, see the Glossary included in this guide.Video differs from computer graphics in these ways:interlacingbroadcast standardscolor encodingvideo signalstape formatsLBL="" HELPID=""ID="96608"InterlacingUnlike the way the screen is typically drawn for computer graphics, most video signals are ID="Media3-1VB1"interlaced: each time the video screen is refreshed, only half of the horizontal lines are drawn. That is, each frame is composed of two fields.During one screen refresh, the video monitor draws the first field, which contains all the odd-numbered lines; during the next refresh, it draws the second field, which contains all the even-numbered lines. Therefore, two refresh cycles are required to draw one frame. ID="Media3-1VB2"The display rate of interlaced video signals can be measured either in terms of field rate, or refresh rate, or in terms of frame rate, which equals half of the field rate, because each frame contains two fields.ID="Media3-1VB3"IDREF="37367" TYPE="GRAPHIC"Figure 11-1 shows a frame and its two fields for NTSC, the broadcast standard used in North America and some other parts of the world.FILE="Media3-1VB.cgm" POSITION="INLINE" SCALE="FALSE"LBL="11-1"Figure 11-1 ID="37367"Fields and FrameID="Media3-1VB4"In contrast, the Silicon Graphics workstation monitor is typically noninterlaced: it draws every line each time it refreshes the screen. Refresh rates vary, depending on the type of monitor your Silicon Graphics workstation has. The video output capability of the graphics subsystem for some Silicon Graphics workstation models supports interlaced monitor formats, including component RGB at 525 and 625 lines per frame.ID="Media3-1VB5"LBL="" HELPID=""Broadcast StandardsBroadcast standards, or video timing formats, are ways of encoding video information for broadcast to television receivers. These standards are also used to describe the display capabilities of video monitors and are thus also called video timing formats or video output formats (VOFs). The three broadcast standards are:ID="Media3-1VB6"ID="Media3-1VB7"NTSCID="Media3-1VB8"Named after the National Television Systems Committee, which developed it, this standard is used in all of North and South America, except Brazil, and in much of East Asia. PAL ID="Media3-1VB9"(Phase Alternated by Line) This standard is used in western Europe, including the United Kingdom but excluding France, and in East Asia, including Australia. SECAM ID="Media3-1VB10"(Sequentiel Couleur avec Memoire) This standard is used in France, eastern Europe, the Near East and Mideast, and parts of Africa and the Caribbean.NoteNTSC implementations can vary slightly by country; PAL and SECAM implementations can vary considerably. NTSC employs a total of 525 horizontal lines per frame, with two fields per frame of 262.5 lines each. Each field refreshes at 60Hz (actually 59.94Hz). NTSC encodes brightness, color, and synchronizing information in one signal.ID="Media3-1VB11"PAL employs a total of 625 horizontal lines per frame, with two fields per frame of 312.5 lines per frame. Each field refreshes at 50Hz. PAL encodes brightness, color, and synchronizing information in one signal also, but in a different way from NTSC. SECAM transmits the same number of lines at the same rate as PAL, but transmits each color difference signal on alternate lines, using the frequency modulation of the subcarrier.These numbers of horizontal linesname='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'525 and 625, respectivelyname='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'are a shorthand description of what actually happens. For NTSC, the first (odd) field starts with a whole line and ends with a half line; the second (even) field starts with a half line and ends with a whole line. Each NTSC field contains 242.5 active lines and 20 lines of vertical blanking. Similarly, for PAL, the first (even) field starts with a half line and ends with a whole line; the second (odd) field starts with a whole line and ends with a half line. Each PAL field contains 287.5 active lines and 25 lines of vertical blanking.In each case, the numbers 525 and 625 refer to transmitted lines; the active video lines are fewername='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'typically, 485 for NTSC and 575 for PAL. The remaining lines are used for delimiting frame boundaries and for synchronization and other information.To minimize frame flickering and reduce the bandwidth of the video signal, the active video lines are interlaced, as explained earlier in this chapter.NTSC and PAL can be recorded digitally; these recording techniques are referred to as D2 525 (digital NTSC) and D2 625 (digital PAL).ID="Media3-1VB12"ID="Media3-1VB13"ID="Media3-1VB14"ID="Media3-1VB15"LBL="" HELPID=""ID="21423"Color EncodingColor-encoding methods are:ID="Media3-1VB16"RGB (component)YUV (component)YIQ (component)YC (separate luminance (Y) and chrominance (C)), YC-358, YC-443, S-Videocomposite videoLBL="" HELPID=""RGBID="Media3-1VB17"RGB is the color-encoding method used by most graphics computers, as well as some professional-quality video cameras. The three colors red, green, and blue are generated separately; each is carried on a separate wire. LBL="" HELPID=""YUVID="Media3-1VB18"YUV, a form of which is used by the PAL video standard and by Betacam and D1 cameras and VCRs, is also a component color-encoding method, but in a different way from RGB. In this case, brightness, or luminance, is carried on a signal known as Y, and color, or chrominance, is carried on the U and V signals. The two chrominance signals U and V are two-phase amplitude-modulated: the U component modulates the subcarrier at an angle of 0 degrees, but the V component modulates it at 90 degrees or 180 degrees on alternate lines. The color burst is also line-alternated at +135 and -135 degrees relative to the U signal. ID="Media3-1VB19"ID="Media3-1VB20"ID="Media3-1VB21"ID="Media3-1VB22"ID="Media3-1VB23"ID="Media3-1VB24"The YUV matrix multiplier derives colors from RGB via the following formula:ID="Media3-1VB25"Y = .299R + .587 G + .114 BCR = R-YCB = B-Yin which Y represents luminance and R-Y and B-Y represent the color difference signals used by this format. In this system, which is sometimes referred to as Y/R-Y/B-Y, R-Y corresponds to CR and V, and B-Y corresponds to CB and U. R-Y and B-Y are obtained by subtracting luminance (Y) from the red (R) and blue (B) camera signals, respectively. CR, CB, V, and U are derived through different normalization methods, depending on the video format used. The U and V signals are carried on the same signal. ID="Media3-1VB26"YUV component color encoding can be recorded digitally, according to the CCIR 601 standard; this recording technique is referred to as D1.ID="Media3-1VB27"LBL="" HELPID=""YIQID="Media3-1VB28"YIQ color encoding, which is typically used by the NTSC video format, encodes color onto two signals called I and Q (for intermodulation and quadrature, respectively). These two signals have different phase modulation in NTSC transmission. Unlike the U and V components of YUV, I and Q are carried on different bandwidths.ID="Media3-1VB29"The YIQ formula is as follows:ID="Media3-1VB30"Y = .299 R + .587 G + .114 B (the same as for YUV)
I = .596 R - .275 G - .321 B
Q = .212 R - .523 G + .311 BLBL="" HELPID=""YC, YC-358, YC-443, or S-VideoYC, a two-wire signal, results when I and Q are combined into one signal, called chrominance. YC-358 is the most common NTSC version of this luminance/chrominance format; YC-443 is the most common PAL version. These formats are also known as S-Video; S-Video is one of the formats used for S-VHSÔ videotape recorders.ID="Media3-1VB31"ID="Media3-1VB32"ID="Media3-1VB33"ID="Media3-1VB34"ID="Media3-1VB35"ID="Media3-1VB36"LBL="" HELPID=""Composite VideoID="Media3-1VB37"The composite color-encoding schemes combine the brightness and color signals into one signal for broadcast. NTSC and PAL both combine brightness and color but use different methods.ID="Media3-1VB38"ID="Media3-1VB39"IDREF="16942" TYPE="GRAPHIC"Figure 11-2 shows the relationships between color-encoding methods and video formats.ID="Media3-1VB40"FILE="Media3-1VB.cgm2" POSITION="INLINE" SCALE="FALSE"LBL="11-2"Figure 11-2 ID="16942"Relationships Between Color-encoding Methods and Video FormatsID="Media3-1VB41"LBL="" HELPID=""Video SignalsThe video signal, whatever the broadcast standard being used, carries other information besides video (luminance and chrominance) and audio. For example, horizontal and vertical synchronization information is required, as well as a color phase reference, which is called color sync burst. ID="Media3-1VB42"IDREF="18458" TYPE="GRAPHIC"Figure 11-3 shows a composite video signal waveform. FILE="11-3.online.bw" POSITION="INLINE" SCALE="FALSE"LBL="11-3"Figure 11-3 ID="18458"Composite Video WaveformID="Media3-1VB43"LBL="" HELPID=""ID="14650"Videotape FormatsID="Media3-1VB44"Videotape recorders are available for analog and digital recording in various formats. They are further classified by performance level, or market: consumer, professional, and broadcast. In addition, during postproduction (editing, including addition of graphics), the original footage can be transferred to digital media; digital videotape formats are available for composite and component video formats. There are no official standards for videotape classifications. IDREF="26172" TYPE="TABLE"Table 11-1 summarizes the formats.COLUMNS="5"LBL="11-1"Table 11-1 ID="26172"Tape Formats and Video Formats ID="Media3-1VB45"LEFT="0" WIDTH="49"ElectronicsLEFT="55" WIDTH="74"ConsumerID="Media3-1VB46"LEFT="135" WIDTH="73"ProfessionalID="Media3-1VB47"LEFT="215" WIDTH="69" Broadcast LEFT="290" WIDTH="66"PostproductionID="Media3-1VB48"LEFT="0" WIDTH="49"AnalogLEFT="55" WIDTH="74"VHS cassette 
(composite)ID="Media3-1VB49"LEFT="135" WIDTH="73"U-Matic (SP) 
cassette, 3/4-
inch (composite)ID="Media3-1VB50"LEFT="215" WIDTH="69"Type C reel-to-
reel, 1-inch 
(composite)ID="Media3-1VB51"LEFT="290" WIDTH="66"LEFT="0" WIDTH="49"LEFT="55" WIDTH="74"S-VHS (YC, composite)ID="Media3-1VB52"LEFT="135" WIDTH="73"LEFT="215" WIDTH="69"Type B (Europe) 
(composite)ID="Media3-1VB53"LEFT="290" WIDTH="66"LEFT="0" WIDTH="49"LEFT="55" WIDTH="74"S-Video (YC-358)ID="Media3-1VB54"LEFT="135" WIDTH="73"S-Video (YC-358)ID="Media3-1VB55"LEFT="215" WIDTH="69"LEFT="290" WIDTH="66"LEFT="0" WIDTH="49"LEFT="55" WIDTH="74"Beta (composite)8mm(composite)ID="Media3-1VB56"Hi-8mmÔ(YC, composite)ID="Media3-1VB57"LEFT="135" WIDTH="73"Hi-8mm (YC)LEFT="215" WIDTH="69"Betacam(component)ID="Media3-1VB58"Betacam SP(YUV, YIQ,composite)ID="Media3-1VB59"MIIÔ(YUV, YIQ, 
composite)ID="Media3-1VB60"LEFT="290" WIDTH="66"LEFT="0" WIDTH="49"DigitalID="Media3-1VB61"LEFT="55" WIDTH="74"LEFT="135" WIDTH="73"LEFT="215" WIDTH="69"LEFT="290" WIDTH="66"D1 525 (YUV)ID="Media3-1VB62"D1 625 (YUV)ID="Media3-1VB63"D2 525 (NTSC)ID="Media3-1VB64"D2 625 (PAL)ID="Media3-1VB65"Although the VL and other software for Silicon Graphics video options do not distinguish between videotape formats, you need to know what kind of connector your video equipment uses. For example, the Galileo board has composite and S-Video connectors.ID="Media3-1VB66"Most home VCRs use composite connectors. S-Video, on the other hand, carries the color and brightness components of the picture on separate wires; hence, S-Video connectors are also called Y/C connectors. Most S-VHS and Hi-8mm VCRs feature S-Video connectors.LBL="12"ID="68158"Getting Started with the Video LibraryThe Video Library (VL) is a collection of device-independent C language calls for Silicon Graphics® workstations equipped with video options, such as Sirius VideoÔ, IndigoID="Media3-2GS1"ID="Media3-2GS2"2 VideoÔ, Indy VideoÔ, or Galileo Video Ô, or workstations equipped with on-board video (VINOÔ: video in, no out), such as IndyÔ. The VL includes generic video tools, including simple tools for importing and exporting digital data to and from current and future Silicon Graphics products, as well as to and from third-party video devices that adhere to the Silicon Graphics architectural model for video devices. VL calls enable you to perform video teleconferencing on platforms that support it, to blend computer-generated graphics with frames from videotape or any video source, and to present video in a window on the workstation screen and to digitize video data. NoteThe range of VL capabilities you can use depends on the capabilities of your workstation and the video options installed in it.Topics in this chapter include:IDREF="75341" TYPE="TEXT"VL FeaturesIDREF="13279" TYPE="TEXT"VL System Software ArchitectureIDREF="89470" TYPE="TEXT"VL Architectural Model of Video DevicesIDREF="93349" TYPE="TEXT"VL Programming ModelIDREF="28051" TYPE="TEXT"Opening a Connection to the Video Daemon and Setting up a Data PathIDREF="73892" TYPE="TEXT"Setting Parameters for Data Transfer to or from MemoryIDREF="92003" TYPE="TEXT"Displaying Video Data OnscreenIDREF="82124" TYPE="TEXT"Transferring Video Data to and from DevicesIDREF="37265" TYPE="TEXT"Ending Data TransferThe chapter concludes with example code illustrating a simple screen application and frame grabs (video to memory, memory to video, and continuous frame capture).LBL="" HELPID=""ID="75341"VL FeaturesLBL="" HELPID=""ID="87942"How the VL Works with HardwareThe VL includes calls for querying features of all supported Silicon Graphics video options. The VL supports conversion from one video format to another (for example, YUV to RGB or RGB to YUV).In some cases, the VL can support multiple devices of the same or of different types. For example, the CHALLENGEID="Media3-2GS3"Ô architecture supports multiple Sirius Video boards; Indy supports its built-in video (VINO) and the Indy Video option.ID="Media3-2GS4"LBL="" HELPID=""ID="19241"How the VL Works with Other SoftwareThe Video Library works with other Silicon Graphics libraries, such as the OpenGL, the IRIS GL and the IRIS ImageVision Library (IL). Software supplied with optional video hardware provides additional video capabilities through extensions to the VL. For example, Sirius Video software includes controls specific to that hardware.The VL allows programs to get events 60 times per second on a quiescent system; it also enables programs to share resources or to gain exclusive use of resources. It supports input and output of video data to or from locked-down memory at the nominal frame rate. Frame rate depends on the capabilities of the hardware you are using. NoteThe VL does not depend on the X Window System, but you can use X Window System libraries or toolkits to create a windowing interface.LBL="" HELPID=""ID="13279"VL System Software ArchitectureThis section describes features of these VL system components and tools:video daemongeneric video toolslibrary and header filesIDREF="12695" TYPE="GRAPHIC"Figure 12-1 diagrams the interaction between the VL, the video daemon, the kernel, the hardware, and the X Window System server. FILE="Media3-2GS.cgm" POSITION="INLINE" SCALE="FALSE"LBL="12-1"Figure 12-1 ID="12695"VL System Components ID="Media3-2GS5"The VL communicates with the IRIX kernel for device initialization, vertical retrace, setup, and maintenance of any device-supported direct memory access (DMA).Besides these components, the VL includes a collection of applications that support device configuration and control setting and retrieval, generic tools that display video on a workstation, and video control panels.LBL="" HELPID=""Video DaemonThe video daemon, ID="Media3-2GS6"/usr/etc/videod, which has device-dependent and device-independent portions, handles video device management and status information. LBL="" HELPID=""Device ManagementManagement that the video daemon performs includes:ID="Media3-2GS7"multiple device managementSome hardware devices support multiple video products in one system. The video daemon is responsible for establishing and coordinating the availability of all video devices installed.multiple client access to multiple devicesThe library supports connections from multiple client applications and manages their access to a limited number of video devices.ID="Media3-2GS8"dispatching eventsAs events are handled and noted by devices, the daemon notifies applications that have expressed interest in those events.handling eventsAs events are generated by the various devices, the daemon initiates any action required by an event before it hands the event off to interested applications.maintaining exclusive useTypes of data or control usage for video clients in a Video Library application are Done Using, Read-only, Lock, and Shared. These usage levels apply only to write access on controls, not read access. Any application can open and read the control's values at any time.client cleanup on exitWhen a client exits or is terminated abnormally, its connection to the daemon is broken; the daemon performs any cleanup required of the system. Any exclusive-use modes that have been set are cleared; interested clients are notified that the device is no longer in exclusive use. Controls set by the client might persist, but are not guaranteed to remain after the client closes the connection.LBL="" HELPID=""Status InformationStatus information for which the video daemon is responsible includes:system status of video devicesThe video devices installed in a system can be queried as to availability and control status.video positioning (offset) informationcontrol setting and retrievalDevice-independent and device-dependent controls are set and retrieved through the video daemon.LBL="" HELPID=""Generic Video ToolsThe generic video tools include:ID="Media3-2GS9"videopanel (vcp)ID="Media3-2GS10"ID="Media3-2GS11"Use this graphical user interface to set controls, such as hue or contrast, on devices. The panel resizes itself dynamically to reflect available video devices. videoinID="Media3-2GS12" Use the video input window tool to view video in a window. videooutID="Media3-2GS13"Use the video output tool to output video from a rectangular or full-screen area of the screen on hardware that supports the screen-to-video path.vlinfoID="Media3-2GS14"Use the video info tool to display information about video devices available through the VL, such as the name of the server, number of devices on the server, and the types and ID numbers of nodes, sources, and drains on each device.vidtomemID="Media3-2GS15"Use this tool to capture a single frame (the current video input) or a specified number of frames, depending on the hardware limits for burst capture, and write the data to disk. Capture size can also be specified. The data can be translated or left as raw data, which can be used by the memtovid tool.memtovidID="Media3-2GS16"Use this tool to output single frames (images) to video out on hardware that supports the memory-to-video path.The vlinfo, vidtomem, and memtovid tools are command-line tools. In addition to their man pages, these tools have explanations in the Media Control Panels User's Guide, which you can view using the IRIS InSight viewer; similar applications are supplied in source-code form as examples in the 4Dgifts directory (/usr/people/4Dgifts/examples/dmedia/video/vl). NoteAdditional video tools may be available for specific on-board video or video options; see the documentation for those products.LBL="" HELPID=""Library and Header FilesThe client library is /usr/lib/libvl.a. The header files for the VL are in /usr/include/dmedia/vl; the main file is vl.h. This file contains the main definition of the VL API and controls that are common across all hardware.ID="Media3-2GS17"Device-dependent files use the form vl_XXX.h, where XXX is replaced with the device-dependent name. IDREF="38336" TYPE="TABLE"Table 12-1 lists header files for hardware options that use the VL. These files contain additional controls specific to the devices.COLUMNS="2"LBL="12-1"Table 12-1 ID="38336"Header Files for Video Options LEFT="0" WIDTH="205"HardwareLEFT="210" WIDTH="126"Header FileLEFT="0" WIDTH="205"Galileo Video, Indigo2 Video, Indy Video LEFT="210" WIDTH="126"vl_ev1.hLEFT="0" WIDTH="205"Sirius Video LEFT="210" WIDTH="126"vl_sirius.hLEFT="0" WIDTH="205"Video capability built into Indy workstation(VINO: video in, no out)LEFT="210" WIDTH="126"vl_vino.hLBL="" HELPID=""ID="89470"VL Architectural Model of Video DevicesThe two central concepts for VL are:path: an abstraction for a way of moving data aroundID="Media3-2GS18"node: an endpoint of the path, such as a video source (such as a VTR), video drain (such as the screen), a device (such as Indy Video), or the blender in which video sources are combined for output to a drainVL routines explained in this chapter enable you to build a fully connected topology of sources and drains.ID="Media3-2GS19"ID="Media3-2GS20"A path defines the useful connections between video sources and video drains. IDREF="15207" TYPE="GRAPHIC"Figure 12-2 shows a simple path in which a frame from a videotape is displayed in a workstation window. FILE="12-2.online.bw" POSITION="INLINE" SCALE="FALSE"LBL="12-2"Figure 12-2 ID="15207"Simple VL PathID="Media3-2GS21"Some Silicon Graphics platforms are capable of supporting more than one video device; for example, Indy supports VINO and Indy Video. Each video device has its own data paths with sources and drains. The application is responsible for looking at the capabilities of the platform and choosing the video device it will run on.IDREF="38727" TYPE="GRAPHIC"Figure 12-3 shows a more complex path with two video sources: a frame from a videotape and a computer-generated image are blended and output to a workstation window. This path is set up in stages. FILE="12-3.online.bw" POSITION="INLINE" SCALE="FALSE"LBL="12-3"Figure 12-3 ID="38727"Simple VL BlendingID="Media3-2GS22"LBL="" HELPID=""ID="93349"VL Programming ModelThe VL recognizes five classes of objects: devices, each including sets of nodes nodes: sources, drains, and internal nodespaths, connecting sources and drains controls, or parameters that modify how data flows through nodes; for example:video device parameters, such as blanking width, gamma value, horizontal phase, sync source video data capture parameters blending parametersbuffers, for sending and receiving frame data to and from host memory; the VL buffers are implemented as ring buffers containing a number of blocks; each maintains a pointer, a size, and pointers to the head (oldest) and tail (newest) valid dataData transfers fall into two categories: transfers involving memory (video to memory, memory to video), which require setting up a ring buffertransfers not involving memory (such as video to screen and graphics to video), which do not require a ring buffer (such transfers are not supported on VINO)Syntax elements are as follows:VL types and constants begin with uppercase VL; for example, VLServerVL functions begin with lowercase vl; for example, vlOpenVideo()For the two categories of data transfer, based on the VL programming model, the process of creating a VL application consists of these steps:opening a connection to the video daemon (vlOpenVideo()); if necessary, determining which device the application will use (vlGetDevice(), vlGetDeviceList()) specifying nodes on the data path (vlGetNode()) creating the path (vlCreatePath())optional step: adding more connections to a path (vlAddNode()) setting up the hardware for the path (vlSetupPaths())specifying path-related events to be captured (vlSelectEvents())setting input and output parameters (controls) for the nodes on the path (vlSetControl())transfers involving memory: creating a ring buffer to hold data for memory transfers (vlGetTransferSize(), vlCreateBuffer())transfers involving memory: registering the buffer (vlRegisterBuffer())starting the data transfer (vlBeginTransfer())transfers involving memory: getting the data (vlGetNextValid() or vlGetLatestValid(), vlGetActiveRegion(), vlPutFree()) to manipulate frame datacleanup (vlEndTransfer(), vlDeregisterBuffer(), vlDestroyPath(), vlDestroyBuffer(), vlCloseVideo())IDREF="73835" TYPE="TABLE"Table 12-2 lists calls explained in this chapter.COLUMNS="3"LBL="12-2"Table 12-2 ID="73835"Video Library Calls for Data TransferLEFT="0" WIDTH="90"All TransfersLEFT="95" WIDTH="115"Transfers Involving MemoryLEFT="215" WIDTH="126"Setting ControlsLEFT="0" WIDTH="90"vlOpenVideo() vlGetDevice() vlGetDeviceList()vlGetNode() vlCreatePath() vlAddNode()vlRemoveNode()vlSetupPaths() vlSelectEvents() vlBeginTransfer() vlEndTransfer()vlDestroyPath() vlCloseVideo() LEFT="95" WIDTH="115"vlGetTransferSize() vlCreateBuffer()vlRegisterBuffer() vlGetNextValid()vlGetLatestValid()vlPutValid()vlGetNextFree()vlGetActiveRegion()vlPutFree()vlGetDMediaInfo()vlGetImageInfo()vlDeregisterBuffer() vlDestroyBuffer()LEFT="215" WIDTH="126"vlSetControl() vlGetControl() vlControlList()vlGetControlInfo()LBL="" HELPID=""ID="28051"Opening a Connection to the Video Daemon and Setting up a Data PathPreliminary procedures required to create the data path are:opening the devicespecifying nodes on the data pathcreating and setting up the data pathEach procedure is explained separately.LBL="" HELPID=""Opening a Connection to the Video DaemonThe first thing a VL application must do is open the device with vlOpenVideo(). Its function prototype is:VLServer vlOpenVideo(const char *sName) 
ID="Media3-2GS23"where sName is the name of the server to which to connect; set it to a NULL string for the local server. For example:svr = vlOpenVideo("")LBL="" HELPID=""Specifying Nodes on the Data PathUse vlGetNode() to specify nodes; this call returns the node's handle. Its function prototype is:VLNode vlGetNode(VLServer vlServer, int type, int kind, int number)where:VLNodeis a handle for the node, used when setting controls or setting up pathsvlServernames the server (as returned by vlOpenVideo())typespecifies the type of node: VL_SRC: sourceVL_DRN: drainVL_DEVICE: device for device-global controlsNoteIf you are using VL_DEVICE, the kind should be set to 0.VL_INTERNAL: internal node, such as the blend node kindspecifies the kind of node:VL_VIDEO: connection to a video device; for example, a video tape deck or cameraVL_GFX: graphics system (Sirius Video only)VL_MEM: region of workstation memoryVL_SCREEN: workstation screen (Galileo Video, Indigo2 Video, and Indy Video only) VL_TEXTURE: texture RAM (Sirius Video only)VL_BLENDER: a blender node NoteThe use of VL_BLENDER is explained in IDREF="57047" TYPE="TITLE"Chapter 15, "VL Blending," later in this guide.VL_ANY: use any available node numberis the number of the node in cases of two or more identical nodes, such as two video source nodesTo use the default node kind, use VL_ANY. nodehandle = vlGetNode(svr, VL_SRC, VL_VIDEO, VL_ANY);To discover which node the default is, use the control VL_DEFAULT_SOURCE after getting the node handle the normal way. The default video source is maintained by the VL. For example:vlGetControl(svr, path, VL_ANY, VL_DEFAULT_SOURCE, &ctrlval);
nodehandle = vlGetNode(svr, VL_SRC, VL_VIDEO, ctrlval.intVal);In the second line above, the last argument is a struct that gets the value.NoteIf either VINO analog channel is active when the first video application starts, the default is analog; otherwise, the default is digital.LBL="" HELPID=""Creating and Setting Up the Data PathOnce nodes are specified, use VL calls to:determine the device ID (optional step) create the pathget the device IDadd nodes (optional step)set up the data pathspecify the path-related events to be capturedLBL="" HELPID=""Determining the Device IDIn this optional step, use one of the following calls to determine the device on which the data path will be created, depending on the situation.If you do not know which device of several available is appropriate for the data path, get the device list with vlGetDeviceList(). Its function prototype is:int vlGetDeviceList(VLServer vlServer, VLDevList * devlist) If you know the device you want, parse devlist to get its handle. Otherwise, the VL selects the first device that the path you have specified can run on. Use this step for systems with multiple devices of different capabilities; for example, on an Indy workstation with VINO and Indy Video, for full frame rate capture, specify VINO, though both devices support the video input path.The struct for nodeinfo in vl.h is: typedef struct __vlNodeInfo {
 char name[VL_NAME_SIZE]; /* name of node */
 int type; /* see list above */
 int number; /* number of this node */
 int kind; /* see list above */
} VLNodeInfo;The struct for dev in vl.h is: typedef struct __vlDevice {
 VLDev dev;
 char name[VL_NAME_SIZE]; /* name of device */
 uint numNodes; /* number of nodes on this device */
 VLNodeInfo *nodes; /* list of nodes */
} VLDevice;The struct for devlist in vl.h is: typedef struct __vlDevList {
    uint numDevices;            /* number of devices */ 
    VLDevice *devices;          /* list of devices */ 
} VLDevList; LBL="" HELPID=""Creating the PathUse vlCreatePath() to create the data path. Its function prototype is:VLPath vlCreatePath(VLServer vlServer, VLDev vlDev                    VLNode src, VLNode drn) 
ID="Media3-2GS24"This code fragment creates a path if the device is unknown:if ((path = vlCreatePath(svr, VL_ANY, src, drn)) < 0) {
    vlPerror(_progName);
    exit(1);
}This code fragment creates a path that uses a device specified by parsing a devlist: if ((path = vlCreatePath(svr, devlist[devicenum].dev, src,
    drn)) < 0) {
    vlPerror(_progName);
    exit(1);
}NoteIf the path contains one or more invalid nodes, vlCreatePath() returns VLBadNode.LBL="" HELPID=""Getting the Device IDIf you specify VL_ANY as the device when you create the path, use vlGetDevice() to discover the device ID selected. Its function prototype is:VLDev vlGetDevice(VLServer vlServer, VLPath path)For example:devicenum = vlGetDevice(svr, path);
deviceName = devlist.devices[devicenum].name;printf("Device is: %s/n", deviceName);LBL="" HELPID=""Adding a NodeFor this optional step, use vlAddNode(). Its function prototype is:int vlAddNode(VLServer vlServer, VLPath path, VLNodeId node)where:vlServernames the server to which the path is connectedvlPathis the path as defined with vlCreatePath()nodeis the node IDThis example fragment adds a source node and a blend node:vlAddNode(vlSvr, vlPath, src_vid);
vlAddNode(vlSvr, vlPath, blend_node);LBL="" HELPID=""Setting Up the Data PathUse vlSetupPaths() to set up the data path. Its function prototype is:int vlSetupPaths(VLServer vlServer, VLPathList paths,                 u_int count, VLUsageType ctrlusage,                 VLUsageType streamusage) 
ID="Media3-2GS25"where:vlServernames the server to which the path is connectedpathsspecifies a list of paths you are setting upcountspecifies the number of paths in the path listctrlusagespecifies usage for path controls: VL_SHARE: other paths can set controls on this node; this control is the desired setting for other paths, including vcp, to workNoteWhen using VL_SHARE, pay attention to events. If another user has changed a control, a VLControlChanged event occurs.VL_READ_ONLY: controls cannot be set, but can only be read; for example, this control can be used to monitor controlsVL_LOCK: prevents other paths from setting controls on this path; controls cannot be used by another pathVL_DONE_USING: the resources are no longer required; the application releases this set of paths for other applications to acquirestreamusagespecifies usage for the data: VL_SHARE: transfers can be preempted by other users; paths contend for ownership NoteWhen using VL_SHARE, pay attention to events. If another user has taken over the device, a VLStreamPreempted event occurs.VL_READ_ONLY: the path cannot perform transfers, but other resources are not locked; set this value to use the path for controlsVL_LOCK: prevents other paths that share data transfer resources with this path from transferring; existing paths that share resources with this path will be preemptedVL_DONE_USING: the resources are no longer required; the application releases this set of paths for other applications to acquireThis example fragment sets up a path with shared controls and a locked stream:if (vlSetupPaths(svr, (VLPathList)&path, 1, VL_SHARE,
    VL_LOCK) < 0)
{
    vlPerror(_progName);
    exit(1);
}LBL="" HELPID=""Specifying the Path-related Events to Be CapturedUse vlSelectEvents() to specify the events you want to receive. Its function prototype is:int vlSelectEvents(VLServer vlServer, VLPath path, VLEventMask eventmask)
ID="Media3-2GS26"where:vlServernames the server to which the path is connected.pathspecifies the data path.eventmaskspecifies the event mask; IDREF="53784" TYPE="TABLE"Table 12-3 lists the possibilities.IDREF="53784" TYPE="TABLE"Table 12-3 lists and describes the VL event masks.COLUMNS="2"LBL="12-3"Table 12-3 ID="53784"VL Event MasksLEFT="0" WIDTH="133"SymbolLEFT="140" WIDTH="200"MeaningLEFT="0" WIDTH="133"VLStreamBusyMaskLEFT="140" WIDTH="200"Stream is lockedLEFT="0" WIDTH="133"VLStreamPreemptedMaskLEFT="140" WIDTH="200"Stream was grabbed by another applicationLEFT="0" WIDTH="133"VLAdvanceMissedMaskLEFT="140" WIDTH="200"Time was already reachedLEFT="0" WIDTH="133"VLSyncLostMaskLEFT="140" WIDTH="200"Irregular or interrupted signalLEFT="0" WIDTH="133"VLSequenceLostMaskLEFT="140" WIDTH="200"Field or frame droppedLEFT="0" WIDTH="133"VLControlChangedMaskLEFT="140" WIDTH="200"A control has changedLEFT="0" WIDTH="133"VLControlRangeChangedMask LEFT="140" WIDTH="200"A control range has changedLEFT="0" WIDTH="133"VLControlPreemptedMask LEFT="140" WIDTH="200"Control of a node has been preempted, typically 
by another user setting VL_LOCK on a path that 
was previously set with VL_SHARELEFT="0" WIDTH="133"VLControlAvailableMask LEFT="140" WIDTH="200"Access is now availableLEFT="0" WIDTH="133"VLTransferCompleteMaskLEFT="140" WIDTH="200"Transfer of field or frame completeLEFT="0" WIDTH="133"VLTransferFailedMaskLEFT="140" WIDTH="200"Error; transfer terminated; perform cleanup at 
this point, including vlEndTransfer()LEFT="0" WIDTH="133"VLEvenVerticalRetraceMaskLEFT="140" WIDTH="200"Vertical retrace event, even fieldLEFT="0" WIDTH="133"VLOddVerticalRetraceMaskLEFT="140" WIDTH="200"Vertical retrace event, odd fieldLEFT="0" WIDTH="133"VLFrameVerticalRetraceMaskLEFT="140" WIDTH="200"Frame vertical retrace eventLEFT="0" WIDTH="133"VLDeviceEventMaskLEFT="140" WIDTH="200"Device-specific event, such as a trigger on a 
Galileo Video deviceLEFT="0" WIDTH="133"VLDefaultSourceMaskLEFT="140" WIDTH="200"Default source changedFor example:vlSelectEvents(svr, path, VLTransferCompleteMask); Event masks can be ORed together. For example:vlSelectEvents(svr, path, VLTransferCompleteMask | VLTransferFailedMask); LBL="" HELPID=""ID="73892"Setting Parameters for Data Transfer to or from MemoryTransferring data to or from memory requires creating a ring buffer; its size is determined by the size of the frame data you are transferring. To set frame data size and to convert from one video format to another, apply controls to the nodes. The use of source node and drain node controls is explained separately in this section.NoteAll controls are available for all platforms unless otherwise noted. The reference "Galileo Video" includes Indigo2 Video and Indy Video, unless otherwise noted.LBL="" HELPID=""Setting Source Node Controls for Data TransferImportant data transfer controls for source nodes are summarized in IDREF="14007" TYPE="TABLE"Table 12-4. They should be set in the order in which they appear in the table.COLUMNS="3"LBL="12-4"Table 12-4 ID="14007"Data Transfer Controls for Source NodesLEFT="0" WIDTH="88"ControlLEFT="95" WIDTH="116"ValuesLEFT="220" WIDTH="128"Basic UsageLEFT="0" WIDTH="88"VL_MUXSWITCHLEFT="95" WIDTH="116"See IDREF="67789" TYPE="TABLE"Table 12-5LEFT="220" WIDTH="128"Determines physical input for 
pathLEFT="0" WIDTH="88"VL_TIMINGLEFT="95" WIDTH="116"Default: timing produce by active signalVL_TIMING_525_SQ_PIXVL_TIMING_625_SQ_PIX VL_TIMING_525_CCIR601 VL_TIMING_625_CCIR601 VL_TIMING_525_4FSCVL_TIMING_625_4FSCLEFT="220" WIDTH="128"Set or get video timingFor Betacam, MII, composite 
tape formats:Analog: 12.27 MHz, 646 x 486Analog: 14.75 MHz, 768 x 576For D1 tape formats:Digital component:13.50 MHz, 720 x 486Digital component:13.50 MHz, 720 x 576For D2 tape formats:4X NTSC subcarrier:14.32 MHz, 768 x 4864X PAL subcarrier:17.72 MHz, 948 x 576LEFT="0" WIDTH="88"VL_SIZELEFT="95" WIDTH="116"CoordinatesLEFT="220" WIDTH="128"Set or get active unmodified 
video areaLEFT="0" WIDTH="88"VL_SYNC_SOURCELEFT="95" WIDTH="116"Galileo Video:Composite 1: set 0Composite 2: set 1Composite 3: set 2Indigo2 Video and Indy 
Video:Composite 1: set 0Composite 2: set 2LEFT="220" WIDTH="128"Not applicable to VINOThe use of VL_MUXSWITCH and VL_TIMING is explained in further detail in the following sections.LBL="" HELPID=""Using VL_MUXSWITCHUse VL_MUXSWITCH to switch between physical inputs on a single path. IDREF="67789" TYPE="TABLE"Table 12-5 summarizes values for VL_MUXSWITCH, which vary, depending on the platform.COLUMNS="2"LBL="12-5"Table 12-5 ID="67789"VL_MUXSWITCH ValuesLEFT="0" WIDTH="125"PlatformLEFT="130" WIDTH="207"ValuesLEFT="0" WIDTH="125"Galileo VideoLEFT="130" WIDTH="207"S-Video input 1: set 0; input 2: set 1; input 3: set 2Composite input 1: set 3; input 2: set 4; input 3: set 5Y/R-Y/B-Y input 1: set 6; input 2: set 7LEFT="0" WIDTH="125"Indigo2 Video and Indy VideoLEFT="130" WIDTH="207"Y/C (RCA jacks): set 0Y/C (S-Video connector): set 1Composite input 1: set 3; input 2: set 5LEFT="0" WIDTH="125"VINOLEFT="130" WIDTH="207"Node VL_VINO_SRC_DV_IN:VL_VINO_INDYCAMVL_VINO_CCIR601Node VL_VINO_SRC_AV_IN:VL_VINO_COMPOSITEVL_VINO_SVIDEOFor Indy Video, the default source depends on which input is active; that is, which input has equipment that is both plugged in and powered on. In other words, the VL assumes that you want to use the piece of equipment that is plugged in and powered on, without you having to tell it so. If the S-Video input is active, it is the default. If the composite input is active, it is the default. If both S-Video and composite equipment are inactive and the IndyCam is active (plugged in), the IndyCam is the default. Composite becomes the default video in two cases: if it is active or if all other inputs are inactive.You can control the default by unplugging or plugging in equipment and/or turning equipment power on or off. For example:set S-Video active by switching on the equipment plugged into the S-Video inputset composite active by switching off the equipment plugged into the S-Video inputOf course, you can change the settingsname='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'this just gives you an idea of what default to expect. If equipment is plugged in to all the inputs, the default VINO input is established by the precedence listed in top-down order in   IDREF="49276" TYPE="TABLE"Table 12-6.COLUMNS="4"LBL="12-6"Table 12-6 ID="49276"Default Sources for VINO InputsLEFT="0" WIDTH="57"InputLEFT="65" WIDTH="57"S-VideoLEFT="130" WIDTH="57"CompositeLEFT="195" WIDTH="57"IndyCamLEFT="0" WIDTH="57"S-VideoLEFT="65" WIDTH="57"ActiveLEFT="130" WIDTH="57"N/ALEFT="195" WIDTH="57"N/ALEFT="0" WIDTH="57"CompositeLEFT="65" WIDTH="57"InactiveLEFT="130" WIDTH="57"ActiveLEFT="195" WIDTH="57"N/ALEFT="0" WIDTH="57"IndyCamLEFT="65" WIDTH="57"InactiveLEFT="130" WIDTH="57"InactiveLEFT="195" WIDTH="57"ActiveLEFT="0" WIDTH="57"CompositeLEFT="65" WIDTH="57"InactiveLEFT="130" WIDTH="57"InactiveLEFT="195" WIDTH="57"InactiveLBL="" HELPID=""Using VL_TIMINGIDREF="23682" TYPE="TABLE"Table 12-7 summarizes VL_TIMING choices for combinations of nodes and mux switches for VINO.COLUMNS="3"LBL="12-7"Table 12-7 ID="23682"VINO Timing ChoicesLEFT="0" WIDTH="101"Node ValueLEFT="110" WIDTH="104"VL_MUXSWITCH ValueLEFT="220" WIDTH="175"Timing ChoicesLEFT="0" WIDTH="101"VL_VINO_SRC_DV_INLEFT="110" WIDTH="104"VL_VINO_INDYCAMVL_VINO_CCIR601LEFT="220" WIDTH="175"VL_TIMING_525_SQ_PIX (NTSC)VL_TIMING_525_SQ_PIX (NTSC)VL_TIMING_625_SQ_PIX (PAL)VL_TIMING_525_CCIR601(digital NTSC)VL_TIMING_625_CCIR601 (digital PAL)LEFT="0" WIDTH="101"VL_VINO_SRC_AV_INLEFT="110" WIDTH="104"VL_VINO_COMPOSITEVL_VINO_SVIDEOLEFT="220" WIDTH="175"VL_TIMING_525_SQ_PIX (NTSC)VL_TIMING_625_SQ_PIX (PAL)VL_TIMING_525_SQ_PIX (NTSC)VL_TIMING_625_SQ_PIX (PAL)Timing type expresses the timing of video presented to a source or drain. IDREF="55976" TYPE="TABLE"Table 12-8 summarizes dimensions for VL_TIMING. COLUMNS="4"LBL="12-8"Table 12-8 ID="55976"Dimensions for Timing ChoicesLEFT="0" WIDTH="180"TimingLEFT="185" WIDTH="50"Maximum 
WidthLEFT="240" WIDTH="50"Maximum 
HeightLEFT="295" WIDTH="50"First 
Active 
Line 
(Offset=0)LEFT="0" WIDTH="180"VL_TIMING_525_SQ_PIX (12.27 MHz)LEFT="185" WIDTH="50"640LEFT="240" WIDTH="50"480LEFT="295" WIDTH="50"22LEFT="0" WIDTH="180"VL_TIMING_625_SQ_PIX (14.75 MHz)LEFT="185" WIDTH="50"768LEFT="240" WIDTH="50"576LEFT="295" WIDTH="50"32LEFT="0" WIDTH="180"VL_TIMING_525_CCIR601 (13.50 MHz)LEFT="185" WIDTH="50"720LEFT="240" WIDTH="50"480LEFT="295" WIDTH="50"1LEFT="0" WIDTH="180"VL_TIMING_625_CCIR601(13.50 MHz)LEFT="185" WIDTH="50"720LEFT="240" WIDTH="50"576LEFT="295" WIDTH="50"1LEFT="0" WIDTH="180"VL_TIMING_525_SQ_PIX (12.27 MHz)(with input VL_VINO_INDYCAM)LEFT="185" WIDTH="50"640LEFT="240" WIDTH="50"480LEFT="295" WIDTH="50"2LBL="" HELPID=""Setting Drain Node Controls for Data TransferImportant data transfer controls for drain nodes are summarized in IDREF="96908" TYPE="TABLE"Table 12-9. They should be set in the order in which they appear in the table.COLUMNS="5"LBL="12-9"Table 12-9 ID="96908"Data Transfer Controls for Drain NodesLEFT="0" WIDTH="68"ControlLEFT="75" WIDTH="114"Basic UsageLEFT="195" WIDTH="68"Video NodesLEFT="270" WIDTH="156"Memory NodesLEFT="435" WIDTH="96"Screen NodesLEFT="0" WIDTH="68"VL_FORMATLEFT="75" WIDTH="114"Video format on the 
physical connectorLEFT="195" WIDTH="68"See IDREF="48408" TYPE="TITLE""Using 
VL_FORMAT"LEFT="270" WIDTH="156"LEFT="435" WIDTH="96"LEFT="0" WIDTH="68"VL_TIMINGLEFT="75" WIDTH="114"Video timingLEFT="195" WIDTH="68"See IDREF="14007" TYPE="TABLE"Table 12-4 
for valuesLEFT="270" WIDTH="156"Not applicableLEFT="435" WIDTH="96"Not applicableLEFT="0" WIDTH="68"VL_CAP_TYPELEFT="75" WIDTH="114"Setting type of field(s) or 
frame(s) to capture; see 
IDREF="96608" TYPE="TITLE""Interlacing" in Chapter 11LEFT="195" WIDTH="68"Not applicableLEFT="270" WIDTH="156"VL_CAPTURE_NONINTERLEAVEDVL_CAPTURE_INTERLEAVEDVL_CAPTURE_EVEN_FIELDSVL_CAPTURE_ODD_FIELDSLEFT="435" WIDTH="96"Not applicableLEFT="0" WIDTH="68"VL_PACKINGLEFT="75" WIDTH="114"Pixel packing (conversion) 
formatLEFT="195" WIDTH="68"Not applicableLEFT="270" WIDTH="156"Changes pixel format of captured 
data; see IDREF="75350" TYPE="TABLE"Table 12-10 for valuesLEFT="435" WIDTH="96"Not applicableLEFT="0" WIDTH="68"VL_ZOOMLEFT="75" WIDTH="114"Decimation or zoom factor 
(fraction)Galileo Video:1/1, 1/2, 1/3, 1/4, 1/5, 
1/6, 1/7, 1/8, 2/1, 4/1VINO: 1/1, 1/2, 1/3, 1/4, 1/5, 
1/6, 1/7, 1/8LEFT="195" WIDTH="68"Not applicableLEFT="270" WIDTH="156"Decimation or zoom: resizes data to 
remain within limitsLEFT="435" WIDTH="96"Decimation or zoom: 
resizes data to remain 
within limitsLEFT="0" WIDTH="68"VL_SIZELEFT="75" WIDTH="114"Clipping sizeLEFT="195" WIDTH="68"Full size of 
video; read 
onlyLEFT="270" WIDTH="156"Clipped sizeLEFT="435" WIDTH="96"Clipped sizeLEFT="0" WIDTH="68"VL_OFFSETLEFT="75" WIDTH="114"Position within larger areaLEFT="195" WIDTH="68"Position of 
active regionLEFT="270" WIDTH="156"Offset relative to video offsetLEFT="435" WIDTH="96"Pan within the videoLEFT="0" WIDTH="68"VL_ORIGINLEFT="75" WIDTH="114"Position within videoLEFT="195" WIDTH="68"Not applicable LEFT="270" WIDTH="156"Not applicable LEFT="435" WIDTH="96"Screen position of first 
pixel displayed; not 
applicable to VINOLEFT="0" WIDTH="68"VL_WINDOWLEFT="75" WIDTH="114"Setting window ID for 
video in a windowLEFT="195" WIDTH="68"Not applicableLEFT="270" WIDTH="156"Not applicableLEFT="435" WIDTH="96"Window ID; not 
applicable to VINOLEFT="0" WIDTH="68"VL_RATELEFT="75" WIDTH="114"Field or frame transfer 
speedLEFT="195" WIDTH="68"Depends on 
capture type as 
specified by 
VL_CAP_TYPELEFT="270" WIDTH="156"Not applicableLEFT="435" WIDTH="96"Not applicableThese controls are highly interdependent, so the order in which they are set is important. In most cases, the value being set takes precedence over other values that were previously set. For all devices, VL_PACKING must be set first. For VINO, set offset before size. Note that changes in one parameter may change the values of other parameters set earlier; for example, clipped size may change if VL_PACKING is set after VL_SIZE.To determine default values, use vlGetControl() to query the values on the video source or drain node before setting controls. The initial offset of the video node is the first active line of video.Similarly, the initial size value on the video source or drain node is the full size of active video being captured by the hardware, beginning at the default offset. Because some hardware can capture more than the size given by the video node, this value should be treated as a default size.For all these controls, it pays to track return codes. If the value returned is VLValueOutOfRange, the value set will not be what you requested.To specify the controls, use vlSetControl(), for which the function prototype is:int vlSetControl(VLServer vlServer, VLPath vlPath,                 VLNode node, VLControlType type,                 VLControlValue * value) 
ID="Media3-2GS27"The use of VL_FORMAT, VL_PACKING, VL_ZOOM, VL_SIZE, VL_OFFSET, VL_RATE, and VL_CAP_TYPE is explained in more detail in the following sections. LBL="" HELPID=""ID="48408"Using VL_FORMATTo specify video input and output formats of the video signal on the physical connector, use VL_FORMAT. Each video platform has a video format native to it; for example, YUV 4:2:2 is native to Galileo Video and RGB is native to Sirius Video. The native format is always the fastest for that platform. To discover the native format for your video platform, consult the release notes or other documentation for the product.NoteTo convert formats, use VL_PACKING, which is explained in the next section.When VL_FORMAT is applied to a source or drain that is a VL_MEM (memory) node, it selects the format of the video stored in memory. This may imply a software conversion of the video data after the transfer is completed.Values for VL_FORMAT for Galileo Video are:VL_FORMAT_RGB (output only)VL_FORMAT_BETACAM (input and output)VL_FORMAT_SMPTE_YUV (input and output)LBL="" HELPID=""Using VL_PACKINGTo convert a video output format to another in memory, use the control VL_PACKING. Packing type expresses the packing in memory of the video data at the source or drain.Packing types are summarized in IDREF="75350" TYPE="TABLE"Table 12-10, which shows the most significant byte on the left. An X means don't care; this bit is not used. COLUMNS="3"LBL="12-10"Table 12-10 ID="75350"Packing Types and Their Sizes and FormatsLEFT="0" WIDTH="134"TypeLEFT="140" WIDTH="58"SizeLEFT="205" WIDTH="224"FormatMSB ------------------------------------------------------------------LSBLEFT="0" WIDTH="134"VL_PACKING_RGB_332_P LEFT="140" WIDTH="58"8-bit wordLEFT="205" WIDTH="224"BBGGGRRR (four pixels packed into a 32-bit word)LEFT="0" WIDTH="134"VL_PACKING_RGBA_8 LEFT="140" WIDTH="58"32-bit wordLEFT="205" WIDTH="224"AAAAAAAA BBBBBBBB GGGGGGGG RRRRRRRRLEFT="0" WIDTH="134"VL_PACKING_RGB_8 LEFT="140" WIDTH="58"24-bit wordLEFT="205" WIDTH="224"XXXXXXXX BBBBBBBB GGGGGGGG RRRRRRRRLEFT="0" WIDTH="134"VL_PACKING_Y_8_P LEFT="140" WIDTH="58"8-bit wordLEFT="205" WIDTH="224"YYYYYYYY (four pixels packed into a 32-bit word)LEFT="0" WIDTH="134"VL_PACKING_YVYU_422_8 LEFT="140" WIDTH="58"32-bit wordLEFT="205" WIDTH="224"UUUUUUUU YYYYYYYY VVVVVVVV YYYYYYYYNoteThe packing names follow the naming conventions used by the IRIS GL; other libraries such as the OpenGL may use different names.For example:VLControlValue val;

val.intVal = VL_PACKING_RGB;
vlSetControl(svr, path, memdrn, VL_PACKING, &val);LBL="" HELPID=""Using VL_ZOOMVL_ZOOM controls the expansion or decimation of the video image. Values greater than one expand the video; values less than one perform decimation. IDREF="98856" TYPE="GRAPHIC"Figure 12-4 illustrates zooming and decimation.FILE="12-4.online.bw" POSITION="INLINE" SCALE="FALSE"LBL="12-4"Figure 12-4 ID="98856"Zoom and DecimationNoteZooming, that is, VL_ZOOM values greater than one, is not supported on VINO.VL_ZOOM takes a nonzero fraction as its argument; do not use negative values. For example, this fragment captures half-size decimation video to memory: val.fractVal.numerator = 1;
val.fractVal.denominator = 2;
if (vlSetControl(server, memory_path, memory_drain_node, VL_ZOOM, &val)){
 vlPerror("Unable to set zoom");
 exit(1);
}CautionNot all video devices support all aspects of zooming. If you use a control on a video device that does not support it, a VLValueOutOfRange error is returned. Use vlGetControl() to show what your results were. NoteFor a source, zooming takes place before blending; for a drain, blending takes place before zooming.This fragment captures half-size decimation video to memory, with clipping to 320 name='times' font=symbol charset=fontspecific code=180
	TeX='\times ' descr='[times]' 240 (NTSC size minus overscan).val.fractVal.numerator = 1;
val.fractVal.denominator = 2;
if (vlSetControl(server, memory_path, memory_drain_node,
VL_ZOOM, &val))
{
    vlPerror("Unable to set zoom");
    exit(1);
}
val.xyVal.x = 320;
val.xyVal.y = 240;
if (vlSetControl(server, memory_path, memory_drain_node,
VL_SIZE, &val))
{
    vlPerror("Unable to set size");
    exit(1);
}This fragment captures xsizename='times' font=symbol charset=fontspecific code=180
	TeX='\times ' descr='[times]'ysize video with as much decimation as possible, assuming the size is smaller than the video stream.if (vlGetControl(server, memory_path, video_source, VL_SIZE, &val))
{
 vlPerror("Unable to get size");
 exit(1);
}
if (val.xyVal.x/xsize < val.xyVal.y/ysize)
 zoom_denom = (val.xyVal.x + xsize - 1)/xsize;
else
 zoom_denom = (val.xyVal.y + ysize - 1)/ysize;
val.fractVal.numerator = 1;
val.fractVal.denominator = zoom_denom;
if (vlSetControl(server, memory_path, memory_drain_node, VL_ZOOM,
&val))
{
 /* allow this error to fall through */
 vlPerror("Unable to set zoom");
}
val.xyVal.x = xsize;
val.xyVal.y = ysize;
if (vlSetControl(server, memory_path, memory_drain_node, VL_SIZE,
&val))
{
 vlPerror("Unable to set size");
 exit(1);
}LBL="" HELPID=""Using VL_SIZEVL_SIZE controls how much of the image sent to the drain is used, that is, how much clipping takes place. This control operates on the zoomed image; for example, when the image is zoomed to half size, the limits on the size control change by a factor of 2. IDREF="67549" TYPE="GRAPHIC"Figure 12-5 illustrates clipping.FILE="12-5.online.bw" POSITION="INLINE" SCALE="FALSE"LBL="12-5"Figure 12-5 ID="67549"Clipping an ImageFor example, to display PAL video in a 320 name='times' font=symbol charset=fontspecific code=180
	TeX='\times ' descr='[times]' 240 space, clip the image to that size, as shown in the following fragment:VLControlValue value;value.xyval.x=320;
value.xyval.y=240;
vlSetControl(svr, path, drn, VL_SIZE, &value); NoteBecause this control is device-dependent and interacts with other controls, always check the error returns. For example, if offset is set before size and an error is returned, set size before offset.LBL="" HELPID=""Using VL_OFFSETVL_OFFSET puts the upper left corner of the video data at a specific position; it sets the beginning position for the clipping performed by VL_SIZE. The values you enter are relative to the origin. VL_OFFSET operates on the unzoomed image; it does not change if the zoom factor is changed. This example places the data ten pixels down and ten pixels in from the left:VLControlValue value;value.xyval.x=10; 
value.xyval.y=10; 
vlSetControl(svr, path, drn, VL_OFFSET, &value); To capture the blanking region, set offset to a negative value.LBL="" HELPID=""Using VL_RATE and VL_CAP_TYPEVL_RATE determines the data transfer rate by field or frame, depending on the capture type as specified by VL_CAP_TYPE, as shown in IDREF="18417" TYPE="TABLE"Table 12-11.COLUMNS="2"LBL="12-11"Table 12-11 ID="18417"VL_RATE Values (Items per Second)LEFT="0" WIDTH="170"VL_CAP_TYPE ValueLEFT="175" WIDTH="162"VL_RATE ValueLEFT="0" WIDTH="170"VL_CAPTURE_NONINTERLEAVED 
onlyLEFT="175" WIDTH="162"NTSC: 10, 12, 20, 24, 30, 36, 40, 48, 50, 60PAL: 5, 10, 15, 20, 25LEFT="0" WIDTH="170"VL_CAPTURE_INTERLEAVED, VL_CAPTURE_EVEN_FIELDS, andVL_CAPTURE_ODD_FIELDSLEFT="175" WIDTH="162"NTSC: 5, 6, 10, 12, 15, 18, 20, 24, 25, 30PAL: 10, 20, 30, 40, 50IDREF="25404" TYPE="GRAPHIC"Figure 12-6 shows the relationships between the source and drain zoom, size, offset, and origin.FILE="Media3-2GS.cgm6" POSITION="INLINE" SCALE="FALSE"LBL="12-6"Figure 12-6 ID="25404"Zoom, Size, Offset, and OriginLBL="" HELPID=""ID="92003"Displaying Video Data OnscreenTo set up a window for live video on Galileo Video, Indigo2 Video, or Indy Video, follow these steps, as outlined in the example program simplev2s.c.NoteThis information does not apply to VINO. Because the video resident in the Indy workstation has no screen node, use the memory node. Capture the video and use the lrectwrite() function or the analogous X or OpenGL function.Open an X display window; for example:if (!(dpy = XOpenDisplay("")))    exit(1);Connect to the video daemon; for example: if (!(svr = vlOpenVideo("")))      exit(1);Create a window to show the video; for example:vwin = XCreateSimpleWindow(dpy, RootWindow(dpy, 0), 10,
                      10, 640, 480, 0,
                      BlackPixel(dpy,DefaultScreen(dpy)),
                      BlackPixel(dpy, DefaultScreen(dpy));
XMapWindow(dpy, vwin);
XFlush(dpy);Create a source node on a video device and a drain node on the screen; for example:src = vlGetNode(svr, VL_SRC, VL_VIDEO, VL_ANY);drn = vlGetNode(svr, VL_DRN, VL_SCREEN, VL_ANY);Create a path on the first device that supports it; for example:if((path = vlCreatePath(svr, VL_ANY, src, drn)) < 0)    exit(1);Set up the hardware for the path and define the path usage; for example:vlSetupPaths(svr, (VLPathList)&path, 1, VL_SHARE,              VL_SHARE); Set the X window to be the drain; for example:val.intVal = vwin;vlSetControl(svr, path, drn, VL_WINDOW, &val);Get X and VL into the same coordinate system; for example:XTranslateCoordinates(dpy, vwin, RootWindow(dpy, DefaultScreen(dpy)), 0, 0,&x, &y, &dummyWin);Set the live video to the same location and size as the window; for example:val.xyVal.x = x;val.xyVal.y = y;vlSetControl(svr, path, drn, VL_ORIGIN, &val);XGetGeometry(dpy, vwin, &dummyWin, &x, &y, &w, &h, &bw, &d);val.xyVal.x = w;val.xyVal.y = h;vlSetControl(svr, path, drn, VL_SIZE, &val);Begin the data transfer:vlBeginTransfer(svr, path, 0, NULL);Wait until the user finishes; for example: printf("Press return to exit.\n");c = getc(stdin);End the data transfer, clean up, and exit:vlEndTransfer(svr, path);vlDestroyPath(svr, path);vlCloseVideo(svr);LBL="" HELPID=""ID="82124"Transferring Video Data to and from DevicesThe processes for data transfer are:creating a buffer for the frames (transfers involving memory)registering the ring buffer with the path (transfers involving memory)starting data transferreading data from the buffer (transfers involving memory)Each process is explained separately.LBL="" HELPID=""Creating a Buffer for the FramesOnce you have specified frame parameters in a transfer involving memory (or have determined to use the defaults), create a buffer for the frames. Like other libraries in the IRIS digital media development environment, the VL uses ring buffers. Ring buffers provide a way to read and write varying sizes of frames of data. A frame of data consists of the actual frame data and an information structure describing the underlying data, including device-specific information. When a ring buffer is created, constraints are specified that control the total size of the data segment and the number of information buffers to allocate.A head and a tail flag are automatically set in a ring buffer so that the latest frame can be accessed. A sector is locked down if it is not called; that is, it remains locked until it is read. When the ring buffer is written to and all sectors are occupied, data transfer stops. The sector last written to remains locked down until it is released.The ring buffer can accommodate data of varying size. You can specify a ring buffer at a fixed size or can determine the size of the data in the buffer.To determine frame data size, use vlGetTransferSize(). Its function prototype is:long vlGetTransferSize(VLServer svr, VLPath path)For example:transfersize = vlGetTransferSize(svr, path); where transfersize is the size of the data in bytes.To create a ring buffer for the frame data, use vlCreateBuffer(). Its function prototype is: VLBuffer vlCreateBuffer(VLServer vlServer, VLPath path,                        VLNode node, int numFrames)where:VLBufferis the handle of the buffer to be createdvlServernames the server to which the path is connectedpathspecifies the data pathnodespecifies the memory node containing data to transfer to or from the ring buffernumFramesspecifies the number of frames in the bufferFor example:buf = vlCreateBuffer(svr, path, src, 1); LBL="" HELPID=""Registering the Ring BufferUse vlRegisterBuffer() to register the ring buffer with the data path. Its function prototype is:int vlRegisterBuffer(VLServer vlServer, VLPath path,                     VLNode memnodeid, VLBuffer buffer)where:vlServernames the server to which the path is connectedpathspecifies the data pathmemnodeidspecifies the memory node IDbufferspecifies the ring buffer handleFor example:vlRegisterBuffer(svr, path, drn, Buffer);LBL="" HELPID=""Starting Data TransferTo begin data transfer, use vlBeginTransfer(). Its function prototype is:int vlBeginTransfer(VLServer vlServer, VLPath path,                   int count, VLTransferDescriptor* xferDesc) 
ID="Media3-2GS28"where:vlServernames the server to which the path is connectedpathspecifies the data pathcountspecifies the number of transfer descriptorsTailor the data transfer by means of transfer descriptors. This example fragment transfers the entire contents of the buffer immediately. xferDesc.mode = VL_TRANSFER_MODE_DISCRETE;xferDesc.count = imageCount;
xferDesc.delay = 0;
xferDesc.trigger = VLTriggerImmediate;The transfer descriptors are:xferDesc.modeTransfer method:VL_TRANSFER_MODE_DISCRETE: a specified number of frames are transferred (burst mode)VL_TRANSFER_MODE_CONTINUOUS (default): frames are transferred continuously, beginning immediately or after a trigger event occurs (such as a frame coincidence pulse), and continues until transfer is terminated with vlEndTransfer()VL_TRANSFER_MODE_AUTOTRIGGER: frame transfer takes place each time a trigger event occurs; this mode is a repeating version of VL_TRANSFER_MODE_DISCRETExferDesc.countNumber of frames to transfer; if mode is VL_TRANSFER_MODE_CONTINUOUS, this value is ignoredxferDesc.delayNumber of frames from the trigger at which data transfer beginsxferDesc.triggerSet of events to trigger on; an event mask. This transfer descriptor is always required. VLTriggerImmediate specifies that transfer begins immediately, with no pause for a trigger eventThis fragment shows the default descriptor, which is the same as passing in a null for the descriptor pointer. Transfer begins immediately; count is ignored.xferDesc.mode = VL_TRANSFER_MODE_CONTINUOUS;xferDesc.count = 0;
xferDesc.delay = 0;
xferDesc.trigger = VLTriggerImmediate;LBL="" HELPID=""Reading Data from the BufferIf your application uses a buffer, use various VL calls for reading frames, getting pointers to active buffers, freeing buffers, and other operations. IDREF="53312" TYPE="TABLE"Table 12-12 lists the buffer-related calls.COLUMNS="2"LBL="12-12"Table 12-12 ID="53312"Buffer-Related CallsLEFT="0" WIDTH="83"CallLEFT="90" WIDTH="249"PurposeLEFT="0" WIDTH="83"vlGetNextValid()LEFT="90" WIDTH="249"Returns a handle on the next valid frame of dataLEFT="0" WIDTH="83"vlGetLatestValid() LEFT="90" WIDTH="249"Reads only the most current frame in the buffer, discarding the 
rest LEFT="0" WIDTH="83"vlPutValid() LEFT="90" WIDTH="249"Puts a frame into the valid list (memory to video)LEFT="0" WIDTH="83"vlPutFree()LEFT="90" WIDTH="249"Puts a valid frame back into the free list (video to memory) LEFT="0" WIDTH="83"vlGetNextFree()LEFT="90" WIDTH="249"Gets a free buffer into which to write data (memory to video) LEFT="0" WIDTH="83"vlBufferDone()LEFT="90" WIDTH="249"Informs you if the buffer has been vacatedLEFT="0" WIDTH="83"vlBufferReset()LEFT="90" WIDTH="249"Resets the buffer so that it can be used againIDREF="44670" TYPE="GRAPHIC"Figure 12-7 illustrates the difference between vlGetNextValid() and vlGetLatestValid(), and their interaction with vlPutFree().FILE="12-7.online.bw" POSITION="INLINE" SCALE="FALSE"LBL="12-7"Figure 12-7 ID="44670"vlGetNextValid(), vlGetLatestValid(), and vlPutFree()IDREF="86210" TYPE="TABLE"Table 12-13 lists the calls that extract information from a buffer.COLUMNS="2"LBL="12-13"Table 12-13 ID="86210"Calls for Extracting Data from a BufferLEFT="0" WIDTH="90"CallLEFT="95" WIDTH="241"PurposeLEFT="0" WIDTH="90"vlGetActiveRegion()LEFT="95" WIDTH="241"Gets a pointer to the data region of the buffer (video to 
memory); called after vlGetNextValid() and 
vlGetLatestValid()LEFT="0" WIDTH="90"vlGetDMediaInfo()LEFT="95" WIDTH="241"Gets a pointer to the DMediaInfo structure associated with 
a frame; this structure contains timestamp and field count 
informationLEFT="0" WIDTH="90"vlGetImageInfo()LEFT="95" WIDTH="241"Gets a pointer to the DMImageInfo structure associated 
with a frame; this structure contains image size informationCautionNone of these calls has count or block arguments; appropriate calls in the application must deal with a NULL return in cases of no data being returned.In summary, for video-to-memory transfer use: buffer = vlCreateBuffer(svr, path, memnode1);vlRegisterBuffer(svr, path, memnode1, buffer); vlBeginTransfer(svr, path, 0, NULL); info = vlGetNextValid(svr, buffer);/* OR vlGetLatestValid(svr, buffer); */dataptr = vlGetActiveRegion(svr, buffer, info); /* use data for application */name='hellip' font=symbol charset=fontspecific code=188vlPutFree(svr, buffer); For memory-to-video transfer, use:buffer = vlCreateBuffer(svr, path, memnode1);vlRegisterBuffer(svr, path, memnode1, buffer); vlBeginTransfer(svr, path, 0, NULL); buffer = vlGetNextFree(svr, buffer, bufsize); /* fill buffer with data */name='hellip' font=symbol charset=fontspecific code=188vlPutValid(svr, buffer); These calls are explained in separate sections.LBL="" HELPID=""Reading the Frames to Memory from the Buffer Use vlGetNextValid() to read all the frames in the buffer or get a valid frame of data. Its function prototype is:VLInfoPtr vlGetNextValid(VLServer vlServer, VLBuffer vlBuffer)Use vlGetLatestValid() to read only the most current frame in the buffer, discarding the rest. Its function prototype is:VLInfoPtr vlGetLatestValid(VLServer vlServer, VLBuffer vlBuffer)After removing interesting data, return the buffer for use with vlPutFree() (video to memory). Its function prototype is:int vlPutFree(VLServer vlServer, VLBuffer vlBuffer)LBL="" HELPID=""Sending Frames from Memory to VideoUse vlGetNextFree() to get a free buffer to which to write data. Its function prototype is: VLInfoPtr vlGetNextFree(VLServer vlServer,                        VLBuffer vlBuffer, int size)After filling the buffer with the data you want to send to video output, use vlPutValid() to put a frame into the valid list for output to video (memory to video). Its function prototype is:int vlPutValid(VLServer vlServer, VLBuffer vlBuffer)CautionThese calls do not have count or block arguments; appropriate calls in the application must deal with a NULL return in cases of no data being returned.LBL="" HELPID=""Getting DMediaInfo and Image Data from the BufferUse vlGetActiveRegion() to get a pointer to the active buffer. Its function prototype is:void * vlGetActiveRegion(VLServer vlServer,                         VLBuffer vlBuffer, VLInfoPtr ptr)Use vlGetDMediaInfo() to get a pointer to the DMediaInfo structure associated with a frame. This structure contains timestamp and field count information. The function prototype for this call is:DMediaInfo * vlGetDMediaInfo(VLServer vlServer,                             VLBuffer vlBuffer, VLInfoPtr ptr)Use vlGetImageInfo() to get a pointer to the DMImageInfo structure associated with a frame. This structure contains image size information. The function prototype for this call is:DMImageInfo * vlGetImageInfo(VLServer vlServer,                             VLBuffer vlBuffer, VLInfoPtr ptr)LBL="" HELPID=""ID="37265"Ending Data TransferTo end data transfer, use vlEndTransfer(). Its function prototype is:int vlEndTransfer(VLServer vlServer, VLPath path)
ID="Media3-2GS29"To accomplish the necessary cleanup to exit gracefully, use:for transfer involving memory: vlDeregisterBuffer(), vlDestroyPath(), vlDestroyBuffer()for all transfers: vlCloseVideo()The function prototype for vlDeregisterBuffer() is:int vlDeregisterBuffer(VLServer vlServer, VLPath path,                    VLNode memnodeid, VLBuffer ringbufhandle) where:vlServeris the server handlepathis the path handlememnodeidis the memory node IDringbufhandleis the ring buffer handleThe function prototypes for vlDestroyPath(), vlDestroyBuffer() and vlCloseVideo() are, respectively:int vlDestroyPath(VLServer vlServer, VLPath path)
int vlDestroyBuffer(VLServer vlServer, VLBuffer vlBuffer)
int vlCloseVideo(VLServer vlServer)This example ends a data transfer that used a buffer: vlEndTransfer(svr, path);vlDeregisterBuffer(svr, path, memnodeid, buffer);vlDestroyPath(svr, path);vlDestroyBuffer(svr, buffer);vlCloseVideo(svr);LBL="" HELPID=""VL ExamplesThe example code in this section illustrates:a simple screen applicationa video-to-memory frame grab a memory-to-video frame output a continuous frame capture Source code for these programs is in /usr/people/4Dgifts/examples/dmedia/video/vl.NoteTo simplify the code, these examples do not check returns. The programmer should, however, always check returns.LBL="" HELPID=""Simple Screen ApplicationIDREF="62142" TYPE="TEXT"Example 12-1 shows how to send live video to the screen (for systems that have a video output port).LBL="12-1"Example 12-1 ID="62142"Sending Live Video to the Screen: simplev2s.c/*
 * File:          simplev2s.c
 *
 * Usage:         simplev2s 
 *
 * Description:   Simplev2s demonstrates live video to screen.
 *                This application only runs on video hardware 
 *                that has a video output port. It will not run 
 *                on a VINO video board. 
 *
 * Functions:     SGI Video Library functions used
 *
 *                vlOpenVideo()
 *                vlGetNode()
 *                vlCreatePath()
 *                vlSetupPaths()
 *                vlSetControl()
 *                vlBeginTransfer()
 *                vlEndTransfer()
 *                vlDestroyPath()
 *                vlCloseVideo()
 */
#include <stdlib.h>
#include <stdio.h>
#include <strings.h>
#include <X11/X.h>
#include <X11/Xlib.h>
#include <Xm/MwmUtil.h>
#include <X11/Xutil.h>
#include <vl/vl.h>
main(int argc, char **argv)
{
    VLServer svr;
    VLPath path;
    Display *dpy;
    Window vwin;
    VLNode src, drn;
    VLControlValue val;
    char *progname, *ptr;
    int x, y, c;
    uint w, h, bw, d;
    Window dummyWin;
    XSizeHints size_hints;
    XClassHint class_hints;

    /* get basename of argv */
    if ((ptr = strrchr(*argv, '/')) != NULL) progname = ++ptr;
    else progname = *argv;

    /* Open an X display */
    if (!(dpy = XOpenDisplay("")))
        exit(1);
        
    /* Connect to the video daemon */
    if (!(svr = vlOpenVideo("")))
        exit(1);
        
    /* Create a window to show the video */
    vwin = XCreateSimpleWindow(dpy, DefaultRootWindow(dpy),
                               10, 10, 640, 480, 0,
                               BlackPixel(dpy, DefaultScreen(dpy)),
                               BlackPixel(dpy, DefaultScreen(dpy)));

    /* Ignore window manager placement set the window to 10, 10 */
    size_hints.flags = USPosition;
    size_hints.x = 10;
    size_hints.y = 10;

    /* set class properties for 4Dwm desktop */
    class_hints.res_name = progname;
    class_hints.res_class = progname;
    XSetClassHint(dpy, vwin, &class_hints);
    XSetWMNormalHints(dpy, vwin, &size_hints);
    XMapWindow(dpy, vwin);
    XFlush(dpy);
    /* Create a source node on a video device */
    src = vlGetNode(svr, VL_SRC, VL_VIDEO, VL_ANY);
    
    /* Create a drain node on the screen */
    drn = vlGetNode(svr, VL_DRN, VL_SCREEN, VL_ANY);
    
    /* Create a path on the first device that supports it */
    if((path = vlCreatePath(svr, VL_ANY, src, drn)) < 0)
        exit(1);
    
     /* Set up the hardware for and define the usage of the path */
    vlSetupPaths(svr, (VLPathList)&path, 1, VL_SHARE, VL_SHARE);
    
    /* Set the X window to be the drain */
    val.intVal = vwin;
    vlSetControl(svr, path, drn, VL_WINDOW, &val); 
    
    /* Get X and VL into the same coordinate system */   
    XTranslateCoordinates(dpy, vwin, DefaultRootWindow(dpy),
                          0, 0,&x, &y, &dummyWin);
                          
    /* Set the live video to the same location and size as the X window */   
    val.xyVal.x = x;
    val.xyVal.y = y;
    vlSetControl(svr, path, drn, VL_ORIGIN, &val); 
    
    XGetGeometry(dpy, vwin, &dummyWin, &x, &y, &w, &h, &bw, &d);
    val.xyVal.x = w;
    val.xyVal.y = h;
    vlSetControl(svr, path, drn, VL_SIZE, &val);
    
    /* Begin the data transfer */
    vlBeginTransfer(svr, path, 0, NULL);        

    /* Wait until the user presses a key */
    printf("Press return to exit.\n");
    c = getc(stdin);   
    
    /* End the data transfer */
    vlEndTransfer(svr, path); 
    
    /* Clean up and exit */   
    vlDestroyPath(svr, path);
    vlCloseVideo(svr);
}LBL="" HELPID=""Video-to-memory Frame GrabIDREF="53324" TYPE="TEXT"Example 12-2 demonstrates video frame grabbing.LBL="12-2"Example 12-2 ID="53324"Video Frame Grabbing: simplegrab.c/*
 * File:          simplegrab.c
 * Usage:         simplegrab 
 * Description:   simplegrab grabs a video frame to memory and screen 
 * Functions:     IRIS Video Library functions used
 *
 *                vlOpenVideo()
 *                vlGetNode()
 *                vlCreatePath()
 *                vlSetupPaths()
 *                vlSetControl()
 *                vlCreateBuffer()
 *                vlRegisterBuffer()
 *                vlGetActiveRegion()
 *                vlGetNextValid()
 *                vlPutFree()
 *                vlBeginTransfer()
 *                vlEndTransfer()
 *                vlDeregisterBuffer()
 *                vlDestroyPath()
 *                vlDestroyBuffer()
 *                vlCloseVideo()
 *                vlPerror()
 */
#include <stdlib.h>
#include <stdio.h>
#include <gl/gl.h>
#include <dmedia/vl.h>

char *_progName;

/* Report errors */
void
error_exit(void)
{
    vlPerror(_progName);
    exit(1);
}
void
main(int argc, char **argv)
{
    VLServer svr;
    VLPath path;
    VLNode src, drn;
    VLControlValue val;
    VLBuffer buffer;
    VLInfoPtr info;
    char *dataPtr;
    int c;
    int xsize;
    int ysize;
    long win;
    
    _progName = argv[0];
    
    foreground();
        
    /* Connect to the daemon */
    if (!(svr = vlOpenVideo(""))) 
        error_exit();

    /* Set up a drain node in memory */
    drn = vlGetNode(svr, VL_DRN, VL_MEM, VL_ANY);
    
    /* Set up a source node on any video source  */
    src = vlGetNode(svr, VL_SRC, VL_VIDEO, VL_ANY);

    /* Create a path using the first device that will support it */
    path = vlCreatePath(svr, VL_ANY, src, drn); 

    /* Set up the hardware for and define the usage of the path */
    if ((vlSetupPaths(svr, (VLPathList)&path, 1,        VL_SHARE, VL_SHARE)) < 0)
        error_exit();

    /* Set the packing to RGB */
    val.intVal = VL_PACKING_RGB_8;
    vlSetControl(svr, path, drn, VL_PACKING, &val);
    
    /* Get the video size */
    vlGetControl(svr, path, drn, VL_SIZE, &val);
    xsize = val.xyVal.x;
    ysize = val.xyVal.y;
    /* Set up and open a GL window to display the data */
    prefsize(xsize,ysize);
    win = winopen("Simplegrab Window");
    RGBmode();
    pixmode(PM_TTOB, 1);
    gconfig();
    
    /* Create and register a buffer for 1 frame */
    buffer = vlCreateBuffer(svr, path, drn, 1);
    if (buffer == NULL)
        error_exit();   
    vlRegisterBuffer(svr, path, drn, buffer);
    
    /* Begin the data transfer */
    if (vlBeginTransfer(svr, path, 0, NULL))
        error_exit();

    /* Wait for a frame */
    do {
        info = vlGetNextValid(svr, buffer);
    } while (!info);

    /* Get a pointer to the frame */
    dataPtr = vlGetActiveRegion(svr, buffer, info);
            
    /* Write the data to the screen */
    lrectwrite(0,0, xsize-1, ysize-1, (ulong *)dataPtr);

    /* Finished with frame, unlock the buffer */
    vlPutFree(svr, buffer);

    /* End the data transfer */
    vlEndTransfer(svr, path);
     
    /* Wait until the user presses a key */
    printf("Press <Enter> to exit: ");
    c = getc(stdin);
            
    /* Cleanup before exiting */
    vlDeregisterBuffer(svr, path, drn, buffer);
    vlDestroyBuffer(svr, buffer);
    vlDestroyPath(svr, path);
    vlCloseVideo(svr);
}LBL="" HELPID=""Memory-to-video Frame OutputIDREF="11237" TYPE="TEXT"Example 12-3 sends a frame to the video output (for systems that have a video output port).LBL="12-3"Example 12-3 ID="11237"Frame Output: simplem2v.c /*
 * Files:         simplem2v.c
 *
 * Usage:         simplem2v <filename>
 *
 * Description:   Simplem2v sends a frame of image data from memory 
 *                to the video output. Image data must be in YUV422
 *                format. Images in this format may be generated by
 *                running the vidtomem application with the -r option
 *                on an Indy Video board. 
 *                Simplem2v only runs on video hardware that has a 
 *                video output port. It will not run on a VINO video 
 *                board.
 *                
 *
 * Functions:     SGI Video Library functions used
 *
 *                vlOpenVideo()
 *                vlGetNode()
 *                vlCreatePath()
 *                vlSetupPaths()
 *                vlRegisterBuffer()
 *                vlCreateBuffer()
 *                vlGetTransferSize()
 *                vlGetNextFree()
 *                vlGetActiveRegion()
 *                vlBufferDone()
 *                vlBeginTransfer()
 *                vlEndTransfer()
 *                vlDeregisterBuffer()
 *                vlDestroyBuffer()
 *                vlDestroyPath()
 *                vlCloseVideo()
 *                vlGetErrno()
 *                vlPerror()
 *                vlStrError()
 */
#include <stdio.h>
#include <stdlib.h>
#include <unistd.h>
#include <sys/types.h>
#include <sys/stat.h>
#include <fcntl.h>
#include <sys/errno.h>
#include <dmedia/vl.h>

#define MIN(x,y) ((x>y)?y:x)

extern int errno;
main(int argc, char **argv)
{
    VLServer svr;
    VLPath MEMtoVIDPath;
    VLNode src, drn;
    VLBuffer buf;
    VLInfoPtr info;
    struct stat status_buffer;
    char *dataPtr;
    ulong transferSize;
    int ret, fd;
    int c;
    char *_progName;
    char *fileName;

    _progName = argv[0];
    
    if (argc != 2)
    {
        fprintf(stderr,"%s <filename>\n", _progName);
        exit(1);
    }
    
    fileName = argv[1];

    /* Connect to the daemon */
    if (!(svr = vlOpenVideo(""))) 
    {
        fprintf(stderr,"%s: can't open video: %s\n", _progName,                vlStrError(vlGetErrno()));
        exit(1);
    }
    /* Set up a source node in memory */
    src = vlGetNode(svr, VL_SRC, VL_MEM, VL_ANY);
     /* Set up a video drain node on the first device that has one */
    drn = vlGetNode(svr, VL_DRN, VL_VIDEO, VL_ANY); 
    
    /* Create a path using the selected devices */
    MEMtoVIDPath = vlCreatePath(svr, VL_ANY, src, drn);

    /* Set up the hardware for and define the usage of the path */
    if (vlSetupPaths(svr, (VLPathList)&MEMtoVIDPath, 1, VL_SHARE, VL_SHARE)<0)
    {
        fprintf(stderr,"%s: can't setup path: %s\n", _progName,                vlStrError(vlGetErrno()));
        exit(1);
    }

    /* Find out what size this path supports */
    transferSize = vlGetTransferSize(svr,MEMtoVIDPath);

    /* Create a ring buffer for the data transfers */
    buf = vlCreateBuffer(svr, MEMtoVIDPath, src, 1);

    /* Associate the ring buffer with the path */
    vlRegisterBuffer(svr, MEMtoVIDPath, src, buf);
   
    /* Get the next free frame in the buffer, reserve it for data */
    do
    {
        info = vlGetNextFree(svr, buf, transferSize);
    } while (!info && !vlBufferDone(buf)); 

    /* Get a pointer to where the data will go */
    dataPtr = vlGetActiveRegion(svr, buf, info);

    /*  Open raw YUV data file */
    fd = open(fileName, O_RDONLY);
    if (!fd)
    {
        fprintf(stderr,"%s: cannot open file %s.\n", _progName, fileName);
        exit(1);
    }
    /* Get the file's size (image size of this data)*/
    if (fstat(fd,&status_buffer) == -1) 
    {
        perror(fileName);
        exit(1);
    }
    /* Make sure the hardware supports this image size */
    if (status_buffer.st_size != transferSize) 
    {
        fprintf(stderr,"%s: The image is not the right size for this device\n",
           _progName);
        exit(1);
    }
    
    /* Read in the data */
    ret = read(fd, dataPtr, transferSize);
    close(fd);

    /* Check the size of the data read in */
    if (ret != transferSize) 
    {
        fprintf(stderr, "%s: Unable to read the image data\n", _progName);
        exit(1);
    }

    /* Put the data into the ring buffer  */
    vlPutValid(svr, buf);

    /* Begin the data transfer */
    vlBeginTransfer(svr, MEMtoVIDPath, 0, NULL);
    
    /* Wait until user presses a key */
    printf("Hit return to exit.\n");
    c = getc(stdin);
    
    /* End the data transfer */
    vlEndTransfer(svr, MEMtoVIDPath);
    
    /* Clean up and exit */
    vlDeregisterBuffer(svr, MEMtoVIDPath, src, buf);
    vlDestroyPath(svr, MEMtoVIDPath);
    vlDestroyBuffer(svr, buf);
    vlCloseVideo(svr);
}LBL="" HELPID=""Continuous Frame CaptureIDREF="99035" TYPE="TEXT"Example 12-4 demonstrates continuous frame capture.LBL="12-4"Example 12-4 ID="99035"Continuous Frame Capture: simplecapt.c /*==================A Simple Continuous Capture Application==========
 *
 *
 * File:          simpleccapt.c
 *
 * Usage:         simpleccapt 
 *
 * Description:   simpleccapt captures a stream of video to memory 
 *
 * Functions:     IRIS Video Library functions used
 *
 *                vlOpenVideo()
 *                vlGetNode()
 *                vlCreatePath()
 *                vlSetupPaths()
 *                vlSetControl()
 *                vlCreateBuffer()
 *                vlRegisterBuffer()
 *                vlGetActiveRegion()
 *                vlGetNextValid()
 *                vlPutFree()
 *                vlBeginTransfer()
 *                vlEndTransfer()
 *                vlDeregisterBuffer()
 *                vlDestroyPath()
 *                vlDestroyBuffer()
 *                vlCloseVideo()
 *                vlPerror()
 */
#include <stdlib.h>
#include <stdio.h>
#include <unistd.h>
#include <gl/gl.h>
#include <dmedia/vl.h>

char *_progName;
/* Report errors */
void
error_exit(void)
{
    vlPerror(_progName);
    exit(1);
}

void
main(int argc, char **argv)
{
    VLServer svr;
    VLPath path;
    VLNode src, drn;
    VLControlValue val;
    VLBuffer buffer;
    VLInfoPtr info;
    char *dataPtr;
    int c;
    int xsize;
    int ysize;
    long win;
    
    _progName = argv[0];
    
    foreground();
    /* Connect to the daemon */
    if (!(svr = vlOpenVideo(""))) 
        error_exit();

    /* Set up a drain node in memory */
    drn = vlGetNode(svr, VL_DRN, VL_MEM, VL_ANY);
    
    /* Set up a source node on any video source  */
    src = vlGetNode(svr, VL_SRC, VL_VIDEO, VL_ANY);

    /* Create a path using the first device that will support it */
    path = vlCreatePath(svr, VL_ANY, src, drn); 

    /* Set up the hardware for and define the usage of the path */
    if ((vlSetupPaths(svr, (VLPathList)&path, 1, VL_SHARE, VL_SHARE)) < 0)
        error_exit();
    /* Set the packing to RGB */
    val.intVal = VL_PACKING_RGB_8;
    vlSetControl(svr, path, drn, VL_PACKING, &val);
    
    /* Get the video size */
    vlGetControl(svr, path, drn, VL_SIZE, &val);
    xsize = val.xyVal.x;
    ysize = val.xyVal.y;
    
    /* Set up and open a GL window to display the data */
    prefsize(xsize,ysize);
    win = winopen("Simpleccapt Window");
    RGBmode();
    pixmode(PM_TTOB, 1);
    gconfig();
    
    /* Create and register a buffer for 1 frame */
    buffer = vlCreateBuffer(svr, path, drn, 1);
    if (buffer == NULL)
        error_exit();   
    vlRegisterBuffer(svr, path, drn, buffer);
    
    /* Begin the data transfer */
    if (vlBeginTransfer(svr, path, 0, NULL))
        error_exit();
    
    printf("Type <control-c> to exit.\n");
    
    for(;;) {
        do {
            sginap(1);          /* wait a tick */
            info = vlGetNextValid(svr, buffer);
        } while (!info);
    
        /* Get a pointer to the frame */
        dataPtr = vlGetActiveRegion(svr, buffer, info);
                
        /* Write the data to the screen */
        lrectwrite(0,0, xsize-1, ysize-1, (ulong *)dataPtr);
    
        /* Finished with frame, unlock the buffer */
        vlPutFree(svr, buffer);
    }
    /* End the data transfer */
    vlEndTransfer(svr, path);
     
    /* Cleanup before exiting */
    vlDeregisterBuffer(svr, path, drn, buffer);
    vlDestroyBuffer(svr, buffer);
    vlDestroyPath(svr, path);
    vlCloseVideo(svr);
}LBL="13"ID="61872"Using VL ControlsVL controls enable you to:specify data transfer parameters, such as the frame rate or countspecify the capture region and decimation, or output windowspecify video format and timingadjust signal parameters, such as hue, brightness, vertical sync, horizontal syncspecify sync source Topics in this chapter include:IDREF="13848" TYPE="TEXT"VL Control Type and ValuesIDREF="77682" TYPE="TEXT"VL Control Fraction RangesIDREF="24371" TYPE="TEXT"VL Control ClassesIDREF="99880" TYPE="TEXT"VL Control GroupingsIDREF="80341" TYPE="TEXT"Galileo Video ControlsIDREF="26544" TYPE="TEXT"VINO ControlsDevice-independent controls are documented in /usr/include/dmedia/vl.h. Device-dependent controls are documented in the respective header files for the devices: dmedia/vl_vino.h (VINO), dmedia/vl_ev1.h (Galileo Video), and dmedia/vl_sv1.h (Sirius Video).IDREF="78099" TYPE="TABLE"Table 13-1 lists device-independent VL controls alphabetically, along with their values or ranges. See the Sirius Video Owner's and Programming Guide for Sirius Video controls.NoteAll controls are available for all platforms unless otherwise noted. The reference "Galileo Video" includes Indigo2 Video and Indy Video, unless otherwise noted.COLUMNS="5"LBL="13-1"Table 13-1 ID="78099"Device-Independent VL ControlsLEFT="0" WIDTH="138"Control LEFT="145" WIDTH="103"SetsLEFT="255" WIDTH="93"VINOLEFT="355" WIDTH="61"Indigo2 Video 
and Indy 
VideoLEFT="425" WIDTH="106"GalileoVideoLEFT="0" WIDTH="138"VL_BLEND_ALEFT="145" WIDTH="103"Input source for foreground 
(channel A) imageLEFT="255" WIDTH="93"N/ALEFT="355" WIDTH="61"VLNode type derived from vlGetNode(); 
must be one of the source nodesLEFT="425" WIDTH="106"LEFT="0" WIDTH="138"VL_BLEND_BLEFT="145" WIDTH="103"Input source for 
background (channel B) 
imageLEFT="255" WIDTH="93"N/ALEFT="355" WIDTH="61"VLNode type derived from vlGetNode(); 
must be one of the source nodesLEFT="425" WIDTH="106"LEFT="0" WIDTH="138"VL_BLEND_A_ALPHALEFT="145" WIDTH="103"Input source for foreground 
(channel A) alphaLEFT="255" WIDTH="93"N/ALEFT="355" WIDTH="61"N/ALEFT="425" WIDTH="106"N/ALEFT="0" WIDTH="138"VL_BLEND_B_ALPHALEFT="145" WIDTH="103"Input source for 
background (channel B) 
alphaLEFT="255" WIDTH="93"VLNode type derived from vlGetNode(); must be one of the source 
nodesLEFT="355" WIDTH="61"LEFT="425" WIDTH="106"LEFT="0" WIDTH="138"VL_BLEND_A_FCNLEFT="145" WIDTH="103"Blend function that controls 
mixing of foreground 
(channel A) signalsLEFT="255" WIDTH="93"N/ALEFT="355" WIDTH="61"VL_BLDFCN_ZEROVL_BLDFCN_ONEVL_BLDFCN_A_ALPHA:(foreground alpha)/255VL_BLDFCN_MINUS_A_ALPHA:1 - ((foreground alpha) / 255)LEFT="425" WIDTH="106"LEFT="0" WIDTH="138"VL_BLEND_B_FCNLEFT="145" WIDTH="103"Blend function that controls 
mixing of background 
(channel B) signalsLEFT="255" WIDTH="93"N/ALEFT="355" WIDTH="61"VL_BLDFCN_ZEROVL_BLDFCN_ONEVL_BLDFCN_B_ALPHA:(background alpha)/255VL_BLDFCN_MINUS_B_ALPHA:1 - ((background alpha) / 255)LEFT="425" WIDTH="106"LEFT="0" WIDTH="138"VL_BLEND_A_NORMALIZELEFT="145" WIDTH="103"Follows Porter-Duff model 
(background (channel A) 
pixels premultiplied by 
their corresponding alphas 
before blending)LEFT="255" WIDTH="93"N/ALEFT="355" WIDTH="61"N/ALEFT="425" WIDTH="106"N/ALEFT="0" WIDTH="138"VL_BLEND_B_NORMALIZELEFT="145" WIDTH="103"Premultiplies foreground 
(channel B) by alphaLEFT="255" WIDTH="93"N/ALEFT="355" WIDTH="61"0 = off1 = onLEFT="425" WIDTH="106"0 = off1 = onLEFT="0" WIDTH="138"VL_BLEND_OUT_NORMALIZELEFT="145" WIDTH="103"Scaled output from blenderLEFT="255" WIDTH="93"N/ALEFT="355" WIDTH="61"N/ALEFT="425" WIDTH="106"N/ALEFT="0" WIDTH="138"VL_BRIGHTNESSLEFT="145" WIDTH="103"BrightnessLEFT="255" WIDTH="93"N/ALEFT="355" WIDTH="61"N/ALEFT="425" WIDTH="106"N/ALEFT="0" WIDTH="138"VL_CAP_TYPE LEFT="145" WIDTH="103"Type of frame(s) or field(s) 
to capture; see IDREF="96608" TYPE="TITLE""Interlacing" 
in Chapter 11LEFT="255" WIDTH="93"VL_CAPTURE_NONINTERLEAVEDVL_CAPTURE_INTERLEAVEDVL_CAPTURE_EVEN_FIELDSVL_CAPTURE_ODD_FIELDSLEFT="355" WIDTH="61"LEFT="425" WIDTH="106"LEFT="0" WIDTH="138"VL_CONTRASTLEFT="145" WIDTH="103"LEFT="255" WIDTH="93"N/ALEFT="355" WIDTH="61"N/ALEFT="425" WIDTH="106"N/ALEFT="0" WIDTH="138"VL_DEFAULT_SOURCELEFT="145" WIDTH="103"Default source for the video 
pathLEFT="255" WIDTH="93"VL_VINO_SRC_DV_INVL_VINO_SRC_AV_INIf either VINO analog 
channel is active when 
the first video 
application starts, the 
default is analog; 
otherwise, the default is 
digital (corresponding to 
the IndyCam).LEFT="355" WIDTH="61"LEFT="425" WIDTH="106"Analog: set 0Digital 1: set 1Digital 2: set 2Indigo2 Video, Indy Video:Analog only: set 0LEFT="0" WIDTH="138"VL_FORMAT LEFT="145" WIDTH="103"Video formatLEFT="255" WIDTH="93"N/ALEFT="355" WIDTH="61"LEFT="425" WIDTH="106"VL_FORMAT_RGB(output only)VL_FORMAT_BETACAM 
(input and output)VL_FORMAT_SMPTE_YUV
 (input and output)LEFT="0" WIDTH="138"VL_FREEZE LEFT="145" WIDTH="103"Data transfer freeze; 
suspends transfer at the 
memory drain node, with 
no picture regeneration LEFT="255" WIDTH="93"0 = off1 = on LEFT="355" WIDTH="61"0 = off1 = on LEFT="425" WIDTH="106"0 = off1 = on LEFT="0" WIDTH="138"VL_H_PHASE LEFT="145" WIDTH="103"Horizontal phaseLEFT="255" WIDTH="93"N/ALEFT="355" WIDTH="61"LEFT="425" WIDTH="106"IntegerVL_EV1_H_PHASELEFT="0" WIDTH="138"VL_HUE LEFT="145" WIDTH="103"Hue; the control panel vcp 
does numerator and 
denominator calculationsLEFT="255" WIDTH="93"(-180,178 19/32) in steps of 1 13/32 degreesLEFT="355" WIDTH="61"LEFT="425" WIDTH="106"LEFT="0" WIDTH="138"VL_MUXSWITCH LEFT="145" WIDTH="103"Switch between inputs on a 
single path, corresponding 
to the physical connector to 
the optionLEFT="255" WIDTH="93"Analog node 
VL_VINO_SRC_AV_IVL_VINO_COMPOSITEVL_VINO_SVIDEODigital node 
VL_VINO_SRC_DV_IN:VL_VINO_INDYCAMVL_VINO_CCIR601LEFT="355" WIDTH="61"Y/C (RCA 
jacks)set 0Y/C (S-Video 
connector):set 1Composite 
input 1: set 3; 
input 2: set 5LEFT="425" WIDTH="106"S-Videoinput 1: set 0input 2: set 1input 3: set 2Compositeinput 1: set 3input 2: set 4;input 3: set 5Y/R­Y/B­Yinput 1: set 6input 2: set 7LEFT="0" WIDTH="138"VL_OFFSET LEFT="145" WIDTH="103"On VL_VIDEO nodes, the 
offset to the active region of 
the video; on all other 
nodes, the offset within the 
videoBecause the default is 0,0, 
use negative values to get 
blanking dataLEFT="255" WIDTH="93"Coordinates; default is 0,0LEFT="355" WIDTH="61"LEFT="425" WIDTH="106"LEFT="0" WIDTH="138"VL_ORIGIN LEFT="145" WIDTH="103"Upper left corner of image 
in drain (usually a 
window); the offset within 
the node;LEFT="255" WIDTH="93"N/ALEFT="355" WIDTH="61"Coordinates; default is 0,0LEFT="425" WIDTH="106"LEFT="0" WIDTH="138"VL_PACKING LEFT="145" WIDTH="103"Packing of video data at 
source or drainLEFT="255" WIDTH="93"VL_PACKING_ABGR_8VL_PACKING_AUYV_8VL_PACKING_AYU_AYV_10VL_PACKING_A_2_BGR_10VL_PACKING_A_2_UYV_10VL_PACKING_BGR_332VL_PACKING_BGR_332_IPVL_PACKING_MAXVL_PACKING_RBG_323VL_PACKING_RGBA_8VL_PACKING_RGB_10VL_PACKING_RGB_332VL_PACKING_RGB_332_IPVL_PACKING_RGB_332_P (VINO default)VL_PACKING_RGB_565VL_PACKING_RGB_565_IPVL_PACKING_RGB_565_PVL_PACKING_RGB_8VL_PACKING_RGB_8_PVL_PACKING_VUY_411_SVVL_PACKING_YUVA_4444_10VL_PACKING_YUVA_4444_8VL_PACKING_YUV_444_10VL_PACKING_YUV_444_8VL_PACKING_YVYU_422_8VL_PACKING_Y_8_IPVL_PACKING_Y_8_PLEFT="355" WIDTH="61"LEFT="425" WIDTH="106"LEFT="0" WIDTH="138"VL_RATE LEFT="145" WIDTH="103"Transfer rate in fields or 
framesLEFT="255" WIDTH="93"With a VL_CAP_TYPE of VL_CAPTURE_NONINTERLEAVED only:NTSC: 5, 6, 10, 12, 15, 18, 20, 24, 25, 30PAL: 5, 10, 15, 20, 25With a VL_CAP_TYPE of VL_CAPTURE_INTERLEAVED, VL_CAPTURE_EVEN_FIELDS, orVL_CAPTURE_ODD_FIELDS:NTSC: 10, 12, 20, 24, 30, 36, 40, 48, 50, 60PAL: 10, 20, 30, 40, 50LEFT="355" WIDTH="61"LEFT="425" WIDTH="106"LEFT="0" WIDTH="138"VL_SIGNALLEFT="145" WIDTH="103"LEFT="255" WIDTH="93"N/ALEFT="355" WIDTH="61"N/ALEFT="425" WIDTH="106"N/ALEFT="0" WIDTH="138"VL_SIZE LEFT="145" WIDTH="103"On VL_VIDEO nodes, the 
size of the video; on all 
other nodes, the clipped 
size of the videoLEFT="255" WIDTH="93"Coordinates; default depends on signalLEFT="355" WIDTH="61"LEFT="425" WIDTH="106"LEFT="0" WIDTH="138"VL_SYNC LEFT="145" WIDTH="103"Sync modeLEFT="255" WIDTH="93"N/ALEFT="355" WIDTH="61"VL_SYNC_INTERNALVL_SYNC_GENLOCKLEFT="425" WIDTH="106"LEFT="0" WIDTH="138"VL_SYNCLEFT="145" WIDTH="103"Slave sync modeLEFT="255" WIDTH="93"N/ALEFT="355" WIDTH="61"N/ALEFT="425" WIDTH="106"VL_EV1_SYNC_SLAVELEFT="0" WIDTH="138"VL_SYNC_SOURCELEFT="145" WIDTH="103"Sets sync source for analog 
breakout boxLEFT="255" WIDTH="93"N/Aonly one sync inputLEFT="355" WIDTH="61"Composite 1: 
set 0Composite 2: 
set 2LEFT="425" WIDTH="106"Composite 1: set 0Composite 2: set 1Composite 3: set 2LEFT="0" WIDTH="138"VL_TIMING LEFT="145" WIDTH="103"Video timingLEFT="255" WIDTH="93"Default: timing produced by active signalFor Betacam, MII, composite tape formats:Analog: 12.27 MHz, 646 x 486 (NTSC): VL_TIMING_525_SQ_PIXAnalog: 14.75 MHz, 768 x 576 (PAL): VL_TIMING_625_SQ_PIX (VINO 
default)For D1 tape formats:Digital component: 13.50 MHz, 720 x 486: VL_TIMING_525_CCIR601 Digital component: 13.50 MHz, 720 x 576: VL_TIMING_625_CCIR601 For D2 tape formats:4X NTSC subcarrier, 14.32 MHz, 768 x 486: VL_TIMING_525_4FSC4X PAL subcarrier, 17.72 MHz, 948 x 576: VL_TIMING_625_4FSCLEFT="355" WIDTH="61"LEFT="425" WIDTH="106"LEFT="0" WIDTH="138"VL_V_PHASE LEFT="145" WIDTH="103"Vertical phaseLEFT="255" WIDTH="93"N/ALEFT="355" WIDTH="61"IntegerLEFT="425" WIDTH="106"IntegerLEFT="0" WIDTH="138"VL_WINDOW LEFT="145" WIDTH="103"Window ID for video in a 
window (screen node only)LEFT="255" WIDTH="93"N/ALEFT="355" WIDTH="61"IntegerLEFT="425" WIDTH="106"IntegerLEFT="0" WIDTH="138"VL_ZOOMLEFT="145" WIDTH="103"Zoom factor for video 
stream; fractions greater 
than 1 expand the picture, 
fractions less than one 
reduce the pictureLEFT="255" WIDTH="93"1/1, 1/2, 1/3, 1/4, 1/5, 
1/6, 1/7, 1/8 LEFT="355" WIDTH="61"4/1, 2/1, 1/1, 
1/2, 1/3, 1/4, 
1/5, 1/6, 1/7, 
1/8LEFT="425" WIDTH="106"4/1, 2/1, 1/1, 1/2, 1/3, 1/4, 
1/5, 1/6, 1/7, 1/8NoteFor information on controls for keying, blending, or wipes, see IDREF="57047" TYPE="TITLE"Chapter 15, "VL Blending."For detailed information on using VL_CAP_TYPE, VL_FORMAT, VL_MUXSWITCH, VL_OFFSET, VL_PACKING, VL_RATE, VL_SIZE, VL_TIMING, and VL_ZOOM, see IDREF="73892" TYPE="TITLE""Setting Parameters for Data Transfer to or from Memory" in Chapter 12.LBL="" HELPID=""ID="13848"VL Control Type and ValuesThe type of VL controls is:typedef long VLControlType;Common types used by the VL to express the values returned by the controls are:typedef struct __vlControlInfo {
 char name[VL_NAME_SIZE]; /* name of control */
 VLControlType type; /* e.g. WINDOW, HUE */
 VLControlClass ctlClass; /* SLIDER, DETENT, KNOB, BUTTON */
 VLControlGroup group; /* BLEND, VISUAL QUALITY, SYNC */
 VLNode node; /* associated node */
 VLControlValueType valueType; /* what kind of data */
 int valueCount; /* how many data items */
 uint numFractRanges; /* number of ranges */
 VLFractionRange *ranges; /* range of values of control */

 uint numItems; /* number of enumerated items */
 VLControlItem *itemList; /* the actual enumerations */
} VLControlInfo;To store the value of different controls, libvl.a uses the struct:typedef union {
 VLFraction fractVal;
 VLBoolean boolVal;
 int intVal;
 VLXY xyVal;
 uint pad[24];
} VLControlValue;

typedef struct {
 int x, y;
} VLXY;

typedef struct {
 int numerator;
 int denominator;
} VLFraction;The control info structure is returned by a vlGetControlInfo() call, and it contains many of the items discussed above.VLControlInfo.number is the number of the VLControlInfo.node that the info pertains to. There may be several controls of the same type on a particular node, but usually there is just one.VLControlInfo.numFractRanges is the number of fraction ranges for a particular control. The names correspond 1-to-1 with the rangeNames, up to the number of range names, numRangeNames. That is, there may be fewer names than ranges, but never more.LBL="" HELPID=""ID="77682"VL Control Fraction RangesThe VL uses fraction ranges to represent the values possible for a control. A VLFractionRange generated by the VL is guaranteed never to generate a fraction with a zero denominator, or a fractional numerator or denominator. For a VLProgressionType of VL_LINEAR, numerator.increment and denominator.increment are guaranteed to be greater than zero, and the limit is always guaranteed to be {numerator,denominator}.base, plus some integral multiple of {numerator,denominator}.increment. The type definition for fraction types in the header file is: typedef struct {
    VLRange numerator;
    VLRange denominator;
} VLFractionRange;LBL="" HELPID=""ID="24371"VL Control ClassesThe VL defines control classes for user-interface developers. The classes are hints only; they are the VL developer's idea of how the control is commonly represented in the real world.#define VL_CLASS_NO_UI            0
#define VL_CLASS_SLIDER           1
#define VL_CLASS_KNOB             2
#define VL_CLASS_BUTTON           3
#define VL_CLASS_TOGGLE           4
#define VL_CLASS_DETENT_KNOB      5
#define VL_CLASS_LIST             6In the list above, VL_CLASS_NO_UI is often used for controls that have no user-interface metaphor and are not displayed in the video control panel or saved in the defaults file. The VL controls can be read-only, write-only, or both. The VL includes these macros:#define VL_CLASS_RDONLY      0x8000    /* control is read-only */
#define VL_CLASS_WRONLY      0x4000    /* control is write-only */

#define VL_IS_CTL_RDONLY(x) ((x)->class & VL_CLASS_RDONLY)
#define VL_IS_CTL_WRONLY(x) ((x)->class & VL_CLASS_WRONLY)
#define VL_IS_CTL_RW(x)      (!(VL_IS_CTL_RDONLY(x) &&
                                VL_IS_CTL_WRONLY(x)))to test these conditions:#define VL_CLASS_MASK        0xfff

typedef unsigned long VLControlClass; /* from list above */
LBL="" HELPID=""ID="99880"VL Control GroupingsLike control class, control grouping is an aid for the user-interface developer. The groupings are the VL developer's idea of how the controls would be grouped in the real world. These groupings are implemented in the video control panel vcp.The type definition for groupings is: typedef char NameString[80];
#define VL_CTL_GROUP_PATH     9    /* Path Controls */The maximum length of a control or range name is VL_NAME_SIZE. IDREF="60145" TYPE="TABLE"Table 13-2 summarizes the VL control groupings.COLUMNS="2"LBL="13-2"Table 13-2  ID="60145"VL Control GroupingsLEFT="0" WIDTH="209"GroupingLEFT="215" WIDTH="294"Includes controls for...LEFT="0" WIDTH="209"VL_CTL_GROUP_BLENDINGLEFT="215" WIDTH="294"Blending; for example, VL_BLEND_BLEFT="0" WIDTH="209"VL_CTL_GROUP_VISUALQUALITYLEFT="215" WIDTH="294"Visual quality of sources or drains; for example, VL_H_PHASE or 
VL_V_PHASELEFT="0" WIDTH="209"VL_CTL_GROUP_SIGNALLEFT="215" WIDTH="294"Signal of sources or drains; for example, VL_MUXSWITCH or VL_HUELEFT="0" WIDTH="209"VL_CTL_GROUP_CODINGLEFT="215" WIDTH="294"Encoding or decoding sources or drains; for example, VL_TIMING or 
VL_FORMATLEFT="0" WIDTH="209"VL_CTL_GROUP_SYNCLEFT="215" WIDTH="294"Synchronizing video sources or drains; for example, VL_SYNCLEFT="0" WIDTH="209"VL_CTL_GROUP_ORIENTATIONLEFT="215" WIDTH="294"Orientation or placement of video signals; for example, VL_ORIGIN LEFT="0" WIDTH="209"VL_CTL_GROUP_SIZINGLEFT="215" WIDTH="294"Setting the size of the video signal; for example, VL_SIZELEFT="0" WIDTH="209"VL_CTL_GROUP_RATESLEFT="215" WIDTH="294"Setting the rate of the video signal; for example, VL_RATELEFT="0" WIDTH="209"VL_CTL_GROUP_WSLEFT="215" WIDTH="294"Specifying the windowing system of the workstation; for example, 
VL_WINDOWLEFT="0" WIDTH="209"VL_CTL_GROUP_PATHLEFT="215" WIDTH="294"Specifying the data path through the system; these controls, often marked 
with the VL_CLASS_NO_UI, are often internal to the VL, with no direct 
access for the userLEFT="0" WIDTH="209"VL_CTL_GROUP_SIGNAL_ALLLEFT="215" WIDTH="294"Specifying properties of all signalsLEFT="0" WIDTH="209"VL_CTL_GROUP_SIGNAL_COMPOSITELEFT="215" WIDTH="294"Specifying properties of composite signals LEFT="0" WIDTH="209"VL_CTL_GROUP_SIGNAL_COMPONENTLEFT="215" WIDTH="294"Specifying properties of component signals LEFT="0" WIDTH="209"VL_CTL_GROUP_SIGNAL_CLUT_COMPOSITELEFT="215" WIDTH="294"Specifying properties of composite color lookup table (CLUT) controlsLEFT="0" WIDTH="209"VL_CTL_GROUP_SIGNAL_CLUT_COMPONENTLEFT="215" WIDTH="294"Specifying properties of component CLUT controlsLEFT="0" WIDTH="209"VL_CTL_GROUP_KEYINGLEFT="215" WIDTH="294"Specifying properties of chroma or luma keying controls, such as 
VL_KEYER_FG_OPACITYLEFT="0" WIDTH="209"VL_CTL_GROUP_PROLEFT="215" WIDTH="294"Specifying values not commonly found on the front panel of a real-world 
video device; for example, a wipe controlLEFT="0" WIDTH="209"VL_CTL_GROUP_MASKLEFT="215" WIDTH="294"Masking optional bits to extract only the control groupLBL="" HELPID=""ID="80341"Galileo Video ControlsVL controls that are used only for Galileo Video and VINO on the Indy workstation fall into several categories:general controls for Galileo Video and VINOGalileo Video encoder and color-space conversion controlsGalileo 601 Video digital breakout box controlsVINO analog input controlsIndyCam controlsNoteGalileo Video keying controls are documented in IDREF="57047" TYPE="TITLE"Chapter 15, "VL Blending."Each category is explained separately in this section.LBL="" HELPID=""General Controls for Galileo VideoThe Galileo Video controls are summarized in IDREF="41362" TYPE="TABLE"Table 13-3 in alphabetical order. COLUMNS="2"LBL="13-3"Table 13-3 ID="41362"Galileo Video vcp Controls LEFT="0" WIDTH="162"Galileo VideoLEFT="170" WIDTH="341"PurposeLEFT="0" WIDTH="162"VL_EV1_AGC_CONTROLLEFT="170" WIDTH="341"Sets automatic gain control speed for chrominance for composite or Y/CLEFT="0" WIDTH="162"VL_EV1_ALPHA_NOT_PIXELValue: LEFT="170" WIDTH="341"Determines whether information is derived from the alpha out or the pixel out 
channelLEFT="0" WIDTH="162"VL_EV1_ANTI_DITHERValue: (0,1) where 0 = off, 1 = onLEFT="170" WIDTH="341"Removes interference between frequency components generated by dithered 
graphics images (Y/C and composite out only) and chrominance frequency present 
in video signals by using a notch filter in luminanceLEFT="0" WIDTH="162"VL_EV1_APERTURELEFT="170" WIDTH="341"Sets aperture factors for luminance for composite and Y/C inputsLEFT="0" WIDTH="162"VL_EV1_BANDPASSLEFT="170" WIDTH="341"Selects bandpass filters for luminance for composite and Y/C inputsLEFT="0" WIDTH="162"VL_EV1_BLANK_LINERange: (0,63)LEFT="170" WIDTH="341"Sets first unblanked line on all analog video outputsLEFT="0" WIDTH="162"VL_EV1_BLEND_B_FLATLEFT="170" WIDTH="341"Sets a flat backgroud colorLEFT="0" WIDTH="162"VL_EV1_BLEND_B_ULEFT="170" WIDTH="341"Sets the U value of a flat background colorLEFT="0" WIDTH="162"VL_EV1_BLEND_B_VLEFT="170" WIDTH="341"Sets the V value of a flat background colorLEFT="0" WIDTH="162"VL_EV1_BLEND_B_YLEFT="170" WIDTH="341"Sets the Y value of a flat background colorLEFT="0" WIDTH="162"VL_EV1_BLEND_H_FILTLEFT="170" WIDTH="341"Controls blendingLEFT="0" WIDTH="162"VL_EV1_BLEND_SHADOW_GAINLEFT="170" WIDTH="341"Controls blendingLEFT="0" WIDTH="162"VL_EV1_BLEND_SHADOW_OFFSETLEFT="170" WIDTH="341"Controls blendingLEFT="0" WIDTH="162"VL_EV1_BLEND_SHADOW_ONLEFT="170" WIDTH="341"Controls blendingLEFT="0" WIDTH="162"VL_EV1_BOTTOM_FLUTTERLEFT="170" WIDTH="341"For CCIR 601 (13.5 MHz) sampling only, compensates for horizontal phase jump on 
the selected line numberLEFT="0" WIDTH="162"VL_EV1_C_GAINRange: (0,255)LEFT="170" WIDTH="341"Adjusts burst and chrominance output level of composite and Y/C simultaneouslyLEFT="0" WIDTH="162"VL_EV1_CHROMA_BANDValue: enhanced = 0, standard = 1LEFT="170" WIDTH="341"Selects standard chrominance bandwidth of about 1.3 MHz or enhanced bandwidth 
(nonstandard) of about 2.5 MHz for composite and Y/C outputsLEFT="0" WIDTH="162"VL_EV1_CHROMA_DELAYLEFT="170" WIDTH="341"For CCIR 601 (13.5 MHz) sampling only, changes composite or Y/C chrominance 
delay without affecting luminance delayLEFT="0" WIDTH="162"VL_EV1_CHROMA_GAINLEFT="170" WIDTH="341"Fine-tunes chroma gain for composite and Y/C inputsLEFT="0" WIDTH="162"VL_EV1_COLOR_IN_ONLEFT="170" WIDTH="341"LEFT="0" WIDTH="162"VL_EV1_COLOR_KILL_THRESLEFT="170" WIDTH="341"Controls level at which burst amplitude decides if composite or Y/C input is color or 
monochrome when color mode is automatically set LEFT="0" WIDTH="162"VL_EV1_COLOR_OUT_ONLEFT="170" WIDTH="341"Makes composite or Y/C output into monochrome by turning off color burst and 
chrominance LEFT="0" WIDTH="162"VL_EV1_CORINGLEFT="170" WIDTH="341"Selects coring levels for luminance for composite and Y/C inputsLEFT="0" WIDTH="162"VL_EV1_DEINTERLACELEFT="170" WIDTH="341"For graphics to video, turns off interlace for flicker reduction; for video to graphics, 
interlaces video image LEFT="0" WIDTH="162"VL_EV1_DELAY_SYNCRange: (0,63)LEFT="170" WIDTH="341"Same as VL_EV1_H_OFFSET but with a narrow range: resolution in pixel clock stepsLEFT="0" WIDTH="162"VL_EV1_DOMINANCE_FIELDValue: 0 = odd field, 1 = even fieldLEFT="170" WIDTH="341"Triggers on odd or even fieldsLEFT="0" WIDTH="162"VL_EV1_FILTERLEFT="170" WIDTH="341"Filters decimated video images to smooth jagged edges LEFT="0" WIDTH="162"VL_EV1_GENLOCK_SRCLEFT="170" WIDTH="341"LEFT="0" WIDTH="162"VL_EV1_H_OFFSETLEFT="170" WIDTH="341"Delays timing of entire video signal (sync and picture) relative to timing reference 
such as genlock; no effect in slave mode for output timingLEFT="0" WIDTH="162"VL_EV1_H_PHASERange: (0,63)LEFT="170" WIDTH="341"In genlock output timing, provides a small-range horizontal phase adjustment for all 
outputs; in slave output timing mode, moves both h-phase and horizontal picture 
position for composite and Y/C output (no effect on RGB or Y/R-Y/B-Y in slave 
mode)LEFT="0" WIDTH="162"VL_EV1_H_PICTURE_POSITIONLEFT="170" WIDTH="341"Controls horizontal position of input picture: 2 pixel steps for composite and Y/C 
resolution; 1 pixel step for Y/R-Y/B-Y input resolutionLEFT="0" WIDTH="162"VL_EV1_LOCK_PORT0LEFT="170" WIDTH="341"LEFT="0" WIDTH="162"VL_EV1_LUMA_DELAYLEFT="170" WIDTH="341"Changes composite or Y/C luminance delay without affecting chrominance delayLEFT="0" WIDTH="162"VL_EV1_PEAK_WHITELEFT="170" WIDTH="341"Sets expected peak white amplitude; sets setup/no setup (pedestal) on black for Y or 
Y/R-Y/B-Y inputBetacam: use 714 mVSMPTE and EBU: use 700 mV or no setupLEFT="0" WIDTH="162"VL_EV1_PREFILTERLEFT="170" WIDTH="341"Boosts luminance frequency response for composite and Y/C formatsLEFT="0" WIDTH="162"VL_EV1_QUALITYLEFT="170" WIDTH="341"Allows locking to unstable video source, such as videotape recorders with no 
timebase correctionLEFT="0" WIDTH="162"VL_EV1_RGB_GAINRange: (0,63)LEFT="170" WIDTH="341"Adjusts output level of Y/R-Y/B-Y simultaneously (no effect on sync pulse 
amplitude)LEFT="0" WIDTH="162"VL_EV1_SCH_PHASERange: (0,255)LEFT="170" WIDTH="341"Adjusts SC-H phase +/- 180 degreesLEFT="0" WIDTH="162"VL_EV1_SUB_FREQRange: (0,255); total range +/- 450 ppmLEFT="170" WIDTH="341"Provides fine adjustment of composite and Y/C output color subcarrier frequencyLEFT="0" WIDTH="162"VL_EV1_SVHS_CHROMAValues; the first is the default:VL_EV1_COLOR_MODE_AUTO VL_EV1_COLOR_MODE_COLORVL_EV1_COLOR_MODE_MONOLEFT="170" WIDTH="341"Selects automatic detection of color or monochrome from the burst, forces 
monochrome, or forces color LEFT="0" WIDTH="162"VL_EV1_SYNC_LEVELValue: 286 mV = 0, 300 mV = 1LEFT="170" WIDTH="341"Selects sync pulse amplitude on G/Y outputLEFT="0" WIDTH="162"VL_EV1_SYNC_SLAVELEFT="170" WIDTH="341"Selects the slave sync mode when genlocking is not required. In this mode, the Galileo 
board is slaved to the input source. This is the best mode for capturing images because 
it provides the highest capture rate.LEFT="0" WIDTH="162"VL_EV1_TBC_MODEValue: (0,1) where 0 = off, 1 = on LEFT="170" WIDTH="341"When video input and output timings are not frequency-locked, removes small 
timing errors in analog video input; video input must go directly into a video 
framebuffer for proper operation; can also be used to achieve frame synchronization; 
makes no digital input unavailableLEFT="0" WIDTH="162"VL_EV1_TRIGGER_LINERange: (0,100)LEFT="170" WIDTH="341"Determines line number on which trigger event happensLEFT="0" WIDTH="162"VL_EV1_TRIGGER_POLARITYValues: 0 = negative, 1 = positiveLEFT="170" WIDTH="341"Determines whether a trigger event occurs on a 0 to 1 or a 1 to 0 transitionLEFT="0" WIDTH="162"VL_EV1_UV_LEVELValue: high = 0, low = 1LEFT="170" WIDTH="341"Changes R-Y and B-Y analog output levels to accommodate different standards, such 
as Beta, SMPTE, and EBU; set to low for RGB outLEFT="0" WIDTH="162"VL_EV1_U_GAIN_ROUGHRange: (0,63)LEFT="170" WIDTH="341"Controls gain of B-Y component input in coarse stepsLEFT="0" WIDTH="162"VL_EV1_U_GAIN_VERNIERRange: (0,63)LEFT="170" WIDTH="341"Controls gain of B-Y component input in fine stepsLEFT="0" WIDTH="162"VL_EV1_V_GAIN_ROUGHRange: (0,63)LEFT="170" WIDTH="341"Controls gain of R-Y component input in coarse stepsLEFT="0" WIDTH="162"VL_EV1_V_GAIN_VERNIERRange: (0,63)LEFT="170" WIDTH="341"Controls gain of R-Y component input in fine stepsLEFT="0" WIDTH="162"VL_EV1_V_OFFSETLEFT="170" WIDTH="341"Delays timing of entire video signal (sync and picture) relative to timing reference 
such as genlock; no effect in slave mode for output timingLEFT="0" WIDTH="162"VL_EV1_VNOISE_REDUCERLEFT="170" WIDTH="341"Selects mode of vertical noise reductionLEFT="0" WIDTH="162"VL_EV1_YC_GAINRange: (0,255)LEFT="170" WIDTH="341"Adjusts output level (sync, burst, luminance, and chrominance) of composite and Y/
C simultaneouslyLEFT="0" WIDTH="162"VL_EV1_YG_SYNCValue: (0,1) where 0 = off, 1 = on LEFT="170" WIDTH="341"Turns sync pulse on or offLBL="" HELPID=""Galileo Video IndyCam ControlsIDREF="57385" TYPE="TABLE"Table 13-4 lists the Galileo Video IndyCam controls.COLUMNS="3"LBL="13-4"Table 13-4 ID="57385"Galileo Video IndyCam ControlsLEFT="0" WIDTH="180"Galileo Video ControlLEFT="185" WIDTH="170"ValuesLEFT="360" WIDTH="135"SetsLEFT="0" WIDTH="180"VL_EV1_INDYCAM_AGCENALEFT="185" WIDTH="170"0 = off1 (default) = onLEFT="360" WIDTH="135"Automatic gain control (AGC)LEFT="0" WIDTH="180"VL_EV1_INDYCAM_AWBCTLLEFT="185" WIDTH="170"0 (momentary default) = off1 = onLEFT="360" WIDTH="135"Automatic white balanceLEFT="0" WIDTH="180"VL_EV1_INDYCAM_BLUE_BALANCELEFT="185" WIDTH="170"0,255,255; set by camera's white balanceLEFT="360" WIDTH="135"Blue balanceLEFT="0" WIDTH="180"VL_EV1_INDYCAM_BLUE_SATURATIONLEFT="185" WIDTH="170"LEFT="360" WIDTH="135"LEFT="0" WIDTH="180"VL_EV1_INDYCAM_BRIGHTNESSLEFT="185" WIDTH="170"Read-only value determined by input 
from IndyCamLEFT="360" WIDTH="135"Luma level (read-only)LEFT="0" WIDTH="180"VL_EV1_INDYCAM_GAINLEFT="185" WIDTH="170"0,255,255; set by AGC in cameraLEFT="360" WIDTH="135"GainLEFT="0" WIDTH="180"VL_EV1_INDYCAM_RED_BALANCELEFT="185" WIDTH="170"0,255,255; set by camera's white balanceLEFT="360" WIDTH="135"Red balanceLEFT="0" WIDTH="180"VL_EV1_INDYCAM_SATURATIONLEFT="185" WIDTH="170"0,170,170LEFT="360" WIDTH="135"SaturationLEFT="0" WIDTH="180"VL_EV1_INDYCAM_SHUTTERLEFT="185" WIDTH="170"VL_EV1_INDYCAM_SHUTTER_60VL_EV1_INDYCAM_SHUTTER_100VL_EV1_INDYCAM_SHUTTER_125VL_EV1_INDYCAM_SHUTTER_250VL_EV1_INDYCAM_SHUTTER_500VL_EV1_INDYCAM_SHUTTER_1000VL_EV1_INDYCAM_SHUTTER_2000VL_EV1_INDYCAM_SHUTTER_4000VL_EV1_INDYCAM_SHUTTER_10000Default:VL_EV1_INDYCAM_SHUTTER_1000LEFT="360" WIDTH="135"Shutter speedLEFT="0" WIDTH="180"VL_EV1_INDYCAM_SHUTTER_SNAPLEFT="185" WIDTH="170"LEFT="360" WIDTH="135"LBL="" HELPID=""Galileo Video Encoder and Color-Space Conversion ControlsEncoder controls for Galileo Video encode digital video from the system into the analog data stream. The controls are summarized in IDREF="59114" TYPE="TABLE"Table 13-5. COLUMNS="3"LBL="13-5"Table 13-5 ID="59114"Galileo Video Encoder and Color-Space Conversion ControlsLEFT="0" WIDTH="115"Encoder controlLEFT="120" WIDTH="126"Color-space conversion controlLEFT="255" WIDTH="90"PurposeLEFT="0" WIDTH="115"VL_EV1_ENC_BLANKLEFT="120" WIDTH="126"VL_EV1_CSC_BLANKLEFT="255" WIDTH="90"Sets digital level for 
blankingLEFT="0" WIDTH="115"VL_EV1_ENC_BLACKLEFT="120" WIDTH="126"VL_EV1_CSC_BLACKLEFT="255" WIDTH="90"Sets digital level for 
blackLEFT="0" WIDTH="115"VL_EV1_ENC_WHITELEFT="120" WIDTH="126"VL_EV1_CSC_WHITELEFT="255" WIDTH="90"Sets digital level for 
whiteLEFT="0" WIDTH="115"VL_EV1_ENC_UVGAINLEFT="120" WIDTH="126"VL_EV1_CSC_UVGAINLEFT="255" WIDTH="90"Sets color difference 
gain factorLEFT="0" WIDTH="115"VL_EV1_ENC_QUANTIZELEFT="120" WIDTH="126"VL_EV1_CSC_QUANTIZELEFT="255" WIDTH="90"Sets number of 
quantization levels; 
set to maximum for 
no effectLEFT="0" WIDTH="115"VL_EV1_ENC_LOADLEFT="120" WIDTH="126"VL_EV1_CSC_LOADLEFT="255" WIDTH="90"Loads default tableLEFT="0" WIDTH="115"N/ALEFT="120" WIDTH="126"VL_EV1_CSC_SUBADDRLEFT="255" WIDTH="90"Selects component 
table to load with 
custom setupLBL="" HELPID=""Galileo 601 Video Digital Breakout Box ControlsGeneral controls for the Galileo 601 Video Digital Breakout Box are summarized in IDREF="48292" TYPE="TABLE"Table 13-6. COLUMNS="2"LBL="13-6"Table 13-6 ID="48292"Galileo 601 Video Digital Breakout Box General ControlsLEFT="0" WIDTH="141"Control LEFT="150" WIDTH="191"PurposeLEFT="0" WIDTH="141"VL_EV1_DBOB_INPUTLEFT="150" WIDTH="191"Selects serial or parallel inputLEFT="0" WIDTH="141"VL_EV1_DBOB_INPUT2LEFT="150" WIDTH="191"Selects serial or parallel input/outputLEFT="0" WIDTH="141"VL_EV1_DBOB_VBSELECTValue: 0 = normal, 1 = narrowLEFT="150" WIDTH="191"Determines whether vertical blanking 
information passes through the Galileo 601 
Video option: normal setting blanks out the 
vertical blanking information; narrow setting 
passes it throughLEFT="0" WIDTH="141"VL_EV1_DBOB_PIXEL_MODEValues:0 = 13.5 (601CCIR), 1 = squareLEFT="150" WIDTH="191"Sets Galileo 601 Video pixel formatLEFT="0" WIDTH="141"VL_EV1_DBOB_PIXEL_FORMATLEFT="150" WIDTH="191"Selects analog output format on digital 
breakout box outputsLEFT="0" WIDTH="141"VL_EV1_DBOB_LINEValues: 0 = 525, 1 = 625LEFT="150" WIDTH="191"Sets 525 (NTSC) or 625 (PAL) timingLEFT="0" WIDTH="141"VL_EV1_DBOB_DELAYRange: (0,255)LEFT="150" WIDTH="191"Sets analog output delay on Galileo 601 VideoSee the dmedia/cl_cosmo.h header file for Cosmo Compress video parameters (CL_COSMO_VIDEO_*) that work with the Galileo 601 Video Digital Breakout Box and the Compression Library.LBL="" HELPID=""Color-Space Conversion ControlsIDREF="15532" TYPE="TABLE"Table 13-7 summarizes color-space conversion controls for the Galileo 601 Video Digital Breakout Box. COLUMNS="2"LBL="13-7"Table 13-7 ID="15532"Galileo Video Digital Breakout Box Color-Space Conversion ControlsLEFT="0" WIDTH="162"ControlLEFT="170" WIDTH="171"PurposeLEFT="0" WIDTH="162"VL_EV1_DBOB_CSC_BLANKLEFT="170" WIDTH="171"Sets digital level for blankingLEFT="0" WIDTH="162"VL_EV1_DBOB_CSC_BLACKLEFT="170" WIDTH="171"Sets digital level for blackLEFT="0" WIDTH="162"VL_EV1_DBOB_CSC_WHITELEFT="170" WIDTH="171"Sets digital level for whiteLEFT="0" WIDTH="162"VL_EV1_DBOB_CSC_UVGAINLEFT="170" WIDTH="171"Sets color difference gain factorLEFT="0" WIDTH="162"VL_EV1_DBOB_CSC_QUANTIZELEFT="170" WIDTH="171"Sets number of quantization levels; set to 
maximum for no effectLEFT="0" WIDTH="162"VL_EV1_DBOB_CSC_LOADLEFT="170" WIDTH="171"Loads default tableLEFT="0" WIDTH="162"VL_EV1_DBOB_CSC_SUBADDRLEFT="170" WIDTH="171"Selects component table to load with 
custom setupLBL="" HELPID=""Galileo Video DAC ControlsIDREF="30068" TYPE="TABLE"Table 13-8 summarizes the Galileo Video digital-to-analog converter (DAC) controls; all but the last two appear in the 
Allsubmenu of the vcp Pro menu. The range for each control is (0,63).COLUMNS="2"LBL="13-8"Table 13-8 ID="30068"Galileo Video DAC controlsLEFT="0" WIDTH="171"ControlLEFT="180" WIDTH="162"SetsLEFT="0" WIDTH="171"VL_EV1_DBOB_DAC_0LEFT="180" WIDTH="162"Y or green channel gainLEFT="0" WIDTH="171"VL_EV1_DBOB_DAC_1LEFT="180" WIDTH="162"Y or green channel offsetLEFT="0" WIDTH="171"VL_EV1_DBOB_DAC_2LEFT="180" WIDTH="162"B-Y or blue channel gainLEFT="0" WIDTH="171"VL_EV1_DBOB_DAC_3LEFT="180" WIDTH="162"B-Y or blue channel offsetLEFT="0" WIDTH="171"VL_EV1_DBOB_DAC_4LEFT="180" WIDTH="162"R-Y or red channel gainLEFT="0" WIDTH="171"VL_EV1_DBOB_DAC_5LEFT="180" WIDTH="162"R-Y or red channel offsetLEFT="0" WIDTH="171"VL_EV1_DBOB_DAC_6LEFT="180" WIDTH="162"Chroma gain; not in vcp (no UI)LEFT="0" WIDTH="171"VL_EV1_DBOB_DAC_7LEFT="180" WIDTH="162"Chroma offset (no UI)LBL="" HELPID=""ID="26544"VINO ControlsThis section describes the VINO controls.LBL="" HELPID=""VINO Video Control Panel ControlsIDREF="67976" TYPE="TABLE"Table 13-9 lists the general VINO controls. COLUMNS="2"LBL="13-9"Table 13-9 ID="67976"VINO vcp Controls LEFT="0" WIDTH="157"VINOLEFT="165" WIDTH="346"PurposeLEFT="0" WIDTH="157"VL_VINO_APERTURE; default 1LEFT="165" WIDTH="346"Sets aperture factors for luminance for composite and Y/C inputsLEFT="0" WIDTH="157"VL_VINO_BANDPASS; default 0LEFT="165" WIDTH="346"Selects bandpass filters for luminance for composite and Y/C inputsLEFT="0" WIDTH="157"VL_VINO_CHROMA_AGC; default 0LEFT="165" WIDTH="346"Sets automatic gain control speed for chrominance for composite or Y/CLEFT="0" WIDTH="157"VL_VINO_CHROMA_GAINdefault: 44 LEFT="165" WIDTH="346"Fine-tunes chroma gain for composite and Y/C inputsLEFT="0" WIDTH="157"VL_VINO_COLOR_KILL_THRESdefault: 30 dBLEFT="165" WIDTH="346"Controls level at which burst amplitude decides if composite or Y/C input is color or 
monochrome when color mode is automatically set LEFT="0" WIDTH="157"VL_VINO_CORING; default 0LEFT="165" WIDTH="346"Selects coring levels for luminance for composite and Y/C inputsLEFT="0" WIDTH="157"VL_VINO_H_PICTURE_POSITIONdefault: 244 (both NTSC and PAL)LEFT="165" WIDTH="346"Controls horizontal position of input picture: 2 pixel steps for composite and Y/C 
resolution; 1 pixel step for Y/R-Y/B-Y input resolutionLEFT="0" WIDTH="157"VL_VINO_LUMA_DELAY; default 0LEFT="165" WIDTH="346"Changes composite or Y/C luminance delay without affecting chrominance delayLEFT="0" WIDTH="157"VL_VINO_PREFILTER; default 0LEFT="165" WIDTH="346"Boosts luminance frequency response for composite and Y/C formatsLEFT="0" WIDTH="157"VL_VINO_COLOR_MODEValues; the first is the default:VL_VINO_COLOR_MODE_AUTO VL_VINO_COLOR_MODE_COLORVL_VINO_COLOR_MODE_MONOLEFT="165" WIDTH="346"Selects automatic detection of color or monochrome from the burst, forces 
monochrome, or forces color LEFT="0" WIDTH="157"VL_VINO_VNOISE_REDUCERValues: normal: set 0 (the default)search: set 1auto: set 2bypass: set 3LEFT="165" WIDTH="346"Selects mode of vertical noise reductionLBL="" HELPID=""VINO Analog Input ControlsIDREF="59204" TYPE="TABLE"Table 13-10 summarizes input controls specific to VINO.COLUMNS="5"LBL="13-10"Table 13-10 ID="59204"VINO Analog Input ControlsLEFT="0" WIDTH="144"ControlLEFT="150" WIDTH="72"RangeLEFT="230" WIDTH="56"Default:60 Hz (NTSC)LEFT="295" WIDTH="49"Default:50 Hz (PAL)LEFT="350" WIDTH="145"UseLEFT="0" WIDTH="144"VL_VINO_HREF_GENERATIONLEFT="150" WIDTH="72"(0,1)0 = off, 1 = onLEFT="230" WIDTH="56"1LEFT="295" WIDTH="49"1LEFT="350" WIDTH="145"Shifts the line 8 pixels to the right.LEFT="0" WIDTH="144"VL_VINO_PAL_SENSLEFT="150" WIDTH="72"Fraction range:0,255,1LEFT="230" WIDTH="56"N/ALEFT="295" WIDTH="49"144LEFT="350" WIDTH="145"In PAL timing, the chroma 
modulation phase inverts every 
line. Dropouts off the tape can 
disrupt this pattern. Use this 
control to set the recovery time 
constant (maximum for poor 
quality tape).LEFT="0" WIDTH="144"VL_VINO_AUFDLEFT="150" WIDTH="72"(0,1)0 = off, 1 = onLEFT="230" WIDTH="56"1LEFT="295" WIDTH="49"1LEFT="350" WIDTH="145"Sets automatic field detect.LEFT="0" WIDTH="144"VL_VINO_ZOOM_XLEFT="150" WIDTH="72"(0,1)0 = off, 1 = onLEFT="230" WIDTH="56"0LEFT="295" WIDTH="49"0LEFT="350" WIDTH="145"Specifies zoom or decimation in x 
direction only, to maintain aspect 
ratio for capturing only even or odd 
fields.LEFT="0" WIDTH="144"VL_VINO_ALPHALEFT="150" WIDTH="72"(0,255)LEFT="230" WIDTH="56"255LEFT="295" WIDTH="49"255LEFT="350" WIDTH="145"Sets value placed in user's 
framebuffers for the alpha value of 
the RGBA pixel format. Can be 
changed while capture is in 
progress. See Graphics Library 
documentation for typical uses.LEFT="0" WIDTH="144"VL_VINO_EVEN_OFFSETLEFT="150" WIDTH="72"0..MaxHeightLEFT="230" WIDTH="56"0LEFT="295" WIDTH="49"0LEFT="350" WIDTH="145"Sets offset or clipping value 
separately for even fields; for 
example, if this control is set to 2 
and VL_VINO_ODD_OFFSET is 
set to 0, the following lines are 
captured in NTSC: odd line 21, 
even line 26, odd line 23, even line 
28.LEFT="0" WIDTH="144"VL_VINO_ODD_OFFSETLEFT="150" WIDTH="72"0..MaxHeightLEFT="230" WIDTH="56"0LEFT="295" WIDTH="49"0LEFT="350" WIDTH="145"Sets offset or clipping value 
separately for odd fields.LBL="" HELPID=""VINO IndyCam ControlsThe controls for the IndyCam are summarized in IDREF="62497" TYPE="TABLE"Table 13-11.COLUMNS="3"LBL="13-11"Table 13-11 ID="62497"IndyCam ControlsLEFT="0" WIDTH="173"VINOLEFT="180" WIDTH="172"ValuesLEFT="360" WIDTH="126"SetsLEFT="0" WIDTH="173"VL_VINO_INDYCAM_AGCENALEFT="180" WIDTH="172"0 = off1 (default) = onLEFT="360" WIDTH="126"Automatic gain control (AGC)LEFT="0" WIDTH="173"VL_VINO_INDYCAM_AWBCTLLEFT="180" WIDTH="172"0 (momentary default) = off1 = onLEFT="360" WIDTH="126"Automatic white balanceLEFT="0" WIDTH="173"VL_VINO_INDYCAM_BLUE_BALANCELEFT="180" WIDTH="172"0,255,255; set by camera's white balanceLEFT="360" WIDTH="126"Blue balanceLEFT="0" WIDTH="173"VL_VINO_INDYCAM_BRIGHTNESSLEFT="180" WIDTH="172"Read-only value determined by input 
from IndyCamLEFT="360" WIDTH="126"Luma level (read-only)LEFT="0" WIDTH="173"VL_VINO_INDYCAM_GAINLEFT="180" WIDTH="172"0,255,255; set by AGC in cameraLEFT="360" WIDTH="126"GainLEFT="0" WIDTH="173"VL_VINO_INDYCAM_RED_BALANCELEFT="180" WIDTH="172"0,255,255; set by camera's white balanceLEFT="360" WIDTH="126"Red balanceLEFT="0" WIDTH="173"VL_VINO_INDYCAM_SATURATIONLEFT="180" WIDTH="172"0,170,170LEFT="360" WIDTH="126"SaturationLEFT="0" WIDTH="173"VL_VINO_INDYCAM_SHUTTERLEFT="180" WIDTH="172"VL_VINO_INDYCAM_SHUTTER_60VL_VINO_INDYCAM_SHUTTER_100VL_VINO_INDYCAM_SHUTTER_125VL_VINO_INDYCAM_SHUTTER_250VL_VINO_INDYCAM_SHUTTER_500VL_VINO_INDYCAM_SHUTTER_1000VL_VINO_INDYCAM_SHUTTER_2000VL_VINO_INDYCAM_SHUTTER_4000VL_VINO_INDYCAM_SHUTTER_10000Default:VL_VINO_INDYCAM_SHUTTER_1000LEFT="360" WIDTH="126"Shutter speedLBL="14"ID="92735"VL Event HandlingThe VL provides several ways of handling data stream events, such as completion or failure of data transfer, vertical retrace event, loss of the path to another client, lack of detectable sync, or dropped fields or frames. The method you use depends on the kind of application you're writing:For a strictly VL application, use:vlSelectEvents() to choose the events to which you want the application to respondvlAddCallback() to specify the function called when the event occursyour own event loop or a main loop (vlMainLoop()) to dispatch the eventsFor an application that also accesses another program or device driver, or if you're adding video capability to an existing X or OpenGL application, set up an event loop in the main part of the application and use the IRIX file descriptor (FD) of the event(s) you want to add.Topics in this chapter include:IDREF="83978" TYPE="TEXT"Querying VL EventsIDREF="30316" TYPE="TEXT"Creating a VL Event LoopIDREF="39194" TYPE="TEXT"Creating a Main Loop with CallbacksThis chapter concludes with an example illustrating a main loop and event loops.LBL="" HELPID=""ID="83978"Querying VL EventsGeneral VL event handling routines are summarized in IDREF="48594" TYPE="TABLE"Table 14-1.COLUMNS="2"LBL="14-1"Table 14-1 ID="48594"VL Event Handling RoutinesLEFT="0" WIDTH="106"RoutineLEFT="115" WIDTH="225"UseLEFT="0" WIDTH="106"vlGetFD()LEFT="115" WIDTH="225"Get a file descriptor for a VL serverLEFT="0" WIDTH="106" vlNextEvent()LEFT="115" WIDTH="225"Gets the next event; blocks until you get the next event 
from the queueLEFT="0" WIDTH="106"vlCheckEvent()LEFT="115" WIDTH="225"Like a nonblocking vlNextEvent(), this call checks to see 
if you have an event waiting of the type you specify and 
reads it off the queue without blockingLEFT="0" WIDTH="106"vlPeekEvent()LEFT="115" WIDTH="225"Copies the next event from the queue but, unlike 
vlNextEvent(), does not update the queue, so that you 
can see the event without processing itLEFT="0" WIDTH="106"vlSelectEvents()LEFT="115" WIDTH="225"Selects video events of interest LEFT="0" WIDTH="106"vlPending()LEFT="115" WIDTH="225"Queries whether there is an event waiting for the 
applicationLEFT="0" WIDTH="106"vlEventToName()LEFT="115" WIDTH="225"Gets the character string with the name of the event; for 
example, to use in messagesLEFT="0" WIDTH="106"vlAddCallback()LEFT="115" WIDTH="225"Adds a callback; use for VL eventsLEFT="0" WIDTH="106"vlRemoveCallback()LEFT="115" WIDTH="225"Removes a callback for the events specified if the client 
data matches that supplied when adding the callbackLEFT="0" WIDTH="106"vlRemoveAllCallbacks()LEFT="115" WIDTH="225"Removes all callbacks for the specified path and eventsLEFT="0" WIDTH="106"vlCallCallbacks()LEFT="115" WIDTH="225"Creates a handler; used when creating a main loop or 
using a supplied, non-VL main loopLEFT="0" WIDTH="106"vlRegisterHandler()LEFT="115" WIDTH="225"Registers an event handler; use for non-VL eventsLEFT="0" WIDTH="106"vlRemoveHandler()LEFT="115" WIDTH="225"Removes an event handlerThe event type is an integer. vlEventToName() allows you to get the character string with the name of the event, so that you can use the event name, for example, in messages.IDREF="20475" TYPE="TABLE"Table 14-2 summarizes VL event masks.COLUMNS="2"LBL="14-2"Table 14-2 ID="20475"VL Event MasksLEFT="0" WIDTH="133"SymbolLEFT="140" WIDTH="197"MeaningLEFT="0" WIDTH="133"VLStreamBusyMaskLEFT="140" WIDTH="197"Stream is lockedLEFT="0" WIDTH="133"VLStreamPreemptedMaskLEFT="140" WIDTH="197"Stream was grabbed by another applicationLEFT="0" WIDTH="133"VLAdvanceMissedMaskLEFT="140" WIDTH="197"Time was already reachedLEFT="0" WIDTH="133"VLSyncLostMaskLEFT="140" WIDTH="197"Irregular or interrupted signalLEFT="0" WIDTH="133"VLSequenceLostMaskLEFT="140" WIDTH="197"Field or frame droppedLEFT="0" WIDTH="133"VLControlChangedMaskLEFT="140" WIDTH="197"A control has changedLEFT="0" WIDTH="133"VLControlRangeChangedMask LEFT="140" WIDTH="197"A control range has changedLEFT="0" WIDTH="133"VLControlPreemptedMask LEFT="140" WIDTH="197"Control of a node has been preempted, typically 
by another user setting VL_LOCK on a path that 
was previously set with VL_SHARELEFT="0" WIDTH="133"VLControlAvailableMask LEFT="140" WIDTH="197"Access is now availableLEFT="0" WIDTH="133"VLTransferCompleteMaskLEFT="140" WIDTH="197"Transfer of field or frame completeLEFT="0" WIDTH="133"VLTransferFailedMaskLEFT="140" WIDTH="197"Error; transfer terminated; perform cleanup at 
this point, including vlEndTransfer() LEFT="0" WIDTH="133"VLEvenVerticalRetraceMaskLEFT="140" WIDTH="197"Vertical retrace event, even fieldLEFT="0" WIDTH="133"VLOddVerticalRetraceMaskLEFT="140" WIDTH="197"Vertical retrace event, odd fieldLEFT="0" WIDTH="133"VLFrameVerticalRetraceMaskLEFT="140" WIDTH="197"Frame vertical retrace eventLEFT="0" WIDTH="133"VLDeviceEventMaskLEFT="140" WIDTH="197"Device-specific event, such as a timing change 
on a Galileo Video nodeLEFT="0" WIDTH="133"VLDefaultSourceMaskLEFT="140" WIDTH="197"Default source changedCall vlGetFD() to get a file descriptor usable from select(2) or poll(2).Call vlSelectEvents() to express interest in one or more event. For example:vlSelectEvents(svr, path, VLTransferCompleteMask); Event masks can be or'ed together. For example:vlSelectEvents(svr, path, VLTransferCompleteMask |               VLTransferFailedMask);Depending on whether you want to block processing or not, use vlNextEvent() ID="Media3-4EH1"(blocking) or vlCheckEvent() (nonblocking) to get the next event.Use vlPeekEvent() to see what the next event in the queue is without removing it from the queue. For example, the part of the code that actually gets the event from the event loop uses vlNextEvent(), whereas another part of the code that just wants to know about it, for example, for priority purposes, uses vlPeekEvent().LBL="" HELPID=""ID="30316"Creating a VL Event LoopYou can set an event loop to run until a specific condition is fulfilled. The routine vlSelectEvents() allows you to specify which event the application will receive.Using an event loop requires creating an event mask to specify the events you want. The VL event mask symbols are combined with the bitwise OR operator. For example, to set an event mask to express interest in either transfer complete or control changed events, use:VLTransferCompleteMask | VLControlChangedMaskTo create an event loop, follow these steps:Define the event; for example:VLEvent ev;
name='hellip' font=symbol charset=fontspecific code=188Set the event mask; for example:vlSelectEvents(vlServer, path, VLTransferCompleteMask | VLControlChangedMask)Block on the transfer process until at least one event is waiting:for(;;){
vlNextEvent(vlServer, &ev);Create the loop and define the choices; for example:switch(ev.reason){
        case VLTransferComplete:
        name='hellip' font=symbol charset=fontspecific code=188
        break;
    case VLControlChanged:
        name='hellip' font=symbol charset=fontspecific code=188
        break;
    }
}LBL="" HELPID=""ID="39194"Creating a Main Loop with CallbacksvlMainLoop() is provided as a convenience routine for the application programmer and constitutes the main loop of VL applications. This routine first reads the next incoming video event; it then dispatches the event to the appropriate registered procedure. Note that the application does not return from this call.Applications are expected to exit in response to some user action. There is nothing special about vlMainLoop(); it is simply an infinite loop that calls the next event and then dispatches it. An application can provide its own version of this loop, for example, to test a global termination flag or to test that the number of top-level widgets is larger than zero before circling back to the call to the next event.To specify callbacks, that is, routines which are called when a particular VL event arrives, use vlAddCallback(). Its function prototype is:int vlAddCallback(VLServer vlServer, VLEvent * event,                  void * clientdata, VLEventMask events,                  VLCallbackProc callback, void *clientData)IDREF="11293" TYPE="TEXT"Example 14-1 illustrates the use of vlAddCallback().LBL="14-1"Example 14-1 ID="11293"Using VL Callbacksmain()
{
  name='hellip' font=symbol charset=fontspecific code=188
      /* Set up the mask for control changed events and Stream preempted events */
   if (vlSelectEvents(vlSvr, vlPath, VLTransferComplete | VLStreamPreemptedMask))
         doErrorExit("select events");

   /* Set ProcessEvent() as the callback for VL events */
   vlAddCallback(vlSvr, vlPath, VLTransferCompleteMask | VLStreamPreemptedMask,
                 ProcessEvent, NULL);
 
   /* Start the data transfer immediately (i.e. don't wait for trigger) */
   if (vlBeginTransfer(vlSvr, vlPath, 0, NULL))
        doErrorExit("begin transfer");
        
   /* Get and dispatch events */
   vlMainLoop();
}
 
/* Handle VL events */
void
ProcessEvent(VLServer svr, VLEvent *ev, void *data)
{ 
   switch (ev->reason)
   {
      case VLTransferComplete:
        /* Get the valid video data from that frame */
           dataPtr = vlGetActiveRegion(vlSvr, transferBuf, info);
       /* Done with that frame, free the memory used by it */
             vlPutFree(vlSvr, transferBuf);
             frameCount++;
    break;
 
    case VLStreamPreempted:
        fprintf(stderr, "%s: Stream was preempted by another Program\n",
         _progname);
          docleanup(1);
    break;
 
    default:
    break;
   }
}Delete a callback with vlRemoveCallback() or vlRemoveAllCallbacks(). Their function prototypes are:int vlRemoveCallback(VLServer vlServer, VLPath * path,       VLEventMask events, VLCallbackProc callback, void *clientData)
int vlRemoveAllCallbacks(VLServer vlServer, VLPath * path, VLEventMask events)The functions vlAddHandler() and vlRemoveHandler() are analogous to vlAddCallback() and vlRemoveCallback(), respectively. Use them for non-VL events.IDREF="89741" TYPE="TEXT"Example 14-2 illustrates how to create a main loop and event loops. CautionTo simplify the code, this example does not check returns. The programmer should, however, always check returns.LBL="14-2"Example 14-2 ID="89741"VL Event Handling: eventex.c /*================An Event Driven Application==========
 *
 *
 * File:          eventex.c
 *
 * Usage:         eventex 
 *
 * Description:   event demonstrates VL eventloop with the IRIS GL
 *
 * Functions:     IRIS Video Library functions used
 *
 *                vlOpenVideo()
 *                vlGetNode()
 *                vlCreatePath()
 *                vlSetupPaths()
 *                vlSetControl()
 *                vlCreateBuffer()
 *                vlRegisterBuffer()
 *                vlRegisterHandler()
 *                vlAddCallback()
 *                vlSelectEvents()
 *                vlMainLoop()
 *                vlGetActiveRegion()
 *                vlGetNextValid()
 *                vlPutFree()
 *                vlBeginTransfer()
 *                vlEndTransfer()
 *                vlDeregisterBuffer()
 *                vlDestroyPath()
 *                vlDestroyBuffer()
 *                vlCloseVideo()
 *                vlPerror()
 */
#include <stdlib.h>
#include <stdio.h>
#include <gl/gl.h>
#include <gl/device.h>
#include <dmedia/vl.h>

/*
 *  Function Prototypes
 */
void error_exit(void);
void ProcessEvent(VLServer svr, VLEvent *ev, void *data);
void ProcessGlEvent(int fd, void *win);
void exit_capture(void);

/*
 * Global Variables
 */
char *_progName;
VLBuffer buffer;
VLServer svr;
VLPath path;
VLNode src, drn;
int xsize;
int ysize;

/* Report errors */
void
error_exit(void)
{
    vlPerror(_progName);
    exit(1);
}
void
main(int argc, char **argv)
{
    VLControlValue val;
    int c;
    long win;
    
    _progName = argv[0];
    
    foreground();
        
    /* Connect to the daemon */
    if (!(svr = vlOpenVideo(""))) 
        error_exit();

    /* Set up a drain node in memory */
    drn = vlGetNode(svr, VL_DRN, VL_MEM, VL_ANY);
    
    /* Set up a source node on any video source  */
    src = vlGetNode(svr, VL_SRC, VL_VIDEO, VL_ANY);

    /* Create a path using the first device that will support it */
    path = vlCreatePath(svr, VL_ANY, src, drn); 

    /* Set up the hardware for and define the usage of the path */
    if ((vlSetupPaths(svr, (VLPathList)&path, 1, VL_SHARE,        VL_SHARE)) < 0)
        error_exit();

    /* Set the packing to RGB */
    val.intVal = VL_PACKING_RGB_8;
    vlSetControl(svr, path, drn, VL_PACKING, &val);
    
    /* Get the video size */
    vlGetControl(svr, path, drn, VL_SIZE, &val);
    xsize = val.xyVal.x;
    ysize = val.xyVal.y;
    
    /* Set up and open a GL window to display the data */
    prefsize(xsize,ysize);
    win = winopen("Eventex Window");
    RGBmode();
    pixmode(PM_TTOB, 1);
    gconfig();
    /* 
     * Allow these key presses, mouseclicks, etc to be 
     * entered in the event queue 
     */
    qdevice(ESCKEY);
    qdevice(WINSHUT);
    qdevice(WINQUIT);
    
    /* Create and register a buffer for 1 frame */
    buffer = vlCreateBuffer(svr, path, drn, 1);
    if (buffer == NULL)
        error_exit();   
    vlRegisterBuffer(svr, path, drn, buffer);
    
    /* Begin the data transfer */
    if (vlBeginTransfer(svr, path, 0, NULL))
        error_exit();
    
    /* 
     * Specify what path-related events we want to receive.
     * In this example we only want transfer complete events.
     */
    vlSelectEvents(svr, path, VLTransferCompleteMask);
    
    /* Set ProcessEvent() is the callback for transfer complete */
    vlAddCallback(svr, path, VLTransferCompleteMask, ProcessEvent,                  NULL);
    
    /* Set ProcessGlEvent() as the GL event handler */
    vlRegisterHandler(svr, qgetfd(), (VLEventHandler)ProcessGlEvent,
                      (VLPendingFunc) qtest, (void *)win);
                  
    /* Loop and dispatch events */
    vlMainLoop();    
}
/* Handle video library events */
void ProcessEvent(VLServer svr, VLEvent *ev, void *data)
{
    VLInfoPtr info;
    char *dataPtr;

    switch (ev->reason)
    {
        case VLTransferComplete:
            info = vlGetNextValid(svr, buffer);
            if(!info)
                break;
        
            /* Get a pointer to the frame */
            dataPtr = vlGetActiveRegion(svr, buffer, info);
                    
            /* Write the data to the screen */
            lrectwrite(0,0, xsize-1, ysize-1, (ulong *)dataPtr);
        
            /* Finished with frame, unlock the buffer */
            vlPutFree(svr, buffer);
        break;
    
        default:
            printf("Got Event %d\n", ev->reason);
        break;
    }
}
/* Handle graphics library events */
void ProcessGlEvent(int fd, void *win)
{
    static short val;

    switch (qread(&val))
    {
        /* Quit */
        case ESCKEY:
            if (val == 1) /* Respond to keydowns only */
                exit_capture();
        break;
                
        case WINSHUT:
        case WINQUIT:
            exit_capture();
        break;
        
        default:
        break;
    }
}

void
exit_capture()
{
    /* End the data transfer */
    vlEndTransfer(svr, path);
    
    /* Disassociate the ring buffer from the path */
    vlDeregisterBuffer(svr, path, drn, buffer);
    
    /* Destroy the path, free the memory it used */
    vlDestroyPath(svr,path);    
    
    /* Destroy the ring buffer, free the memory it used */
    vlDestroyBuffer(svr, buffer);
    
    /* Disconnect from the daemon */
    vlCloseVideo(svr);
    
    exit(0);LBL="15"ID="57047"VL BlendingThis chapter explains how to combine video frame information and computer-generated graphics, if your equipment supports such operations.Use the VL to perform three types of blending:Chroma keying: overlaying one image on another by choosing a key color. For example, if chroma keying is set to blue, image A might show through image B everywhere the color blue appears in image B. A common example is the TV weather reporter standing in front of the satellite weather map. The weather reporter, wearing any color but blue, stands in front of a blue background; keying on blue shows the satellite picture everywhere blue appears. Because there is no blue on the weatherperson, he or she appears to be standing in front of the weather map.Luma keying: overlaying one image on another by choosing a level of luminance. For example, to overlay bright text (such as a caption) on video, a graphics source is created with the text on a dark background. The video source is made to show through the dark areas of the graphics; the bright text remains on top of the video.Transitions: fades, tiles, and wipes, such as single, double, or corner wipes, for which you can set the angle or center. This chapter explains how the VL performs blending. Topics in this chapter include:IDREF="14898" TYPE="TEXT"The VL Key GeneratorIDREF="95188" TYPE="TEXT"The VL Blend NodeIDREF="28169" TYPE="TEXT"VL Blending ControlsIDREF="14600" TYPE="TEXT"VL KeyingThe chapter concludes with example application programs.LBL="" HELPID=""ID="14898"The VL Key GeneratorBlending in the VL is based on values that the key generator assigns to each pixel in the sources to be blended. The key generator generates a key for each pixel in the two source nodes (foreground and background):If luma keying is set, the key generator assesses the brightness of each pixel. If chroma keying is set, the key generator assesses the color of each pixel. If spatial, or transition, keying (fade, tile, wipe) is set, the key generator assesses the (x,y) coordinates for each pixel.In addition, the key generator determines the alpha value (opacity) of a pixel and sets a value for it ranging from 0 (completely transparent) to 1 (completely opaque). This alpha value can be used downstream for further layering operations.Key generation is usually for one alpha source only. The other alpha source can be constant or can come from a matte signal or from graphics on systems that support alpha rendering.LBL="" HELPID=""ID="95188"The VL Blend NodeBlending takes place in the VL's internal blend node, which is created with the vlGetNode() function. NoteNot all connections are possible on all video options.The code fragment in IDREF="72772" TYPE="TEXT"Example 15-1 sets up source, drain and blend nodes.LBL="15-1"Example 15-1 ID="72772"Setting Up Source, Drain, and Blend Nodes/* variable definitions */
{
 VLServer svr;
 VLPath path;
 VLNode src, drn; 
 VLControlValue val;
 int x, y, c;
 uint w, h, bw{

/* Open a video device */ 
svr = vlOpenVideo("")

/* Set up drain nodes on the screen and video */
drn_scr = vlGetNode(vlSvr, VL_DRN, VL_SCREEN, VL_ANY);
drn_vid = vlGetNode(vlSvr, VL_DRN, VL_VIDEO, VL_ANY);

/* Set up source nodes on the screen and video */
src_scr = vlGetNode(vlSvr, VL_SRC, VL_SCREEN, VL_ANY);
src_vid = vlGetNode(vlSvr, VL_SRC, VL_VIDEO, vin);

/* Set up internal blending node */
blend_node = vlGetNode(vlSvr, VL_INTERNAL, VL_BLENDER,
                       VL_ANY);IDREF="81442" TYPE="GRAPHIC"Figure 15-1 diagrams the blender setup.FILE="15-1.online.bw" POSITION="INLINE" SCALE="FALSE"LBL="15-1"Figure 15-1 ID="81442"Setting Up the Blend NodeThe blend node mixes the foreground and background video signals by applying a blend function to the foreground and background pixels. IDREF="79354" TYPE="GRAPHIC"Figure 15-2 diagrams the Galileo Video alpha blender.FILE="15-2.online.bw" POSITION="INLINE" SCALE="FALSE"LBL="15-2"Figure 15-2  ID="79354"Galileo Video Alpha BlenderThe blend node then sends the data to the drain node. For example, blending analog video with part of a graphics screen and sending it to video out can be diagrammed as shown in IDREF="72248" TYPE="GRAPHIC"Figure 15-3.FILE="15-3.online.bw" POSITION="INLINE" SCALE="FALSE"LBL="15-3"Figure 15-3 ID="72248"Blending Analog Video with Part of a Graphics ScreenBlending analog video with static frame data and sending it to video out can be diagrammed as shown in IDREF="22891" TYPE="GRAPHIC"Figure 15-4.FILE="15-4.online.bw" POSITION="INLINE" SCALE="FALSE"LBL="15-4"Figure 15-4 ID="22891"Blending Analog Video with Static Frame DataAdding another drain, such as a screen location at which to preview the output, can be diagrammed as shown in IDREF="45175" TYPE="GRAPHIC"Figure 15-5.FILE="15-5.online.bw" POSITION="INLINE" SCALE="FALSE"LBL="15-5"Figure 15-5 ID="45175"Adding Another Drain to Preview the BlendIn IDREF="45175" TYPE="GRAPHIC"Figure 15-5, the source called 
Screen in and the drain called 
Screen out are shaded to indicate that although they are separate and distinct nodes, they overlap physically; that is, they are set for the same screen location.LBL="" HELPID=""ID="28169"VL Blending ControlsThe VL uses blending controls to set blending options. All blending controlsname='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'that is, all the controls discussed in this chaptername='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'apply only to blend nodes, except for VL_EV1_ALPHA_NOT_PIXEL, which applies to drain nodes. The order of blending and zooming depends on the node type: for a source, zooming takes place before blending; for a drain, blending takes place before zooming. All controls are available for all platforms unless otherwise noted. See the Sirius Video Owner's and Programming Guide for Sirius Video blending controls.NoteThe reference "Galileo Video" includes Indigo2 Video and Indy Video, unless otherwise noted.IDREF="77275" TYPE="TABLE"Table 15-1 summarizes the VL controls for blending.COLUMNS="3"LBL="15-1"Table 15-1 ID="77275"VL Blend ControlsLEFT="0" WIDTH="134"ControlLEFT="140" WIDTH="144"ValuesLEFT="290" WIDTH="225"SelectsLEFT="0" WIDTH="134"VL_BLEND_A_FCNtype intValLEFT="140" WIDTH="144"VL_BLDFCN_ZEROVL_BLDFCN_ONEVL_BLDFCN_B_ALPHA    (background alpha)/255VL_BLDFCN_MINUS_B_ALPHA: 1 - ((background alpha) / 255)LEFT="290" WIDTH="225"Blend function that controls mixing of foreground 
signalsLEFT="0" WIDTH="134"VL_BLEND_B_FCNtype intValLEFT="140" WIDTH="144"VL_BLDFCN_ZEROVL_BLDFCN_ONEVL_BLDFCN_A_ALPHA    (foreground alpha)/255VL_BLDFCN_MINUS_A_ALPHA    1 - ((foreground alpha) / 255)LEFT="290" WIDTH="225"Blend function that controls mixing of background 
signalsLEFT="0" WIDTH="134"VL_BLEND_Atype intValLEFT="140" WIDTH="144"VLNode type, derived from 
vlGetNode(); must be one of the 
two source nodesLEFT="290" WIDTH="225"Input source for foreground imageLEFT="0" WIDTH="134"VL_BLEND_Btype intValLEFT="140" WIDTH="144"VLNode type, derived from 
vlGetNode(); must be one of the 
two source nodesLEFT="290" WIDTH="225"Input source for background imageLEFT="0" WIDTH="134"VL_BLEND_A_ALPHAtype intValLEFT="140" WIDTH="144"VLNode type, derived from 
vlGetNode(); must be one of the 
two source nodesLEFT="290" WIDTH="225"Input source for foreground alpha; cannot be used on 
Galileo Video, Indigo2 Video, or Indy VideoLEFT="0" WIDTH="134"VL_BLEND_B_ALPHAtype intValLEFT="140" WIDTH="144"VLNode type, derived from 
vlGetNode(); must be one of the 
two source nodesLEFT="290" WIDTH="225"Input source for background alphaLEFT="0" WIDTH="134"VL_BLEND_A_NORMALIZEtype boolValLEFT="140" WIDTH="144"(0,1)0 = off, 1 = on LEFT="290" WIDTH="225"Follows Porter-Duff model (background pixels 
premultiplied by their corresponding alphas before 
blending)LEFT="0" WIDTH="134"VL_BLEND_B_NORMALIZEtype boolValLEFT="140" WIDTH="144"(0,1)0 = off, 1 = onLEFT="290" WIDTH="225"Follows Porter-Duff model LEFT="0" WIDTH="134"VL_BLEND_OUT_NORMALIZEtype boolValLEFT="140" WIDTH="144"(0,1)0 = off, 1 = onLEFT="290" WIDTH="225"Follows Porter-Duff modelLBL="" HELPID=""ID="14600"VL KeyingFor each kind of keyingname='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'luma keying, chroma keying, and transitionsname='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'further VL controls enable you to specify the properties of the blend.NoteKeying parameters are implemented as device-dependent VL controls; this section explains Galileo Video (including Indigo2 Video and Indy Video) keying controls.The values for the Galileo Video "master" keyer control VL_EV1_KEYER_MODE determine the type of keying performed:luma keying: VL_EV1_KEYERMODE_LUMAchroma keying: VL_EV1_KEYERMODE_CHROMAtransitions, that is, fades, tiles, or wipes: VL_EV1_KEYERMODE_SPATIALFor example, the following fragment specifies that chroma keying is to be performed:vlSetControl(vlSvr, vlPath, blend_node, VL_EV1_KEYER_MODE             VL_EV1_KEYERMODE_CHROMA); Keying controls fall into three groups:luma keyingchroma keyingfades, tiles, and wipesEach type is explained separately in this section.LBL="" HELPID=""Galileo Video Luma KeyingLuma keying is typically used to overlay a fixed image on video, such as the name and title of an individual being interviewed, a cable channel's logo, or a symbol that denotes an ongoing news story during a newscast. IDREF="49749" TYPE="TEXT"IDREF="49749" TYPE="GRAPHIC"Figure 15-6 diagrams an application. FILE="luma.key.app.ai" POSITION="INLINE" SCALE="FALSE"LBL="15-6"Figure 15-6 ID="49749"Luma Keying Application: TitlingThe four Galileo Video luma keying controls are summarized in IDREF="19402" TYPE="TABLE"Table 15-2; each is of type intVal.COLUMNS="3"LBL="15-2"Table 15-2 ID="19402"Galileo Video Luma Keying ControlsLEFT="0" WIDTH="139"ControlLEFT="145" WIDTH="36"RangeLEFT="190" WIDTH="328"SetsLEFT="0" WIDTH="139"VL_EV1_KEYER_VALUE_LUMALEFT="145" WIDTH="36"(0,255)LEFT="190" WIDTH="328"Central luma value. This control sets the luma value at which the background 
shows through the foreground.LEFT="0" WIDTH="139"VL_EV1_KEYER_RANGE_LUMALEFT="145" WIDTH="36"(0,255)LEFT="190" WIDTH="328"One-sided range of the center value. This control determines the range of luma 
values where the background shows through the foreground.LEFT="0" WIDTH="139"VL_EV1_KEYER_FG_OPACITYLEFT="145" WIDTH="36"(0,255)LEFT="190" WIDTH="328"Opacity of the foreground, thus limiting the value of foreground alpha at any point.LEFT="0" WIDTH="139"VL_EV1_KEYER_DETAILLEFT="145" WIDTH="36"(-8,7)LEFT="190" WIDTH="328"Sharpness of transition between foreground and background allowing blurring of 
edges. The value -8 yields the most gradual transition, +7 the sharpest.IDREF="26060" TYPE="GRAPHIC"Figure 15-7 diagrams the relationships between these controls.FILE="15-7.online.bw" POSITION="INLINE" SCALE="FALSE"LBL="15-7"Figure 15-7 ID="26060"Relationships Between Galileo Video Luma Keying ControlsLBL="" HELPID=""Galileo Video Chroma KeyingChroma keying overlays one image on another based on the color value. IDREF="27748" TYPE="GRAPHIC"Figure 15-8 diagrams a common application.FILE="15-8.online.bw" POSITION="INLINE" SCALE="FALSE"LBL="15-8"Figure 15-8 ID="27748"Chroma Keying Application: TV Weather MapIDREF="14820" TYPE="TABLE"Table 15-3 summarizes the controls for Galileo Video chroma keying and gives their ranges. These controls are all of type intVal.COLUMNS="3"LBL="15-3"Table 15-3 ID="14820"Galileo Video Chroma Keying ControlsLEFT="0" WIDTH="180"ControlLEFT="185" WIDTH="54"RangeLEFT="245" WIDTH="270"SetsLEFT="0" WIDTH="180"VL_EV1_KEYER_VALUE_CHROMA_U LEFT="185" WIDTH="54"(-226,226)LEFT="245" WIDTH="270"Central U value at which the background shows through the 
foreground.LEFT="0" WIDTH="180"VL_EV1_KEYER_RANGE_CHROMA_ULEFT="185" WIDTH="54"(0,452)LEFT="245" WIDTH="270"One-sided range of U where the background shows through the 
foreground.LEFT="0" WIDTH="180"VL_EV1_KEYER_VALUE_CHROMA_VLEFT="185" WIDTH="54"(-179,179)LEFT="245" WIDTH="270"Central V value at which the background shows through the 
foreground.LEFT="0" WIDTH="180"VL_EV1_KEYER_RANGE_CHROMA_VLEFT="185" WIDTH="54"(0,358)LEFT="245" WIDTH="270"One-sided range of V where the background shows through the 
foreground.LEFT="0" WIDTH="180"VL_EV1_KEYER_DETAILLEFT="185" WIDTH="54"(-8,7)LEFT="245" WIDTH="270"Sharpness of transition between foreground and backgroundNoteVL_EV1_KEYER_FG_OPACITY has no effect on Galileo Video in chroma key mode.IDREF="60715" TYPE="GRAPHIC"Figure 15-9 diagrams the relationships between these controls.FILE="Media3-5BL.cgm9" POSITION="INLINE" SCALE="FALSE"LBL="15-9"Figure 15-9 ID="60715"Relationships Between Galileo Video Chroma Keying ControlsLBL="" HELPID=""Galileo Video Fades, Tiles, and WipesThe values used with the control VL_EV1_WIPE_TYPE determine the type of blending performed:from all-foreground to all-background: VL_EV1_WIPETYPE_FADEfrom all-foreground to all-background by randomly tiling screen with rectangles of a specified size: VL_EV1_WIPETYPE_TILEwipe to cross the screen as a vertical, diagonal, or horizontal "front," with a specified angle: VL_EV1_WIPETYPE_SINGLEwipe in two orthogonal directions simultaneously (two single wipes at the same time): VL_EV1_WIPETYPE_DOUBLEwipe in two orthogonal directions, with the perpendicular position locked to the normal, or in-line position: VL_EV1_WIPETYPE_CORNERFor example, the following fragment specifies that a fade is to be performed:VLControlType val;
val.intVal = VL_EV1_WIPETYPE_FADE;
vlSetControl(vlSvr, vlPath, blend_node, VL_EV1_WIPE_TYPE,
             val); Fades, tiles, and wipes go from all-foreground (VL_EV1_WIPE_POSN=0) to all-background (VL_EV1_WIPE_POSN=1000), unless VL_EV1_WIPE_INVERT control is set, in which case they go from all-background (VL_EV1_WIPE_POSN = 0) to all-foreground (VL_EV1_WIPE_POSN = 1000). IDREF="54850" TYPE="TABLE"Table 15-4 summarizes controls common to all wipe types.COLUMNS="3"LBL="15-4"Table 15-4 ID="54850"Controls for Fades, Tiles, and WipesLEFT="0" WIDTH="144"Control LEFT="150" WIDTH="90"ValuesLEFT="245" WIDTH="252"SetsLEFT="0" WIDTH="144"VL_EV1_WIPE_POSNtype fractValLEFT="150" WIDTH="90"numerator (0,1000)denominator (1000)LEFT="245" WIDTH="252"Amount of progress of wipe, from none (numerator = 0) to full 
(numerator = 1000). LEFT="0" WIDTH="144"VL_EV1_WIPE_REPTtype intValLEFT="150" WIDTH="90"(0,15)LEFT="245" WIDTH="252"Number of repetitions of pattern in direction of wipe, usually 
louvers on single, corner, or double wipe, and length of one 
side of rectangles for a tile wipe.Note: This control does not apply to fades.LEFT="0" WIDTH="144"VL_EV1_WIPE_INVERTtype intValLEFT="150" WIDTH="90"(0,1)0 = off, 1 = onLEFT="245" WIDTH="252"Reversal of foreground and background regions of a wipe. 
When set to 0, wipes proceed from foreground (position = 
minimum) to background (position = maximum). When set to 
1, wipes proceed from background (position = minimum) to 
foreground (position = maximum).This value is buffered (does not go into effect) until another 
blending control is set.IDREF="82626" TYPE="TABLE"Table 15-5 summarizes the controls specific to wipes or that work differently for wipes. Some of these controls work in conjunction with each other.COLUMNS="3"LBL="15-5"Table 15-5 ID="82626"Galileo Video Controls Specific to WipesLEFT="0" WIDTH="122"Control LEFT="130" WIDTH="119"ValuesLEFT="255" WIDTH="261"SetsLEFT="0" WIDTH="122"VL_EV1_WIPE_ANGLEtype intValLEFT="130" WIDTH="119"VL_EV1_WIPEANGLE_EVL_EV1_WIPEANGLE_NEVL_EV1_WIPEANGLE_NVL_EV1_WIPEANGLE_NWVL_EV1_WIPEANGLE_WVL_EV1_WIPEANGLE_SWVL_EV1_WIPEANGLE_SVL_EV1_WIPEANGLE_SELEFT="255" WIDTH="261"Wipe vector direction, that is, the direction the wipe appears to be 
proceeding in as its position increases.Note: VL_EV1_WIPEANGLE_N and VL_EV1_WIPEANGLE_S 
do not work for the wipe types VL_EV1_WIPETYPE_DOUBLE 
and VL_EV1_WIPETYPE_CORNER on Galileo Video.LEFT="0" WIDTH="122"VL_EV1_WIPE_FUZZtype intValLEFT="130" WIDTH="119"(-8,7)LEFT="255" WIDTH="261"Sharpness of wipe transition band. As for 
VL_EV1_KEYER_DETAIL, -8 is most gradual, +7 is sharpest.LEFT="0" WIDTH="122"VL_EV1_WIPE_SYMMETRYtype intValLEFT="130" WIDTH="119"(0,1)0 = off, 1 = onLEFT="255" WIDTH="261"Wipe symmetry (on or off) so that wipe proceeds in both 
directions at once from the center line. Effect depends on type of 
wipe: no effect for fades or tiling; enables VL_EV1_WIPE_CENT 
for single, double, and corner wipes; enables 
VL_EV1_WIPE_CENT_PERP control for double and corner 
wipes.LEFT="0" WIDTH="122"VL_EV1_WIPE_POSN_PERPtype fractValLEFT="130" WIDTH="119"numerator (0,1000)denominator (1000)LEFT="255" WIDTH="261"Amount of progress of wipe, from none (numerator = 0) to full 
(numerator = 1000), along a direction perpendicular to normal 
wipe position VL_EV1_WIPE_POSN.LEFT="0" WIDTH="122"VL_EV1_WIPE_CENTtype fractValLEFT="130" WIDTH="119"numerator (0,1000)denominator (1000)LEFT="255" WIDTH="261"Offset that is center of a symmetrical wipe along wipe position. 0 
means center is where VL_EV1_WIPE_POSN is 0, and 1000 
means center is where VL_EV1_WIPE_POSN is 1000. For this 
control to work for single, double, and corner wipes, 
VL_EV1_WIPE_SYMMETRY must be on.LEFT="0" WIDTH="122"VL_EV1_WIPE_CENT_PERPtype fractValLEFT="130" WIDTH="119"numerator (0,1000)denominator (1000)LEFT="255" WIDTH="261"Offset that is center of a symmetrical wipe along a perpendicular 
wipe position. 0 means center is where VL_WIPE_POSN_PERP is 
0, and 1000 means center is where VL_WIPE_POSN_PERP is 
1000. VL_WIPE_SYMMETRY must be on for this control to work 
for double and corner wipes.LEFT="0" WIDTH="122"VL_EV1_WIPE_REPT_PERPtype intValLEFT="130" WIDTH="119"(0,15)LEFT="255" WIDTH="261"Number of repetitions perpendicular to wipe direction for single, 
double, and corner wipes, and length of other side of rectangles 
for tile wipe.IDREF="77273" TYPE="GRAPHIC"Figure 15-10 shows relationships between the Galileo Video keying controls.FILE="15-10.online.bw" POSITION="INLINE" SCALE="FALSE"LBL="15-10"Figure 15-10 ID="77273"Galileo Video Keying ControlsLBL="" HELPID=""VL Blending ExamplesThis section explains two example programs from /usr/people/4Dgifts/examples/dmedia/video/vl:simpleblend.csimplewipe.cBecause the programs are lengthy, they are not duplicated here. Look at the source code in a separate window, or print them out to look at while you read their descriptions.CautionTo simplify the code, these examples do not check returns. The programmer should, however, always check returns. LBL="" HELPID=""Blending Video and GraphicsAPP="/usr/sbin/jot /usr/people/4Dgifts/examples/dmedia/video/vl/simpleblend.c" PARMS=""simpleblend.c, which blends video with graphics and outputs it to both a graphics window and video out. The program:constrains the window's aspect ratiochecks that the device the user requested is in the device listsets up a path between the source (screen) and the drain (video)adds video source and a screen drain nodes to create the blendsets the keyer mode, keyer source, and blend controlsdisplays the drain window and sets the video to appear in itspecifies appropriate event handlingstarts data transferspecifies that video is updated if the user changes the size of the windowLBL="" HELPID=""Creating a Simple Wipe EffectLike simpleblend.c, APP="/usr/sbin/jot /usr/people/4Dgifts/examples/dmedia/video/vl/simpleblend.c" PARMS=""APP="/usr/sbin/jot /usr/people/4Dgifts/examples/dmedia/video/vl/simplewipe.c" PARMS=""simplewipe.c
 blends video with graphics and outputs it to a graphics window and video out. When the user presses the 
w key, it executes a wipe. Specifically, in addition to doing everything that simpleblend.c does, simplewipe.c:sets up blend parameters (VL_WIPE_TYPE, VL_WIPE_ANGLE, VL_WIPE_CENT, VL_WIPE_REPT)calls a loop that sets the keyer mode to spatial and sets the position in the loop; doswitchloop() and dowipe() execute the loopchecks for the 
w key and calls dowipe(), which in turn calls doswitchloop()LBL="IV"ID="24419"IndigoVideo ProgrammingIDREF="58977" TYPE="TITLE"Chapter 16, "Introduction to IndigoVideo Programming,"introduces the IndigoVideo library and gives an overview of the features of the IndigoVideo board.IDREF="91930" TYPE="TITLE"Chapter 17, "Getting Started with the IndigoVideo Library,"describes basic concepts for using the IndigoVideo board, and presents a sample video application that displays live video input in a window.IDREF="48005" TYPE="TITLE"Chapter 18, "Controlling the IndigoVideo Input Window," explains how applications can position and scale the video input. It also explains how to select different video sources, formats, and broadcast standards.IDREF="50847" TYPE="TITLE"Chapter 19, "Producing IndigoVideo Output,"explains how to encode a portion of your screen to video in real time. This chapter also covers single-frame output.IDREF="46637" TYPE="TITLE"Chapter 20, "Capturing Video from IndigoVideo,"explains how to capture frames of video to memory.IDREF="61431" TYPE="TITLE"Chapter 21, "Handling IndigoVideo Events,"explains how to handle video events, such as video parameters being changed by another process.IDREF="59277" TYPE="TITLE"Chapter 22, "Using the IndigoVideo Utilities," explains how to use the IndigoVideo end-user tools.LBL="16"ID="58977"Introduction to IndigoVideo ProgrammingThe IndigoVideo board provides video input and output for IRIS Indigo workstations equipped with Entry Graphics. The IndigoVideo Library provides a software interface to the IndigoVideo board, enabling applications to:display live video in a windowcapture live video to system memoryencode graphics to video in real timeproduce high-quality single-frame video outputThe IndigoVideo Library provides a C language API; this part of this guide describes the use of those routines. The IndigoVideo library header file, /usr/include/svideo.h, is compatible with the ANSI-C standard; however, the IndigoVideo library does not comply with ANSI-C namespace conventions.For an introduction to basic video concepts, read IDREF="17607" TYPE="TITLE"Chapter 11, "Video Basics," in IDREF="13297" TYPE="TITLE"Part III, "Video Programming," of this guide, and consult the Glossary at the end of this guide for definitions of video terms.IDREF="24419" TYPE="TITLE"Part IV, "IndigoVideo Programming,"presents the IndigoVideo library from a task-oriented perspective. Chapters are organized to cover topics in roughly the order you would be concerned about them as you write IndigoVideo programs. LBL="" HELPID=""Using the IndigoVideo ExamplesThe code examples in this part of this guide are online in the directory /usr/people/4Dgifts/examples/dmedia/video/indigovideo. The README file in that directory gives an overview of the programs and instructions for compiling and running them. You must use the 4Dgifts login to compile and run these programs.LBL="" HELPID=""References for Video ProgrammingFor more information on video, consult these references:Television Engineering Handbook, Benson, K. Blair, McGraw-Hill (New York) 1986.Television Technology: Fundamentals and Future Prospects, Noll, A. Michael, Artech House (MA) 1988.Lenk's Video Handbook: Operation and Troubleshooting, Lenk, John D., McGraw-Hill (New York), 1991.Basic Television and Video Systems, Fifth Edition, Grob, Bernard, McGraw-Hill (New York), 1984.LBL="17"ID="91930"Getting Started with the IndigoVideo LibraryThis chapter describes the features and capabilities of the IndigoVideo board and presents an annotated sample program that displays live video input in a graphics window to help you get started with the IndigoVideo Library.In this chapter:IDREF="56784" TYPE="TITLE""IndigoVideo Basics" describes the features and I/O functions of the IndigoVideo board.IDREF="23831" TYPE="TITLE""A Simple Program for Getting Started with IndigoVideo" presents a simple IndigoVideo application that demonstrates the use of the most basic IndigoVideo Library routines.For an introduction to basic video concepts, read IDREF="17607" TYPE="TITLE"Chapter 11, "Video Basics," in IDREF="13297" TYPE="TITLE"Part III, "Video Programming," and consult the Glossary at the end of this guide for definitions of video terms.LBL="" HELPID=""ID="56784"IndigoVideo BasicsThis section describes the IndigoVideo board and its I/O interface.LBL="" HELPID=""IndigoVideo The IndigoVideo board attaches to the Indigo Entry Graphics board in your Indigo workstation. On the back of the board is an I/O panel with a number of video connectors, which you can use to attach video devices to the IndigoVideo board. The IndigoVideo board translates video signals into a form usable by the IRIS Indigo workstation. It also does the reverse, translating graphics from the IRIS Indigo display into video signals.LBL="" HELPID=""Broadcast StandardsThe IndigoVideo hardware supports the two most popular broadcast standards, the National Television Systems Committee (NTSC) composite video standard and the Phase Alternated by Line (PAL) standard.LBL="" HELPID=""Video and Videotape FormatsYou can record video signals in a variety of videotape formats: S-VHS and Hi-8mm are two examples of common videotape formats. Although IndigoVideo doesn't distinguish between individual tape formats, you need to know what kind of connector your video equipment uses. The IndigoVideo board has two kinds of input connectors: composite and S-Video.Most home VCRs use composite connectors. S-Video, on the other hand, carries the color and brightness components of the picture on separate wires; hence, S-Video connectors are also called Y/C connectors. Most S-VHS and Hi-8mm VCRs feature S-Video connectors.In addition to composite and S-Video output, the IndigoVideo board provides analog RGB output, in which the image data is carried as three separate components: red, green, and blue intensity. The RGB output can be used in conjunction with an external encoder to produce formats that IndigoVideo does not support, or it can be used to drive an external video monitor.See IDREF="14650" TYPE="TITLE""Videotape Formats" in Chapter 11 for more information about videotape formats, which are summarized in IDREF="26172" TYPE="TABLE"Table 11-1 in that chapter.LBL="" HELPID=""ID="98955"IndigoVideo Data FormatsThis section describes the image data formats used by IndigoVideo. IndigoVideo uses three formats for image data: 32-bit RGB, 8-bit RGB, and 4:1:1 YUV. You should note that data coming from or going to the IndigoVideo board is always ordered top-to-bottom. The IRIS Indigo, on the other hand, commonly stores image data with lines ordered bottom-to-top. Both IRIS Indigo and IndigoVideo store pixels from left to right within lines.ID="Media4-1GS1"In the diagrams that follow, the bits are numbered from right to left, with the least significant (rightmost) bit numbered zero.LBL="" HELPID=""32-bit RGBID="Media4-1GS2"IndigoVideo uses 32-bit RGB format for single frame output, and it is produced by some of the video capture convenience routines. This format could also be called 24-bit RGB data, since each 32-bit pixel consists of 24 bits of RGB data and 8 bits of unused space.The format of these pixels is shown in IDREF="16609" TYPE="GRAPHIC"Figure 17-1.FILE="RGB32.ai" POSITION="INLINE" SCALE="FALSE"LBL="17-1"Figure 17-1 ID="16609"Format of 32-bit RGB Pixels32-bit RGB is produced by the functions svCaptureOneFrame(), svYUVtoRGB(), and svRGB8toRGB32(). The IRIS GL lrectwrite() function also uses this format, but it expects rows of pixels to be ordered bottom-to-top unless the default ordering has been changed using the IRIS GL pixmode() function. As noted above, data sent to the IndigoVideo board using the svPut24Frame() function should be ordered top-to-bottom. The svYUVtoRGB() and svRGB8toRGB32() functions will return data ordered top-to-bottom unless the invert parameter is TRUE.LBL="" HELPID=""8-bit RGBThe 8-bit RGB format stores 2 bits of blue and 3 bits each of red and green in an 8-bit pixel. The format of the 8-bit RGB pixels is shown in ID="Media4-1GS3"IDREF="32988" TYPE="GRAPHIC"Figure 17-2.FILE="RGB8.ai" POSITION="INLINE" SCALE="FALSE"LBL="17-2"Figure 17-2 ID="32988"Format of 8-bit RGB PixelsWhen you capture 8-bit RGB frames using IndigoVideo, you receive them with the fields uninterleaved, that is, the data contains all of the even lines (from top to bottom) followed by all of the odd lines.LBL="" HELPID=""4:1:1 YUVThe 4:1:1 YUV format is much closer in form to the original video signal than the RGB formats described above. The YUV format is also much more complicated. Each pixel is described by three components: one luminance value (Y) and a pair of chrominance values (U and V); however, there are four luminance samples to each pair of chrominance samples has (hence 4:1:1 YUV). This means that sets of 4 pixels share the same chrominance values but have individual luminance values. ID="Media4-1GS4"To further complicate matters, data from odd and even lines are interleaved within individual 32-bit words. The format of these words is shown in IDREF="51791" TYPE="GRAPHIC"Figure 17-3.FILE="YUV.ai" POSITION="INLINE" SCALE="FALSE"LBL="17-3"Figure 17-3 ID="51791"Format of YUV Data WordsEach 32-bit word consists of 8 bits of unused space, followed by one Y sample (8 bits) and 2 bits each of U and V from the odd line, then a Y sample and 2 bits each of U and V from the even line. By collecting the chrominance data from 4 consecutive 32-bit words, you get a pair of chrominance samples for the odd line, and a pair of chrominance samples for the even line. The upper 2 bits of U and V come in the first word, the next highest set of bits comes in the next word, and so on. By putting these chrominance samples together with the 4 odd and 4 even luminance samples, you can reconstruct 8 YUV pixels. You can derive 8-bit red, green, and blue values from the YUV values using the following formulas:R = 1.164 name='times' font=symbol charset=fontspecific code=180
	TeX='\times ' descr='[times]' (Y - 16) + 1.596 name='times' font=symbol charset=fontspecific code=180
	TeX='\times ' descr='[times]' (V - 128)G = 1.164 name='times' font=symbol charset=fontspecific code=180
	TeX='\times ' descr='[times]' (Y - 16) - 0.392 name='times' font=symbol charset=fontspecific code=180
	TeX='\times ' descr='[times]' (U - 128) -0.813 name='times' font=symbol charset=fontspecific code=180
	TeX='\times ' descr='[times]' (V - 128)B = 1.164 name='times' font=symbol charset=fontspecific code=180
	TeX='\times ' descr='[times]' (Y - 16) + 2.017 name='times' font=symbol charset=fontspecific code=180
	TeX='\times ' descr='[times]' (U - 128)As with the other forms of video data, lines of YUV data are ordered top-to-bottom.LBL="" HELPID=""IndigoVideo I/OIDREF="25494" TYPE="GRAPHIC"Figure 17-4 shows the IndigoVideo I/O panel, which is accessible from the back of the Indigo workstation.FILE="IndigoVideoports.ai" POSITION="MARGIN" SCALE="FALSE"LBL="17-4"Figure 17-4 ID="25494"IndigoVideo I/O PortsLBL="" HELPID=""IndigoVideo Board Input Port SpecificationsAt the top of the panel are three pairs of input connectors, labeled 1, 2, and 3. Each pair is made up of a composite video connector and an S-Video connector. You can use only one of the connectors in a given pair at a timename='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'if you connect a composite video source and an S-Video source to the same input pair, the IndigoVideo hardware will not be able to decode either source properly.The three input port pairs on the IndigoVideo board have the following specifications:Composite VIDEO: 1.0 Vpp, 75\xbd , sync negativeS-VIDEO: DIN 4-pinBelow the input connectors, there are three output connectorsname='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'one composite connector, one S-Video connector, and one RGB connector.LBL="" HELPID=""IndigoVideo Board Output Port SpecificationsThe three output ports on IndigoVideo board have the following specifications:Composite VIDEO output: 1.0 Vpp, 75\xbd , sync negativeS-VIDEO output: DIN 4-pinRGB output: 15 pin D-connector, .714 Vpp, 75\xbd The sync specification is:Sync: RS-170 television sync: -4Vpp @ 75\xbd IDREF="61134" TYPE="GRAPHIC"Figure 17-5 shows video equipment connected to the I/O panel of the IndigoVideo board.FILE="IndigoVideoEquip.ai" POSITION="INLINE" SCALE="FALSE"LBL="17-5"Figure 17-5 ID="61134"Connecting Video Equipment to the Indigo Video BoardThe next two sections briefly describe how the IndigoVideo board operates.LBL="" HELPID=""Video InputFive basic steps are used to convert a video input signal to pixels that can be displayed on the Indigo workstation:The IndigoVideo board synchronizes with the video source by looking for sync information in the video signal (see IDREF="18458" TYPE="GRAPHIC"Figure 11-3).The analog video signal is converted to a digital signal.The digital signal is decoded into a series of pixels in 4:1:1 YUV colorspace.A colorspace is simply a way of encoding color information. In RGB colorspace, a color is defined by its red, green, and blue levels. In YUV colorspace, a color is defined by its luminance and by two chrominance components that determine its color. 4:1:1 means that the chrominance components are subsampled; that is, for each four Y samples, there is only one U sample and one V sample.The YUV pixels are converted to 24-bit RGB pixels.The 24-bit pixels are dithered down to 8-bit pixels.Before they can be displayed on the Indigo workstation, the video pixels must be reduced to 8-bit RGB pixels. IndigoVideo accomplishes this by dithering the video image. Dithering uses a small number of colors to simulate a larger number of colors using patterns of different colored pixels; for example, a pattern of red and yellow pixels can be used to represent orange.The video image is scaled down, if necessary.LBL="" HELPID=""Video OutputIn live video output mode, IndigoVideo receives 8-bit RGB pixels from the Indigo graphics subsystem. These 8-bit pixels are converted to 24-bit RGB pixels using a set of look-up tables. These 24-bit pixels drive a set of digital-to-analog converters, which produce the analog RGB output, and also a digital encoder, which produces composite and S-Video output. The digital encoder also generates a composite sync signal, which can be used to synchronize video devices with the IndigoVideo board. The timing for this signal can be generated by the IndigoVideo board itself, or it can be derived from either the input source or from a separate composite sync signal connected to input #3.LBL="" HELPID=""ID="23831"A Simple Program for Getting Started with IndigoVideoThis section presents a sample program to help you get started with writing a simple video application. Source code for the sample programs is located in the /usr/people/4Dgifts/examples/dmedia/video/indigovideo directory, which is also referred to as 4Dgifts. You must have the svideodev option installed to get the gifts source, svideodev.sw.gifts. You also must have the IRIS Development Option, dev, and the C language software, c, loaded before you can compile the sample programs (use the versions command to find out which software is loaded on your system). See the Svideo Release Notes for complete system software requirements. Login as 4Dgifts to compile the examples and copy files to your home directory before modifying them. IDREF="57654" TYPE="TEXT"Example 17-1 contains a listing of simpleinput.c, a program that opens up a window and displays live video input. This process has five basic steps:Create a GL/X window. Open the video device.Set video parameters.Associate video input with the GL/X window.Wait for the user to quit.To compile simpleinput.c, enter: cc -o simpleinput simpleinput.c -lsvideo -lXext -lgl_sYou must link with the IndigoVideo Library (-lsvideo) to use the IndigoVideo software. The linking order is specific: -lsvideo must appear first, followed by -lXext to link with the X extensions Library.Programs that use IRIS GL windowing and event handling must include gl/gl.h and gl/device.h and must link with the shared IRIS GL (-lgl_s). Programs that use that use the X Window System must include X11/Xlib.h before including svideo.h, and must link with the X11 shared library (-lX11_s).NoteThis program uses IRIS GL windowing and event handling; however, a mixed-model GL/X window and X11 event handling is recommended for greater portability. NoteCPU-intensive programs that use IRIS GL windowing and event handling should do a qtest() followed by sginap() if no events are waiting. LBL="17-1"Example 17-1 ID="57654"Opening a Window to Display Live Video Input: simpleinput,c #include <stdlib.h>
#include <svideo.h>
#include <gl/gl.h>
#include <gl/device.h>

main()
{
    long win, dev, params[2];
    short val;
    SVhandle V;

    /* Step 1: Open window */
    prefsize(SV_NTSC_XMAX, SV_NTSC_YMAX);
    win = winopen("video test");

    /* Step 2: Open video device */
    if ((V = svOpenVideo()) == NULL) {
        svPerror("open");
        exit(1);
    }

    /* Step 3: Set video source */
    params[0] = SV_SOURCE;
    params[1] = SV_SOURCE1;
    if (svSetParam(V, params, 2) < 0) {
        svCloseVideo(V);
        svPerror("set param");
        exit(1);
    }

    /* Step 4: Associate video input with window */
    if (svBindGLWindow(V, win, SV_IN_REPLACE) < 0) {
        svPerror("bind gl window");
        svCloseVideo(V);
        exit(1);
    }
    /* Step 5: wait for user to quit */
    qdevice(ESCKEY);
    qdevice(WINQUIT);
    qdevice(WINSHUT);
    while (1) {
        dev = qread(&val);
        switch (dev) {
        case ESCKEY:
            if (val)    /* exit on key up */
                break;
        case WINQUIT:
        case WINSHUT:
            svCloseVideo(V);
            exit(0);
            break;
        }
    }
}This program isn't very flexiblename='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'it assumes that a composite NTSC video source is connected to input #1name='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'however, it demonstrates the basic principles of programming with the IndigoVideo Library. Here's a step-by-step description of what the program does:The program opens a window in which to display the video. First it specifies the preferred size for the window using prefsize(), then it calls winopen() to create the window. The winopen() routine returns a window identifier, which is used in step 4 to associate the video input with the window. The symbolic constants SV_NTSC_XMAX and SV_NTSC_YMAX represent the dimensions of an NTSC video frame. The program opens the video device. The svOpenVideo() routine returns a video handle that is passed to other routines in the IndigoVideo Library. This step also demonstrates the use of the IndigoVideo error reporting routine, svPerror(). The program sets up the IndigoVideo board to receive video input from source #1. This is the default input, so you could omit this step; however, this step is included to demonstrate the use of the svSetParam() routine. The svSetParam() routine can be used to select input source, broadcast standard, and more. Note that it is not necessary to set these parameters before associating video input with a window; video parameters can be changed at any time. Also note that once set up, live video display requires no CPU intervention.One of the arguments to svSetParam() is an array of long integers. Even-numbered elements of this array represent parameters to be changed, and the corresponding odd-numbered elements represent parameter values. In this program, the parameter being set is represented by the symbolic constant SV_SOURCE, and the value is represented by the symbolic constant SV_SOURCE1.After the setup is complete, the program attaches the video input to the window, using the svBindGLWindow() routine. This routine takes three arguments: a video handle, a window identifier, and a third argument indicating how to display the video input. The sample program uses the symbolic constant SV_IN_REPLACE for the third argument, indicating that video should replace the contents of the window. The other possible values for the third argument are SV_IN_UNDER, for video underlay, SV_IN_OVER, for video overlay, and SV_IN_OFF, to deactivate video display in the window.Finally, the program enters an IRIS GL event handling loop, displaying video input until the user elects to quit by pressing the <Esc> key. Once set up, live video display requires no CPU intervention. When the user exits, the program closes the video device by using the svCloseVideo() routine. This routine deallocates the data structures associated with the video handle.More sophisticated event handling methods are presented later in this guidename='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'new development should use X event handling rather than IRIS GL event handling. See IDREF="61431" TYPE="TITLE"Chapter 21, "Handling IndigoVideo Events,"for more information on handling events.LBL="18"ID="48005"Controlling the IndigoVideo Input WindowIDREF="91930" TYPE="TITLE"Chapter 17, "Getting Started with the IndigoVideo Library," showed you how to create a video input window. This chapter presents more details on the process and demonstrates how to control various aspects of the video display.In this chapter:IDREF="22182" TYPE="TITLE""Setting Input Parameters" explains how to set input parameters that provide information about the input source and signal for the IndigoVideo board.IDREF="58105" TYPE="TITLE""Querying Video Parameters" describes how to obtain information about the IndigoVideo board status.IDREF="66827" TYPE="TITLE""Positioning and Scaling the Video Input" explains how to create and configure the video input window.IDREF="49701" TYPE="TITLE""Preventing Other Programs from Using Video" explains how to take control of the IndigoVideo board for exclusive use.IDREF="32750" TYPE="TITLE""Combining Video and Graphics" provides suggestions for using video with graphics, including overlays and underlays.LBL="" HELPID=""ID="22182"Setting Input ParametersYou can use the svSetParam() routine to set a number of different video parameters. This section describes some of the parameters that affect the video input window. See svGetParam(3V) for a complete list of parameters.svSetParam() takes three arguments: a video handle, an array of long integers specifying parameters and values, and an argument specifying the length of the array. The values in the array are interpreted in pairs: the first member of each pair represents the parameter to be changed; the second member represents the new value for that parameter. The code fragment in IDREF="29703" TYPE="TEXT"Example 18-1 sets up the IndigoVideo board to receive PAL input over an S-Video connector.LBL="18-1"Example 18-1 ID="29703"Setting up the IndigoVideo Board for PAL Input SVhandle V;
long param[4];
/* . . . */ 
param[0] = SV_BROADCAST;
param[1] = SV_PAL;
param[2] = SV_VIDEO_MODE;
param[3] = SV_SVIDEO;
svSetParam(V, param, 4);LBL="" HELPID=""Selecting an Input SourceYou can plug up to three video inputs into the IndigoVideo board and select between them by changing the SV_SOURCE parameter with svSetParam(). Set SV_SOURCE to SV_SOURCE1, SV_SOURCE2, or SV_SOURCE3. These constants correspond to the input connectors labeled 1, 2, and 3. The code fragment in IDREF="23994" TYPE="TEXT"Example 18-2 demonstrates setting up the input source.LBL="18-2"Example 18-2 ID="23994"Selecting a Video Input Sourcevoid setSource(SVhandle V, int source)
{
long param[2];
if (source < 1 || source > 3) {
/* error */
}
param[0] = SV_SOURCE;
switch (source) {
case 1: param[1] = SV_SOURCE1;
break;
case 2: param[1] = SV_SOURCE2;
break;
case 3: param[1] = SV_SOURCE3;
break;
}
svSetParam(V, param, 2);
}LBL="" HELPID=""Selecting the Input Signal TypeTo enable the IndigoVideo board to display video input correctly, you must set it to the correct broadcast standard and format. These may be set using svSetParam(). The broadcast standard parameter, SV_BROADCAST, may be set to one of two values: SV_NTSC for NTSC video, or SV_PAL for PAL video. The SV_VIDEO_MODE parameter controls input format; it can be set to SV_COMP for composite video input, or SV_SVIDEO for S-Video input.You can also set the default video mode and broadcast standard for each input source using the Video Control Panel. This procedure is explained in IDREF="59277" TYPE="TITLE"Chapter 22, "Using the IndigoVideo Utilities."LBL="" HELPID=""Freezing and Restarting Video InputFreeze the video input by setting the SV_FREEZE parameter to TRUE. Doing so holds the current frame in the IndigoVideo frame buffer. Setting SV_FREEZE to FALSE restarts live input.LBL="" HELPID=""ID="58105"Querying Video ParametersYou can determine the value of a parameter by using the svGetParam() routine. This routine takes exactly the same arguments as svSetParam(), but instead of reading new parameter values from the array, it fills in the current parameter values in the appropriate places.The code fragment in IDREF="92868" TYPE="TEXT"Example 18-3 displays the number of the currently selected input source.LBL="18-3"Example 18-3 ID="92868"Getting the Input Source NumberSVhandle V;
long param[2];
int source;
/* ... */
param[0] = SV_SOURCE;
svGetParam(V, param, 2);
switch(param[1]) {
case SV_SOURCE1: source = 1;
                 break;
case SV_SOURCE2: source = 2;
                 break;
case SV_SOURCE3: source = 3;
                 break;
}
printf("Current video source is %d\n", source);In addition to the input parameters already mentioned, two special read-only parameters can be used in conjunction with svGetParam() to get information about the video source (if any) connected to the current input: SV_SIGNAL_STDcan be used to query the broadcast standard used by the current source. The parameter value returned is one of:SV_NTSC for NTSC input SV_PAL for PAL input SV_NOSIGNAL if there is no signal on the current input source SV_SIGNAL_COLORcan be used to query whether the current input signal contains color information. The returned value is either TRUE or FALSE.LBL="" HELPID=""ID="66827"Positioning and Scaling the Video InputBy default, the IndigoVideo software displays video input at full resolution, with the origin of the video image at the upper left corner of the video input window. You can set the size and position of the video image using the svSetSize() and svWindowOffset() routines. If you call one of these routines after binding video to a window, you must rebind the video using either svBindWindow() or svBindGLWindow() for the change to take effect.NoteThe IndigoVideo positioning and scaling routines take arguments in pixels, not in IRIS GL coordinates. Because arguments to svSetSize() and svWindowOffset() are not in IRIS GL coordinates, they are not affected by the current IRIS GL transformation matrix. LBL="" HELPID=""Setting the Size of the Video ImageYou can set the size of the video image within certain constraints:The aspect ratio of the image (ratio of horizontal size to vertical size) must remain constant.The image can be scaled only in increments of 8 pixels of horizontal size and 6 pixels of vertical size.The image cannot be larger than the video frame size (640 by 480 pixels for NTSC, 768 by 576 pixels for PAL).To allow the user to resize the video input window, you can use the IRIS GL minsize(), maxsize(), and stepunit() routines to constrain the window to a useful size.IDREF="65351" TYPE="TEXT"Example 18-4 contains a listing of APP="/usr/sbin/jot /usr/people/4Dgifts/examples/dmedia/video/indigovideo/sizeinput.c" PARMS=""sizeinput.c
, in  /usr/people/4Dgifts/examples/dmedia/video/indigovideo, which implements a user-resizable video input window.LBL="18-4"Example 18-4 ID="65351"Creating a Scalable Video Input Window: sizeinput.c /*
 * Scalable GL Video Input Window
 */

#include <stdlib.h>
#include <svideo.h>
#include <gl/gl.h>
#include <gl/device.h>

main()
{
    short val;
    long win, dev, x, y;
    SVhandle V;

    /* Open window */
    minsize(80, 60);
    stepunit(8, 6);
    maxsize(SV_NTSC_XMAX, SV_NTSC_YMAX);
    keepaspect(SV_NTSC_XMAX, SV_NTSC_YMAX);
    win = winopen("video in");
    if ((V = svOpenVideo()) == NULL) {
        svPerror("open");
        exit(1);
    }
    getsize(&x, &y); 
    svSetSize(V, x, y);
    if (svBindGLWindow(V, win, SV_IN_REPLACE) < 0) {
        svPerror("bindwindow");
        svCloseVideo(V);
        exit(1);
    }

    /* Event loop */
    qdevice(ESCKEY);
    qdevice(WINQUIT);
    qdevice(WINSHUT);
    while (1) {
        dev = qread(&val);
        switch (dev) {
        case REDRAW:
            getsize(&x, &y); /* may have been resized */
            svSetSize(V, x, y);
            /* Re-bind window to scale input */
            if (svBindGLWindow(V, win, SV_IN_REPLACE) < 0){
                svPerror("bindwindow");
                svCloseVideo(V);
                exit(1);
            }
            break;

        case ESCKEY:
        case WINQUIT:
        case WINSHUT:
            svCloseVideo(V);
            winclose(win);
            exit(0);
            break;
        }
    }
}In the preceding example, the window is constrained to sizes supported by the IndigoVideo software. The event loop handles the resizing of the video input. When the program receives a REDRAW event, which could indicate a size change, it determines the new size of the window, calls svSetSize() to scale the video input appropriately, and rebinds the video input to the window.If you use svSetSize() to specify a size that IndigoVideo cannot produce, it will select the closest possible size. To determine what size will result from a given pair of arguments, use the svQuerySize() routine. The code fragment in IDREF="63176" TYPE="TEXT"Example 18-5 finds the closest match to the desired size and resizes the window accordingly.LBL="18-5"Example 18-5 ID="63176"Approximating the Requested Video Window Size SVhandle V;
long win;
int x, y, new_x, new_y;
/* ... */
svQuerySize(V, x, y, &new_x, &new_y);
prefsize(new_x, new_y);
winconstraints();
svSetSize(V, new_x, new_y);
svBindGLWindow(V, win, SV_IN_REPLACE);LBL="" HELPID=""Positioning the Video ImageBy default, the origin of the live video image is at the upper left corner of the live video input window. You can change this position using the svWindowOffset() routine. To do this, specify vertical and horizontal offsets, in pixels, from the upper left corner of the input window to the upper left corner of the video image. These values may be negative, meaning that you can use a small window to "pan" across the video image. The code fragment in IDREF="42939" TYPE="TEXT"Example 18-6 demonstrates the use of svWindowOffset():LBL="18-6"Example 18-6 ID="42939"Specifying a Video Window Offset SVhandle V;
int xoffset, yoffset, win;
/* ... */
svWindowOffset(V, xoffset, yoffset);
svBindGLWindow(V, win, SV_IN_REPLACE);NoteThe live video image cannot be positioned such that any part of the image is off the edge of the screen. Thus, if you have a window in the upper left corner of the screen, negative window offsets will be ignored. The entire video image must remain within the screen dimensions, even if you are viewing only a small portion of the image. LBL="" HELPID=""ID="49701"Preventing Other Programs from Using VideoTo prevent other programs from changing video parameters while your program is running, you can request exclusive use of the IndigoVideo board by calling the svUseExclusive() routine with a value of TRUE for the onoff parameter, as demonstrated in IDREF="77007" TYPE="TEXT"Example 18-7.LBL="18-7"Example 18-7 ID="77007"Getting Exclusive Use of the IndigoVideo Board SVhandle V;
int status;
/* ... */
status = svUseExclusive(V, TRUE, SV_INPUT) 
if (status == -1) {
svPerror("Couldn't get exclusive use");
/* error handling*/
}While one process has exclusive use of the IndigoVideo board, any other process that makes a call to the IndigoVideo Library will receive an error.To get out of exclusive use mode, call svUseExclusive() FALSE.LBL="" HELPID=""ID="32750"Combining Video and GraphicsYou can combine video and graphics in a window using either of two modes: video underlay or video overlay. In video replace mode, which has been used in this guide until now, all of the pixels in the video image are displayed. In video underlay and overlay modes, video pixels replace only selected graphics pixels. In video underlay mode, the decision whether to display a video pixel or a graphics pixel at a given location is based on the value of the graphics pixel. In video overlay mode, this decision is based on the value of the video pixelname='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'meaning that in video underlay mode you specify which parts of your graphic image should be replaced by video; in video overlay mode you specify which parts of the video image should be replaced by graphics.Video underlay mode is typically used for such applications as video titling, where you want to display text or graphics superimposed over video. See IDREF="49749" TYPE="GRAPHIC"Figure 15-6 on IDREF="49749" TYPE="TEXT"page 368 in IDREF="57047" TYPE="TITLE"Chapter 15, "VL Blending," for an illustration of this technique, but use the method described in this section for producing this effect with the IndigoVideo board.You might use video overlay mode for a "TV weatherman" effect, superimposing a live video image over a computer-generated backdrop. See IDREF="27748" TYPE="GRAPHIC"Figure 15-8 on IDREF="27748" TYPE="TEXT"page 370 in IDREF="57047" TYPE="TITLE"Chapter 15, "VL Blending,"for an illustration of this technique, but use the method described in this section for producing this effect with the IndigoVideo board.LBL="" HELPID=""Video Underlay ModeIn video underlay mode, video pixels replace graphics pixels that have a value of zero. In IRIS GL programs that use RGB mode, video pixels replace black graphics pixels. In X programs, and in IRIS GL programs that use color map mode, video replaces the color that is mapped to zero. There are advantages and disadvantages to both IRIS GL modes.LBL="" HELPID=""RGB Mode IRIS GL ProgramsRGB mode allows you to take better advantage of the IRIS GL's special 3D effects such as lighting and shading; however, all black pixels are replaced by video, so you can't display black objects over video. Furthermore, in RGB mode, the Indigo workstation simulates 24-bit color using a dithering algorithm, which produces several black pixels in any expanse of a dark color. This means that dark objects appear partially transparent, which is usually the opposite of the desired effect. This effect is particularly noticeable in double-buffered mode. Getting the effect you want in RGB mode can require some experimentation.A detailed description of IRIS GL lighting and shading routines is beyond the scope of this guide, but here are some hints for producing good-looking graphics in RGB mode:When drawing Gouraud shaded polygons, use fairly light colorsname='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'they appear more opaque than darker colors because light colors do not dither to black.When drawing lighted objects, use more than one light. With one light, objects are defined by the contrast between light and shadow; however, in video underlay mode, the shadows will look transparent. If you use two lights of contrasting colors on opposite sides of an object, you can define the object using the contrast between the two colors. For example, light a sphere from above using a white light, and fill the shadow by lighting it from below using a blue light. In some cases, using more than one light can affect the graphics performance.The Graphics Library Programming Guide covers Gouraud shading and IRIS GL lighting in the chapters "Display and Color Modes," and "Lighting."LBL="" HELPID=""Color Map Mode IRIS GL ProgramsColor map mode provides a certain flexibility in that it allows you to use any color you want without the side effects dithering can produce; however, it is much more difficult to produce lighted or Gouraud shaded polygons in color map mode. For applications that do not require these effects, you're better off using color map mode. The Graphics Library Programming Guide chapters listed in the previous section also describe lighting and Gouraud shading in color map mode.IDREF="16406" TYPE="TEXT"Example 18-8 demonstrates the use of video underlay mode with IRIS GL color map mode. The program effectively clips the video input to a circle by drawing a circle of color zero on a white background.LBL="18-8"Example 18-8 ID="16406"Using IndigoVideo Underlay Mode#include <gl/gl.h>
#include <svideo.h>
#include <gl/device.h>

void 
drawScene(void)
{
color(7);
clear();
color(0);
arcf(320.0, 240.0, 200.0, 1.0, 0.0); 
}

main(void)
{
long win, dev;
short val;
SV_nodeP V;
prefsize(640, 480);
win = winopen("Video underlay test");
ortho2(1, 640, 1, 480);
if ((V = svOpenVideo()) == NULL) {
svPerror("open video");
exit(1);
}
if (svBindGLWindow(V, win, SV_IN_UNDER) < 0) {
svPerror("bind window");
svCloseVideo(V);
exit(2);
}
drawScene();
qdevice(ESCKEY);
qdevice(WINQUIT);
qdevice(WINSHUT);
while(1) {  
dev = qread(&val);
            switch (dev) {   
case ESCKEY:
case WINQUIT:   
case WINSHUT:     
    svCloseVideo(V);     
    exit(0);     
    break;   
case REDRAW:  
    drawScene(); 
    break;
}
}
}LBL="" HELPID=""Video Overlay Mode and Chroma KeyingIn video overlay mode, video pixel values can be "keyed" out. Video pixels replace graphics pixels, except where the value of a video pixel matches one of the keyed values. This allows you to select certain colors in the video image to be replaced by graphics.The IndigoVideo board has a 256-entry array of chroma keys, called the chroma key map. This array is indexed by pixel value (the 8-bit RGB pixels are treated as 8-bit unsigned integers), so entry zero in the chroma key map corresponds to pixels of value zero (black pixels). If the value of this entry is 1, black video pixels will be keyed out. If the value is zero, black video pixels will be displayed.You can load a new chroma key map by using the svLoadMap() routine. The chroma key map is passed to svLoadMap() as a 256-entry array of rgb_tuple structures, which are red, green, blue triplets. The red portion of the array is used for the chroma key map, and the rest of the array is ignored. The code fragment in IDREF="12387" TYPE="TEXT"Example 18-9 keys out black pixels by turning on the chroma key for pixel value zero.LBL="18-9"Example 18-9 ID="12387"Using Chroma Keying to Key Out Black Pixels rgb_tuple chromamap[256];
SVhandle V;
/* ... */
chomamap.red[0] = 1;
svLoadMap(V, SV_CHROMA_KEY_MAP, chromamap);IDREF="25191" TYPE="TEXT"Example 18-10 and IDREF="82544" TYPE="TEXT"Example 18-11 contain listings of two 4Dgifts programs that work together. IDREF="25191" TYPE="TEXT"Example 18-10, APP="/usr/sbin/jot /usr/people/4Dgifts/examples/dmedia/video/indigovideo/voverlay.c" PARMS=""voverlay.c
, demonstrates how to use IndigoVideo overlay mode. IDREF="82544" TYPE="TEXT"Example 18-11, APP="/usr/sbin/jot /usr/people/4Dgifts/examples/dmedia/video/indigovideo/chromamap.c" PARMS=""chromamap.c
, demonstrates how to use the chroma key map to set chroma key entries that voverlay.c can use.LBL="18-10"Example 18-10 ID="25191"Using IndigoVideo Overlay Mode: voverlay.c /* 
 *  voverlay.c:
 *
 *           "pool" ball that "bounces" around a 2-d "surface". 
 *                RIGHTMOUSE stops ball
 *                MIDDLEMOUSE increases y velocity
 *                LEFTMOUSE increases x velocity
 *
 * Adapted to show IndigoVideo overlay mode. To use, also compile and run 
 * the chromamap.c example. As you set chroma key entries with chromamap,
 * the graphics generated by this program will begin to appear.
 */

#include <stdio.h>
#include <svideo.h>
#include <gl/gl.h>
#include <gl/device.h>

long xmaxscrn, ymaxscrn;         /* maximum size of screen in x and y       */

#define XMIN 100
#define YMIN 100
#define XMAX 900
#define YMAX 700

long xvelocity = 0, yvelocity = 0;

main()
{
    Device dev;
    short val;
    long sizex, sizey;

    initialize();
    while (TRUE) {
       while (qtest()) {
          dev = qread(&val);
          switch (dev) {
                case REDRAW:     /* redraw window re: move/resize/push/pop */
                    reshapeviewport();
                    ortho2(XMIN - 0.5, XMAX + 0.5, YMIN - 0.5, YMAX + 0.5);
                    drawball();
                    break;
                case LEFTMOUSE:                /* increase xvelocity */
                    if (xvelocity >= 0)
                        xvelocity++;
                    else
                        xvelocity--;
                    break;
                case MIDDLEMOUSE:        /* increase yvelocity */
                    if (yvelocity >= 0)
                        yvelocity++;
                    else
                        yvelocity--;
                    break;
                case RIGHTMOUSE:         /* stop ball */
                    xvelocity = yvelocity = 0;
                    break;
                case ESCKEY:
                    gexit();
                    exit(0);
            }
        }
        drawball();
    }
}

initialize() {
    SVhandle V;
    long win;

    xmaxscrn = getgdesc(GD_XPMAX)-1;
    ymaxscrn = getgdesc(GD_YPMAX)-1;
    prefposition(xmaxscrn/4,xmaxscrn*3/4,ymaxscrn/4,ymaxscrn*3/4);
    win = winopen("voverlay");
    winconstraints();

    doublebuffer();
    gconfig();
    shademodel(FLAT);

    ortho2(XMIN - 0.5, XMAX + 0.5, YMIN - 0.5, YMAX + 0.5);

    qdevice(ESCKEY);
    qdevice(LEFTMOUSE);
    qdevice(MIDDLEMOUSE);
    qdevice(RIGHTMOUSE);

    /* Open video device */
    if ((V = svOpenVideo()) == NULL) {
        svPerror("open");
        exit(1);
    }
    /* Associate video input with this window */
    if (svBindGLWindow(V, win, SV_IN_OVER) < 0) {
        svPerror("bindwindow");
        exit(1);
    }
}

drawball() {
    static xpos = 500,ypos = 500;
    long radius = 50;

    color(BLUE);
    clear();
    xpos += xvelocity;
    ypos += yvelocity;
    if (xpos > XMAX - radius ||
        xpos < XMIN + radius) {
        xpos -= xvelocity;
        xvelocity = -xvelocity;
    }
    if (ypos > YMAX - radius ||
        ypos < YMIN + radius) {
        ypos -= yvelocity;
        yvelocity = -yvelocity;
    }
    color(YELLOW);
    circfi(xpos, ypos, radius);
    swapbuffers();
}
IDREF="82544" TYPE="TEXT"Example 18-11 contains a listing of chromamap.c, a program that demonstrates how to manipulate the chroma key map for programs that use video overlay.LBL="18-11"Example 18-11 ID="82544"Using the Chroma Key Map: chromamap.c /*
 * chromamap.c
 *
 *  This program demonstrates how to manipulate the IndigoVideo
 *  chroma key map for programs that use the video overlay feature.
 *  It shows the colors that correspond to the 256 entries in the map,
 *  where 0 is the lower left corner and 255 is the upper right.
 *  Clicking the left mouse button over a color toggles the value for that
 *  entry. An X mark in a box means that color will be keyed out (i.e.,
 *  the underlying graphics will show through.) A pull-down menu can
 *  be used to clear or set all of the entries. The program initializes
 *  all entries to 0 when it starts up.
 *
 *  To demonstrate keying, compile and run the voverlay program in
 *  this directory. As you set entries in the key map, the graphics
 *  in voverlay will begin to appear. If you set all of the entries,
 *  only the graphics will appear. If you clear all of the entries,
 *  only video will appear.
 */

#include <stdio.h>
#include <gl/gl.h>
#include <gl/device.h>
#include <svideo.h>

static SVhandle V;
static SVcolorMap keymap;

static long rgb8to32[256];

#define grey9() cpack(0x00E0E0E0)
#define grey7() cpack(0x00B0B0B0)
#define grey5() cpack(0x00808080)
#define bordercolor()   grey5()

static long xsize, ysize;
static long xorg, yorg;
#define YSIZE           16
#define XSIZE           16
#define BORDERSIZE      0.25
#define MOUSEXMAP(x)    ( ((XSIZE+2*BORDERSIZE)*((x)-xorg))/(xsize) )
#define MOUSEYMAP(y)    ( ((YSIZE+2*BORDERSIZE)*((y)-yorg))/(ysize) )

static void
drawX(int i, int j)
{
    grey7();
    move2i(i,j);
    draw2i(i+1,j+1);
    move2i(i,j+1);
    draw2i(i+1,j);
}

static void
drawborder(int i, int j)
{
    bordercolor();
    move2i(i,j);
    draw2i(i+1,j);
    draw2i(i+1,j+1);
    draw2i(i,j+1);
    draw2i(i,j);
}

static void
drawcolor(int i, int j)
{
    cpack(rgb8to32[(j*XSIZE)+i]);
    rectfi(i,j,i+1,j+1);
}

static void
showmap(void)
{
    int i, j;

    /* Clear background */
    grey9();
    clear();

    ortho2(-BORDERSIZE, XSIZE+BORDERSIZE, -BORDERSIZE,YSIZE+BORDERSIZE);
    /* Draw colored boxes for the 256 RGB colors */
    for (j=0; j<YSIZE; j++) {
        for (i=0; i<XSIZE; i++) {
            drawcolor(i,j);
            if (keymap[i+(j*XSIZE)].red) {
                drawX(i,j);
            }
        }
    }

    /* Draw borders around all the boxes */
    bordercolor();
    if ((xsize/XSIZE)>4) {
        for (j=0; j<=YSIZE; j++) {
            move2i(0,j);
            draw2i(XSIZE,j);
        }
        for (j=0; j<=XSIZE; j++) {
            move2i(j,0);
            draw2i(j,YSIZE);
        }
    }
}
static void
fillmap(int fill)
{
    int i;
    for (i = 0; i < SV_CMAP_SIZE; i++)
        keymap[i].red = fill;
    showmap();
    if (svLoadMap(V, SV_CHROMA_KEY_MAP, keymap) < 0)
        printf("load map failed\n");
}
main(void)
{
    short val;
    int menu;
    int r, g, b;
    float mx, my;

    /* Open video device */
    if ((V = svOpenVideo()) == NULL) {
        svPerror("open");
        exit(1);
    }

    /* Create mapping of 8-bit RGB to 32-bit equivalents */
    for (r=0; r<8; r++) {
        for (b=0; b<4; b++) {
            for (g=0; g<8; g++) {
                rgb8to32[(r<<5)|(b<<3)|g] =
                     ((r<<5)|(r<<2)|(r>>1))    |
                     (((g<<5)|(g<<2)|(g>>1)) << 8 )  |
                     (((b<<6)|(b<<4)|(b<<2)|b) << 16);
            }
        }
    }

    keepaspect(XSIZE, YSIZE);
    winopen("chromamap");
    RGBmode();
    gconfig();

    qdevice(LEFTMOUSE);
    qdevice(MOUSEX);
    qdevice(MOUSEY);
    qdevice(MENUBUTTON);
    menu = defpup("chromamap %t|clear all|set all|exit");

    getsize(&xsize,&ysize);
    getorigin(&xorg,&yorg);

    /* Put map in known state */
    fillmap(0);
    while (1) {
        switch(qread(&val)) {
            case REDRAW:
                reshapeviewport();
                getsize(&xsize,&ysize);
                getorigin(&xorg,&yorg);
                showmap();
                break;

            case MOUSEX:
                mx = MOUSEXMAP(val) - .25;
                if (mx < 0.0)
                    mx = 0.0;
                else if (mx >= XSIZE)
                    mx = XSIZE-1;
                break;

            case MOUSEY:
                my = MOUSEYMAP(val) - .25;
                if (my < 0.0)
                    my = 0.0;
                else if (my >= YSIZE)
                    my = YSIZE-1;
                break;

            case LEFTMOUSE:
                /* Toggle the entry's key */
                if (val) {
                    int i = (int)mx + (int)my * XSIZE;

                    keymap[i].red = !keymap[i].red;

                    drawcolor((int)mx, (int)my);
                    if (keymap[i].red) {
                        drawX((int)mx, (int)my);
                    }
                    drawborder((int)mx, (int)my);

                    if (svLoadMap(V, SV_CHROMA_KEY_MAP, keymap) < 0)
                        printf("load map failed\n");
                }
                break;
            case MENUBUTTON:
                if (val) {
                    switch (dopup(menu)) {
                        case 1:
                            fillmap(0);
                            break;
                        case 2:
                            fillmap(1);
                            break;
                        case 3:
                            exit(0);
                    }
                }
                break;

        }
    }
}LBL="19"ID="50847"Producing IndigoVideo OutputProducing live video output from the IndigoVideo board is simple. The IndigoVideo board constantly encodes a portion of the screen to video unless output has been explicitly turned off. You can also use IndigoVideo to produce single-frame output. In single-frame output mode, both live input and live output are disabled.In this chapter:IDREF="94429" TYPE="TITLE""Selecting the IndigoVideo Live Output Area" explains how to designate a portion of the screen to be output to video.IDREF="48523" TYPE="TITLE""Setting Output Parameters" explains how to set up the output configuration.IDREF="89362" TYPE="TITLE""Generating Single-frame Output" explains how to output video one frame at a time.LBL="" HELPID=""ID="94429"Selecting the IndigoVideo Live Output AreaSelect the portion of the screen to be output using the svOutputOffset() routine. This routine specifies the upper left corner of the output "window." The broadcast standard parameter (SV_BROADCAST) determines the size of the output areaname='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'640 by 480 pixels for NTSC, 768 by 576 pixels for PAL. svOutputOffset() does not create an actual window, and the output area is not in any way delineated on the Indigo workstation monitor.IDREF="46124" TYPE="TEXT"Example 19-1 demonstrates setting the location of the IndigoVideo output area.LBL="19-1"Example 19-1 ID="46124"Setting the Location of the IndigoVideo Output Window#include <stdio.h>
#include <stdlib.h>
#include <svideo.h>

/*
 * Simple Output Window
 * Change output location of NTSC video window
 */
 
main(int argc, char *argv[])
{
int xstart, ystart;
SVhandle V;

if (argc != 3) {
fprintf(stderr, "Usage: %s x_start y_start\n", argv[0]);
exit(1);
}
xstart = atoi(argv[1]);
ystart = atoi(argv[2]);

/* open video device */
if ((V = svOpenVideo()) == NULL) {
svPerror("open");
exit(1);
}

/* change location of output window */
svOutputOffset(V, xstart, ystart);
exit(0);
}You can use this program to record the output of any graphics program to videotape.To select an output window:Use xwininfo(1) to get the location of the desired window on the screen by entering:xwininfoPlace the mouse cursor in the window that you want information about and click any of the mouse buttons. xwininfo displays information about the selected window, including the coordinates of its upper left corner:xwininfo ==> Window id: 0x3400001 (Hubert)==> Absolute upper-left X: 14==> Absolute upper-left Y: 156...You can then use these coordinates as arguments to vout to place the output area over the selected window:vout 14 156To incorporate video output into a program, you can use the code fragment from IDREF="13210" TYPE="TEXT"Example 19-2 to place the upper left corner of the output area at the upper left corner of the current IRIS GL window.LBL="19-2"Example 19-2 ID="13210"Aligning a Video Output Area with an IRIS GL Window long x, y, xsize, ysize, screensize;
SVhandle V;
/* ... */
getorigin(&x, &y);
getsize(&xsize, &ysize);
screensize = getgdesc(GD_YPMAX);
y = screensize - (y + ysize);
svOutputOffset(V, x, y);Your program should recalculate the output offset whenever a "REDRAW" event is received so that the program relocates the output area whenever you move the window.LBL="" HELPID=""ID="48523"Setting Output ParametersThis section explains how to set parameters that affect live video output. LBL="" HELPID=""Turning Output On and OffThe SV_VIDEO_OUTPUT parameter can be used to turn live video output on and off. The default value for SV_VIDEO_OUTPUT is TRUE. Setting SV_VIDEO_OUTPUT to FALSE disables live video output.LBL="" HELPID=""Synchronizing Output with InputThe IndigoVideo board normally synchronizes video input and output so that they have the same field rates. This is necessary if you are doing any input; however, when you are using the IndigoVideo board for output only, you will get better results if you disable this synchronization by setting SV_SLAVE to FALSE. The default value for SV_SLAVE is TRUE.LBL="" HELPID=""Filtering OutputWhen encoding dithered graphics to video, you may encounter vertical bands on the composite output. You can eliminate these by setting SV_OUTPUT_FILTER to TRUE. The default value for SV_OUTPUT_FILTER is FALSE.LBL="" HELPID=""ID="89362"Generating Single-frame OutputIn single-frame output mode, the IndigoVideo board is configured as a 24-bit RGB framebuffer, and both live input and live output are disabled. Sending the board a still frame automatically reconfigures the board into single-frame output mode.Use the svPutFrame() routine to send a still frame to the IndigoVideo board. The data must be in the format used by the IRIS GL lrectwrite() routine (32-bit pixels, ordered 0x00BBGGRR, that is, 1 empty byte, followed by 1 byte each of blue, green, and red). The rows of pixels must be ordered top-to-bottom. The video capture routines also use this 32-bit RGB format. See IDREF="46637" TYPE="TITLE"Chapter 20, "Capturing Video from IndigoVideo," for information on the video capture routines, and IDREF="98955" TYPE="TITLE""IndigoVideo Data Formats" in Chapter 17 for information on the data formats used by IndigoVideo.Exit single-frame mode by calling svPutFrame() with a NULL frame pointer.IDREF="51372" TYPE="TEXT"Example 19-3 reads an RGB image file and sends it to the IndigoVideo board as a still frame.LBL="19-3"Example 19-3 ID="51372"Sending a RGB Image as a Still Video Frame/*
 * Use video card as a 24 bit RGB framebuffer.
 * ipaste SGI image file to screen, vpaste image file to video
 */

#include <stdio.h>
#include <stdlib.h>
#include <svideo.h>
#include <gl/image.h>

static void imgerror(char *);
static long getvideoparam(SVhandle, long);
static void sgiimage_to_buf(IMAGE *, unsigned long *, 
                            unsigned long, unsigned long);

int main(int argc, char *argv[])
{
unsigned long *rgb_buf, x_size, y_size;
IMAGE* ip;
SVhandle V;
int pal_mode = 0;
char line[30];
if (argc != 2) {
fprintf(stderr, "Usage: %s imagefile\n", argv[0]);
exit(1);
}
i_seterror(imgerror);

/* Open video device */
if ((V = svOpenVideo()) == NULL {
svPerror("open");
exit(1);
}
/* Size image according to broadcast standard */
pal_mode = (getvideoparam(V, SV_BROADCAST) == SV_PAL);
if (pal_mode) {
x_size = SV_PAL_XMAX;
y_size = SV_PAL_YMAX;
} else {
x_size = SV_NTSC_XMAX;
y_size = SV_NTSC_YMAX;
}
/* Open image */
if((ip = iopen(argv[1],"r")) <= (IMAGE*)0) {
fprintf(stderr, "could not open image file %s\n",
        argv[1]);
exit(1);
}

rgb_buf = (unsigned long *)malloc(x_size * y_size * sizeof(long));

/* Convert to RGB buffer */
sgiimage_to_buf(ip, rgb_buf, x_size, y_size);

iclose(ip);

/* Output 24-bit RGB image */ 
if (svPutFrame(V, (char *)rgb_buf) < 0) {
svPerror("putframe");
svCloseVideo(V);
exit(1);
}
printf("Type <Enter> to exit:");
(void) gets(line);
exit(0);
}
static void
imgerror(char *s)
{
fputs(s, stderr);
}
/* center SGI image file in buffer */
static void
sgiimage_to_buf(IMAGE *ip, unsigned long *rgb_buf, unsigned long bxsize, unsigned long bysize)
{
short *red, *green, *blue, *r, *g, *b;
int bxstart, ixstart, bystart, iystart;
int iy, by, x, nx, ny;
unsigned long *rgb;
red = malloc(ip->xsize * sizeof(short));
green = malloc(ip->xsize * sizeof(short));
blue = malloc(ip->xsize * sizeof(short));
bzero(rgb_buf,bxsize*bysize*sizeof(long));
if (ip->xsize > bxsize) {
bxstart = 0;
ixstart = (ip->xsize - bxsize)/2;
nx = bxsize;
} else {
ixstart = 0;
bxstart = (bxsize - ip->xsize)/2;
nx = ip->xsize;
}
if (ip->ysize > bysize) {
bystart = 0;
iystart = (ip->ysize - bysize)/2;
ny = bysize;
} else {
iystart = 0;
bystart = (bysize - ip->ysize)/2;
ny = ip->ysize;
}
for (iy=iystart, by=bystart; iy<iystart + ny; iy++, by++) {
getrow(ip, red, iy, 0);
getrow(ip, green, iy, 1);
getrow(ip, blue, iy, 2);

rgb = &rgb_buf[(by*bxsize) + bxstart];
r = &red[ixstart];
g = &green[ixstart];
b = &blue[ixstart];
for (x = 0; x < nx; x++) {
    *rgb++ = (*b++ << 16) | (*g++ << 8) | *r++;
}
}

free(red);
free(green);
free(blue);

return 0;
}

static long
getvideoparam(SVhandle V, long arg)
{
    long pvbuf[2];
    pvbuf[0] = arg;
    if (svGetParam(V, pvbuf, 2) < 0)
        svPerror("svGetParam");
    return pvbuf[1];
}LBL="20"ID="46637"Capturing Video from IndigoVideoThis chapter explains how to capture video using the IndigoVideo board. The IndigoVideo Library provides these three methods for capturing video:single frameburst modecontinuous modeIn this chapter:IDREF="85649" TYPE="TITLE""Captured Video Data Formats" provides a brief introduction to the data formats for capturing video.IDREF="86973" TYPE="TITLE""Capturing a Single Video Frame" describes the single-frame capture method. You can easily capture a single frame of video with just one call to the convenience routine svCaptureOneFrame(). IDREF="34690" TYPE="TITLE""Capturing Video Frames in Burst Mode" describes the burst-mode capture method. In burst mode, IndigoVideo captures a buffer full of sequential frames at full frame rate in most formats. You can use burst mode capture in conjunction with a computer-controllable video device to read in segments of video and stop the deck while your program processes the buffer full of frames.IDREF="82689" TYPE="TITLE""Capturing Video Frames in Continuous Mode" describes the continuous-mode capture method. In continuous mode, IndigoVideo captures frames of video into a queue at less than full frame rate. In this mode, you can capture reduced size and reduced frame rate video directly to disk, allowing you to collect images for a movie without a computer-controllable video device. In continuous capture mode, you can capture frames at no more than half the normal frame rate.IDREF="10851" TYPE="TITLE""Using Data Conversion Routines" describes how to convert video data for graphics display.LBL="" HELPID=""ID="85649"Captured Video Data FormatsThis section describes the data formats used by the IndigoVideo Library capture routines. For more information on these formats, see IDREF="98955" TYPE="TITLE""IndigoVideo Data Formats" in Chapter 17. The data formats are listed below, along with the symbolic constants that the IndigoVideo Library uses to identify them.SV_RGB8_FRAMESThis is the default format. In this mode, IndigoVideo captures 8-bit RGB frames. The fields that make up these frames are not interleaved; that is, all of the odd lines in the image come first, followed by all the even lines. 8-bit RGB frames are dithered unless the SV_DITHER parameter is set to FALSE.SV_YUV411_FRAMES_AND_BLANKING_BUFFERIn this mode, the IndigoVideo board captures full-size YUV frames, complete with the data carried in the blanking interval of the video signal. YUV format provides the best resolution and the most accurate color representation of the available options. The blanking portion of the video signal is sometimes used to carry extra information, such as the closed captioning provided on some television broadcasts. The blanking buffer takes the form of an extra 22 lines of data preceding the picture data; therefore, frames in this format are 640 by 502 pixels (NTSC) or 768 by 598 pixels (PAL). Use the svFindVisibleRegion() routine to find the first line of the image data following the blanking buffer.SV_RGB32_FRAMESThis format is not produced directly by IndigoVideo, but is produced by the convenience routines svCaptureOneFrame() and svYUVtoRGB(). In these frames, each pixel is represented by a 32-bit word containing 24 bits of RGB data.SV_YUV411_FRAMESThis format consists of interleaved 4:1:1 YUV frames without the blanking data mentioned above. This format is not produced directly by the IndigoVideo hardware, so it cannot be captured in burst capture mode; however, the other capture routines produce this format. Use svYUVtoRGB() to convert YUV frames to 24-bit RGB.IDREF="75229" TYPE="TABLE"Table 20-1 summarizes the storage requirements for the various data formats:COLUMNS="2"LBL="20-1"Table 20-1 ID="75229"Pixel Sizes for Video DataLEFT="0" WIDTH="108"FormatLEFT="115" WIDTH="216"Bytes per PixelLEFT="0" WIDTH="108"8-bit RGBLEFT="115" WIDTH="216"1LEFT="0" WIDTH="108"32-bit RGBLEFT="115" WIDTH="216"4LEFT="0" WIDTH="108"YUVLEFT="115" WIDTH="216"2Several of the capture routines take an svCaptureInfo structure as an argument. The fields in the svCaptureInfo structure are listed in IDREF="71331" TYPE="TABLE"Table 20-2.COLUMNS="2"LBL="20-2"Table 20-2 ID="71331"Fields in the svCaptureInfo StructureLEFT="0" WIDTH="108"FieldLEFT="115" WIDTH="216"ValueLEFT="0" WIDTH="108"formatLEFT="115" WIDTH="216"format (one of the symbolic constants listed above)LEFT="0" WIDTH="108"widthLEFT="115" WIDTH="216"width of captured frames, in pixelsLEFT="0" WIDTH="108"heightLEFT="115" WIDTH="216"height of captured frames, in pixelsLEFT="0" WIDTH="108"sizeLEFT="115" WIDTH="216"size of the capture buffer, in framesLEFT="0" WIDTH="108"samplingrateLEFT="115" WIDTH="216"used for continuous captureNote that the width and height members of the svCaptureInfo structure are input-output parameters; that is, if you set them to an unsupported size such as 321 by 243 pixels, they will be set to the nearest approximation of the requested size by the capture routines.If you are capturing 8-bit RGB frames, and you do not specify width and height, the IndigoVideo Library will use the width and height of the live video input window, if one is active. The video capture routines set the size of the video image to the requested size, so if you have a live video input window active, you may have to reset the size of the video image after capturing frames.LBL="" HELPID=""ID="86973"Capturing a Single Video FrameThe IndigoVideo Library provides a convenient way to capture a single frame of video in any of the supported formats. The svCaptureOneFrame() routine captures data into a user-allocated buffer. This buffer must be big enough to hold a single frame in the specified format.You can use the svQueryCaptureBufferSize() routine to determine the size of buffer required by svCaptureOneFrame(), then use malloc() to reserve a buffer of the appropriate size, as demonstrated in IDREF="42124" TYPE="TEXT"Example 20-1.LBL="20-1"Example 20-1 ID="42124"Determining the Capture Buffer Size SVhandle V;
svCaptureInfo capInfo;
char *buffer;
int width, height, bufSize;
    /* ... */
capInfo.format = SV_RGB8_FRAMES;
capInfo.width = width;
capInfo.height = height;
(void) svQueryCaptureBufferSize(V, &capInfo, &bufSize);
buffer = malloc(bufSize);IDREF="89757" TYPE="TEXT"Example 20-2 contains a listing of the /usr/people/4Dgifts/examples/dmedia/video/indigovideo program APP="/usr/sbin/jot /usr/people/4Dgifts/examples/dmedia/video/indigovideo/rgbgrab.c" PARMS=""rgbgrab.c
, which demonstrates the use of svCaptureOneFrame(). This program lets the user click the left mouse button to grab a frame of 8-bit RGB data and display it in a window. See oneframe.c for an example of capturing other data formats.LBL="20-2"Example 20-2 ID="89757"Grabbing a Single Frame of 8-bit RGB data: rgbgrab.c /*
 * Simple frame grabbing using video capture.
 *
 * To use: click on the left mouse button in either window 
 * to grab a frame and display it.
 */

#include <stdio.h>
#include <stdlib.h>
#include <svideo.h>
#include <gl/gl.h>
#include <gl/device.h>

#define RGBBUFSIZE (SV_NTSC_XMAX*SV_NTSC_YMAX)
static char captureData[RGBBUFSIZE], rgbbuf[RGBBUFSIZE];

main()
{
    SVhandle V;
    long dev, live_win, still_win;
    short val;
    int w, h;

    /* Open window */
    foreground();
    prefsize(SV_NTSC_XMAX, SV_NTSC_YMAX);
    still_win = winopen("Grabbed frame");
    RGBmode();
    gconfig();
    pixmode(PM_SIZE, 8);
    prefsize(SV_NTSC_XMAX, SV_NTSC_YMAX);
    live_win = winopen("Live video");

    /* Open video device */
    if ((V = svOpenVideo()) == NULL) {
        svPerror("open");
        exit(1);
    }
    /* Associate video input with this window */
    if (svBindGLWindow(V, live_win, SV_IN_REPLACE) < 0) {
        svPerror("bindwindow");
        svCloseVideo(V);
        exit(1);
    }
    printf("Use leftmouse to grab frame\n");

    /* Event loop */
    qdevice(LEFTMOUSE);
    qdevice(WINQUIT);
    qdevice(WINSHUT);
    qdevice(ESCKEY);
    while (1) {
        dev = qread(&val);
        switch (dev) {
        case LEFTMOUSE:
            if (val != 1)       /* button-press */
                break;
            w = SV_NTSC_XMAX;
            h = SV_NTSC_YMAX;
            if (svCaptureOneFrame(V, SV_RGB8_FRAMES,
                        &w, &h, (char *)captureData) < 0) {
                svPerror("captureburst");
                exit(-1);
            }
            svInterleaveFields(TRUE, captureData, rgbbuf, w, h);
            winset(still_win);
            lrectwrite(0, 0, w-1, h-1, (unsigned long *) rgbbuf);
            winset(live_win);
            break;

        case ESCKEY:
            if (val)    /* exit on key up */
                break;
        case WINQUIT:
        case WINSHUT:
            svCloseVideo(V);
            winclose(live_win);
            winclose(still_win);
            exit(0);
            break;
        }
    }
}LBL="" HELPID=""ID="34690"Capturing Video Frames in Burst ModeUse the svCaptureBurst() routine to capture a contiguous series of frames into a previously allocated buffer. In burst mode, you can capture full-sized YUV frames with blanking buffers or 8-bit RGB frames at full or reduced size. Initiating a burst capture puts the IndigoVideo board into exclusive mode, which remains set until the capture is complete or an error occurs. You must pass svCaptureBurst() a pointer to an svCaptureInfo structure, which determines the number and type of frames to be captured. This svCaptureInfo structure can also be passed to svQueryCaptureBufferSize() to determine how much memory to allocate for the capture buffer. The fields in the svCaptureInfo structure are listed in IDREF="71331" TYPE="TABLE"Table 20-2. Remember that the width and height members of the svCaptureInfo structure are input-output parameters; if you set them to an unsupported size, they will be set to the nearest approximation of the requested size when you call svQueryCaptureBufferSize() or svCaptureBurst().The final argument to svCaptureBurst() is an optional pointer to a bitvector for the SV_RGB8_FRAMES format, which can be used to determine whether any fields have been dropped during the capture. Fields must occasionally be dropped during capture to avoid visible tearing in the image due to scan rate conversion. The SV_FIELDDROP parameter controls whether fields are dropped, see svGetParam(3) for details.Every captured frame is represented by a pair of status bits (one for each field) in the bitvector. Each bit is set to either SV_EVEN_FIELD or SV_ODD_FIELD. Every frame should consist of an even field followed by an odd field; if this is not true, then one or more fields have been dropped during the capture. Use the SV_GET_FIELD macro, which is defined in svideo.h, to determine the even or odd value for a specific field. The code fragment in IDREF="81352" TYPE="TEXT"Example 20-3 prints the even/odd values for the fields that make up the frames from a captured burst.LBL="20-3"Example 20-3 ID="81352"Using the SV_GET_FIELD Macro for (f = 0; f < info->size; f++) {
  printf("%s-%s ",
SV_GET_FIELD(framevec, 2*f) == SV_EVEN_FIELD ? "even" : "odd",
SV_GET_FIELD(framevec, 2*f+1) == SV_EVEN_FIELD ? "even" : "odd");
}
putchar('\n');The status bits are filled in starting with the least significant bit in the first byte of the bitvector. You must allocate one byte of bitvector for every four frames or fraction thereof. Use the SV_BITVEC_SIZE macro to calculate the size of the buffer in bytes for a given number of frames.The format can be either 8-bit RGB or YUV with blanking buffer. This is because data is transferred directly from IndigoVideo to memory, and the IndigoVideo hardware produces only these two formats. If you need 32-bit RGB frames, you can capture YUV frames and use svFindVisibleRegion() and svYUVtoRGB() to convert the data, as discussed in IDREF="54738" TYPE="TITLE""Converting YUV Data to RGB". See the APP="/usr/sbin/jot /usr/people/4Dgifts/examples/dmedia/video/indigovideo/burstcapt.c" PARMS=""burstcapt.c
 sample program in /usr/people/4Dgifts/examples/dmedia/video/indigovideo for a demonstration of how to capture and display a burst of frames in either format.The number of frames that can be captured in burst mode is limited to what will fit in the memory buffer, which has a maximum size of 8 MB. The frame size affects how many frames will fit in the buffer, because larger frames have more data.NoteCurrently, svCaptureBurst() can only capture YUV frames with blanking data at half the full frame rate.The code fragment in IDREF="64535" TYPE="TEXT"Example 20-4 shows how to use svCaptureBurst().LBL="20-4"Example 20-4 ID="64535"Capturing Frames in Burst Mode SVhandle V;
char *buffer, *bitVector;
int numberOfFrames=8, bufSize, bitVectorSize;
svCaptureInfo capInfo;
/* ... */
capInfo.format = SV_YUV411_FRAMES;
capInfo.width = SV_PAL_XMAX;
capInfo.height = SV_PAL_YMAX;
capInfo.size = numberOfFrames;
(void) svQueryCaptureBufferSize(V, &capInfo, &bufSize);
buffer = malloc(bufSize);
bitVectorSize = numberOfFrames / 4 + 1; 
bitVector = malloc(bitVectorSize);
if (svCaptureBurst(V, &capInfo, buffer, bitVector) < 0) {
svPerror("capture burst");
} else {    /* process frames */
}LBL="" HELPID=""ID="82689"Capturing Video Frames in Continuous ModeIn continuous capture mode, IndigoVideo writes frames of video into a queue while your program reads frames out of the queue. To enter continuous capture mode, call svInitContinousCapture(), which allocates memory for the capture queue and begins capturing frames. You can read frames from the queue using the svGetCaptureData() routine. Once you are done with a frame, you must release it using svUnlockCaptureData(). When you are done capturing frames, call svEndContinuousCapture() to leave continuous capture mode.Continuous capture mode does not give you full frame ratename='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'the maximum frame rate achievable in this mode is one half the normal frame rate. The samplingrate member of the svCaptureInfo structure specifies the number of frames seen for each frame captured. Thus, a sampling rate of two captures gives you every other frame, and a sampling rate of four captures gives you every fourth frame.LBL="" HELPID=""Entering Continuous Capture ModeUse svInitContinousCapture() to enter continuous capture mode.The code fragment in IDREF="42876" TYPE="TEXT"Example 20-5 demonstrates how to initialize continuous capture. See the APP="/usr/sbin/jot /usr/people/4Dgifts/examples/dmedia/video/indigovideo/contcapt.c" PARMS=""contcapt.c
 sample program in /usr/people/4Dgifts/examples/dmedia/video/indigovideo for a complete program using continuous capture.LBL="20-5"Example 20-5 ID="42876"Initializing Continuous Capture Mode SVhandle V;
int success;
int width=320, height=240, queueSize=16, samplingRate=2;
svCaptureInfo capInfo;
/* ... */
capInfo.format = SV_RGB8_FRAMES;
capInfo.width = width;
capInfo.height = height;
capInfo.size = queueSize;
capInfo.samplingrate = samplingRate;
success = svInitContinuousCapture(V, capInfo);svInitContinousCapture() allocates the frame capture queue and takes control of the IndigoVideo board using the svUseExclusive() routine. If another program already has exclusive use of the board, svInitContinousCapture() returns -1; if it succeeds, svInitContinuousCapture() returns 0. In this example, 8-bit RGB frames are being captured at half size and half frame rate.Call svEndContinuousCapture() after you have completed video capture, to release control of the board. If you release control of the video board using svUseExclusive() during continuous capture mode, continuous capture mode is automatically terminated.LBL="" HELPID=""Accessing Captured DataTo access the next frame in the video capture queue, use the svGetCaptureData() routine. When you're done with the frame, release it using svUnlockCaptureData(), so that IndigoVideo can reuse the memory.The code fragment in IDREF="13567" TYPE="TEXT"Example 20-6 demonstrates how to access and release captured frames.LBL="20-6"Example 20-6 ID="13567"Accessing and Releasing Captured Frames SVhandle V;
void *data;
long fieldID;
long curFrame, maxFrames;
/* ... */
while (TRUE) {
svGetCaptureData(V, &data, &fieldID);
if (data == NULL) {
/* no frame available yet */
sginap(1);
} else {
/* process data */
svUnlockCaptureData(V, data);
curFrame++;
if (curFrame == maxFrames) break;
}
}The data parameter is pointed to the next frame in the queue, and the fieldID parameter is filled in to indicate the ID of the first field in the frame. If there are no frames in the queue, the data parameter is set to NULL (this will happen if you are processing frames faster than they are captured).Because there are two fields in each frame, the fieldID value increases by 2 for each video frame. Therefore, for a sampling rate of S, the fieldID should increase by 2 name='times' font=symbol charset=fontspecific code=180
	TeX='\times ' descr='[times]'S between captured frames. If the fieldID increases by more than 2 name='times' font=symbol charset=fontspecific code=180
	TeX='\times ' descr='[times]'S between captured frames, you have missed one or more frames. This could be because the system load is high, or because the queue has filled up, forcing the IndigoVideo board to drop frames.Although you don't have to unlock a frame before you get the next one, it's a good idea; if you keep a lot of frames locked, you will probably force the IndigoVideo software to drop frames. At any rate, you should unlock frames in the same order in which you received them. The IndigoVideo software writes the frames in order, and if it encounters a locked frame, it will block until the frame is unlocked.The data parameter points to memory in the queue, which may be overwritten as soon as you release the element. Once you have the queue element, you'll probably want to write it to disk or copy it to a location where it won't be overwritten.LBL="" HELPID=""Leaving Continuous Capture ModeTo leave continuous capture mode, call svEndContinuousCapture(), as demonstrated below:SVhandle V;
/* ... */
svEndContinuousCapture(V);LBL="" HELPID=""ID="10851"Using Data Conversion RoutinesThis section describes the convenience routines for converting data from IndigoVideo into other formats. For more information on the various data formats that IndigoVideo uses, see IDREF="98955" TYPE="TITLE""IndigoVideo Data Formats" in Chapter 17.LBL="" HELPID=""ID="54738"Converting YUV Data to RGBThe IndigoVideo Library provides a utility routine, svYUVtoRGB(), for converting YUV frames to RGB format. This routine produces 32-bit pixels, as used by the IRIS GL lrectwrite() routine. Each pixel contains 24 bits of RGB data and 8 bits of unused space. (In the YUV format, a pixel takes up only 16 bits, so if you're trying to record frames to disk as fast as possible, you should store the YUV data directly and convert it later.)Rows of pixels in the YUV frames are ordered top-to-bottom. This differs from the default ordering used by the IRIS GL lrectwrite() routine, which is bottom-to-top. If you set the invert parameter to TRUE, the svYUVtoRGB() routine will return an RGB frame with lines ordered bottom-to-top. If invert is FALSE, svYUVtoRGB() will not perform this inversion (this is useful because the X Window System expects the top-to-bottom ordering).Alternately, if you want to display the frame, you can set up lrectwrite() to use top-to-bottom ordering by using the IRIS GL pixmode() routine, as shown in IDREF="66476" TYPE="TEXT"Example 20-7.LBL="20-7"Example 20-7 ID="66476"Setting Top-to-Bottom pixmode for YUV boolean invert = FALSE;
int width, height;
long *yuv_buf;
long rgb_buf;
 /* ... */
rgb_buf = malloc(width*height*sizeof(*rgbbuf));
if (svYUVtoRGB(invert, yuv_buf, rgb_buf, width, height)==-1) {
 /* error */
}
pixmode(PM_TTOB, 1); /* pixel ordering top-to-bottom */
lrectwrite(0, 0, width-1, height-1, rgb_buf);NoteThis code may not work on some older IRIS workstations. If you capture YUV frames with blanking data, you can use svFindVisibleRegion() to locate the start of the YUV image data, as demonstrated in IDREF="76637" TYPE="TEXT"Example 20-8.LBL="20-8"Example 20-8 ID="76637"Finding Image Data in YUV with Blanking Frames SVhandle V;
void *frame_with_blanking, *frame;
long fieldID;
 /* ... */
svGetCaptureData(V, &frame_with_blanking, &fieldID);
svFindVisibleRegion(V, frame_with_blanking, &frame);
svUnlockCaptureData(frame_with_blanking);
 /* process frame */IDREF="93760" TYPE="TEXT"Example 20-9 contains a listing of APP="/usr/sbin/jot /usr/people/4Dgifts/examples/dmedia/video/indigovideo/vgrab.c" PARMS=""vgrab.c
, in /usr/people/4Dgifts/examples/dmedia/video/indigovideo, which demonstrates how to convert grabbed YUV frames to RGB images.LBL="20-9"Example 20-9 ID="93760"Grabbing YUV Frames to Save as RGB Images: vgrab.c /*
 *      vgrab.c
 *      Grab YUV frames, save as SGI RGB images
 */

#include <stdio.h>
#include <gl/gl.h>
#include <gl/image.h>
#include <gl/device.h>
#include <svideo.h>

#define GRABFILE        "out.rgb"
#define RGBBUFSIZE      (SV_PAL_XMAX*SV_PAL_YMAX*sizeof(long))
static char rgbbuf[RGBBUFSIZE];

/*
 * Dump rgb data to image file
 */
void
dumpImage(char *data, int xsize, int ysize)
{
    IMAGE *image;
    short rbuf[SV_PAL_XMAX];
    short gbuf[SV_PAL_XMAX];
    short bbuf[SV_PAL_XMAX];
    int x, y, z;
    image = iopen(GRABFILE, "w", RLE(1), 3, xsize, ysize, 3);
    for (y=0;y<ysize;y++) {
        for(x=0;x<xsize;x++) {
            bbuf[x] = *(data+1);
            gbuf[x] = *(data+2);
            rbuf[x] = *(data+3);
            data += 4;
        }
        putrow(image, rbuf, y, 0);
        putrow(image, gbuf, y, 1);
        putrow(image, bbuf, y, 2);
        
    }
    iclose(image);
}
main(int argc, char **argv)
{
    short val;
    long livewin, stillwin, x, y;
    int width, height;
    SVhandle V;
    long param[2];
    int videoon = 1;

    /* Open video device */
    if ((V = svOpenVideo()) == NULL) {
        svPerror("open");
        exit(1);
    }

    /* Determine window size based on signal standard */
    param[0] = SV_BROADCAST;
    svGetParam(V, param, 2);
    if (param[1] == SV_PAL) {
        width = SV_PAL_XMAX;
        height = SV_PAL_YMAX;
    } else {
        width = SV_NTSC_XMAX;
        height = SV_NTSC_YMAX;
    }

    /* Open windows */
    foreground();
    prefsize(width, height);
    stillwin = winopen("Grabbed frame");
    RGBmode();
    gconfig();

    /* Set video window background to black */
    cpack(0x0);
    clear();
    maxsize(width, height);
    keepaspect(width, height);
    stepunit(8, 6);
    livewin = winopen("video in");
    RGBmode();
    gconfig();

    getsize(&x, &y);
    svSetSize(V, x, y);
    /* Associate video input with livewin */
    if (svBindGLWindow(V, livewin, SV_IN_REPLACE) < 0) {
        svPerror("bindwindow");
        exit(1);
    }

    printf("Click on left mouse button to grab frame\n");
    qdevice(LEFTMOUSE);
    qdevice(WINQUIT);
    qdevice(WINSHUT);
    qdevice(ESCKEY);

    while (1) {
        switch (qread(&val)) {
            case LEFTMOUSE:
                if (val != 1)
                    break;

                svCaptureOneFrame(V, SV_RGB32_FRAMES, &width,
                                   &height, rgbbuf);
                winset(stillwin);
                lrectwrite(0, 0, width-1, height-1,
                           (unsigned long *) rgbbuf);
                winset(livewin);
                if (svSetSize(V, x, y) < 0) {
                    svPerror("setsize");
                    exit(1);
                }
                /* Re-bind window to re-scale output */
                if (svBindGLWindow(V, livewin,
                    SV_IN_REPLACE) < 0) {
                    svPerror("bindwindow");
                    exit(1);
                }               
                dumpImage(rgbbuf, width, height);
                printf("saved image to file %s\n", GRABFILE);
                break;
            case REDRAW:
                reshapeviewport();
                getsize(&x, &y);
                svSetSize(V, x, y);
                /* Re-bind window to re-scale output */
                if (svBindGLWindow(V, livewin,
                    SV_IN_REPLACE) < 0) {
                    svPerror("bindwindow");
                    exit(1);
                }
                break;
            case ESCKEY:
                if (val)        /* exit on key up */
                    break;
            case WINQUIT:
            case WINSHUT:
                winclose(stillwin);
                winclose(livewin);
                svCloseVideo(V);
                exit(0);
                break;
        }
    }
}LBL="" HELPID=""Using 8-bit RGB Capture DataThe fields in a frame of 8-bit RGB data captured with svGetCaptureData() are not interleaved; all the even rows of pixels are stored before all the odd rows of pixels. In addition, the rows within the fields are ordered top-to-bottom. The IndigoVideo Library provides a convenience routine, svInterleaveFields(), to interleave, and optionally invert, the fields. It produces 8-bit RGB data rather than SV_RGB_FRAMES data.IDREF="23691" TYPE="TEXT"Example 20-10 demonstrates how to invert fields and interleave them.LBL="20-10"Example 20-10 ID="23691"Interleaving 8-bit RGB Fields with Inversion boolean invert = TRUE;
char *fields, *rgb8frame;
int width, height;
/* ... */
frame = malloc(width*height);
svInterleaveFields(invert, fields, rgb8frame,width, height)Once interleaved, you can display the 8-bit RGB data directly on an Indigo workstation with Entry graphics that has the svideo software installed, by using the RGBmode(), pixmode(), and lrectwrite() routines, as demonstrated in IDREF="61462" TYPE="TEXT"Example 20-11.LBL="20-11"Example 20-11 ID="61462"Displaying Interleaved 8-bit RGB Data char *rgb8frame;
int width, height;
    /* ... */
RGBmode();
gconfig();
pixmode(PM_SIZE, 8);
lrectwrite(0, width-1, 0, height-1, rgb8frame);To display the 8-bit RGB data on other systems, convert it to the more common 32-bit RGB format by using the svRGB8toRGB32() routine, which converts the data to 32-bit in addition to interleaving, and, optionally, inverting the fields, as demonstrated in IDREF="73470" TYPE="TEXT"Example 20-12.LBL="20-12"Example 20-12 ID="73470"Converting 8-bit RGB Capture Data to 32-bit RGB char *rgb8fields;
long *rgb32frame;
int width, height;
boolean invert = TRUE;
    /* ... */
rgb32frame = malloc(width*height*sizeof(*rgb32frame));
svRGB8toRGB32(invert, rgb8fields, rgb32frame, width, height);The svRGB8toRGB32() works on 8-bit RGB field data only in the format captured by the IndigoVideo board. Like svInterleaveFields(), it will interleave, and, optionally, invert the data (bottom-to-top) if the invert parameter is true.If you don't want to convert the 8-bit RGB data, you can display it on any system by treating the 8-bit pixels as 8-bit color index values. In IDREF="28741" TYPE="TEXT"Example 20-13, the code fragment, from the /usr/people/4Dgifts/examples/dmedia/video/indigovideo program APP="/usr/sbin/jot /usr/people/4Dgifts/examples/dmedia/video/indigovideo/vmirror.c" PARMS=""vmirror.c
, sets up the IRIS GL color map to display 8-bit RGB data:LBL="20-13"Example 20-13 ID="28741"Setting up the IRIS GL Color Map to Display 8-bit RGB /* Change GL color map to display IndigoVideo RGB8 data */
static void
makevideomap(void)
{
    int r, g, b;
    for (r=0; r<8; r++) {
        for (b=0; b<4; b++) {
            for (g=0; g<8; g++) {
                mapcolor((r<<5)|(b<<3)|g,
                     (r<<5)|(r<<2)|(r>>1),
                     (g<<5)|(g<<2)|(g>>1),
                     (b<<6)|(b<<4)|(b<<2)|b);
            }
        }
    }   
    gflush();
}LBL="21"ID="61431"Handling IndigoVideo EventsPrograms that use live video need to be notified when the setup or status of the IndigoVideo board changes. This chapter explains how to use either the X Window System or IRIS GL event mechanisms to provide your application with notification of status changes to the IndigoVideo board.The X and IRIS GL event mechanisms are different, but they serve the same purpose. X event handling methods distinguish between video activity events and parameter change events; IRIS GL event handling methods lump these event classes together as a single pseudodevice. Wherever possible, X event handling should be used rather than IRIS GL event handling to provide greater portability and flexibility. See the IRIS IM Programming Guide, which you can read online using the IRIS InSight viewer, for more details about X event handling versus IRIS GL event handling.In this chapter:IDREF="87998" TYPE="TITLE""IndigoVideo Event Handling Basics" provides an overview of video events and the event-handling routines in the IndigoVideo library.IDREF="58947" TYPE="TITLE""X Event Handling" describes some event handling methods provided by the X Window System. To learn more about these methods, and to find out about other methods, consult the references recommended at the beginning of this guide.IDREF="99344" TYPE="TITLE""IRIS GL Event Handling" describes pure IRIS GL event handling. Skip this section if you are developing a new IndigoVideo application, and use the methods described in IDREF="58947" TYPE="TITLE""X Event Handling" instead.LBL="" HELPID=""ID="87998"IndigoVideo Event Handling BasicsPrograms receive video activity events only if they use live video, that is, only if they have executed an svBindWindow() or svBindGLWindow() call. The four reasons for which the window system generates a video activity event are: video startedThis event indicates that video has started in this window. It is generated when your program succeeds in turning on video using svBindWindow() or svBindGLWindow().video stoppedThis event indicates that video has stopped in this window. It is generated when your program turns off video using svBindWindow() or svBindGLWindow() with an argument of SV_IN_OFF.video busyYour program tried to turn on video in this window using svBindWindow() or svBindGLWindow(), but failed because another program had exclusive use of the IndigoVideo board.video preemptedYour program had video running in this window, but it was preempted by another program calling svBindWindow() or svBindGLWindow().These event reasons are identified by global variables, listed in IDREF="11409" TYPE="TABLE"Table 21-1.COLUMNS="2"LBL="21-1"Table 21-1 ID="11409"Video Activity Event Variable NamesLEFT="0" WIDTH="171"ReasonLEFT="180" WIDTH="171"Variable NameLEFT="0" WIDTH="171"Video startedLEFT="180" WIDTH="171"SvVideoStartedLEFT="0" WIDTH="171"Video stoppedLEFT="180" WIDTH="171"SvVideoStoppedLEFT="0" WIDTH="171"Video busyLEFT="180" WIDTH="171"SvVideoBusyLEFT="0" WIDTH="171"Video preemptedLEFT="180" WIDTH="171"SvVideoPreemptedThe five types of parameter change events are:active attribute changeThis event indicates that a process has given up live video input; for example, by exiting, or by calling svBindWindow() or svBindGLWindow() with an argument of SV_IN_OFF.signal changeThis event is generated whenever a process changes the broadcast standard and/or video mode by calling svSetParam(). Changing input sources can also generate this event, as IndigoVideo changes to the default broadcast standard and video mode for the new input source.video frozen/unfrozenThis event is generated whenever a process freezes or unfreezes video by calling svSetParam().input source changedThis event is generated whenever a process changes the input source by calling svSetParam().other parameters changedThis event is generated whenever a process changes any other variable by calling svSetParam().These event reasons are identified by the global variables listed in IDREF="86377" TYPE="TABLE"Table 21-2.COLUMNS="2"LBL="21-2"Table 21-2 ID="86377"Video Parameter Change Event Variable NamesLEFT="0" WIDTH="171"ReasonLEFT="180" WIDTH="171"Variable NameLEFT="0" WIDTH="171"Active attributeLEFT="180" WIDTH="171"SvActiveAttributeLEFT="0" WIDTH="171"Signal changeLEFT="180" WIDTH="171"SvEncodingAttributeLEFT="0" WIDTH="171"Video frozen/unfrozenLEFT="180" WIDTH="171"SvFreezeAttributeLEFT="0" WIDTH="171"Source changeLEFT="180" WIDTH="171"SvSourceAttributeLEFT="0" WIDTH="171"Other parameter changeLEFT="180" WIDTH="171"SvParamChangeAttributeLBL="" HELPID=""ID="56579"ID="58947"X Event HandlingTo provide X event handling, you must include the appropriate X11 header files, and you must link your program with the X extensions library (-lXext), the X shared library (-lX11_s), and any X toolkits that you use.Before your program can receive video-related X events, you must call the svSelectXEvents() function.There are two types of video events, indicated by the following variables:SvVideoActivityEventNumberDescribes video events such as video starting, or video stopping, that affect only a specific window. The reason field of the video activity event is set to one of the values listed in IDREF="11409" TYPE="TABLE"Table 21-1. Only programs that use live video receive video activity events.SvParamChangeEventNumberDescribes parameter change events that apply to board settings. The attribute field of the param change event is set to one of the values listed in IDREF="86377" TYPE="TABLE"Table 21-2. In addition, the value field is set to the new value of the changed parameter (if applicable). In the case of SvEncodingAttribute events, the value field is set to one of the values listed in IDREF="30929" TYPE="TABLE"Table 21-3. COLUMNS="3"LBL="21-3"Table 21-3 ID="30929"Encoding Attribute ValuesLEFT="0" WIDTH="108"VariableLEFT="115" WIDTH="108"Broadcast StandardLEFT="230" WIDTH="108"Video Mode  LEFT="0" WIDTH="108"SvNTSCCompositeLEFT="115" WIDTH="108"NTSCLEFT="230" WIDTH="108"CompositeLEFT="0" WIDTH="108"SvNTSCSVideoLEFT="115" WIDTH="108"NTSCLEFT="230" WIDTH="108"S-VideoLEFT="0" WIDTH="108"SvPALCompositeLEFT="115" WIDTH="108"PALLEFT="230" WIDTH="108"CompositeLEFT="0" WIDTH="108"SvPALSVideoLEFT="115" WIDTH="108"PALLEFT="230" WIDTH="108"S-VideoIDREF="34069" TYPE="TEXT"Example 21-1 contains a listing of xevents.c, which demonstrates the use of X events. First, an event mask is set up, to establish interest in exposure, key, and video related events. A connection to the X server is established and a video device is opened with the proper window size for the signal being received. The program prints status messages about the events as they occur.LBL="21-1"Example 21-1 ID="34069"X Event Handling for IndigoVideo events: xevents.c /*
 * xevents.c
 *
 * This X11 program displays live video from the IndigoVideo board and shows
 * how to decode X11 video-related event information.
 *
 * Hit the escape or the 'q' keys to exit.
 */

#include <stdio.h>
#include <stdlib.h>
#include <X11/Xlib.h>
#include <X11/Xutil.h>
#include <svideo.h>             /* must be included after <X11/Xlib.h> */

/* We're interested in exposure, key and video-related events */
#define EVENTMASK   (ExposureMask|KeyPressMask|StructureNotifyMask)

main(int argc, char *argv[])
{
    Window          rootwin, win;
    Display        *display;
    XEvent          event;
    int             screen, width, height;
    SVhandle        V;
    long            param[2];

    /* Open connection to X server */
    if ((display = XOpenDisplay(0)) == NULL) {
        fprintf(stderr, "%s: cannot connect to X server", argv[0]);
        if (getenv("DISPLAY") == NULL)
            fprintf(stderr,
                    ", `DISPLAY' environment variable not set.\n");
        else
            fprintf(stderr, " %s\n", XDisplayName(0));
        exit(1);
    }

    /* Open video device */
    if ((V = svOpenVideo()) == NULL) {
        svPerror("open");
        exit(1);
    }
    /* Determine the window size from the signal standard */
    param[0] = SV_BROADCAST;
    svGetParam(V, param, 2);
    if (param[1] == SV_PAL) {
        width = SV_PAL_XMAX;
        height = SV_PAL_YMAX;
    } else {
        width = SV_NTSC_XMAX;
        height = SV_NTSC_YMAX;
    }
    printf("Default window size: %d by %d\n", width, height);

    /* Create appropriate-sized window */
    screen = DefaultScreen(display);
    rootwin = RootWindow(display, screen);
    win = XCreateSimpleWindow(display, rootwin, 100, 100, width, height,
               5, BlackPixel(display, screen), BlackPixel(display, screen));

    /* Set the window and icon names for the window manager before mapping it */
    XStoreName(display, win, "X Video Event Handler");
    XSetIconName(display, win, argv[0]);
    XSelectInput(display, win, EVENTMASK);
    XMapWindow(display, win);

    /* Associate video with window */
    if (svBindWindow(V, display, win, SV_IN_REPLACE) < 0) {
        svPerror("bindwindow");
        svCloseVideo(V);
        exit(1);
    }

    /* Receive video-related X events */
    svSelectXEvents(V, display);

    /* Event loop */
    while (1) {
        XNextEvent(display, &event);

        if (event.type == Expose) {
            printf("Expose event\n");
            if (svBindWindow(V, display, win, SV_IN_REPLACE) < 0) {
                svPerror("bindwindow");
                svCloseVideo(V);
                exit(1);
            }
        } else if (event.type == KeyPress) {    /* See if we're done */
            XKeyEvent      *kev = (XKeyEvent *) &event;
            KeySym          keysym;
            char            buf[4];

            XLookupString(kev, buf, 1, &keysym, 0);
            printf("Key pressed: '%c' (%d)\n", buf[0], buf[0]);
            if (buf[0] == 'Q' || buf[0] == 'q' || buf[0] == '\033') {
                printf("Quitting...\n");
                svCloseVideo(V);
                exit(0);
            }

        } else if (event.type == SvVideoActivityEventNumber) {
            SVvideoActivityEvent *ev = (SVvideoActivityEvent *) & event;

            if (ev->reason == SvVideoStarted) {
                printf("Video started\n");
            } else if (ev->reason == SvVideoStopped) {
                printf("Video stopped\n");
            } else if (ev->reason == SvVideoBusy) {
                printf("Video busy\n");
            } else if (ev->reason == SvVideoPreempted) {
                printf("Lost video\n");
            } else {
                printf("unknown video activity (%d)?\n", ev->reason);
            }

        } else if (event.type == SvParamChangeEventNumber) {
            SVparamChangeEvent *ev = (SVparamChangeEvent *) &event;

            if (ev->attribute == SvActiveAttribute) {
                /* value always 0 */
                if (svBindWindow(V, display, win, SV_IN_REPLACE) < 0) {
                    svPerror("bindwindow");
                    svCloseVideo(V);
                    exit(1);
                }
                printf("Active attribute: re-bound video\n");
            } else if (ev->attribute == SvEncodingAttribute) {
                printf("Encoding change: %d = ", ev->value);
                if (ev->value == SvNTSCComposite) {
                    printf("NTSC composite\n");
                } else if (ev->value == SvPALComposite) {
                    printf("PAL composite\n");
                } else if (ev->value == SvNTSCSVideo) {
                    printf("NTSC SVideo\n");
                } else if (ev->value == SvPALSVideo) {
                    printf("PAL SVideo\n");
                } else {
                    printf("?\n");
                }
            } else if (ev->attribute == SvFreezeAttribute) {
                printf("Freeze attribute: %s\n", ev->value ? "on" : "off");
            } else if (ev->attribute == SvSourceAttribute) {
                printf("Input source change: %d\n", ev->value + 1);
            } else if (ev->attribute == SvParamChangeAttribute) {
                printf("Parameter changed\n");  /* value always 1 */
            } else {
                printf("unknown param attribute (%d) ?\n", ev->attribute);
            }
        }
    }
}LBL="" HELPID=""ID="99344"IRIS GL Event HandlingHandling IRIS GL events is fairly simple. To receive video-related events, use the IRIS GL ID="Media4-5EH1"qdevice() function to queue events from the VIDEO pseudodevice, and use qread() to read the events from the queue.ID="Media4-5EH2"When you get an event from the event queue using the IRIS GL qread() function, you must pass the function a pointer to a short integer. qread() returns a value indicating the device that generated the event, and fills in the reason for the event in the space pointed to by the argument. In the case of a video event, the reason will correspond to one of the constants listed in IDREF="11409" TYPE="TABLE"Table 21-1 or IDREF="86377" TYPE="TABLE"Table 21-2.The event loop in IDREF="97423" TYPE="TEXT"Example 21-2 handles video events, printing a message when video is preempted by another process and rebinding the video when it is released by another process.LBL="21-2"Example 21-2 ID="97423"Handling Video Events with IRIS GL Routineslong window_id, device;
SVhandle vidnode;
short reason;

/* ... */

qdevice(VIDEO);
qdevice(WINQUIT);
qdevice(WINSHUT);
while(TRUE) {
device=qread(&reason);
switch(device) {
case VIDEO: 
if (reason == SvVideoPreempted)
    printf("Lost video\n");
else if (reason == SvActiveAttribute) {
    svBindGLWindow(vidnode, window_id,
    SV_IN_REPLACE);
    printf("Re-bound video\n");
}
break;
case WINSHUT:
case WINQUIT:
exit(0);
}
}

ID="Media4-5EH3"ID="Media4-5EH4"ID="Media4-5EH5"ID="Media4-5EH6"ID="Media4-5EH7"ID="Media4-5EH8"LBL="22"ID="59277"Using the IndigoVideo UtilitiesThis chapter briefly describes the utilities and end-user tools that are available for running applications written for the IndigoVideo board. These utility programs provide a convenient way of accessing many of the IndigoVideo board's functions. In addition, you may find them helpful as a reference when debugging your IndigoVideo Library programs.Two graphical user interface (GUI) tools are available for controlling video: Video Control Panel (vpanel) and Video Pro Panel (vpro). You can use the Video Control Panel by itself for viewing live video input, grabbing frames of video, and encoding graphics to video, or you can use vpanel in conjunction with your IndigoVideo Library programs. As long as your program does not use exclusive-use mode, you can use vpanel to switch input sources and control display effects, such as dithering and hue. The Video Pro Panel provides access to IndigoVideo's low-level parameters. All the parameters that can be set using the Video Pro Panel are listed on the svSetParams(3V) manual page.Live video windows for input (videoin) and output (videoout) can be launched separately from a command line or from vpanel. See the videoin(1) and videout(1) man pages for details.See the Media Control Panels User's Guide, which is accessible from the InSight viewer, for complete instructions on using the Video Control Panel, Video Pro Panel, and the live video windows. Be sure to read the Svideo Release Notes for important information about software updates and notes about special situations and workarounds.This chapter explains how to use two other utilities: svcmd, an interactive shell-level tool that lets you access IndigoVideo Library routines from a shell command line or a shell script, and svtomovie, a program that lets you turn video input into movie files that you can play using the Movie Maker tool.In this chapter:IDREF="54737" TYPE="TITLE""Using svcmd, the IndigoVideo Shell-level Tool" explains how to use svcmd. The IndigoVideo shell-level tool provides a command-line interface to most of the functions of the IndigoVideo Library, including single-frame output (but not including DMA). You can use the shell-level tool to control IndigoVideo functions from a shell script.IDREF="56432" TYPE="TITLE""Making a Movie File from IndigoVideo and Audio Input" explains how to use svtomovie to create movies that can be viewed with Movie Player.LBL="" HELPID=""ID="54737"Using svcmd, the IndigoVideo Shell-level ToolYou can use the shell-level tool, svcmd, to control the IndigoVideo board from the command line. You can also use svcmd in shell scripts to automate repetitive tasks. You can use the shell-level tool in one of two ways: either by specifying a single operation on the command line, or by using svcmd's interactive mode.To get a listing of svcmd commands, enter:svcmd -hTo execute a single command, use the syntax:svcmd command [parameters]For example, to set the input source to 1, enter:svcmd inputsource 1To start svcmd in interactive mode, use the -i flag. To exit svcmd, use the quit or exit command. For example, to set the input source to 2 and the video mode to composite, type:svcmd -isvcmd >> inputsource 2svcmd >> videomode compsvcmd >> quitFor more information on svcmd, see the svcmd(1) manual page.LBL="" HELPID=""ID="56432"Making a Movie File from IndigoVideo and Audio InputYou can use IndigoVideo input and optional audio input to make a movie. Movies are files that can be played on the Indigo workstation from the Movie Player tool. The svtomovie program provides everything you need to make a movie using your IndigoVideo board, audio input if you wish, and a little imagination.To start svtomovie from a shell command line, enter:svtomovie [options] filenameto which the system replies:svtomovie: Press <ENTER> when you are ready to collect video:You must now "queue" the video source, that is, advance it to the location where the recording is to begin. You can use vpanel to set up the video source and apanel to set up the audio sourcename='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'svtomovie responds to control from both of these tools.When you press 
<Enter>, svtomovie collects video, and optionally audio, and writes it to the file you specified in filename.When svtomovie finishes capturing the video (and audio) frames, it rewrites the movie file to a playable format. You can play your new movie with Movie Player or edit it with Movie Maker.The svtomovie options are: -aTurn audio off; audio is on by default. This makes a silent movie and has the possible advantage of capturing more video data, because the audio capturing is disabled and is not using CPU bandwidth. -bUse burst mode for capture; continuous mode is the default. Burst mode is useful to get short bursts of 30 frames per second. It captures only 30 frames per second, and can capture directly to only a maximum 8MB memory buffer; hence, there is no audio, and, depending on the chosen size, a limited number of frames. -dTurn diagnostic messaging on; diagnostics are off by default. See svtomovie(1) for a list of diagnostics.-f framerateSpecifies movie frame rate; the default rate is 15 frames per second. This option is ignored if -b is used. In continuous capture mode, the only legal values for this parameter are 15, 10, 6, 5, 3, 2, and 1. All other values will gather data at one of these rates but movie playback will be at the requested rate. In burst capture mode, the only legal value is 30. All other values will be ignored. -mTurn audio monitoring on; audio monitoring is off by default. This option is ignored if -a or -b is used. -n numframesSpecifies number of frames of movie; 100 is the default. -sUse stereo audio input; mono is the default. This option is ignored if -a or -b is used. -w widthSpecifies the video width in pixels; 320 is the default. Height is chosen to preserve 8:6 aspect ratio.LBL="V"ID="22118"Compression ProgrammingIDREF="36492" TYPE="TITLE"Chapter 23, "Introduction to the Compression Library,"introduces the CL and describes its applications and features. It provides basic background information on compression technology and on digital audio and video data formats.IDREF="44980" TYPE="TITLE"Chapter 24, "Getting Started with the Compression Library,"describes how to use the three types of interfaces supplied by the CL.IDREF="95875" TYPE="TITLE"Chapter 25, "Using Compression Library Algorithms and Parameters,"explains how to use the CL algorithms and global parameters.IDREF="63275" TYPE="TITLE"Chapter 26, "Customizing the Compression Library,"explains how to add your own algorithms and parameters to the CL.LBL="23"ID="36492"Introduction to the Compression LibraryThe Compression LibraryID="Media5-0ZIN1", libcl.so, provides a flexible, extensible, and algorithm-independent software interface for compressing and decompressing audio, video, and image data. Developers may also choose to incorporate the licensable built-in interface to third-party audio compression software from Aware, Inc., which is described in ID="Media5-0ZIN2"IDREF="63476" TYPE="TITLE"Appendix B, "Aware Scalable Audio Compression Software."Using the Compression Library (CL) involves three concepts, each of which are discussed in a separate chapter in this part of this guide:using the application interface (API)using algorithms and parametersusing/adding algorithmsIn this chapter:IDREF="18863" TYPE="TITLE""Overview of the Compression Library" describes the features and applications of the CL and provides fundamental information essential for working with compression.IDREF="58292" TYPE="TITLE""Compression Library Data Formats" describes the data formats that you are likely to encounter when using the CL.LBL="" HELPID=""ID="18863"Overview of the Compression LibraryCompression is the process of shrinking the size of the data without changing its content significantly. Compact data can be stored more efficiently and can be transmitted faster than raw data. For example, certain compression methods can allow you to store 10 to 20 times as many compressed images in the space required to store a single uncompressed image. Compression extends the capabilities of digital media delivery and storage systems because it encodes data more efficiently.ID="Media5-0ZIN3"LBL="" HELPID=""Compression Library ApplicationsCompression Library applications are far-reaching. The primary goal of the CL is to improve the data delivery and storage capabilities of applications that use digital media.ID="Media5-0ZIN4"The Compression Library can be used with the Audio File Library, and data used by the IRIS MediaMosaicÔ tools, Movie Player and Movie Maker. Other applications include:Information delivery and storage, including multimedia presentations, publications, interactive training, archiving, and annotation. For example, you can use MoviePlayer as the playback mechanism for an information delivery application. ShowcaseÔ can be used as the base medium from which to launch separate executables of the MoviePlayer to play back prerecorded movies.Telecommunications (video/voice mail, phone, and teleconferencing)ID="Media5-0ZIN5"ID="Media5-0ZIN6"Compression allows faster transmission of data. This is especially useful when the data rate is limited by the transmission medium. Cost savings can also be realized when transmitting data over a medium where you are billed on the basis of either access time or number of bytes transferred.Animation previewingImages can be compressed frame-by-frame, as they are rendered, for previewing 2D and 3D graphics animations in live action before recording to video tape. Previewing saves time for animators because they don't have to render and record a full-data animation to tape every time they want to check the motion sequence.ID="Media5-0ZIN7"Movie (audio and video) editingMovie editing can be done entirely in the digital domain using a tool such as MovieMaker, instead of editing a tape recording. Compression lets you store more data and decompress it as you open files for editing.ID="Media5-0ZIN8"IDREF="94177" TYPE="GRAPHIC"Figure 23-1 shows a few of the applications that are possible in a server-client environment.ID="Media5-0ZIN9"FILE="23-1.online.bw" POSITION="INLINE" SCALE="FALSE"LBL="23-1"Figure 23-1 ID="94177"Server-Client Compression ApplicationsLBL="" HELPID=""Compression Library FeaturesThe Compression Library features:ID="Media5-0ZIN10"algorithm independencehardware independencesupport of industry standard algorithmssupport of Silicon Graphics proprietary algorithmsbinary compatibility across Silicon Graphics platformsBecause the CL is algorithm-independent, you need to know only the basic application interface (API) to use any of the supplied algorithms. You can query the library for the available algorithms, and you can add your own algorithms to the library. A pass-through capability allows you to pass data through the routines without using an algorithm.ID="Media5-0ZIN11"The libcl API provides facilities for working with audio, still images, sequential frames of data (movies), and a buffering mechanism for random access of compressed data.The buffering facility allows independent buffering of compressed data and decompressed frames, with synchronous or asynchronous access, either external or internal to the library. Separate processes can be used for supplying data, compressing/decompressing, and retrieving data.The API also uses a set of global state parameters, similar to those found in the Audio Library, libaudio, to establish and manipulate compression attributes.LBL="" HELPID=""Compression Library BasicsThis section introduces compression technology and compression standards. It provides useful background information that you should know before using the Compression Library.LBL="" HELPID=""Lossy versus Lossless Compression MethodsCompressed data isn't always a perfect representation of the original data. Information can be lost in the compression process. A ID="Media5-0ZIN12"lossless compression method retains all of the information present in the original data. A ID="Media5-0ZIN13"lossy compression method does not preserve 100% of the information in the original method. Some methods incur more loss than others, so the amount of loss that can be tolerated by your application might affect your decision about which compression method to use.NoteIn general, video compression algorithms are designed to work on camera-generated images. Computer-generated images often contain text and line drawings that compression algorithms can't compress as well as smooth-shaded computer images, which approximate camera video.ID="Media5-0ZIN14"LBL="" HELPID=""Compression StandardsStandards provide a common ground for developers to share technology. Standards for the audio and video industries are constantly being developed and changed in response to new technology. The Compression Library supports these standards through the use of algorithms and parameters.ID="Media5-0ZIN15"LBL="" HELPID=""Compression Library AlgorithmsAlgorithms are provided within libcl for audio and video standards and for Silicon Graphics proprietary algorithms that have significant benefits. You can query the library for the available algorithms, and you can add your own algorithms to the library. Algorithms are grouped according to the type of data they operate on: still images, motion video, or audio.LBL="" HELPID=""Still Image AlgorithmsAlthough any algorithm can be used for still images, the JPEG ID="Media5-0ZIN16"(Joint Photographic Experts Group)-baseline algorithm, which is referred to simply as JPEG for the remainder of this guide, is the best for most applications.JPEG is a compression standard for compressing full-color or grayscale digital images. JPEG is most useful for still images; it is usable, but slow when performed in software, for video. You can use the Cosmo Compress option, a hardware JPEG accelerator, in conjunction with the Compression Library for compressing video to and decompressing video from memory or for compressing to and decompressing from a special video connection to Galileo Video, IndyVideo, or Indigo2 video.JPEG is a lossy algorithm, meaning that the compressed image is not a perfect representation of the original image, but you may not be able to detect the differences with the naked eye.The amount of compression and the quality of the resulting image are independent of the image data. The quality depends on the compression ratio. The Compression Library lets you select the compression ratio that best suits your application needs.JPEG is designed for still images and is usable, but slow, for video. JPEG is typically used to compress each still frame during the writing or editing process, with the intention being to apply another type of compression to the final version of the movie or to leave it uncompressed. JPEG works better on high-resolution, continuous-tone images such as photographs, than on crisp-edged, high-contrast images like line drawings.LBL="" HELPID=""Movie AlgorithmsFor the best quality in a final movie, all image manipulation and storage should be with uncompressed images until the final movie is produced, at which time the images can be compressed. Repeatedly compressing, decompressing, and then recompressing images reduces the image quality.The Compression Library supports the following algorithms for motion video compression/decompression: CL_MPEG_VIDEOID="Media5-0ZIN17"Moving Pictures Expert Group is a standard that is designed for extreme compression of motion video while maintaining high image quality. It is a lossy algorithm that is capable of producing higher compression ratios than both JPEG and MVC1.MPEG I is designed to give the best possible quality for a 1.2 million bits per second (Mbps) data rate for audio as well as video data. Other data rates are possible.The quality depends on the sophistication of the encoder. Quality (subjectively evaluated) between VHS and S-VHS can be achieved for images whose frame size is 352 name='times' font=symbol charset=fontspecific code=180
	TeX='\times ' descr='[times]' 240 with the 1.2 Mbps data rate, which is possible to obtain from a CD-ROM in real time.MPEG is an asymmetric coding techniquename='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'compression requires considerably more processing power than decompression because MPEG examines the sequence of frames and compresses it in a optimized way, including compressing the difference between frames using motion estimation.The compressed data stream is designed so that the video can be played forward or backward. This makes MPEG well suited for video publishing, where a video is compressed once and decompressed many times for playback.CL_MVC1ID="Media5-0ZIN18"Motion Video Compressor 1 is a Silicon Graphics proprietary algorithm that is a good general-purpose compression scheme. It is a color-cell compression technique that works well for video, but can cause fuzzy edges in high-contrast animation. MVC1 is a fairly lossy algorithm that does not produce compression ratios as high as JPEG, but it is well suited to movies.ID="Media5-0ZIN19"CL_MVC2Motion Video Compressor 2 provides results similar to MVC1 in terms of image quality. MVC2 compresses the data more than MVC1, but takes longer to perform the compression. Playback is faster for MVC2, because there is less data to read in, and decompression is faster than for MVC1.CL_RLEID="Media5-0ZIN20"8-bit Run Length Encode is a lossless algorithm for compressing 8-bit RGB. It is the only algorithm currently available to directly compress 8-bit RGB data (CL_RGB332). Although this algorithm is lossless, it doesn't save as much space as the other compression algorithmsname='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'typically less than 2:1 compression is achieved. The libcl implementation of RLE does not use a standard RLE method.  This is a lossless compression method that uses run-length encoding (RLE). Run-length encoding compresses images by storing a color and its run-length (the number of pixels of that color) every time the color changes. It is a good technique for animations where there are large areas that have identical colorsRun-length encoding replaces pixel values that are repeated for several pixels in a row with a single pixel at the first occurrence of a particular value, followed by a repeat count representing the number of subsequent pixels of the same value. CL_RLE2424-bit Run Length Encode is a lossless algorithm for compressing 24-bit RGB.CL_RTR1ID="Media5-0ZIN21"Real Time Record is a Silicon Graphics proprietary algorithm designed for recording directly from a camera or VTR to disk or digital audio tape (DAT) by compressing on the fly. The quality achieved is dependent upon the processor performance and video hardware that is available.LBL="" HELPID=""Audio AlgorithmsThe Compression Library supports two audio algorithms that are based on international standards:CCITT/TSB G.711 name='mgr' font=symbol charset=fontspecific code=109
	TeX='\mu '      descr='[mgr]'-lawID="Media5-0ZIN22"compresses 16-bit audio to 8-bit audio using a geometric function that takes advantage of the fact that human hearing is more sensitive to differences at lower volume levels. It is designed for rapid compression and decompression at a 2:1 compression ratio. CCITT /TSB G.711 A-lawID="Media5-0ZIN23"compresses 16-bit audio to 8-bit audio using a different geometric function that takes advantage of the fact that human hearing is more sensitive to differences at lower volume levels. It is designed for rapid compression and decompression at a 2:1 compression ratio.In America, name='mgr' font=symbol charset=fontspecific code=109
	TeX='\mu '      descr='[mgr]'-law compression is generally used. In Europe, A-law is more prevalent.LBL="" HELPID=""ID="58292"Compression Library Data FormatsThis section provides a brief introduction to digital media formats. It describes the fundamental nature of digital data and introduces some basic terminology that you should know before using the Compression Library.Many different formats exist for audio, image, and video data. The Compression Library supports the most common formats, but it doesn't restrict you to using one of these formats. In fact, you can define your own unique format to suit your application needs. For example, you can define a file format that contains interleaved frames of audio and video for a movie application, or you can define a file format that contains multiple tracks of audio data for an audio-mixing application.The following sections describe some of the data formats you are likely to encounter when developing applications that use the Compression Library.ID="Media5-0ZIN24"LBL="" HELPID=""Audio Data FormatsAudio data occurs in a stream, which can be divided into units called blocks. Audio data can be monaural (mono), which has one channel embedded in the audio stream, or stereo, which has two channels embedded in the audio stream. In a stereo audio stream, the left and right channels are interleaved. The Compression Library provides support for both mono and stereo audio. Parameters are used to distinguish between the two data types. Depending on the original source of the audio, it may have other distinguishing characteristics such as the resolution. See IDREF="37303" TYPE="TITLE"Part II, "Digital Audio and MIDI Programming," for more information about audio data formats.LBL="" HELPID=""Image Data FormatsImage data is contained in a frame. You need to supply the height and width of an image frame when using the libcl routines that compress/decompress image and video data. The ordering of pixels within the frame depends upon the source of the data. Top-to-bottom is the default data orientation for Compression Library routines. You can use the CL_ORIENTATION parameter to specify how pixels are ordered.ID="Media5-0ZIN25"The Compression Library works with data that is contained in frames. A frame is defined as a sample at one instant of time so that:1 audio sample:mono 8 bit = 1 bytemono 16 bit= 2 bytesstereo 8 bit = 2 bytesstereo 16 bit = 4 bytes1 video frame:width * height * components * bitsPerComponent/8 = n bytesLBL="" HELPID=""Video Data FormatsVideo data is a stream of sequential frames of image data. Some video formats have special frames called ID="Media5-0ZIN26"keyframes that contain information for a block of frames that is treated as a single unit. There are a variety of video formats. The Compression Library supports a set of formats for all algorithms.Video data can be either color or black-and-white. If you are working with video data, you should be familiar with such terms as component video, composite video, chrominance, luminance, and RGBA data.Implicit ID="Media5-0ZIN27"color space conversion occurs whenever the specified original format does not match the specified internal format, that is, the format that is compressed directly. Conversion from the original format to the internal format occurs on compression, and conversion from the internal format to the original format occurs on decompression. A different original format can be used on decompression than was used on compression.ID="Media5-0ZIN28"ID="Media5-0ZIN29"NoteThe parameter CL_BEST_FIT can be used when compressing to automatically choose the best internal format for a given original format.ID="Media5-0ZIN30"The Compression Library supports these video formats:CL_RGBAID="Media5-0ZIN31"R, G, B, and A data are 8-bit components packed into the 32-bit word as:0xAABBGGRRwhere:AA contains the 1-byte alpha value.BB contains the 1-byte blue value.GG contains the 1-byte green value.RR contains the 1-byte red value.RGBA component values range from 0 to 0xFF (255). For this format, compressionFormat.components should be set to 4.CL_RGBXID="Media5-0ZIN32"R, G, B, and X (don't care) data are packed into the 32-bit word as for CL_RGBA. Note that with this format, only the R, G, and B values are compressed.CL_RGBID="Media5-0ZIN33"R, G, and B data are packed into a 24-bit word. Note that with this format, the RGB triplets may cross the 32-bit word boundaries.CL_RGB332ID="Media5-0ZIN34"R, G, and B data are packed into an 8-bit byte as:0xrrrbbgggwhere:rrr is three bits of red.bb is two bits of blue.ggg is three bits of green.CL_GRAYSCALEID="Media5-0ZIN35"Four 8-bit luminance bytes are packed in a 32-bit word. CL_YID="Media5-0ZIN36"Equivalent to CL_GRAYSCALE.CL_YUVID="Media5-0ZIN37"Three 8-bit components, Y, U, and V, are packed into 24 bits as:0xUUYYVVwhere:UU contains the chroma-blue value.YY contains the luminance value.VV contains the chroma-red value.CL_YCbCrID="Media5-0ZIN38"A synonym for YUV The video specification of YUV and YCbCr dictates a scale factor for each component when converting between these formats. For convenience, the CL defines them as equal.
. Y is for luminance, Cb (chroma-blue), and Cr (chroma-red) are for chroma.CL_YUV422ID="Media5-0ZIN39"Two luminance components are packed into a 32-bit word with one U-V pair. In other words, the chroma components are sampled with half of the horizontal rate of the luma, which is known as 4:2:2 sampling. Two pixels are represented by this 32-bit word as (Y1, U1, V1) and (Y2, U1, V1). The order of the components is:0xU1Y1V1Y2where:U1 contains the chroma-blue value.Y1 contains the first luminance value.V1 contains the chroma-red value.Y2 contains the second luminance value.CL_YUV422DCID="Media5-0ZIN40"(duplicate chroma) The chroma is subsampled by 2 vertically in addition to horizontally, and is packed the same as CL_YUV422, except that U and V are duplicated on the odd lines. CL_IMAGE_WIDTH must be even when using this format. This format is convenient for storing 4:1:1 sampled data, which is analogous to 4:2:2 sampling with the addition of half-sampling of the chroma vertically. Sometimes 4:1:1 is used to indicate full vertical and one-quarter horizontal sampling.IDREF="21121" TYPE="TABLE"Table 23-1 shows the formats that are supported directly, that is, formats that do not require color conversionname='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'for each algorithm that is currently implemented in ID="Media5-0ZIN41"libcl.COLUMNS="2"LBL="23-1"Table 23-1 ID="21121"Video Formats Not Requiring Color ConversionLEFT="0" WIDTH="135"AlgorithmLEFT="140" WIDTH="180"FormatLEFT="0" WIDTH="135"UNCOMPRESSEDLEFT="140" WIDTH="180"Any formatLEFT="0" WIDTH="135"JPEGLEFT="140" WIDTH="180"CL_YUV and CL_GRAYSCALELEFT="0" WIDTH="135"MVC1LEFT="140" WIDTH="180"CL_RGBX and CL_GRAYSCALELEFT="0" WIDTH="135"MPEGLEFT="140" WIDTH="180"CL_YUV422DCLEFT="0" WIDTH="135"RLELEFT="140" WIDTH="180"CL_RGB332LEFT="0" WIDTH="135"RTR1LEFT="140" WIDTH="180"CL_YUV, CL_YUV422, CL_YUV422DC, and 
CL_GRAYSCALELBL="" HELPID=""Movie Data FormatsThe Compression Library supports the movie formats used by the Movie Maker and Movie Player tools.LBL="" HELPID=""Header FormatsSometimes data is prefaced by a header that contains information about the data. The CL provides routines for extracting header information, which can also contain CL state parameters.A typical header begins with a start code and a size:Header Start Code
Header size (in bytes)followed by parameter-value pairs such as those listed in IDREF="16603" TYPE="TABLE"Table 23-2.COLUMNS="2"LBL="23-2"Table 23-2 ID="16603"Parameters Contained in Header DataLEFT="0" WIDTH="126"ParameterLEFT="135" WIDTH="207"Information SuppliedLEFT="0" WIDTH="126"CL_ALGORITHM_IDLEFT="135" WIDTH="207"Algorithm schemeLEFT="0" WIDTH="126"CL_ALGORITHM_VERSIONLEFT="135" WIDTH="207"Version of the algorithmLEFT="0" WIDTH="126"CL_INTERNAL_FORMATLEFT="135" WIDTH="207"Format of images immediately before compressionLEFT="0" WIDTH="126"CL_NUMBER_OF_FRAMESLEFT="135" WIDTH="207"Number of frames in the sequenceLEFT="0" WIDTH="126"CL_FRAME_RATELEFT="135" WIDTH="207"Frame rateLEFT="0" WIDTH="126"CL_IMAGE_WIDTHLEFT="135" WIDTH="207"Width (image and video data only)LEFT="0" WIDTH="126"CL_IMAGE_HEIGHTLEFT="135" WIDTH="207"Height (image and video data only)Other parameters are possible, see IDREF="95875" TYPE="TITLE"Chapter 25, "Using Compression Library Algorithms and Parameters," for a complete list of parameters available.LBL="24"ID="44980"Getting Started with the Compression LibraryThis chapter describes how to use the Compression Library API to compress and decompress image, audio, and video data. The Compression Library API has three basic interfaces: ID="Media5-1GS1"still image accesssequential frame accessbuffered random frame accessIn addition, the Compression Library supports Cosmo Compress, an optional hardware JPEG video codec, which connects to the Galileo family of video devices to allow real-time JPEG video capture and playback.In this chapter:IDREF="25228" TYPE="TITLE""Overview of the Compression Library API" describes the three types of interfaces provided by the CL and explains how to use them. It also describes the CL error handling facility.IDREF="73497" TYPE="TITLE""Using the Still Image Interface" explains how to compress still images with a single call.IDREF="82040" TYPE="TITLE""Using the Sequential Frame Interface" explains how to compress or decompress sequential data using a compressor or decompressor.IDREF="82708" TYPE="TITLE""Using the Buffering Interface" explains how to use internal or external buffering to implement random access or multithreaded compression or decompression applications.IDREF="22839" TYPE="TITLE""Programming with the Cosmo Compress JPEG Codec" explains how to add Cosmo Compress support to your application.LBL="" HELPID=""ID="25228"Overview of the Compression Library APIThis section describes how each type of interface is used and provides error handling information.LBL="" HELPID=""Still Image APIThe single image method is designed to make still image compression as simple as possible. It is the simplest, yet most limited of the three. It consists of two calls, one for compression and one for decompression. No interframe compression/decompression, such as the method that takes advantage of similarities between frames in MPEG, is possible with this interface.ID="Media5-1GS2"LBL="" HELPID=""Sequential Access APIThe sequential interface is designed for audio/video-streaming applications where the input is live, or where there is no control over playback and when the amount of compressed data for each frame is known in advance; in fact, an error is reported if insufficient data is passed. This interface is more complex, requiring a series of compress or decompress calls to be encapsulated within an open-close block. Each compressor or decompressor keeps state information appropriate to the selected compression algorithm in parameters that you can query and set.ID="Media5-1GS3"LBL="" HELPID=""Buffered Access APIThe buffered interface is designed for:ID="Media5-1GS4"VCR-like control over the audio/video streammaximum efficiency by buffering compressed data and uncompressed framesblocking and nonblocking accesstransparent buffering for hardware acceleration or for multiprocessor operationmultithreaded applicationsThis interface includes the calls of the sequential interface, plus buffer-management routines to access the compressed data and the uncompressed frame buffers.The buffer management routines allow blocking and nonblocking access and accumulation of compressed data and decompressed frames. The compression or decompression modules can each be placed in separate processes. Separating the processes allows the compression or decompression process to get ahead a few frames, which is advantageous for algorithms such as MPEG, which compress the data using techniques that take advantage of similarities between frames, and it also facilitates hardware acceleration.ID="Media5-1GS5"LBL="" HELPID=""About File I/O and Error HandlingIn the CL, file I/O is handled by the caller. The CL has an error handler that prints error messages to ID="Media5-1GS6"stderr. Most CL routines return a negative error code upon failure.You can override the default error handling routine and establish an alternate compression error handling routine using ID="Media5-1GS7"clSetErrorHandler().The function prototype for clSetErrorHandler() is:CL_ErrFunc clSetErrorHandler(CL_ErrFunc efunc)where:efuncis a pointer to an error handling routine declared as:void ErrorFunc(CLhandle handle, int code, const char* fmt,...)The returned value is a pointer to the previous error handling routine.The code fragment in IDREF="57326" TYPE="TEXT"Example 24-1 demonstrates how to silence error reporting for a section of code.LBL="24-1"Example 24-1 ID="57326"Using a Custom Error Handling Routine #include <cl.h>
...
CL_ErrFunc originalErrorHandler;
void SilentCLError(CLhandle handle, int errorCode,
const char* fmt, ...)
{
/* ignore all CL errors */
}

...
originalErrorHandler = clSetErrorHandler(silentCLError);
/* cl errors here will go unnoticed */

...
clSetErrorHandler(originalErrorHandler);
/* back to normal reporting of CL errors */
...LBL="" HELPID=""ID="73497"Using the Still Image InterfaceA simple interface exists for compressing or decompressing still images with a single call. To compress a still image, use ID="Media5-1GS8"clCompressImage(), which compresses the data from the specified frameBuffer, stores the compressed image in compressedData, and stores its resulting size in compressedBufferSize.Pass to ID="Media5-1GS9"clCompressImage() the compression scheme; the width, height, and format of the image; the desired compression ratio; pointers to reference the buffer containing the image and the buffer that is to store the compressed data; and a pointer to return the size of the compressed data.You should allocate a buffer large enough to store the compressed data. In most cases, a buffer the size of the source image plus the maximum header size, which you can get by calling clQueryMaxHeaderSize(), is sufficient. When calculating the data storage of the source image, you can use the CL macro CL_BytesPerPixel() to determine the number of bytes per pixel for certain packing formats.The function prototypes for the compress and decompress image routines are:int clCompressImage(int compressionScheme, int width,
int height, int originalFormat, float compressionRatio,
void *frameBuffer, int *compressedBufferSizePtr,
void *compressedData)
int clDecompressImage(int decompressionScheme, int width,
int height, int originalFormat,int compressedBufferSize,
void *compressedData, void *frameBuffer)where:compressionSchemeis the compression or decompression scheme to use.widthis the width of the image.heightis the height of the image.originalFormatis the format of the original image to (de)compress. For video, use CL_RGB, CL_RGBX, CL_RGBA, CL_RGB332, CL_GRAYSCALE, CL_YUV, CL_YUV422, or CL_YUV422DC. For audio, use CL_MONO or CL_STEREO_INTERLEAVED.compressionRatiois the target compression ratio. The resulting quality depends on the value of this parameter and on the algorithm that is used. Use 0.0 to specify a nominal value. The nominal values for some of the algorithms are: COLUMNS="2"LEFT="0" WIDTH="72"MVC1LEFT="80" WIDTH="72"5.3:1LEFT="0" WIDTH="72"JPEGLEFT="80" WIDTH="72"15.0:1LEFT="0" WIDTH="72"MPEGLEFT="80" WIDTH="72"48.0:1 frameBufferis a pointer to the frame buffer that contains the uncompressed image data.compressedBufferSizePtris a pointer to the size, in bytes, of the compressed data buffer. If it is specified as a nonzero value, the size indicates the maximum size of the compressed data buffer. The value pointed to is overwritten by clCompressImage() when it returns the actual size of the compressed data.compressedBufferSizeis the size of the compressed data in bytes.compressedBufferis a pointer to the compressed data buffer.Use ID="Media5-1GS10"clDecompressImage() to decompress an image. clDecompressImage() decompresses the data that is stored in compressedBuffer, whose size is compressedBufferSize, and stores the resulting image in frameBuffer.The values of the state parameters that are used in conjunction with the other compression library calls have no effect on these routines, but their defaults do. The arguments width, height, originalFormat, and compressionRatio function the same as the state parameters by the same names but are given as direct arguments to facilitate the single-command interface.IDREF="94038" TYPE="TEXT"Example 24-2 demonstrates how to compress and decompress a color image using the JPEG algorithm. The image is 320 pixels wide by 240 pixels high and its data is in the RGBX format.ID="Media5-1GS11"LBL="24-2"Example 24-2 ID="94038"Compressing and Decompressing a Single Frame/* Compress and decompress a 320 by 240 RGBX image with JPEG */
int frameIndex, compressedBufferSize, maxCompressedBufferSize;
int *compressedBuffer, frameBuffer[320][240];

/* malloc a big enough buffer */
maxCompressedBufferSize = 320 * 240 * CL_BytesPerPixel(CL_RGBX)
                                + clQueryMaxHeaderSize(CL_JPEG);
compressedBuffer = (int *)malloc(maxCompressedBufferSize);

/* Compress and decompress it */
clCompressImage(CL_JPEG, 320, 240, CL_RGBX, 15.0,
    frameBuffer, &compressedBufferSize, compressedBuffer);
clDecompressImage(CL_JPEG, 320, 240, CL_RGBX,
    compressedBufferSize, compressedBuffer, frameBuffer);LBL="" HELPID=""ID="82040"Using the Sequential Frame InterfaceThis section describes how to work with sequential frames of audio or video data. See ID="Media5-1GS12"IDREF="82708" TYPE="TITLE""Using the Buffering Interface" for a description of how to work with nonsequential data, or for situations where the decompression rate is different from the compression rate.LBL="" HELPID=""Compressing a Sequence of FramesTo compress sequential data and audio/video streams, use a ID="Media5-1GS13"compressor. A compressor is an abstraction that modularizes compression operations.To compress a sequence of frames:Open a compressor to establish the beginning of a sequence of compression calls.Compress frames one at a time, storing the compressed data after each frame has been compressed.Close the compressor to deallocate the resources associated with that compressor.Each of these steps is discussed in detail in the following sections.LBL="" HELPID=""Opening a CompressorCall ID="Media5-1GS14"clOpenCompressor() to open a compressor for a given algorithm. Its function prototype is:int clOpenCompressor(int scheme, CLhandle *handlePtr)where:schemeis the compression scheme to use.handlePtris a pointer, which is overwritten by the returned handle of the compressor that is used by subsequent calls to identify the compressor.More than one compressor can be open at a time. Use the handle that is returned in handle to identify a specific compressor.LBL="" HELPID=""Compressing FramesAfter a compressor has been opened, call clCompress() to compress the data. Pass to ID="Media5-1GS15"clCompress() the handle returned by clOpenCompressor(), the number of frames to be compressed, and pointers to reference the frame buffer containing the data frames, the size of the data, and the location of the buffer that is to store the compressed data.The function prototype for clCompress() is:int clCompress(CLhandle handle, int numberOfFrames,
void *frameBuffer, int *compressedDataSize,
void *compressedBuffer);where:handleis a handle to the compressornumberOfFramesis the number of frames to compress: generally 1 for video data, an appropriate block size for audio data, or either CL_CONTINUOUS_BLOCK or CL_CONTINUOUS_NONBLOCK in order to continue compression until either the frame buffer is marked done or clCloseCompressor() is called. With CL_CONTINUOUS_NONBLOCK, the call to clCompress() returns immediately while the compression occurs in a separate thread; CL_CONTINUOUS_BLOCK blocks until compression is completed.frameBufferis a pointer to the location of the buffer that contains the data that is to be compressed. Using a NULL argument here invokes the buffered interface that is described in IDREF="82708" TYPE="TITLE""Using the Buffering Interface". An error is reported if no buffer exists. Some compressors allow a value of CL_EXTERNAL_DEVICE, indicating a direct connection to an external audio or video source.compressedDataSizeis a pointer to the returned size of the compressed data in bytes.compressedBufferis a pointer to the location where the compressed data is to be written. Using a NULL argument here invokes the buffered interface that is described in IDREF="82708" TYPE="TITLE""Using the Buffering Interface".Call clCompress() ID="Media5-1GS16"once to compress numberOfFrames sequential frames. clCompress() reads the raw data from the location pointed to by frameBuffer and writes the compressed data to the location pointed to by compressedBuffer. clCompress() returns either the number of frames successfully compressed, or in the case of CL_CONTINUOUS_NONBLOCK, returns SUCCESS immediately.The size of the compressed data is stored in compressedDataSize, even if this size exceeds the COMPRESSED_BUFFER_SIZE state parameter. If COMPRESSED_BUFFER_SIZE is less than the actual size returned by clCompress(), then the data returned in compressedBuffer is not complete. An application-allocated compressed buffer must be at least COMPRESSED_BUFFER_SIZE bytes. This parameter should be determined by calling clGetParams() after the frame buffer dimensions are defined by clSetParams(). It is not required to set the COMPRESSED_BUFFER_SIZE, because the default is the largest possible compressed data size, which is computed from the given parameters.ID="Media5-1GS17"LBL="" HELPID=""Closing a CompressorTo close a compressor, call ID="Media5-1GS18"clCloseCompressor() with the handle of the compressor you wish to close. This frees resources associated with the compressor.The code fragment in IDREF="84540" TYPE="TEXT"Example 24-3 demonstrates how to compress a series of frames using the CL_MVC1 algorithm. A compressor is opened, then a compression loop is entered, where frames are accessed one at a time and compressed using the selected algorithm, then written to a data buffer. The compressor is closed when all of the frames have been compressed.ID="Media5-1GS19"LBL="24-3"Example 24-3 ID="84540"Compressing a Series of Frames #include <dmedia/cl.h>

int pbuf[][2] = {
    CL_IMAGE_WIDTH,  0,
    CL_IMAGE_HEIGHT, 0,
    CL_COMPRESSED_BUFFER_SIZE, 0
};
 ...
/* Compress a series of frames */
clOpenCompressor(CL_MVC1, &handle);

/* set parameters */
pbuf[0][1] = 320;
pbuf[1][1] = 240;
clSetParams(handle, (int *)pbuf, 4);
/* allocate the required size buffer */
clGetParams(handle, (int *)pbuf, 6);
compressedBuffer = malloc(pbuf[2][1]);

for(i = 0; i < numberOfFrames; i++)
{
    /* Get a frame from somewhere */
    ...
    clCompress(handle, 1, frameBuffer, &compressedBufferSize,
        compressedBuffer);
    /* Write the compressed data to somewhere else. */
    ...
}
clCloseCompressor(handle);LBL="" HELPID=""Decompressing a Sequence of FramesDecompressing sequential data and audio/video streams requires the use of a decompressorID="Media5-1GS20". A decompressor is an abstraction that modularizes decompression operations.To decompress a sequence of frames:Query the stream header to get the compression scheme used.Open a decompressor to establish the beginning of a sequence of decompression calls.Decompress frames one at a time, storing the decompressed data after each frame has been decompressed.Close the decompressor to deallocate the resources associated with that decompressor.Each of these steps is discussed in detail in the following sections.LBL="" HELPID=""Getting Stream InformationTo determine which scheme to pass to the decompressor, use ID="Media5-1GS21"clQueryScheme() to get the scheme from the 16 bytes of the stream header (see IDREF="31534" TYPE="TABLE"Table 24-1 for a list of typical header contents, and ID="Media5-1GS22"IDREF="98792" TYPE="TABLE"Table 24-2 for a list of additional video stream header contents). clQueryScheme() returns the scheme, or the (negative) error code when an error occurs.Once you determine the scheme, you can open the decompressor and read the header using ID="Media5-1GS23"clReadHeader(), which returns the actual size of the header, or zero if none is detected. Use clQueryMaxHeaderSize()ID="Media5-1GS24", which returns the maximum size of the header, or zero if none is detected, to determine the size of the header to send to clReadHeader(). You should free the space used for the header buffer when you are finished with it.clReadHeader() is generally called before ID="Media5-1GS25"clCreateBuf() to help calculate the compressed buffer size. It uses the data passed to it without affecting the buffering. clReadHeader() also sets up any state parameters that can be determined from the header.The function prototypes are:int clQueryScheme(void *header)
int clQueryMaxHeaderSize(int scheme)
int clReadHeader(CLhandle handle, int headerSize,void *header)where:headeris a pointer to a buffer containing at least 16 bytes of the header.schemeis the decompression scheme to use.handleis a handle to the decompressor.headerSize is the maximum size of the header in bytes.header is a pointer to a buffer containing the header.A typical header begins with a start code and a size, followed by parameter-value pairs such as those listed in ID="Media5-1GS26"IDREF="31534" TYPE="TABLE"Table 24-1.COLUMNS="2"LBL="24-1"Table 24-1 ID="31534"Typical Stream Header ContentsLEFT="0" WIDTH="126"ParameterLEFT="135" WIDTH="206"Information suppliedLEFT="0" WIDTH="126"CL_ALGORITHM_IDLEFT="135" WIDTH="206"Algorithm schemeLEFT="0" WIDTH="126"CL_ALGORITHM_VERSIONLEFT="135" WIDTH="206"Version of the algorithmLEFT="0" WIDTH="126"CL_INTERNAL_FORMATLEFT="135" WIDTH="206"Format of images immediately before compressionLEFT="0" WIDTH="126"CL_NUMBER_OF_FRAMESLEFT="135" WIDTH="206"Number of frames in the sequenceLEFT="0" WIDTH="126"CL_FRAME_RATELEFT="135" WIDTH="206"Frame rateIn addition, video algorithms usually supply the width and height parameters listed in the header, as shown in IDREF="98792" TYPE="TABLE"Table 24-2.COLUMNS="2"LBL="24-2"Table 24-2 ID="98792"Additional Video Stream Header ContentsLEFT="0" WIDTH="126"ParameterLEFT="135" WIDTH="207"Information SuppliedLEFT="0" WIDTH="126"CL_IMAGE_WIDTHLEFT="135" WIDTH="207"WidthLEFT="0" WIDTH="126"CL_IMAGE_HEIGHTLEFT="135" WIDTH="207"HeightThe code fragment in IDREF="28733" TYPE="TEXT"Example 24-4 demonstrates how to query a stream header and read its contents.LBL="24-4"Example 24-4 ID="28733"Getting the Decompression Scheme from a Header #include <cl.h>
...
int decompressionScheme;
...
/*
 * Determine the scheme from the first 16 bytes of the
 *  header(from the beginning of video data)
*/
header = malloc(16);
read(inFile, header, 16);
decompressionScheme = clQueryScheme(header);
if(decompressionScheme < 0) {
fprintf(stderr, "Unknown compression scheme in stream
header.0);
exit(0);
}
free(header);

clOpenDecompressor(decompressionScheme, &decompressorHdl);

/* Find out how big the header can be. */
headerSize = clQueryMaxHeaderSize(decompressionScheme);
if(headerSize > 0) {
/* Read the header from the beginning of video data */
header = malloc(headerSize);
lseek(inFile, 0, SEEK_SET);LBL="" HELPID=""Opening a DecompressorCall ID="Media5-1GS27"clOpenDecompressor(), with the desired compression scheme and a pointer for returning a handle, to open a decompressor for a given algorithm. Its function prototype is:int clOpenDecompressor(int scheme, CLhandle *handlePtr)where:schemeis the decompression scheme to usehandlePtris a pointer to the returned handle of the decompressor that is used by subsequent calls to identify the decompressor.More than one decompressor can be open at a time. Use the handle that is returned in handle to identify a specific decompressor.LBL="" HELPID=""Decompressing FramesAfter a decompressor has been opened, call ID="Media5-1GS28"clDecompress() to decompress the data. Pass to clDecompress() the handle returned by clOpenDecompressor(), the number of frames to be decompressed, the size of the data, and pointers to reference the decompressed data and the frame buffer that contains the compressed frames.The function prototype for ID="Media5-1GS29"clDecompress() is:int clDecompress ( CLhandle handle, int numberOfFrames,
                   int compressedDataSize, void *compressedData
                   void *frameBuffer);where:handleis a handle to the decompressor. numberOfFramesis the number of frames to decompress: generally 1 for video data, an appropriate block size for audio data, or either CL_CONTINUOUS_BLOCK or CL_CONTINUOUS_NONBLOCK in order to continue decompression until either the frame buffer is marked done or clCloseDecompressor() is called. With CL_CONTINUOUS_NONBLOCK, the call to clDeCompress() returns immediately while the compression occurs in a separate thread; CL_CONTINUOUS_BLOCK blocks until compression is completed. Using a NULL argument invokes the buffered interface that is described in IDREF="82708" TYPE="TITLE""Using the Buffering Interface".compressedDataSizeis a pointer to the returned size of the decompressed data in bytes.compressedDatais a pointer to the location where the decompressed data is to be written.frameBufferis a pointer to the location of the frame buffer that contains the data that is to be decompressed. Some compressors allow a value of CL_EXTERNAL_DEVICE, indicating a direct connection to an external audio or video source. Using a NULL argument invokes the buffered interface that is described in IDREF="82708" TYPE="TITLE""Using the Buffering Interface". An error is reported if no buffer exists.LBL="" HELPID=""Closing a DecompressorTo close a decompressor, call ID="Media5-1GS30"clCloseDecompressor() with the handle of the decompressor you wish to close.The code fragment in IDREF="95225" TYPE="TEXT"Example 24-5 demonstrates how to decompress a series of 320 name='times' font=symbol charset=fontspecific code=180
	TeX='\times ' descr='[times]' 240 (32-bit) RGBX frames by using the CL_MVC1 algorithm. A decompressor is opened, then a decompression loop is entered, where frames are accessed one at a time and decompressed by using the selected algorithm, then written to a location such as the screen. The decompressor is closed when all of the frames have been compressed.ID="Media5-1GS31"LBL="24-5"Example 24-5  ID="95225"Decompressing a Series of Frames#include <cl.h>
...
int compressedBufferSize;
int compressedBuffer[320][240], frameBuffer[320][240];
int     width, height, k;
static int    paramBuf[][2] = {
    CL_IMAGE_WIDTH, 0,
    CL_IMAGE_HEIGHT, 0,
    CL_ORIGINAL_FORMAT, 0,
};
width = 320;
height = 240;

clOpenDecompressor(CL_MVC1, &decompressorHdl);
paramBuf[0][1] = width;
paramBuf[1][1] = height;
paramBuf[2][1] = CL_RGBX;
clSetParams(decompressorHdl, (int *)paramBuf,
sizeof(paramBuf) / sizeof(int));

for (k = 0; k < numberOfFrames; k++)
{ /* Decompress each frame and display it */
  dataSize = GetCompressedVideo(k, frameSize, data);
  clDecompress(decompressorHdl, 1, dataSize, data,
frameBuffer);
  lrectwrite(0, 0, width-1, height-1,
(unsigned int *)frameBuffer);
}
/* Close Decompressor */
clCloseDecompressor(decompressorHdl);LBL="" HELPID=""ID="82708"Using the Buffering InterfaceBuffers are used to manage compression and decompression for data that is accessed randomly, or when it is necessary to separate the task into several processes or across multiple processors. Buffering allows the accumulation of compressed data to be independent of that of decompressed frames. The buffering interface can be used for multithreaded applications.ID="Media5-1GS32"ID="Media5-1GS33"Buffers are implemented as ring buffers in ID="Media5-1GS34"libcl. A ring buffer contains a number of blocks of arbitrary size. It maintains a pointer to the buffer location, a size, and pointers to the ID="Media5-1GS35"Head of newest and ID="Media5-1GS36"Tail of oldest valid data. Separate processes can be ID="Media5-1GS37"producing (adding to the buffer) and ID="Media5-1GS38"consuming (removing from the buffer).IDREF="93526" TYPE="GRAPHIC"Figure 24-1 shows a conceptual drawing of a ring buffer.FILE="Media5-1GS.cgm" POSITION="INLINE" SCALE="FALSE"LBL="24-1"Figure 24-1 ID="93526"Ring BufferThe circle represents the ring buffer. The shaded part of the circle contains frames or data, depending on the buffer type; the blank part is free space. The size of the data (or the number of frames) available and the size of the space (or the number of frames of space) are shown by the arrows within the circles. Head marks the location where new data or frames, depending on the buffer type, are inserted. Tail marks the location where the oldest data or frames, depending on the buffer type, are removed. The head and tail march around the circle as data or frames, depending on the buffer type, are produced and consumed. The double vertical bar at the top signifies the discontinuity between the end of the buffer and the beginning of the buffer in linear physical memory.ID="Media5-1GS39"LBL="" HELPID=""Creating a BufferThe buffer management routines allow buffer space to be allocated by the library (internal) or by the application (external). A buffer often already exists in memory where the frames exist (on compression) or need to be placed (on decompression). External buffering allows this to happen without having to copy the data to or from an internal buffer. An external buffer is managed entirely within ID="Media5-1GS40"ID="Media5-1GS41"libcl as a ring buffer.Use ID="Media5-1GS42"clCreateBuf() to create an internal or external buffer. Use ID="Media5-1GS43"clDestroyBuf() to destroy an internal or external buffer. If clDecompress() or clCompress() is called with NULL for the compressed data or frame buffer parameters, then the buffer specified by clCreateBuf() is used. An error is reported if no buffer was created.The function prototypes are: CLbufferHdl * clCreateBuf (CLhandle handle, int bufferType,                 int blocks, int blockSize, void **bufferPtr)
int          clDestroyBuf (CLbufferHdl bufferHdl)where:handleis the handle to the compressor or decompressorbufferTypespecifies the type of the ring buffer, which can be either:CL_FRAME for a frame bufferCL_DATA for a data bufferblocksspecifies the number of blocks in the bufferblockSizespecifies the size in bytes of the block. This value is either 1 for data buffering or a multiple of the frame size for frame bufferingbufferPtris a pointer to a pointer to the ring buffer. If it points to a NULL pointer, it specifies an internally allocated buffer, and the value it points to receives the buffer pointerbufferHdlis a handle to the bufferThe handle returned in bufferHdl is used in subsequent buffering calls, with which you can get the buffer handle, and get the compressor or decompressor handle.Use ID="Media5-1GS44"clQueryBufferHdl() to get the buffer handle from a compressor or decompressor handle. Its function prototype is:CLbufferHdl clQueryBufferHdl(CLhandle handle,                           int bufferType, void **bufferPtr2)Use ID="Media5-1GS45"clQueryHandle() to get the compressor or decompressor handle from a buffer handle. Its function prototype is:CLhandle clQueryHandle(CLbufferHdl bufferHdl)The code fragment in IDREF="92263" TYPE="TEXT"Example 24-6 demonstrates how to create and use an internal buffer.LBL="24-6"Example 24-6 ID="92263"Creating and Using an Internal Buffer #include <cl.h>
CLhandle       handle;
CLbufferHdl    bufferHdl;
void    *buffer;
 ...
clOpenCompressor(CL_MVC1, &handle);

/* Create a buffer of 10 blocks of size 10000 */
buffer = NULL;
bufferHdl = clCreateBuf(handle, CL_DATA, 10, 10000, &buffer);
bufferHdl = clQueryBufferHdl(handle, CL_DATA, &buffer);
handle = clQueryHandle(bufferHdl);
 ...
clDestroyBuf(bufferHdl);
clCloseCompressor(handle);The code fragment in IDREF="87325" TYPE="TEXT"Example 24-7 demonstrates how to create and use an external buffer.LBL="24-7"Example 24-7 ID="87325"Creating and Using an External Buffer#include <cl.h>
CLhandle       handle;
CLbufferHdl    bufferHdl;
void    *buffer;
clOpenCompressor(CL_MVC1, &handle);

/* Create a buffer of 10 blocks of size 10000 */
buffer = malloc(10*10000);
bufferHdl = clCreateBuf(handle, CL_DATA, 10, 10000, &buffer);
bufferHdl = clQueryBufferHdl(handle, CL_DATA, &buffer);
handle = clQueryHandle(bufferHdl);
 ...
clDestroyBuf(bufferHdl);
clCloseCompressor(handle);LBL="" HELPID=""Managing BuffersThe buffer management routines are used for both uncompressed (or decompressed) frames and compressed data. When used for compressed data, they return the number of blocks (of selectable byte size) of valid contiguous data (or free space for data). When used for frames, they return the actual number of valid contiguous frames (or free space for frames).Use ID="Media5-1GS46"clQueryFree() to find out how much free space is available and where it is located.Use ID="Media5-1GS47"clUpdateHead() to notify the library that data has been placed in the ring buffer and to update the head pointer.Use ID="Media5-1GS48"clQueryValid() to find out how many blocks of valid data are available and where they are located.Use ID="Media5-1GS49"clUpdateTail() to notify the library that valid data has been consumed from the ring buffer and that data is no longer needed.Use ID="Media5-1GS50"clDoneUpdatingHead() to notify a decompressor that no more data will be arriving, in which case clDecompress() returns when the buffer empties.The function prototypes are:int clQueryFree (CLbufferHdl bufferHdl, int space                void **freeData, int *wrap)
int clUpdateHead (CLbufferHdl bufferHdl, int amountToAdd);
int clQueryValid (CLbufferHdl bufferHdl, int amount,                 void **ValidData, int *wrap)
int clUpdateTail (CLbufferHdl bufferHdl, int amountToRelease)
int clDoneUpdatingHead (CLbufferHdl bufferHdl)where:bufferHdlis a handle to a compressor buffer.spaceis the number of blocks of free space in the frame buffer to wait for. If it is zero, then the current number of blocks of space is returned without waiting.freeDatais a pointer to the returned pointer to the location where data or frames can be placed.wrapis the number of blocks that have wrapped to the beginning of the ring buffer (see IDREF="10542" TYPE="GRAPHIC"Figure 24-2 and the accompanying discussion). If it is greater than zero, then the end of the ring buffer has been reached and the routine return value will not increase (on subsequent calls) until either clUpdateHead() for free space or clUpdateTail() for valid data has been called.amountToAddis the number of blocks of free space that were written by the caller and are ready to be consumed by the library.amountis the number of blocks of valid data in the data buffer to wait for. If it is zero, then the number of blocks currently available is returned without waiting.validDatais a pointer to the returned pointer to the location where valid data can be retrieved.amountToReleaseis the number of blocks of valid data that were consumed by the call and can be reused by the library.Each compressor or decompressor can have a (compressed) data buffer and a (uncompressed) frame buffer.The block size for the uncompressed frame buffer must be a multiple of the size of one frame. This value, multiplied by the number of blocks specified, determines how many frames ahead a decompressor can get if you allow it to work ahead.LBL="" HELPID=""Producing and Consuming Data in BuffersIDREF="10542" TYPE="GRAPHIC"Figure 24-2  shows snapshots of the buffer state over time as a sequence of produce and consume processes operate on the buffer. Initially, the buffer is empty and both Head and Tail point to the beginning of the buffer. When Head and Tail are equal, the buffer is either empty or fullname='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'in this case, the buffer is empty. The library keeps track of whether the buffer is empty or full internally.In the first frame of IDREF="10542" TYPE="GRAPHIC"Figure 24-2, a process begins producingname='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'adding data to the buffer. First, a call is made to ID="Media5-1GS51"ID="Media5-1GS52"clQueryFree() to determine how much free space is available. An amount equal to the entire buffer size is returned. Data is written to the buffer, then the location of Head is updated to point to the beginning of the next available free space.In the second frame of IDREF="10542" TYPE="GRAPHIC"Figure 24-2, the next call to clQueryFree() returns the free space that exists from Head to Tail. More data is written and the Head is updated once again.In the third frame of IDREF="10542" TYPE="GRAPHIC"Figure 24-2, a process begins consumingname='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'taking data from the buffer. A call is made to ID="Media5-1GS53"ID="Media5-1GS54"clQueryValid() to determine the amount of valid data in existence. The size of the data that was written by the producers so far is returned. Data is read from the beginning of the buffer to the desired location, and Tail is updated to point to the next location containing valid data.The final frame of IDREF="10542" TYPE="GRAPHIC"Figure 24-2 shows what happens when the free space is not contiguous. When the next producer queries for the available free space, two pieces of free space existname='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'one on each side of the buffer discontinuity. The first piece of free space, which is from Head to the end of the buffer, is returned as usual. The second piece of free space, which is from the beginning of the buffer to Tail, is returned in the ID="Media5-1GS55"wrap argument. You can't write data across the buffer boundary, so it must be written to the buffer in two steps. First write the data until the end of the buffer is reached, then write the data from the beginning of the buffer until all of the data has been used. Head can then be updated to point to the next available free space. The process for reading data across the frame discontinuity is analogous.FILE="24-2.online.bw" POSITION="INLINE" SCALE="FALSE"LBL="24-2"Figure 24-2 ID="10542"Snapshots of Buffer State During Producing and Consuming ProcessesIDREF="82681" TYPE="GRAPHIC"Figure 24-3 shows the architecture of the buffer management. Rectangles represent code modules that can be placed in separate synchronized processes. The buffer management routines are shown within the boxes. Arrows show the flow of data from the modules to and from the buffers.FILE="24-3.online.bw" POSITION="INLINE" SCALE="FALSE"LBL="24-3"Figure 24-3 ID="82681"Flow of Data in a Buffered Compression and Decompression SchemeLBL="" HELPID=""Creating a Buffered Record and Play ApplicationThis section provides several examples of how to use buffering. Blocking and nonblocking playback and record examples are provided.LBL="" HELPID=""Creating a Basic Buffered Playback ApplicationThe code fragment in IDREF="15214" TYPE="TEXT"Example 24-8 demonstrates how to use buffers for a playback application. The amount of space is queried, the data is read directly into the data buffer, and the decompressor is notified of the change. The data can then be decompressed and retrieved by querying the number of frames, displaying them directly from the frame buffer, then releasing the consumed frames.ID="Media5-1GS56"LBL="24-8"Example 24-8 ID="15214"Using Buffers for Playback #include <cl.h>
 ...
actualLen = clQueryFree(decompressorHdl, len, &buf, &wrap);
read(fd, buf, actualLen);
len = clUpdateHead(dataHdl, actualLen);

clDecompress(decompressorHdl, 1, 0, NULL, NULL);

actualNumberOfFrames = clQueryValid(frameHdl, numberOfFrames,
  &frameBuffer, &wrap);
ConsumeFrames(actualNumberOfFrames, frameBuffer);
numberOfFrames = clUpdateTail(bufferHdl, actualNumberOfFrames);ID="Media5-1GS57"clUpdateHead() indicates to the library that the data has been placed in the data buffer, but does not copy the data.ID="Media5-1GS58"clDecompress() reads compressed data from the data buffer and writes uncompressed frames to the frame buffer. If space for a frame exists in the frame buffer, then it begins decompressing directly to the frame buffer. It consumes data from the data buffer until there is no more data, then it sleeps for a while and periodically continues to check for data until there is enough data. When it finishes decompressing a frame, it updates the frame buffer pointers and returns. clDecompress() does not return until decompression is complete or until an error occurs.If no more data will be added to the buffer, the application can call clDoneUpdatingHead()ID="Media5-1GS59" so that the library will not stall.ID="Media5-1GS60"clQueryValid() returns the pointer into the frame ring buffer. ID="Media5-1GS61"clUpdateTail() is required to free the internal frame buffer space, which you don't want to happen until after you consume it. The pointer to the next valid frame is kept internally, and only the actual number of frame buffers that have been decompressed are returned. The size (or numberOfFrames) returned by the routines are for the contiguous data (or frames, depending on the buffer type). The wrap argument of the clQuery() routines returns the actualLen (orID="Media5-1GS62" numberOfFrames) that have wrapped to the beginning of the buffer.The frame accesses will not cross the buffer boundary, and the wrap argument does not need to be used if both:the allocated size of the frame ring buffer is a multiple of the size of a frame times the numberOfFrames that will be requestedthe same number of frames will always be requestedIf the len (or numberOfFrames) passed to the clQuery() routines is greater than zero, the routine blocks until that much data (or that many frames) is available. If it is less than or equal to zero, then the routine returns immediately with whatever data is available. In either case, the buffer pointers are not adjusted until the clUpdate() routines are called.LBL="" HELPID=""Creating a Nonblocking Buffered Playback ApplicationThe code fragment in IDREF="68532" TYPE="TEXT"Example 24-9 demonstrates how to implement nonblocking playback.ID="Media5-1GS63"LBL="24-9"Example 24-9 ID="68532"Using Buffers for Nonblocking Playback actualLen = clQueryFree(decompressorHdl, 0, &buf, &wrap);
if((actualLen > MIN_READ_SIZE) || (wrap > 0)) {
   read(fd, buf, actualLen);
   len = clUpdateHead(decompressorHdl, actualLen);
}
/* Go do something else */
 ...Each call to clQueryFree() returns the same buf pointer, but increasing values of actualLen until MIN_READ_SIZE is reached, whereupon clUpdateHead(dataHdl) updates the pointers and the next call to clQueryFree() returns a different buf pointer and a reset actualLen. If wrap becomes greater than zero, the end of the buffer has been reached and actualLen will not get any larger, so the amount remaining in the buffer must be consumed.LBL="" HELPID=""Creating a Buffered Record ApplicationThe code fragment in IDREF="71820" TYPE="TEXT"Example 24-10 demonstrates how to use buffers for recording.ID="Media5-1GS64"LBL="24-10"Example 24-10 ID="71820"Using Buffers for Recording actualNumberOfFrames = clQueryFree(bufferHdl, numberOfFrames,
                                   &frameBuffer, &wrap);
ProduceFrames(actualNumberOfFrames, frameBuffer);
numberOfFrames = clUpdateHead(bufferHdl, actualNumberOfFrames);

clCompress(compressorHdl, 1, NULL, 0, NULL);

actualBufSize = clQueryValid(compressorHdl, bufSize, &buf,
                             &wrap);
write(fd, buf, actualBufSize);
bufSize = clUpdateTail(compressorHdl, actualBufSize);The amount of free space is queried, the frames are read directly into the frame buffer, and the compressor is notified of the change. The frames can then be compressed and the data can be retrieved by querying the amount of the data, consuming directly from the data buffer, then releasing the consumed data.clUpdateHead()ID="Media5-1GS65" indicates that the frames have been placed in the frame buffer, but does not copy the data.clCompress()ID="Media5-1GS66" reads from the frame buffer and writes to the data buffer. If a frame exists in the frame buffer, then it begins compressing directly from the frame buffer. It places compressed data in the data buffer until there is no more room, then it blocks until there is enough room. When it completes compression of a frame, it updates the frame buffer pointers and returns. clCompress() does not return until compression is complete (or an error occurs).clQueryValid()ID="Media5-1GS67" returns the pointer into the data ring buffer. ID="Media5-1GS68"clUpdateTail() is required to free the internal data buffer space, which you don't want to happen until after you consume itname='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'in this case, by writing it. The pointer to valid data is kept internally, and clUpdateTail() returns only the actual number of bytes released.The amount/numberOfFrames returned by the routines are for contiguous data/ frames. The wrap parameter of the ID="Media5-1GS69"clQuery() routines returns the amount/numberOfFrames that have wrapped to the beginning of the buffer.If the allocated size of the frame ring buffer is a multiple of the size of a frame times the numberOfFrames that will be requested, assuming that the same number of frames is always requested, then the frame accesses will not cross the buffer boundary, and the wrap parameter does not need to be used.If the amount passed to the clQuery() routines is greater than zero, then the routine blocks until that much data is available. If it is less than or equal to zero, then the routine returns immediately with whatever data is available. In either case, the buffer pointers are not adjusted until the clUpdate() routine is called.LBL="" HELPID=""Creating a Nonblocking Buffered Record ApplicationThe code fragment in IDREF="12309" TYPE="TEXT"Example 24-11 demonstrates how to use buffers for nonblocking recording.ID="Media5-1GS70"LBL="24-11"Example 24-11 ID="12309"Using Buffers for Nonblocking Recording actualLen = clQueryValid(dataHdl, 0, &buf, &wrap);
if((actualLen > MIN_READ_SIZE) || (wrap > 0)){
write(fd, buf, actualLen);
len = clUpdateTail(dataHdl, actualLen);
}Each call to ID="Media5-1GS71"clQueryValid() returns the same buf pointer, but increasing values of actualLen until MIN_READ_SIZE is reached, whereupon clUpdateTail() updates the pointers, and the next call to clQueryValid() returns a different buf pointer and a reset actualLen. If wrap becomes greater than zero, then the end of the buffer has been reached, and actualLen will not get any larger, so the amount remaining in the buffer must be consumed.Note that the consuming, compressing or decompressing, and producing have been separated into different sets of calls. The most powerful use of the interface is to separate these functional groupings into shared processes using sproc()ID="Media5-1GS72" or to allocate them to separate (shared data) processors. See sproc(2) for more information about using sproc().The buffers are set up by clCreateBuf(). In order to use data input buffering, clDecompress() receives NULL for compressedData. In order to use frame output buffering, clDecompress() receives NULL for frameBuffer.ID="Media5-1GS73"clCompress() reads from the frame buffer and writes to the data buffer. If a frame exists in the frame buffer, then it begins compressing directly from the frame buffer. It places compressed data in the data buffer until there is no more room, then it sleeps for a while and checks again until there is enough room. When it finishes compressing a frame, it updates the frame buffer pointers and returns. clCompress() does not return until compression is complete or until an error occurs.LBL="" HELPID=""Creating Buffered Multiprocess Record and Play ApplicationsIn the examples in the previous section, consuming, compressing or decompressing, and producing have been separated into different sets of calls. The most powerful use of the buffering interface is to separate these functional groups into shared processes using ID="Media5-1GS74"sproc() or to allocate them to separate (shared data) processors.The code fragment in IDREF="24002" TYPE="TEXT"Example 24-12 demonstrates how to implement multiprocess playback. The functions in boldface can be implemented as separate processes.ID="Media5-1GS75"LBL="24-12"Example 24-12 ID="24002"Using Buffers for Multiprocess Playback ProduceDataProcess()
  actualLen = clQueryFree(dataHdl, len, &buf, &wrap);
  read(fd, buf, actualLen);
  len = clUpdateHead(dataHdl, actualLen);

DecompressProcess()
  clDecompress(decompressorHdl, 1, 0, NULL, NULL);

ConsumeFrameProcess()
  actualNumberOfFrames = clQueryValid(frameHdl,
    numberOfFrames, &frameBuffer, &wrap);
  lrectwrite(0, 0, width - 1, height - 1, frameBuffer);
  numberOfFrames = clUpdateTail(frameHdl,actualNumberOfFrames);The code fragment in IDREF="59265" TYPE="TEXT"Example 24-13 demonstrates how to use buffers for multiprocess recording. The functions in boldface can be implemented as separate processes.LBL="24-13"Example 24-13 ID="59265"Using Buffers for Multiprocess Recording ProduceFrameProcess()
  actualNumberOfFrames = clQueryFree(frameHdl,
    numberOfFrames, &frameBuffer, &wrap);
  lrectread(0, 0, width - 1, height - 1, frameBuffer);
  numberOfFrames = clUpdateHead(frameHdl,
    actualNumberOfFrames);

CompressProcess()
  clCompress(compressorHdl, 1, NULL, &compressedDataSize,
      NULL);

ConsumeDataProcess()
  actualBufSize = clQueryValid(dataHdl, bufSize,&buf, &wrap);
  write(fd, buf, actualBufSize);
  bufSize = clUpdateTail(dataHdl, actualBufSize);This allows the application nonblocking access to compression and decompression. The application will almost always use ProduceDataProcess() for playback and the ProduceFrameProcess() for record, since the single process will block forever within clDecompress()/clCompress() if insufficient data or frames, depending on the buffer type, are supplied. The other processes can be made parts of the main() process. These processes could also be spread across multiple processors.LBL="" HELPID=""ID="22839"Programming with the Cosmo Compress JPEG CodecCosmo Compress is an optional hardware JPEG accelerator for workstations equipped with Galileo Video options, including: Galileo Video for Indigo2 and Indigo R4000 computers, Indigo2 Video, Indy Video, and Sirius Video. Cosmo Compress is capable of compressing to and decompressing from memory, or directly through a special video connection to Galileo Video options.Cosmo Compress JPEG is a lossy compression scheme based on psychovisual studies of human perception. Picture information that is generally not noticeable is dropped out, reducing the storage requirement anywhere from 2 to 100 times. Cosmo Compress implements a subset of the JPEG standard especially for video-originated images (baseline JPEG, interleaved YCrCb 8-bit components).LBL="" HELPID=""Cosmo Compress BasicsSee the Cosmo Compress Execution Environment Release Notes for important prerequisite information and installation instructions. Your workstation must be equipped with the following hardware and software components in order to be able to use Cosmo Compress:Cosmo Compress option and Cosmo Compress software Video option with output capabilityIris Development Option softwareYou can program Cosmo Compress from the Compression Library (CL), using either the buffered or sequential interface along with JPEG-specific and Cosmo-specific CL parameters.Cosmo Compress has four different modes of operation:compressing video from an external video device into a memory bufferdecompressing video from a buffer to an external video device compressing an image stored in memory into another area of memorydecompressing a stored compressed image into a bufferTo add Cosmo Compress support to your application:Include the dmedia/cl_cosmo header in order to get definitions for Cosmo Compress:#include <dmedia/cl_cosmo.h>Set Cosmo Compress specific compression parameters:Set image formats as described in IDREF="75325" TYPE="TITLE""Cosmo Compress Image Formats"Enable CL_ENABLE_IMAGEINFO as described in IDREF="71731" TYPE="TITLE""Getting Compressed Image Information".Specify CL_JPEG_COSMO as the scheme argument for clOpenCompressor() when opening a compressor or clOpenDecompressor() when opening a decompressor. Only one application can have Cosmo Compress open at a timename='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'an error will be returned to the program if another application has Cosmo Compress open.Compress or decompress frames.LBL="" HELPID=""ID="75325" Cosmo Compress Image FormatsThis section describes CL image parameters supported by Cosmo Compress. Cosmo Compress works with video fields, 2 of which compose a video frame.Cosmo Compress requires that input images have height and width dimensions that are multiples of 8 pixels because the JPEG compression algorithm processes images in blocks of 8x8 pixels. The CL associates two sets of image dimensions with an instance of a video compressor or decompressor: one set for uncompressed images (CL_IMAGE_WIDTH and CL_IMAGE_HEIGHT), and one set for compressed images (CL_INTERNAL_IMAGE_WIDTH and CL_INTERNAL_IMAGE_HEIGHT).For memory-to-memory compression, CL_IMAGE_WIDTH always equals CL_INTERNAL_IMAGE_WIDTH, and CL_IMAGE_HEIGHT always equals CL_INTERNAL_IMAGE_HEIGHT. IDREF="58048" TYPE="TABLE"Table 24-3 summarizes the image format parametersCOLUMNS="4"LBL="24-3"Table 24-3 ID="58048"Cosmo Compress Image Format ParametersLEFT="0" WIDTH="83"Image AttributeLEFT="90" WIDTH="123"DescriptionLEFT="220" WIDTH="145"Parameter(s)LEFT="370" WIDTH="151"ValuesLEFT="0" WIDTH="83"Pixel formatLEFT="90" WIDTH="123"Cosmo Compress supports 
32-bit RGB only for memory-
memory transfers and YCrCb 
4:2:2 only for video-memory 
transfers. LEFT="220" WIDTH="145"CL_ORIGINAL_FORMATLEFT="370" WIDTH="151"CL_RGBX (memory-memory)CL_YUV (video-memory)LEFT="0" WIDTH="83"InterlacingLEFT="90" WIDTH="123"Cosmo Compress operates on 
interlaced NTSC or PAL 
video data for video-to-
memory compression and 
memory-to-video 
decompression. Even and 
odd fields are compressed as 
separate images.LEFT="220" WIDTH="145"DM_IMAGE_INTERLACINGLEFT="370" WIDTH="151"NTSC or CCIR(525):DM_IMAGE_INTERLACED_EVENPAL or CCIR(625):DM_IMAGE_INTERLACED_ODDLEFT="0" WIDTH="83"Image orientationLEFT="90" WIDTH="123"Cosmo Compress 
compresses/decompresses 
images that have top-to-
bottom orientation. By 
default, lrectwrite(3g) draws 
images with bottom-to-top 
orientation. Use pixmode(3g) 
to set graphics orientation to 
PM_TTOB in order to 
correctly display top-to-
bottom images. LEFT="220" WIDTH="145"CL_ORIENTATIONLEFT="370" WIDTH="151"CL_TOP_DOWNDM_TOP_TO_BOTTOM (for SGI 
movies)LEFT="0" WIDTH="83"Uncompressed 
image dimensionsLEFT="90" WIDTH="123"Uncompressed image height 
(in pixels).LEFT="220" WIDTH="145"CL_IMAGE_HEIGHTLEFT="370" WIDTH="151"Range: 16­336, in multiples of 8. 
(NTSC must use either 240 or 248) 
Default: 248LEFT="0" WIDTH="83"LEFT="90" WIDTH="123"Uncompressed image width 
(in pixels ) LEFT="220" WIDTH="145"CL_IMAGE_WIDTHLEFT="370" WIDTH="151"640 (NTSC), 720 (CCIR(525) and 
CCIR(625)), 768 (PAL). Default: 640LEFT="0" WIDTH="83"Compressed 
image dimensionsLEFT="90" WIDTH="123"Compressed image height (in 
pixels) LEFT="220" WIDTH="145"CL_INTERNAL_IMAGE_HEIGHTLEFT="370" WIDTH="151"Range: 16­336, in multiples of 8. LEFT="0" WIDTH="83"LEFT="90" WIDTH="123"Uncompressed image width 
(in pixels) LEFT="220" WIDTH="145"CL_INTERNAL_IMAGE_WIDTHLEFT="370" WIDTH="151"320 (NTSC), 360 (CCIR(525) and 
CCIR(625)), 384 (PAL). Default: 320LBL="" HELPID=""ID="71731"Getting Compressed Image InformationThe CL provides a function exclusively for Cosmo Compress that lets you get information such as the size, timestamp, and a relative image index value for images (fields or frames) as they are compressed or decompressed through Cosmo Compress. When compressing from external video, the timestamp returned represents the time at which the first line of the uncompressed field arrived at the Cosmo Compress board.To get compressed image information:Call clSetParam() to set the CL_ENABLE_IMAGEINFO parameter to TRUE before compressing or decompressing any frames.Call clGetNextImageInfo() to get a structure containing information about the compressed image:int clGetNextImageInfo(CL_Handle handle,                  CLimageInfo *info, int sizeofimageinfo)handlespecifies an open handle which is actively compressing or decompressinginfois a pointer where a CLimageInfo structure is to be placedsizeofimageinfospecifies the size of the CLimageInfo structure in bytesThe CLimageInfo structure is defined in dmedia/cl.h and has the following fields: typedef struct {
    unsigned size; /* size of compressed image in bytes */
    unsigned long long ustime;   /* time in nanoseconds */
    unsigned imagecount;       /*  media stream counter */
    unsigned status;   /* additional status information */
} CLimageInfo;The ustime field returns a meaningful value only when compressing from or decompressing to an external device. The status field is reserved for future use.NoteCurrently, in order to get valid JPEG data, an application using the CL_JPEG_COSMO compressor must enable clGetNextImageInfo() by setting CL_ENABLE_IMAGEINFO, and then read a CLimageInfo structure corresponding to each compressed image, before calling clCompress to read the compressed image data.When using the CL_JPEG_COSMO decompressor, you don't need to read CLimageInfo structures. When clGetNextImageInfo() is enabled, the CL uses a small internal buffer to queue the structures during decompression. When this queue fills, the oldest structures are overwritten by new ones.clGetNextImageInfo() blocks only when it is waiting for the first valid decompressed field to exit the CL_JPEG_COSMO decompressor.LBL="" HELPID=""Memory-to-Memory Compression and DecompressionYou can use Cosmo Compress to compress images from a memory archive to a buffer. For example, you can use Cosmo Compress to compress images from a movie file to a buffer, and then insert the JPEG-compressed images into a movie file to create a compressed movie. Taking this idea a step further, you can then use Cosmo Compress to scale down the images as it decompresses them, in order to display thumbnail images similar to the ones in Movie Player. See APP="/usr/sbin/jot /usr/people/4Dgifts/examples/dmedia/compression/vidmemcomp/vidmemcomp.c" PARMS=""vidmemcomp.c
 in /usr/people/4Dgifts/examples/dmedia/compression/vidmemcomp for example code that demonstrates these concepts.LBL="" HELPID=""Memory-to-Memory CompressionTo compress frames into memory using Cosmo Compress:Set the CL image parameters to characterize the input image data. Open a CL_JPEG_COSMO compressor.Compress frames into memory. Each frame contains 2 fields.When compressing images from memory into a buffer, Cosmo Compress supports image widths of 16­768 and image heights of 16­336, in multiples of 8. You cannot scale images when compressing into memory, therefore, CL_IMAGE_WIDTH equals CL_INTERNAL_IMAGE_WIDTH, and CL_IMAGE_HEIGHT equals CL_INTERNAL_IMAGE_HEIGHT.The uncompressed data format must be 32-bit RGB (CL_RGBX), and the uncompressed image size cannot be larger than PAL video.NoteNTSC frames have a width of 243, but Cosmo Compress supports only input image widths that are multiples of 8. For NTSC, you must specify an image width of either 240 (causing the image to be cropped 3 lines from the bottom) or 248 (causing the image to be padded with 5 extra lines). Output image widths do not have to be multiples of 8.IDREF="36143" TYPE="TEXT"Example 24-14 demonstrates memory-to-memory compression of NTSC video.LBL="24-14"Example 24-14 ID="36143"Cosmo Compress Memory-to-Memory Compression#include <dmedia/cl.h>
...
    int pbuf[][2] = {
        CL_IMAGE_WIDTH,  0,
        CL_IMAGE_HEIGHT, 0,
        CL_COMPRESSED_BUFFER_SIZE, 0
    };
     ...
    /* Compress a series of frames */
    clOpenCompressor(CL_JPEG_COSMO, &handle);

    /* set parameters */
    pbuf[0][1] = 640;
    pbuf[1][1] = 240;
    clSetParams(handle, (int *)pbuf, 4);

    /* allocate the required size buffer */
    clGetParams(handle, (int *)pbuf, 6);
    compressedBuffer = malloc(pbuf[2][1]);

    for(i = 0; i < numberOfFrames; i++)
    {
        /* Get a frame from somewhere */
        ...
        clCompress(handle, 1, frameBuffer,
                   &compressedBufferSize, compressedBuffer);
        /* Write the compressed data to somewhere else. */
        ...
    }
    clCloseCompressor(handle);After compressing the images, you can use mvInsertCompressedImage() to insert the compressed images into a movie file, as described in IDREF="64505" TYPE="TITLE""Reading and Inserting Compressed Images" in Chapter 29. Since the JPEG images are stored in fields, you must read two fields for every frame.LBL="" HELPID=""Memory-to-Memory DecompressionTo decompress JPEG images from memory using Cosmo Compress:Set the CL image parameters to characterize the output image data. Open a CL_JPEG_COSMO decompressor.Decompress frames into a buffer. Each frame contains 2 fields.You can shrink the images as they are decompressed, which is useful for displaying thumbnail images. When decompressing images from memory into a buffer, Cosmo Compress supports image widths of 16­768 and image heights of 16­336. Scaling can be arbitrary, that is, you can scale the image dimensions down by any amount, and the output image dimensions do not have to be multiples of 8. To shrink images as they are decompressed, make the uncompressed image dimensions (CL_IMAGE_WIDTH and CL_IMAGE_HEIGHT) less than the corresponding compressed image dimensions (CL_INTERNAL_IMAGE_WIDTH and CL_INTERNAL_IMAGE_HEIGHT). LBL="" HELPID=""Compressing and Decompressing Video Through External Connections to Cosmo CompressYou can use Cosmo Compress as a real-time JPEG codec between your application and an external video device. This section explains how to use Cosmo Compress to compress images from an external video connection into memory and decompress JPEG images from memory to a video device.LBL="" HELPID=""Video-to-Memory Compression To capture video from an external video device using Cosmo Compress:Connect the video device to the appropriate port. For example, use either analog port 1 or digital port 1. Video port connections are managed from the videopanel control panel.Open a connection to the video server by calling vlOpenVideo( "" ). Create the video transfer paths.Get the source (VL_SRC) node for the video signal connection by calling vlGetNode(). Get the drain(VL_DRN) node for the Cosmo Compress connection by calling vlGetNode(). Create the path from source to drain by calling vlCreatePath(). Set up the path to share (VL_SHARE) data by calling vlSetupPaths(). TipCosmo Compress is not a video node; it is a separate device. Therefore the VL does not have a Cosmo VLnode for video paths. The port to which Cosmo Compress is connected (for example, the digital video output) is the video drain node. Set the appropriate video synchronization mode. Use slave mode (VL_EV1_SYNC_SLAVE) when capturing from the analog port; use internal mode (VL_SYNC_INTERNAL) when capturing from the digital port.Set the CL parameters for image dimensions, quality factor, and compressed image information (CL_ENABLE_IMAGEINFO).Open a CL_JPEG_COSMO compressor.Call clGetNextImageInfo() to get a structure containing information about the compressed image.Start the video transfer.Use the CL buffered interface to compress frames by calling clCompress() with CL_CONTINUOUS_NONBLOCK as the framecount parameter and CL_EXTERNAL_DEVICE as the frameBuffer parameter.NoteInstead of using CL_CONTINUOUS_NONBLOCK, you can call clCompress() from a separate thread within the program. clCompress() does not return until the transfer is complete.See APP="/usr/sbin/jot /usr/people/4Dgifts/examples/dmedia/dmrecord/cosmo_capture.c" PARMS=""cosmo_capture.c
 in /usr/people/4Dgifts/examples/dmedia/dmrecord for an example of capturing external video through Cosmo Compress.Video fields entering Cosmo Compress from the direct video connection are captured into an array of field buffers. The field buffers support field widths from 640 to 768 and field heights from 16 to 336. Field dimensions depend on the video timing, as shown in IDREF="25981" TYPE="TABLE"Table 24-4.IDREF="25981" TYPE="TABLE"Table 24-4 shows video field dimensions for the video formats supported by Cosmo Compress.COLUMNS="3"LBL="24-4"Table 24-4 ID="25981"Cosmo Compress Video Field DimensionsLEFT="0" WIDTH="110"Video FormatLEFT="115" WIDTH="110"WIdth (pixels)LEFT="230" WIDTH="110"Height (pixels)LEFT="0" WIDTH="110"NTSCLEFT="115" WIDTH="110"640LEFT="230" WIDTH="110"243LEFT="0" WIDTH="110"PALLEFT="115" WIDTH="110"768LEFT="230" WIDTH="110"288LEFT="0" WIDTH="110"CCIR(525)LEFT="115" WIDTH="110"720LEFT="230" WIDTH="110"243LEFT="0" WIDTH="110"CCIR(625)LEFT="115" WIDTH="110"720LEFT="230" WIDTH="110"288Lines in the field buffers following the end of valid video data are filled with indeterminate data (that is, they are not blacked out). When the compressed image height is less than the height of the incoming video fields, the video fields are clipped from the bottom before they are sent to the compressor. When the compressed image height is greater than the height of the incoming video fields, additional lines of indeterminate data are appended to the valid video data before the data is sent to the compressor. NoteNTSC fields have a height (243 pixels) which is not a multiple of 8. For NTSC capture, you can choose to have your application either throw away 3 lines from the bottom of each field (240 pixel height) or append 5 extra lines to the bottom of each field (248 pixel height) prior to compression. You can scale the captured image to half-size before compressing it. This allows for an additional increase in data compression by factor of 4. Specify vertical decimation by setting the compressed image height (CL_INTERNAL_IMAGE_HEIGHT) to half the size of the uncompressed image height (CL_IMAGE_HEIGHT). Compressed image heights can range from 16 to 168 and uncompressed image heights can range from 32 to 336.Specify horizontal decimation by setting the compressed image width (CL_INTERNAL_IMAGE_WIDTH) to half the size of the uncompressed image width (CL_IMAGE_WIDTH) as indicated in IDREF="65211" TYPE="TABLE"Table 24-5. COLUMNS="3"LBL="24-5"Table 24-5 ID="65211"Cosmo Compress Field Widths for Compression With DecimationLEFT="0" WIDTH="63"Video FormatLEFT="70" WIDTH="110"CL_IMAGE_WIDTH (pixels)LEFT="185" WIDTH="158"CL_INTERNAL_IMAGE_WIDTH (pixels)LEFT="0" WIDTH="63"NTSCLEFT="70" WIDTH="110"640LEFT="185" WIDTH="158"320LEFT="0" WIDTH="63"PALLEFT="70" WIDTH="110"768LEFT="185" WIDTH="158"384LEFT="0" WIDTH="63"CCIR(525)LEFT="70" WIDTH="110"720LEFT="185" WIDTH="158"360LEFT="0" WIDTH="63"CCIR(625)LEFT="70" WIDTH="110"720LEFT="185" WIDTH="158"360During video compression from an external device, CLimageInfo.imagecount is initialized to 1 when the first field is received by the compressor after calling clCompress(). The count advances when a new field arrives. If the compression data buffer fills up, then a field will be dropped, but the imagecount continues to increase. An application can thus detect a dropped field by noticing a jump in the imagecount field of more than one. The ustime indicates the time the uncompressed field entered the compressor. LBL="" HELPID=""Memory-to-Video DecompressionThe connections for decompressing from memory to an external video are set up similar to those for capturing video, except that a decompressor is opened. See APP="/usr/sbin/jot /usr/people/4Dgifts/examples/dmedia/dmplay/clInit.c" PARMS=""clInit.c
 in /usr/people/4Dgifts/examples/dmedia/dmplay for example code that initializes the CL for JPEG decompression (optionally through Cosmo Compress) from memory to external video.Video playback of the decompressed frames requires media synchronization. See APP="/usr/sbin/jot /usr/people/4Dgifts/examples/dmedia/dmplay/dmplay.c" PARMS=""dmplay.c
 and APP="/usr/sbin/jot /usr/people/4Dgifts/examples/dmedia/dmplay/streamDecompress.c" PARMS=""streamDecompress.c
 in /usr/people/4Dgifts/examples/dmedia/dmplay for more information.Uncompressed fields leaving the JPEG decompressor may optionally be scaled up by a factor of 2 in the horizontal and/or vertical dimensions. NTSC, PAL or CCIR(525)/CCIR(625) fields are then scanned out of the array of field buffers. Horizontal scaling is performed by pixel replication, vertical scaling is performed by line doubling. If the uncompressed fields leaving the decompressor have fewer lines than the field height required by the NTSC, PAL or CCIR(525)/CCIR(625) connection (after optional pistoling), additional lines of indeterminate (not blacked out) data will be scanned out of the field buffers to pad out bottoms of the uncompressed images. If the uncompressed fields leaving the decompressor have more lines than the NTSC/PAL/CCIR(525)/CCIR(625) field height (after optional pistoling), lines will be clipped from the bottom of the uncompressed images. Specify horizontal scaling by setting the uncompressed image width (CL_IMAGE_WIDTH) that is twice the compressed image width (CL_INTERNAL_IMAGE_WIDTH) as indicated in IDREF="24808" TYPE="TABLE"Table 24-6.COLUMNS="3"LBL="24-6"Table 24-6 ID="24808"Cosmo Compress Field Widths for DecompressionLEFT="0" WIDTH="63"Video FormatLEFT="70" WIDTH="110"CL_IMAGE_WIDTH (pixels)LEFT="185" WIDTH="158"CL_INTERNAL_IMAGE_WIDTH (pixels)LEFT="0" WIDTH="63"NTSCLEFT="70" WIDTH="110"640LEFT="185" WIDTH="158"320LEFT="0" WIDTH="63"PALLEFT="70" WIDTH="110"768LEFT="185" WIDTH="158"384LEFT="0" WIDTH="63"CCIR(525)LEFT="70" WIDTH="110"720LEFT="185" WIDTH="158"360LEFT="0" WIDTH="63"CCIR(625)LEFT="70" WIDTH="110"720LEFT="185" WIDTH="158"360Specify vertical scaling by setting the uncompressed image height (CL_IMAGE_HEIGHT) to twice the size of the compressed image height (CL_INTERNAL_IMAGE_HEIGHT). Compressed image heights can range from 16 to 168 and uncompressed image heights can range from 32 to 336.During video decompression to an external device, CLimageInfo.imagecount   reflects the count of fields sent by the application to the decompressor. The ustime indicates the time that field left the decompressor. In certain situations, fields are repeated on output, in which case the imagecount will remain the same, but the ustime will increase. Cosmo Compress decompression has a 1 frame delay through Galileo/IndyVideo before the field actually leaves the machine.When transferring to or from external video, the video can be played continuously (default) or single-stepped a field or frame at a time. In either mode, the frame output is composed of either a single field replicated twice or two different fields. Specify the frame control by setting CL_COSMO_VIDEO_TRANSFER_MODE.For continuous transfer, set CL_COSMO_VIDEO_TRANSFER_MODE to CL_COSMO_VIDEO_TRANSFER_AUTO_1_FIELD for the first field in a frame, and CL_COSMO_VIDEO_TRANSFER_AUTO_2_FIELD for the second field in a frame.For manual control, set CL_COSMO_VIDEO_TRANSFER_MODE to CL_COSMO_VIDEO_TRANSFER_MANUAL_1_FIELD, and CL_COSMO_VIDEO_TRANSFER_MANUAL_2_FIELD. In manual video transfer mode, the output frame can be set to either advance or repeat the current frame or field, as specified by CL_COSMO_VIDEO_FRAME_CONTROL.You can control compression or decompression with CL_COSMO_CODEC_CONTROL. Setting CL_COSMO_CODEC_CONTROL to CL_COSMO_STOP halts compression or decompression. If clCompress() or clDecompress() was called with CL_CONTINUOUS_BLOCK, the function returns. If clCompress() or clDecompress() was called with CL_CONTINUOUS_NONBLOCK, the associated thread terminates. LBL="" HELPID=""Controlling JPEG Compressed Image QualityJPEG is a tunable algorithmname='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'you can trade quality for compression ratio and vice-versa. You can specify a hint (CL_COMPRESSION_RATIO) for an approximate compression ratio or you can set more explicit quality factors, as described next.The source image is compressed in three basic steps. Data is transformed from spatial to frequency form in eight-by- eight blocks using a discrete cosine transform (DCT). The frequency coefficients are filtered down by a linear quantization. The coefficients are Huffman-encoded into a bit stream. The process is reversed for decompression.The quantization step controls the trade-off between image quality and size. A table called the JPEG quantization table is used to scale each of the 64 DCT coefficients. The luminance (Y) and the chrominance (Cr and Cb) components each use a separate table.The CL provides two methods for controlling image quality from these quantization tables. You can specify an overall JPEG quality factor (CL_JPEG_QUALITY_FACTOR) for scaling the default JPEG quantization tables or you can manually set the quantization tables CL_JPEG_QUANTIZATION_TABLES.The JPEG algorithm does not allow you to specify exact compression ratios (or bit rate targets), so the CL_EXACT_COMPRESSION_RATIO parameter is not supported by the CL JPEG codecs.LBL="" HELPID=""Specifying a JPEG Quality FactorYou can use the CL_JPEG _QUALITY_FACTOR parameter to specify a JPEG quantization table scale factor that represents a rough percentage of the image detail preservation. This is one method to control the image loss and therefore the compression ratio for the Cosmo Compress JPEG algorithm.Each time the quality factor is set, the reference quantization tables are scaled and downloaded into the codec. The formula used to obtain the scale factor is:scalefactor = 50/quality          (quality < 50)
scalefactor = 2 - 2*quality/100;  (otherwise)The default quality is CL_JPEG_QUALITY_DEFAULT, which represents a good-quality compressed image. A quality factor of 1 results in coarse quantization, a high compression ratio, and very poor image quality. A quality factor of 100 results in the finest possible quantization, a low compression ratio (perhaps even image expansion), and near-perfect image quality. The most useful quality factor is typically in the range of 25­95. To bypass scaling, specify CL_JPEG_QUALITY_NO_SCALE.LBL="" HELPID=""Defining and Using Custom JPEG Quantization TablesYou can customize the JPEG quantization tables by using the CL_JPEG_QUANTIZATION_TABLES parameter. To set the tables, specify an unsigned short *qtables[4] argument. For each j, qtables[j] must either be NULL or point to a unsigned short[64] area of memory that represents a JPEG-baseline quantization table in natural scanning order. These custom tables are then stored as reference tables, and then scaled versions of them based on the current CL_JPEG_QUALITY_FACTOR are downloaded into the codec, becoming the tables associated with the ID j. When getting the value of CL_JPEG_QUANTIZATION_TABLES, the CL allocates the required memory and returns the currently used tables, as indicated by CL_JPEG_COMPONENT_TABLES, scaled by the value of CL_JPEG_QUALITY_FACTOR. Your application is responsible for freeing the memory allocated to return these tables.You can specify the quantization tables on a per-component basis, by using the CL_JPEG_COMPONENT_TABLES parameter. It specifies the IDs of the AC Huffman table, DC Huffman table, and quantization table to be used for each component. Currently, you cannot change this parameter for Cosmo Compressname='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'it is set up for YUV422 processing. This setting uses AC Huffman table 0, DC Huffman table 0, and quantization table 0 for component 0; AC huffman table 1, DC huffman table 1, and quantization table 1 for components 1 and 2.LBL="25"ID="95875"Using Compression Library Algorithms and ParametersThis chapter describes how to use the Compression Library algorithms and parameters. ID="Media5-2AP1"In this chapter:IDREF="75268" TYPE="TITLE""Using the Compression Library Algorithms" describes the algorithms available in the CL and explains how to use them.IDREF="66069" TYPE="TITLE""Using the Compression Library Parameters" describes the CL global parameters and explains how to use them.LBL="" HELPID=""ID="75268"Using the Compression Library AlgorithmsThis section describes how to use the algorithms that are supplied with libcl. See IDREF="63275" TYPE="TITLE"Chapter 26, "Customizing the Compression Library," for information on adding and using your own algorithms.To use one of the algorithms supplied with libcl, you need to select an appropriate algorithm for your application and specify it in the compress or decompress routines.LBL="" HELPID=""Choosing a Compression Library AlgorithmPerhaps the most important aspect of developing an application that uses libcl is selecting the appropriate algorithm to use for the application. The algorithm affects the data size and quality and the rate of compression and decompression, so it is important to consider how an algorithm will affect the end result and to consider whether a particular algorithm will achieve the desired effect. A certain amount of experimentation may be necessary.If you are interested in a particular quality level, you need to set the compression ratio to achieve that quality; if you are primarily interested in a particular data size or data rate, you need to set the compression ratio to achieve the desired data size or rate.Here are some suggestions for typical application categories:NoteThe performance is quoted for Indigo workstations with 33Mhz MIPS® R3000® processors only.Algorithms for Multimedia Information Delivery ApplicationsVideoThe key factors to consider when choosing a video compression algorithm for multimedia applications are playback speed, data size or rate, and quality.ID="Media5-2AP2"MPEG gives the best video quality for a given data size or rate, but playback speed is limited by the CPU. MVC1 is usually the best choice if MPEG is not fast enough. If an expensive frame-by-frame VCR is not available, recording in real time to disk is important, which can be done with RTR1.ID="Media5-2AP3"ID="Media5-2AP4"ID="Media5-2AP5"Audioname='mgr' font=symbol charset=fontspecific code=109
	TeX='\mu '      descr='[mgr]'-law and A-law audio compression are appropriate for some movies. If higher quality is desired, a license for Aware Inc.'s audio compression can be obtained (see ID="Media5-2AP6"Aware (5)). Algorithms for Telecommunications ApplicationsVideoThe key factors to consider when choosing a video compression algorithm for video/voice mail, video teleconferencing, and other telecommunications applications are the combined compression-decompression speed, data size/rate, and to a lesser extent, quality.ID="Media5-2AP7"MVC1 gives the best result for video of about 10 frames per second for a 160 by 120 frame size at the cost of a very high data rate. More performance can be achieved by using grayscale.ID="Media5-2AP8"AudioEither name='mgr' font=symbol charset=fontspecific code=109
	TeX='\mu '      descr='[mgr]'-law or A-law audio compression at 8KHz can be used with satisfactory results, or the audio can be left uncompressed if the degradation in sound quality is such that it renders the voice data unusable.Algorithms for Previewing AnimationsVideoThe key factors when choosing a video compression algorithm for previewing 2D and 3D animations are playback speed, quality, and, to a lesser extent, data size/rate. MVC1 gives the appropriate speed and quality.ID="Media5-2AP9"AudioAudio compression is usually not an issue for these applications.Algorithms for Editing MoviesVideoThe key factors to consider when choosing a video compression algorithm for movie editing applications are decompression speed, image quality, data size/rate, and compression speed.For motion video applications, MVC1 is the best choice, especially when the playback is provided by the MoviePlayer tool. MVC1 provides rapid decompression. Playback speed can be traded off with image quality. When recording from video hardware to disk, recording in real time to disk is important if a frame-by-frame VCR is not availablename='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'leading to the use of RTR1ID="Media5-2AP10"AudioThe current audio compression algorithms are not particularly suited to editing. Uncompressed audio is recommended.IDREF="48046" TYPE="TABLE"Table 25-1 summarizes the compression and performance relationships of the image and motion video algorithms. Compression, decompression, and codec performance measurements are in frames per second (FPS), as measured for 320 by 240 frames on Indigo workstations with 33Mhz MIPS R3000 processors only.ID="Media5-2AP11"COLUMNS="8"LBL="25-1"Table 25-1 ID="48046"Capabilities of Image and Video AlgorithmsLEFT="0" WIDTH="65"AlgorithmLEFT="70" WIDTH="61"Typicalcompressionratio from24-bit RGBLEFT="140" WIDTH="61"AveragebitsperpixelLEFT="210" WIDTH="61"Bits persecond at15 framesper secondLEFT="280" WIDTH="61"BytesperframecompressionLEFT="350" WIDTH="61"CompressLEFT="420" WIDTH="61"DecompressIDREF="Media5-2APTF0a"aLEFT="490" WIDTH="61"CodecLEFT="0" WIDTH="65"UncompressedLEFT="70" WIDTH="61"1:1LEFT="140" WIDTH="61"24LEFT="210" WIDTH="61"27.65MbLEFT="280" WIDTH="61"230.4KBLEFT="350" WIDTH="61"LEFT="420" WIDTH="61"LEFT="490" WIDTH="61"LEFT="0" WIDTH="65"RLE 8 bitLEFT="70" WIDTH="61"4.8:1LEFT="140" WIDTH="61"5LEFT="210" WIDTH="61"5.76MbLEFT="280" WIDTH="61"48KBLEFT="350" WIDTH="61"6 FPSLEFT="420" WIDTH="61"11.5 FPSLEFT="490" WIDTH="61"3.9 FPSLEFT="0" WIDTH="65"MVC1LEFT="70" WIDTH="61"5.33:1LEFT="140" WIDTH="61"4.5LEFT="210" WIDTH="61"5.2MbLEFT="280" WIDTH="61"43.2KBLEFT="350" WIDTH="61"3 FPSLEFT="420" WIDTH="61"25 FPSLEFT="490" WIDTH="61"2.8 FPSLEFT="0" WIDTH="65"MVC1GrayscaleLEFT="70" WIDTH="61"8:1LEFT="140" WIDTH="61"3LEFT="210" WIDTH="61"3.456MbLEFT="280" WIDTH="61"28.8KBLEFT="350" WIDTH="61"7 FPSLEFT="420" WIDTH="61"28 FPSLEFT="490" WIDTH="61"5.6 FPSLEFT="0" WIDTH="65"RTR1LEFT="70" WIDTH="61"6:1LEFT="140" WIDTH="61"4LEFT="210" WIDTH="61"4.608MbLEFT="280" WIDTH="61"38.4KBLEFT="350" WIDTH="61"NYMIDREF="Media5-2APTF0b"bLEFT="420" WIDTH="61"2.5 FPSLEFT="490" WIDTH="61"2.0 FPSLEFT="0" WIDTH="65"RTR1GrayscaleLEFT="70" WIDTH="61"9:1LEFT="140" WIDTH="61"2.67LEFT="210" WIDTH="61"3.072MbLEFT="280" WIDTH="61"25.6KBLEFT="350" WIDTH="61"NYMLEFT="420" WIDTH="61"8 FPSLEFT="490" WIDTH="61"NYMLEFT="0" WIDTH="65"JPEGLEFT="70" WIDTH="61"16:1LEFT="140" WIDTH="61"1.5LEFT="210" WIDTH="61"1.728MbLEFT="280" WIDTH="61"14.4KBLEFT="350" WIDTH="61"1.1 FPSLEFT="420" WIDTH="61"1.8 FPSLEFT="490" WIDTH="61"0.7 FPSLEFT="0" WIDTH="65"MPEGLEFT="70" WIDTH="61"48:1LEFT="140" WIDTH="61"0.5LEFT="210" WIDTH="61"0.576MbLEFT="280" WIDTH="61"4.8KBLEFT="350" WIDTH="61"<< 1 FPSLEFT="420" WIDTH="61"4.75 FPSLEFT="490" WIDTH="61"<<1 FPSLBL="a" ID="Media5-2APTF0a" Decompressed frame per second is the measured performance, including reading the data from disk, decompressing it, and writing it to the 
screen.LBL="b" ID="Media5-2APTF0b" NYMname='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'Not Yet Measured.LBL="" HELPID=""Querying Compression Library AlgorithmsThis section explains how you can get a list of available algorithms for an audio or video compressor or decompressor, along with the name and type of algorithm, or find the identifier for an algorithm given its name. Other features of the algorithms can also be queried by the application at run time. Querying algorithms, rather than having hard-coded setups, makes it possible to have an algorithm-independent interface, which lets you take advantage of future algorithms as they are implemented without redesigning your code.ID="Media5-2AP12"LBL="" HELPID=""Getting a List of AlgorithmsUse ID="Media5-2AP13"clQueryAlgorithms() to get a list of algorithms for the compressor or decompressor identified by handle. clQueryAlgorithms() returns the size of the buffer needed to contain the list of algorithms and their types.If the size of the algorithmTypeBuffer is smaller than the returned value, a partial list of the algorithms and their types is returned, and you must enlarge the algorithmTypeBuffer in order to receive a complete list.The function prototype for clQueryAlgorithms() is:int clQueryAlgorithms ( int algorithmMediaType,
                        int *algorithmTypebuffer, int bufferLength )where:algorithmMediaTypeis the media type of the algorithm, which can be either of the following values:COLUMNS="2"LEFT="0" WIDTH="72"CL_AUDIOLEFT="80" WIDTH="130"specifies an audio algorithmLEFT="0" WIDTH="72"CL_VIDEOLEFT="80" WIDTH="130"specifies a video algorithmalgorithmTypeBufferis a pointer to an array of ints into which clQueryAlgorithms() can write algorithm name/type pairs for each parameter associated with handle. The even (0,2,4,...) entries receive the algorithm name. The odd entries (1,3,5,...) receive the types. The returned types take on one of three values:COLUMNS="2"bufferLengthis the length of the buffer, in ints, pointed to by paramValueBuffer. If bufferLength is zero, then paramValueBuffer is ignored and only the return value is valid.LBL="" HELPID=""Getting an Algorithm Scheme or NameUse ID="Media5-2AP14"clQuerySchemeFromHandle() or ID="Media5-2AP15"clQuerySchemeFromName() to return the algorithm scheme identifier used by the other compression functions. Use clGetAlgorithmName()ID="Media5-2AP16" to return the algorithm name. Their function prototypes are:int clQuerySchemeFromHandle(CLhandle handle)
int clQuerySchemeFromName(int algorithmMediaType, char *name)
char *clGetAlgorithmName(int scheme)where:handleis a handle to a compressor or a decompressoralgorithmMediaTypeis the media type of the algorithm, which can be either of the following values:COLUMNS="2"nameis the algorithm nameschemeis the algorithm schemeIDREF="67580" TYPE="TEXT"Example 25-1 demonstrates how to query the CL for a list of algorithmsname='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'in this case, video algorithms. The necessary buffer size is returned in the first call to clQueryAlgorithms(), then ID="Media5-2AP17"malloc() is used to allocate enough buffer space to store the returned list of video algorithms.LBL="25-1"Example 25-1 ID="67580"Getting a List of Compression Library Algorithms#include <dmedia/cl.h>
#include <malloc.h>

int *buffer, bufferLength;
char *name;
/*
* Get a buffer containing all the video algorithms and types
*/
bufferLength = clQueryAlgorithms(CL_VIDEO, NULL, 0);
buffer = (int *)malloc(bufferLength * sizeof(int));
clQueryAlgorithms(CL_VIDEO, buffer, bufferLength);

scheme = clQuerySchemeFromName(handle);
name = clGetAlgorithmName(scheme);LBL="" HELPID=""Getting License InformationUse ID="Media5-2AP18"clQueryLicense() to obtain license information about an algorithm. The returned message is text intended for inclusion in a message box that is displayed for a user, explaining how to license an algorithm. Failure returns the license error code.ID="Media5-2AP19"The function prototype is:int clQueryLicense ( int scheme, int functionality,
                     char **message)where:schemeis the algorithm scheme.functionalityis the type of algorithm, which can be one of:CL_COMPRESSOR for compressionCL_DECOMPRESSOR for decompressionCL_CODEC for both compression and decompressionmessageis a pointer to a returned pointer to a character string containing a message.LBL="" HELPID=""ID="66069"Using the Compression Library ParametersThe CL has a group of routines for working with a set of state variables called "parameters" that are unique for each instantiation. These routines are similar to a set of routines in the audio library. You can get and set parameters, either individually or as a group; however, all of the parameters have reasonable defaults that are algorithm-dependent and need not be set. ID="Media5-2AP20"The Compression Library works with data that is contained in frames. A frame is defined as a sample in time so that:1 audio sample:mono 8 bit = 1 bytemono 16 bit = 2 bytesstereo 8 bit = 2 bytesstereo 16 bit = 4 bytes1 video frame:width * height * components * bitsPerComponent/8 = n bytesLBL="" HELPID=""Compression Library Parameter DefinitionsParameters provide state information about frame characteristics, data formats, and algorithms for each compressor/decompressor.These parameters provide information about image frame dimensions:CL_IMAGE_WIDTHThe spatial width of a sample (not relevant for audio); the video default is 320, and the audio default is 1.CL_IMAGE_HEIGHTThe spatial height of a sample (not relevant for audio); the video default is 240, and the audio default is 1. These parameters describe data formats:CL_ORIGINAL_FORMATOn compression, this is the format of the original audio or video. On decompression, this is the format that you want after decompression. The value is a symbolic constant from one of the following lists, depending on its data type:Video values are: CL_RGB, CL_RGBX (default), CL_RGBA, CL_RGB332, CL_GRAYSCALE, CL_YUV, CL_YUV422, or CL_YUV422DC.Audio values are: CL_MONO or CL_STEREO_INTERLEAVED (default).CL_INTERNAL_FORMATSome video algorithms have several "natural" formats that can be compressed without color space conversion. This parameter allows the selection of one of these formats. The video default is algorithm-specific. Not relevant for audio.CL_COMPONENTSA read-only value, as determined by CL_ORIGINAL_FORMAT, that indicates the number of components in the data. For example, audio is 1 for mono, and 2 for stereo, video is generally 1 for grayscale, and 3 or 4 for color. The audio default is 2; video default is 4.CL_BITS_PER_COMPONENTThe number of bits per component. For example, audio data is either 8-bit or 16-bit, video is generally 8-bit. The audio default is 16; video default is 8.CL_ORIENTATIONSpecifies the orientation of compressed data, which can be either:COLUMNS="2"The orientation of compressed data is always top down. When specifying compression or decompression, the original format (or final format) of the data may be bottom up. Specify this inversion by setting the CL_ORIENTATION parameter to CL_BOTTOM_UP instead of the default.These parameters describe buffer sizes and characteristics:CL_FRAME_BUFFER_SIZEThe maximum size, in bytes, of the frame buffer. If clDecompress() is called with numberOfFrames larger than 1, this value should be the frame size name='times' font=symbol charset=fontspecific code=180
	TeX='\times ' descr='[times]'numberOfFrames. Because this is almost always true for audio, CL_FRAME_BUFFER_SIZE should be set when doing audio decompression.CL_COMPRESSED_BUFFER_SIZEThe maximum size of the compressed data buffer. The default is calculated as the maximum possible size, taking into account all the factors such as algorithm, encoding method, data type, and so on. If you want to try to use a smaller buffer, you can set this value explicitly. If clCompress() is called with numberOfFrames larger than 1, this value should be the maximum compressed size of one frame name='times' font=symbol charset=fontspecific code=180
	TeX='\times ' descr='[times]'numberOfFrames. Because this is almost always true for audio, CL_COMPRESSED_BUFFER_SIZE should be set when doing audio compression. CL_NUMBER_OF_FRAMESThe number of frames in the sequence.CL_BLOCK_SIZEThe natural block size of the algorithm in samples. It is most efficient to specify numberOfFrames to be a multiple of the block size when calling clCompress() or clDecompress(). CL_PREROLLThe number of blocks of frames that must be supplied to clDecompress() before decompressed frames will be returned.CL_FRAME_RATEThe requested number of frames per second. CL_FRAME_TYPEThe decompressor fills in the frame type when it decompresses a frame. Frame type is one of:COLUMNS="2"LEFT="0" WIDTH="108"CL_KEYFRAMELEFT="115" WIDTH="153"frame is a keyframeLEFT="0" WIDTH="108"CL_INTRALEFT="115" WIDTH="153"equivalent to CL_KEYFRAMELEFT="0" WIDTH="108"CL_PREDICTEDLEFT="115" WIDTH="153"frame contains information about 
its succeeding framesLEFT="0" WIDTH="108"CL_BIDIRECTIONALLEFT="115" WIDTH="153"frame contains information about 
frames that precede and succeed 
itCL_ACTUAL_FRAME_INDEXThe frame number of the frame most recently decompressed by clDecompress().These parameters control the compression ratio and/or quality:CL_ALGORITHM_IDA parameter that can be queried to find out the scheme identifier of the algorithm of an open compressor or decompressor.CL_COMPRESSION_RATIOThe target compression ratio. Some algorithms (MVC1, JPEG, and MPEG) are tunable, that is, they allow quality to be traded for compression ratio. CL_EXACT_COMPRESSION_RATIOA flag determines whether the compression ratio is a target or must be exact. Some algorithm implementations, such as for JPEG, can be only approximated and can never be exact. For algorithms that do support it, it is generally kept within a small range that over time is guaranteed to average out to the specified compression ratio.CL_SPEEDThe relative speed of playback. A value of 1.0 for this single-precision floating point value means no change. When this value is greater than 1.0, the algorithm tries to use less CPU time by dropping frames or by reducing the quality.JPEG has the following additional parameters:CL_JPEG_COMPONENT_TABLESSpecifies the IDs of the AC Huffman table, DC Huffman table, and quantization table to be used for each component. Currently, this parameter cannot be changed directly, rather, it is set up automatically for processing the selected CL_INTERNAL_FORMAT. YUV formats use AC huffman table 0, DC huffman table 0, and quantization table 0 for component 0; AC huffman table 1, DC huffman table 1, and quantization table 1 for components 1 and 2. RGB formats use tables AC table 0, DC table 0, and quantization table 0 for all components.CL_JPEG_QUANTIZATION_TABLES Sets or gets the quantization tables to be used. For setting, an unsigned short *qtables[4] argument is specified as the argument to this parameter. For each j, qtables[j] must either be NULL or point to a unsigned short[64] area of memory which represents a JPEG base-line quantization table in natural scan order. The user-specified tables are stored as reference tables, and scaled versions of them based on the current CL_JPEG_QUALITY_FACTOR are downloaded into the codec and become the table associated with the ID j. For getting, the library allocates the memory as described above, and the tables returned to the user are those which are specified by CL_JPEG_COMPONENT_TABLES as being in use and are scaled as indicated by CL_JPEG_QUALITY_FACTOR. The user is responsible for freeing the memory. CL_JPEG_QUALITY_FACTORA JPEG quantization table scale factor that represents a rough percentage of the image detail preservation. This is one method to control the image loss and therefore the          compression ratio for the JPEG algorithm. Each time the quality factor is set, the reference quantization tables are scaled and downloaded into the codec. The formula used to obtain the scale factor is:scalefactor = 50/quality          (quality < 50)
scalefactor = 2 - 2*quality/100;  (otherwise)Using a value of 1 results in coarse quantization, a high compression ratio, and very poor image quality. Using a value of 100 results in the finest possible quantization, a low compression ratio (perhaps even image expansion), and near-perfect image quality. The most useful quality factor is typically in the range of 25­95. The default quality is CL_JPEG_QUALITY_DEFAULT, which represents a good-quality compressed image. Use the value CL_JPEG_QUALITY_NO_SCALE to bypass scaling.When CL_QUALITY_FACTOR is set, the approximate value of CL_COMPRESSION_RATIO is calculated; when CL_COMPRESSION_RATIO is set, the approximate value of CL_QUALITY_FACTOR is calculated. When decompressing JPEG, clDecompress() fills in this value. The actual compression ratio is determined by the quality factor and the image content and therefore may not be exactly what you expect.MPEG_VIDEO has the following additional parameters:CL_END_OF_SEQUENCEAn end of sequence flag. When the decompressor arrives at the end of the sequence, it sets this flag. The default is FALSE (0). RTR has the following additional parameters:CL_QUALITY_LEVELSelects a quantization table. 6 is the highest compression ratio and therefore the worst quality; 0 (default) is the lowest compression ratio and therefore the highest quality. IDREF="99650" TYPE="TABLE"Table 25-2 lists the parameters and their types, ranges, and defaults.COLUMNS="3"LBL="25-2"Table 25-2 ID="99650"Compression Library ParametersLEFT="0" WIDTH="163"ParameterLEFT="170" WIDTH="87"RangeLEFT="265" WIDTH="129"DefaultLEFT="0" WIDTH="163"CL_ALGORITHM_IDLEFT="170" WIDTH="87"Current IDLEFT="265" WIDTH="129"Current IDLEFT="0" WIDTH="163"CL_ALGORITHM_VERSIONLEFT="170" WIDTH="87"Current versionLEFT="265" WIDTH="129"Current versionLEFT="0" WIDTH="163"CL_BITS_PER_COMPONENTLEFT="170" WIDTH="87"0­32LEFT="265" WIDTH="129"Audio: 16Video: 8LEFT="0" WIDTH="163"CL_BLOCK_SIZELEFT="170" WIDTH="87"0­2 BillionLEFT="265" WIDTH="129"1, depends on algorithmLEFT="0" WIDTH="163"CL_COMPONENTS (read only)LEFT="170" WIDTH="87"0­8LEFT="265" WIDTH="129"Depends on original formatLEFT="0" WIDTH="163"CL_COMPRESSED_BUFFER_SIZELEFT="170" WIDTH="87"0­2 BillionLEFT="265" WIDTH="129"Maximum amount of 
compressed data needed for 
one frameLEFT="0" WIDTH="163"CL_COMPRESSION_RATIOLEFT="170" WIDTH="87"Depends on original 
format and 
algorithmLEFT="265" WIDTH="129"Uncompressed 1.0:1name='mgr' font=symbol charset=fontspecific code=109
	TeX='\mu '      descr='[mgr]'-law 2.0:1A-law 2.0:1Aware MultiRate I 2­4:1Aware MPEG 2­4:1MVC1 5.3:1JPEG 15.0:1MPEG Video 48.0:1RLE 1.0:1RLE24 1.5:1RTR 5.0:1LEFT="0" WIDTH="163"CL_ENABLE_IMAGEINFO(Cosmo Compress JPEG only)LEFT="170" WIDTH="87"LEFT="265" WIDTH="129"LEFT="0" WIDTH="163"CL_END_OF_SEQUENCE(MPEG_VIDEO only)LEFT="170" WIDTH="87"0 (FALSE)­1 (TRUE)LEFT="265" WIDTH="129"0 (FALSE)LEFT="0" WIDTH="163"CL_EXACT_COMPRESSION_RATIOLEFT="170" WIDTH="87"0 (FALSE)­ 1 (TRUE)LEFT="265" WIDTH="129"Audio: 0 (FALSE)Video: 0 (FALSE)LEFT="0" WIDTH="163"CL_FRAME_BUFFER_SIZELEFT="170" WIDTH="87"0­2 BillionLEFT="265" WIDTH="129"Size of one frameLEFT="0" WIDTH="163"CL_FRAME_RATELEFT="170" WIDTH="87"0­1 MillionLEFT="265" WIDTH="129"Audio: 44100.0Video: 30.0LEFT="0" WIDTH="163"CL_FRAME_TYPELEFT="170" WIDTH="87"0­2LEFT="265" WIDTH="129"Supplied by decompressorLEFT="0" WIDTH="163"CL_IMAGE_HEIGHTLEFT="170" WIDTH="87"0­1MillionLEFT="265" WIDTH="129"Audio: 1Video: 320LEFT="0" WIDTH="163"CL_IMAGE_WIDTHLEFT="170" WIDTH="87"0­1MillionLEFT="265" WIDTH="129"Audio: 1Video: 240LEFT="0" WIDTH="163"CL_INTERNAL_FORMATLEFT="170" WIDTH="87"0­maxIDREF="Media5-2APTF6a"aLEFT="265" WIDTH="129"Audio: 
CL_STEREO_INTERLEAVEDVideo: depends on algorithmLEFT="0" WIDTH="163"CL_INTERNAL_IMAGE_HEIGHTLEFT="170" WIDTH="87"LEFT="265" WIDTH="129"LEFT="0" WIDTH="163"CL_INTERNAL_IMAGE_WIDTHLEFT="170" WIDTH="87"LEFT="265" WIDTH="129"LEFT="0" WIDTH="163"CL_JPEG NUM_PARAMS (JPEG only)LEFT="170" WIDTH="87"0­2 BillionLEFT="265" WIDTH="129"0 (unknown)LEFT="0" WIDTH="163"CL_JPEG_QUALITY_FACTOR(JPEG only)LEFT="170" WIDTH="87"0­100LEFT="265" WIDTH="129"75LEFT="0" WIDTH="163"CL_JPEG_QUANTIZATION_TABLES (JPEG only)LEFT="170" WIDTH="87"LEFT="265" WIDTH="129"LEFT="0" WIDTH="163"CL_LAST_FRAME_INDEXLEFT="170" WIDTH="87"0­2 BillionLEFT="265" WIDTH="129"0LEFT="0" WIDTH="163"CL_NUMBER_OF_FRAMESLEFT="170" WIDTH="87"LEFT="265" WIDTH="129"LEFT="0" WIDTH="163"CL_NUMBER_OF_PARAMSLEFT="170" WIDTH="87"LEFT="265" WIDTH="129"LEFT="0" WIDTH="163"CL_ORIENTATIONLEFT="170" WIDTH="87"CL_TOP_DOWN to 
CL_BOTTOM_UPLEFT="265" WIDTH="129"CL_TOP_DOWNLEFT="0" WIDTH="163"CL_ORIGINAL_FORMAT LEFT="170" WIDTH="87"0­maxIDREF="Media5-2APTF6b"bLEFT="265" WIDTH="129"Audio: 
CL_STEREO_INTERLEAVEDVideo: CL_RGBX LEFT="0" WIDTH="163"CL_PREROLLLEFT="170" WIDTH="87"0­2BillionLEFT="265" WIDTH="129"0, depends on algorithmLEFT="0" WIDTH="163"CL_RTR_QUALITY_LEVEL (RTR only)LEFT="170" WIDTH="87"1­6LEFT="265" WIDTH="129"1LEFT="0" WIDTH="163"CL_SPEEDLEFT="170" WIDTH="87"0 (stopped) to 2 
Billion, depends on 
algorithmLEFT="265" WIDTH="129"1.0 (real-time playback)LBL="a" ID="Media5-2APTF6a"max = CL_MAX_NUMBER_OF_ORIGINAL_FORMATS.LBL="b" ID="Media5-2APTF6b"max = Enumerated type value that depends on the format.LBL="" HELPID=""Setting and Querying Compression Library ParametersAfter a compressor or decompressor is opened, thus specifying the compression scheme to use, various parameters can be modified using clSetParams(). All of these parameters have reasonable defaults that are algorithm-dependent and need not be set. Some parameters, such as CL_IMAGE_WIDTH and CL_IMAGE_HEIGHT for video compression, should be set, but setting them is not required.LBL="" HELPID=""Getting a List of Parameters and Parameter TypesUse clQueryParams() to get a list of valid parameters and their types for a specified a compressor or decompressor. The compressor being queried is identified by its handle. Its function prototype is:int clQueryParams(CLhandle handle,int *paramValuebuffer, int maxLength)where:handleis the handle to a compressor or decompressor.paramValuebufferis a pointer to an array of ints into which clQueryParams() can write parameter identifier/parameter type pairs for each parameter associated with the compressor or decompressor. The even (0,2,4,...) entries receive a string that is the parameter identifier. The odd entries (1,3,5,...) receive the parameter type. Parameter type is one of four values:CL_RANGE_VALUE, meaning that a parameter can assume a range of values in which the relative magnitude of the value is meaningfulname='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'that is, increasing values indicate increasing quantities of whatever this parameter controls, and vice-versa.CL_ENUM_VALUE, meaning that a parameter assumes values from an enumerated type. They have a limited range, but there is no inherent relationship between the range values.CL_FLOATING_RANGE_VALUE, meaning that a parameter can assume a range of floating point values, in which the relative magnitude of the value is meaningfulname='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'that is, increasing values indicate increasing quantities of whatever this parameter controls, and vice-versa.CL_FLOATING_ENUM_VALUE, meaning that a parameter assumes values from an enumerated type. They have a limited floating point range, but there is no inherent relationship between the range values.maxLengthis the length of the buffer, in ints, pointed to by paramValuebuffer. If maxLength is zero, then paramValuebuffer is ignored and only the return value is valid.clQueryParams() returns the size of the buffer, in ints, needed to hold all the parameter identifier/parameter type pairs for the compressor or decompressor identified by handle. The parameters are returned in the even locations of paramValuebuffer, and their types are returned in the odd locations.If the size of the paramValuebuffer is smaller than the returned value, a partial list of the parameter identifier/parameter type pairs is returned, making it necessary to enlarge the paramValuebuffer in order to receive a complete list. To avoid this situation, you can obtain the correct size of the buffer by calling clQueryParams() with a NULL buffer pointer and a maxLength of 0 to return the actual buffer length without writing any data.clQueryParams() also reports whether the parameter is one of a set of enumerated types, any integer number within a specific range, or any floating point number within a specific range. In each case, the values are numbers within the range returned by clGetMinMax() and have the defaults returned by clGetDefault().IDREF="31528" TYPE="TEXT"Example 25-2 demonstrates how to get a list of parameters for a specified compressor/decompressor.LBL="25-2"Example 25-2 ID="31528"Getting a List of Parameters for a Compressor/Decompressor#include <dmedia/cl.h>
#include <malloc.h>

/*
* Get a buffer containing all the parameters for a specified
* compressor or decompressor.
*/

int *buf, bufferLength;
bufferLength = clQueryParams(handle, 0, 0);
buf = (int *)malloc(bufferLength * sizeof(int));
clQueryParams(handle, buf, bufferLength);LBL="" HELPID=""Getting the Parameter ID that Corresponds to a Parameter NameIf you know the name of a parameter, but not its identifier, you can use clGetParamID() to get the identifier of a parameter from its name.Its function prototype is:int clGetParamID(CLhandle handle, char *name)LBL="" HELPID=""Getting and Setting Parameter ValuesYou can get or set parameter values as a group or individually.Use clGetParams() to return the current values for the parameters referenced in the paramValuebuffer array. The values are written into the odd locations of paramValuebuffer immediately after the corresponding parameters.Use clSetParams() to set the current state of the parameters referenced in the paramValuebuffer array.To change a state parameter:Call clQueryParams() to find out which parameters are available. Call clGetParams() to find out the current state.Fill in the even entries of the paramValuebuffer array corresponding to the parameters to be changed and then call clSetParams().The function prototypes are:void clGetParams ( CLhandle handle, int *paramValuebuffer,
                   int bufferLength )
void clSetParams ( CLhandle handle, int *paramValuebuffer,
                   int bufferLength )where:handleis a handle that identifies a compressor or decompressor.paramValuebufferis a pointer to an array of pairs of ints. The even elements of this array select the parameters to be read or changed. The subsequent odd elements are the current or new values of these parameters. bufferLengthis the number of ints in the buffer pointed to by paramValuebuffer.Alternatively, parameters can be changed individually with clSetParam() and clGetParam(). clGetParam() returns the current value of the parameter. clSetParam() returns the previous value of the parameter.The function prototypes are:int clGetParam(CLhandle handle, int paramID)
int clSetParam(CLhandle handle, int paramID, int value)where:handleis a handle that identifies a compressor or decompressor.paramIDis the identifier of the parameter to get or set.valueis the new value of the parameter.IDREF="67976" TYPE="TEXT"Example 25-3 demonstrates how to extract the current value of specific parameters from a list of parameters returned as a group. In this case, the current block size and preroll values are obtained from the list of parameters that are returned in paramValuebuffer from clGetParams().LBL="25-3"Example 25-3 ID="67976"Getting the Current Values of Selected Parameters#include <dmedia/cl.h>
...
/* Get the block size and preroll */
int paramValueBuffer[][2] = {
CL_BLOCK_SIZE, 0,
CL_PREROLL, 0
};
clGetParams(handle, (int *)paramValueBuffer,
sizeof(paramValueBuffer) / sizeof(int));
/* paramValueBuffer[0][1] is the block size */
/* paramValueBuffer[1][1] is the preroll */LBL="" HELPID=""Getting or Setting the Value of a Floating Point ParameterSome parameters, such as CL_COMPRESSION_RATIO and CL_FRAME_RATE, are floating point values. You don't have to cast expressions involving floating point values, because macros are provided within libcl that handle the conversions for you; even though a value is a float you can cast to an int. To set a floating point value, use the macro CL_TypeIsInt(); to retrieve a floating point value, use the macro CL_TypeIsFloat().The argument must be a variable, because the type definitions in /usr/include/dmedia/cl.h are:float*(float *) &valueint*(int *) &valueIDREF="74912" TYPE="TEXT"Example 25-4 demonstrates how to use the libcl macros to get/set a floating point parameter value.LBL="25-4"Example 25-4 ID="74912"Using Macros to Get or Set the Value of a Floating Point Parameterfloat number;
number = 3.0;
...
clSetParam(handle, CL_COMPRESSION_RATIO, CL_TypeIsInt(number));
number = CL_TypeIsFloat(clGetParam(handle,CL_COMPRESSION_RATIO));LBL="" HELPID=""Getting or Setting Individual Parameter AttributesYou can query parameters individually to get the name, defaults, and range of valid values, given the parameter identifier and a handle.Use clGetName() to return a pointer to a null-terminated string that supplies the English name of a parameter. Its function prototype is:char* clGetName(CLhandle handle, int param)where:handleis a handle that identifies a compressor or decompressor.paramis a parameter identifier.Use clGetDefault() to return the default value of the parameter specified by param. Use clSetDefault() to set the default value. Setting the default value is particularly useful when an algorithm has been added and new defaults need to be set.The function prototypes are:int clGetDefault(CLhandle handle, int param)
int clSetDefault(int scheme, int paramID, int value)where:handleis a handle that identifies a compressor or decompressor.paramIDis a parameter identifier.schemeis the identifier of the scheme for which to set the defaults.valueis the new default value associated with param.IDREF="94903" TYPE="TEXT"Example 25-5 demonstrates how to get and set defaults for a parameter. In this case, the default for the CL_ORIGINAL_FORMAT parameter is set to CL_RGBX for the specified decompressor.LBL="25-5"Example 25-5 ID="94903"Getting and Setting Parameter Defaults#include <dmedia/cl.h>
int default;
...
clOpenDecompressor(scheme, &handle);
...
default = clGetDefault(handle, CL_ORIGINAL_FORMAT);
clSetDefault(scheme, CL_ORIGINAL_FORMAT, CL_RGBX);
...Use clGetMinMax() to get the maximum and minimum values for a parameter. Use clSetMin() and clSetMax() to set new minimum and maximum parameter values, or to establish the minimum and maximum values when adding a new algorithm.The function prototypes are:int clGetMinMax ( CLhandle handle, int param, int *minParam,
                  int *maxParam)int clSetMin(int scheme, int paramID, int min)
int clSetMax(int scheme, int paramID, int max)where:handleis a handle that identifies a compressor or decompressor.paramIDis a parameter identifier.minParamis a pointer to the parameter into which clGetMinMax() can write the minimum value associated with paramID.maxParamis a pointer to the parameter into which clGetMinMax() can write the maximum value associated with paramID.schemeis the identifier of the scheme that is to have its minimum or maximum value changed.minis the new minimum value associated with paramID.maxis the new maximum value associated with paramID.IDREF="10511" TYPE="TEXT"Example 25-6 demonstrates how to get and set the minimum and maximum values of a particular parameter for the specified compressor or decompressor.LBL="25-6"Example 25-6 ID="10511"Getting and Setting Minimum and Maximum Parameter Values#include <dmedia/cl.h>
int oldMin, oldMax;
...
clOpenDecompressor(scheme, &handle);
6
...
clGetMinMax(handle, CL_ORIGINAL_FORMAT, &oldMin, &oldMax);
clSetMin(scheme, CL_ORIGINAL_FORMAT, CL_RGB);
clSetMax(scheme, CL_ORIGINAL_FORMAT, CL_RGB332);
...LBL="" HELPID=""Using Frame Type ParametersSome compression algorithms do not allow direct compression or decompression of an arbitrary frame. These algorithmsname='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'MPEG, CCITT H.261, and so onname='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'have blocks of frames, where each frame can be decompressed only if all previous frames in the block have been decompressed. The frame at the beginning of the block is called a keyframe.A frame can be queried for its status as a keyframe by using the CL_FRAME_TYPE state parameter. Legal values are CL_KEYFRAME (or CL_INTRA), CL_PREDICTED, and CL_BIDIRECTIONAL. Predicted frames use information from a previous keyframe, bidirectional frames use information from both previous and future reference frames, where a reference frame is either of the other two typesname='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'CL_KEYFRAME or CL_PREDICTED.The Compression Library interface allows keyframe control from the application.Some algorithms contain only keyframes, such as JPEG, MVC1, RTR, RLE, G.711, and so on. MPEG Video is the only algorithm currently supported that has all three types of frames. LBL="26"ID="63275"Customizing the Compression LibraryAudio and video compression technology is constantly evolving. The Compression Library provides the flexibility to evolve with this changing environment by letting you customize and expand libcl. This chapter explains how to add your own algorithms to the library and how to add new state parameters to these algorithms to provide capabilities that are not addressed by the standard set of parameters. To ensure capability across applications, you must use the existing interface paradigm for your custom implementations; you cannot add to or change the API.ID="Media5-3UP1"In this chapter:IDREF="81540" TYPE="TITLE""Adding Custom Algorithms to the Compression Library" explains how to add algorithms to the CL.IDREF="24443" TYPE="TITLE""Adding Custom Parameters to the Compression Library" explains how to add parameters to the CL.LBL="" HELPID=""ID="81540"Adding Custom Algorithms to the Compression LibraryFor compatibility, an algorithm that is to be added to the Compression Library must meet these requirements:Provide support for all three types of interfaces (single-image, sequential, and buffered) through the clCompress() and clDecompress() entry points.Support interfaces for the query routines clGetParams() and clSetParams().Provide the ability to specify the worst-case size of the compressed data through the CL_COMPRESSED _BUFFER_SIZE parameter so that the application can then allocate buffers of appropriate size.Report errors using clError().Use ID="Media5-3UP2"clAddAlgorithm() to add your own compression algorithms to libcl. clAddAlgorithm() adds compression algorithms to the library by passing function pointers to routines that are unique for each algorithm. When you call this added algorithm, some preprocessing is done, then the routines that have been passed to clAddAlgorithm() are called.The function prototype for clAddAlgorithm() is:int clAddAlgorithm ( char *name, int type,
                     int maximumHeaderSize,
                     FunctionPtr openCompressor,
                     FunctionPtr compress,
                     FunctionPtr closeCompressor,
                     FunctionPtr openDecompressor,
                     FunctionPtr decompress,
                     FunctionPtr closeDecompressor,
                     FunctionPtr readHeader,
                     FunctionPtr queryScheme,
                     FunctionPtr queryLicense,
                     FunctionPtr getParams,
                     FunctionPtr setParams,
                     int *compressionScheme)where:maximumHeaderSizeis the maximum size of the stream header for the specified algorithm.nameis a pointer to a string that contains the name of the algorithm.typeis the type of the algorithm (CL_AUDIO or CL_VIDEO).openCompressoris a pointer to the function that opens a compressor for the new algorithm. The function must have the same arguments as clOpenCompressor().compressis a pointer to the function that compresses for the new algorithm. The function must have the same arguments as clCompress().closeCompressoris a pointer to the function that closes a compressor for the new algorithm. The function must have the same arguments as clCloseCompressor().openDecompressoris a pointer to the function that opens a decompressor for the new algorithm. The function must have the same arguments as clOpenDecompressor().decompressis a pointer to the function that decompresses for the new algorithm. The function must have the same arguments as clDecompress().closeDecompressoris a pointer to the function that closes a decompressor for the new algorithm. The function must have the same arguments as clCloseDecompressor().readHeaderis pointer to the function that reads the stream header for the new algorithm. The function must have the same arguments as clReadHeader().querySchemeis a pointer to the function that identifies the scheme from the stream header for the new algorithm. The function must have the same arguments as clQueryScheme().queryLicenseis a pointer to the function that determines whether there is a license for the new algorithm. The function must have the same arguments as clQueryLicense().getParamsis a pointer to the function that gets compressor or decompressor parameters for the new algorithm. The function must have the same arguments as clGetParams().setParamsis a pointer to the function that sets compressor or decompressor parameters for the new algorithm. The function must have the same arguments as clSetParams().compressionSchemeis a pointer to a value that is to receive the compression scheme identifier.Argument bounds checking is performed before these functions are called. For example, if clCompress() is called, each argument is checked for validity before passing control to the function passed to clAddAlgorithm().Added algorithms must support interfaces to clGetParams() and clSetParams(), which are supplied to clAddAlgorithm(). The algorithm implementation is notified of changes to, or requests for, parameter values through these routines. This allows complete control of parameters to constrain or report errors upon setting or to calculate only when requested.Algorithms and parameters, once added, will show up when queried with clQueryAlgorithms() and clQueryParams() respectively.The algorithm implementation is required to specify the worst-case size of the compressed data for a frame through the CL_COMPRESSED_BUFFER_SIZE parameter. This value must be calculated every time it is requested, using the current value of other parameters such as CL_IMAGE_WIDTH, CL_HEIGHT, and CL_INTERNAL_FORMAT. An algorithm can also specify a value for its most natural number of frames to process at a time in the CL_BLOCK_SIZE parameter.The number of blocks that need to be processed before decompressed data begins to emerge is specified in the CL_PREROLL parameter. For algorithms with a fixed compression ratio, this may allow the application to use the sequential interface. Use ID="Media5-3UP3"clSetUnique() and ID="Media5-3UP4"clGetUnique() to allow the algorithm implementation to store and retrieve algorithm-specific information that is associated with each instantiation of a compressor or decompressor.When clOpenCompressor() or clOpenDecompressor() is called, the implementation sets up the unique pointer that gets stored associated with the compressor handle. Other functions that require the information get it using that handle. clSetUnique() returns the previous unique pointer. clGetUnique() returns the current unique pointer.The function prototypes are:void * clSetUnique(CLhandle handle, void *unique)void * clGetUnique(CLhandle handle)where:handle is a handle to a compressor or decompressor.uniqueis a pointer to unique data that is associated with handle.When adding an algorithm, use ID="Media5-3UP5"clSetMin() and ID="Media5-3UP6"clSetMax() to set its minimum and maximum values, respectively. For example, you may need to set the bounds that define the legal range of the compression ratio. These settings take effect when either clOpenCompressor() or clOpenDecompressor() is called.When adding an algorithm, use clSetDefault() to set defaults. For example, you may need to specify a default compression ratio. These defaults take effect when either clOpenCompressor() or clOpenDecompressor() is called.IDREF="65887" TYPE="TEXT"Example 26-1 demonstrates how to add algorithms to the Compression Library.ID="Media5-3UP7"LBL="26-1"Example 26-1 ID="65887"Adding Algorithms to the Compression Library#include <cl.h>
 ...
int     scheme;
 ...
/* Add the new algorithm */
clAddAlgorithm("New Algorithm", CL_VIDEO,
    NEW_ALGORITHM_MAX_HEADER_SIZE,
    OpenNewCompressor, CompressNew, CloseNewCompressor,
    OpenNewDecompressor, DecompressNew, CloseNewDecompressor,
    ReadNewHeader, QueryNewScheme, QueryLicense, GetNewParams,
    SetNewParams, &newScheme);
/* Compress a series of frames (same as always) */
clOpenCompressor(newScheme, &handle);
for(i = 0; i < numberOfFrames; i++)
{
    /* Get a frame from somewhere */
    ...
    clCompress(handle, i, 1, frameBuffer, &compressedDataSize,
compressedData);
    /* Write the compressed data to somewhere else. */
    ...
}
clCloseCompressor(handle);LBL="" HELPID=""Managing Buffers for Added AlgorithmsWhen you add an algorithm, you must mirror the normal use of the buffer management calls, that is, the calls for compression and decompression are swapped and the calling order is reversed. ID="Media5-3UP8"IDREF="70320" TYPE="TEXT"Example 26-2 sets up decompression buffering for added algorithms.LBL="26-2"Example 26-2 ID="70320"Decompression Bufferinguntil numberOfFrames frames are decompressed:
   until space for a frame is available:
       actualNumberOfFrames = clQueryFree(frameHdl, 1, &frameBuffer, &wrap);
   until a frame is decompressed (and the compressed data is available):
       actualBufSize = clQueryValid(dataHdl, bufSize, &buf, &wrap);
       /* Decompress the data in "buf" and place the result in "frameBuffer" *
       actualSize = clUpdateTail(dataHdl, actualBufSize);
   actualNumberOfFrames = clUpdateHead(frameHdl, numberOfFrames);IDREF="89377" TYPE="TEXT"Example 26-3 sets up decompression buffering for added algorithms.LBL="26-3"Example 26-3 ID="89377"Compression Bufferinguntil numberOfFrames frames are compressed:
   until a frame is available:
       actualNumberOfFrames = clQueryValid(frameHdl, 1, &frameBuffer, &wrap);
   until a frame is compressed (and space for the compressed data is available):
       actualSize = clQueryFree(dataHdl, size, &buf, &wrap);
       /* Compress the frame in "frameBuffer" and place the result in "buf" */
       actualLen = clUpdateHead(dataHdl, len);
   actualNumberOfFrames = clUpdateTail(frameHdl, numberOfFrames);LBL="" HELPID=""Reading Data Across Buffer DiscontinuitiesWhen clDecompress() is called with non-NULL pointers for the compressedData or frameBuffer arguments, the data is available through the buffer management calls, so no special code is required for that case; however, care must be taken not to wait for data that will never arrive. For example, if insufficient data is passed into the compressedData or frameBuffer parameters, you don't want the application to block and wait forever for data.To avoid this situation for compressedData, you can use ID="Media5-3UP9"clReadData(), which provides an interface that removes the need for the application to know about the discontinuity of the compressed data caused by using a ring buffer. Its function prototype is:int clReadData(CLbufferHdl bufferHdl, int requestedDataSize,
void **compressedData)where:COLUMNS="2"LEFT="0" WIDTH="93"bufferHdlLEFT="100" WIDTH="241"is a handle to a compressor/decompressor.LEFT="0" WIDTH="93"requestedDataSizeLEFT="100" WIDTH="241"is the size of the requested data.LEFT="0" WIDTH="93"compressedData LEFT="100" WIDTH="241"is a pointer to the returned pointer to the compressed 
data.Because it is often not known what the size of the compressed data is, clReadData() allows the algorithm to request data of arbitrary size, such as the next piece of data that it knows it needs. When the requested data crosses a discontinuity, it is automatically pieced together in a temporary buffer. A pointer to this temporary buffer is returned. If the size of the requested data is larger than what is present in the buffer, the routine blocks until the data arrives. Alternatively, if the compressed data were passed directly to clDecompress(), no more data would arrive, no matter how long it waited and whatever data was available would be returned.clReadData() calls clQueryValidData() and clUpdateTail(dataHdl). It blocks (unless the compressed data was passed directly to clDecompress()) until the requested amount of data has accumulated and, if necessary (at the end of the ring buffer), copies the data into a temporary buffer, to guarantee one contiguous buffer.clReadData() returns the actual number of bytes read. clDone() returns the actual number of bytes updated. The requested data size is always returned unless there is an error. If the data requested crosses the discontinuity from the end to the beginning of the ring buffer, a temporary buffer is automatically created, the data from the ring buffer is copied to it, and its address is returned in the compressedData argument.An algorithm has two parts: the compressed data and the bitstream that encapsulates it. For some algorithms, such as JPEG and MPEG, the bitstream is fairly complex and must be parsed in very small segments. clReadData() is designed to be very efficient and can be used to read many small segments of a few bytes if so desired.Use ID="Media5-3UP10"clDone() to update the consumed data read by clReadData(). Its function prototype is:int clDone(CLbufferHdl bufferHdl, int amountToUpdate)where:bufferHdlis a handle to a compressor or decompressor.requestedDataSizeis the size of the requested data.compressedData is a pointer to the returned pointer to the compressed data.amountToUpdateis the amount that was consumed from the last read and therefore needs to be updated with a call to clUpdateTail(). If -1 is passed, the amount returned by the last call to clReadData() is used.In each call to clReadData(), clUpdateTail() is called to release data from the previous call to clReadData(), and clQueryValid() is called to get the new data. clDone() is used at the end of the decompress routine (just before returning) to call clUpdateTail() for data used from the last read.New algorithms should report errors with clError(). Generally, the format string starts with the routine name within which the error occurred, followed by a description of the error.The buffer architecture for adding algorithms is shown in IDREF="92916" TYPE="GRAPHIC"Figure 26-1. The routines called by the compressor and decompressor are shown. FILE="26-1.online.bw" POSITION="INLINE" SCALE="FALSE"LBL="26-1"Figure 26-1 ID="92916"Buffer Architecture for Adding AlgorithmsLBL="" HELPID=""ID="24443"Adding Custom Parameters to the Compression LibraryYou can add audio or video compression parameters to ID="Media5-3UP11"libcl. This is useful when using clAddAlgorithm() to add a new algorithm that uses parameters that don't exist in the default set of compression parameters. The application uses the new parameters as it would any of the other compression parameters. The functions for the new compression algorithm access the parameters in the same way as the application.Use ID="Media5-3UP12"clAddParam() to add parameters to the library. Its function prototype is:int clAddParam(int scheme, char *name, int type, int min,
int max, int initial, int *paramID)where:schemeis the compression scheme to add a parameter to. nameis a pointer to a string that contains the name of the parameter. type is the type of the parameter: CL_ENUM_VALUE, CL_RANGE_VALUE, CL_POINTER, CL_FLOATING_ENUM_VALUE, or CL_FLOATING_RANGE_VALUE. min is the minimum value of the parameter. max is the maximum value of the parameter. initial is the default value of the parameter. paramID is a pointer to an int value to receive the compression parameter identifier.The code fragment in IDREF="93736" TYPE="TEXT"Example 26-4 adds a new video algorithm to the CL.LBL="26-4"Example 26-4 ID="93736"Adding Parameters to the Compression Library #include <cl.h>
int paramID;
 ...
/* Add a new algorithm */
clAddAlgorithm("New Algorithm", CL_VIDEO,
    NEW_ALGORITHM_MAX_HEADER_SIZE,
    OpenNewCompressor, CompressNew, CloseNewCompressor,
    OpenNewDecompressor, DecompressNew, CloseNewDecompressor,
    ReadNewHeader, QueryNewScheme, GetNewParams, SetNewParams,
    &newScheme);
/* Add the new parameter */
clAddParam(newScheme, "New Parameter", CL_RANGE_VALUE, 0, 100,
75, &paramID);
/* Compress a series of frames (same as always) */
clOpenCompressor(newScheme, &handle);

clSetParam(handle, paramID, 55);

...LBL="VI"ID="57019"Movie ProgrammingIDREF="36899" TYPE="TITLE"Chapter 27, "Introduction to the Movie Library,"introduces the Movie Library and describes its applications and features.IDREF="23635" TYPE="TITLE"Chapter 28, "Getting Started with the Movie Library,"explains how to set up, compile, and debug Movie Library applications.IDREF="61830" TYPE="TITLE"Chapter 29, "File I/O and Editing Movies with the Movie Library,"explains how to perform movie file I/O and how to edit movies.IDREF="30953" TYPE="TITLE"Chapter 30, "Playing Movies with the Movie Library," describes the Movie Library playback and event-handling facilities.IDREF="12291" TYPE="TITLE"Chapter 31, "Using the Movie Library with QuickTime Movies," describes basic concepts for working with QuickTIme movies, and then it explains how to add QuickTime capability to a Movie Library application. It also describes the optional QuickTime compressor Library, which provides access to QuickTime compressors for Movie Library applications.IDREF="98659" TYPE="TITLE"Chapter 32, "Using the Movie Library Sample Programs,"describes the Movie Library sample programs.LBL="27"ID="36899"Introduction to the Movie LibraryThe Movie Library is a collection of routines within the IRIS Media LibrariesÔ that provides a C language application programming interface (API) for reading, writing, editing, and playing movies on Silicon Graphics workstations. The API provides a uniform interface to movies of various formats and lets you convert movies from one format to another. This chapter describes the features and applications of the Movie Library.LBL="" HELPID=""Overview of Movie Library Features and ApplicationsThis section provides a quick overview of the features and applications of the Movie Library. You need not have specialized knowledge about digital media or synchronization methods to use the Movie Library.LBL="" HELPID=""Movie Library FeaturesThe main features of the Movie Library (libmovie) are:ID="Media6-0ZIN1"the ability to read, write, and play movie filesa file-format-independent APIfile format conversion capabilitiessupport for Silicon Graphics Movie format, versions 2.0 and 3.0support for Apple® Computer QuickTimeÔ movie formatdata compression and decompressionasynchronous playback supportflexible playback controlsupport for movies embedded in applications softwareLBL="" HELPID=""Movie Library ApplicationsYou can easily integrate playback or creation of movies into your existing application without making extensive changes to the application's main event loop, or you can incorporate movies into an application that doesn't already have an event loop, because the Movie Library uses its own event queue.Application categories include:ID="Media6-0ZIN2"3D graphics animationYou can use the Movie Library to save 3D graphics images as a movie that can be used to provide rapid playback of computationally intensive graphics scenes.scientific visualizationYou can use the Movie Library to assemble a series of images from an IRIS ExplorerÔ application into a movie in order to look for patterns that aren't apparent when looking at the still images one by one.computer-based training (CBT)You can use the Movie Library to play embedded movies to illustrate complicated procedures in interactive tutorials.live video recordingYou can use the Movie Library to convert video input from your video option board (IndigoVideo, Sirius VideoÔ, or Galileo VideoÔ) or video from Indy VideoÔ into a movie.digital video editing systemsYou can build video editing applications on top of the low-level data handling routines provided by the Movie Library.LBL="" HELPID=""Using the Movie Library with Other Silicon Graphics LibrariesOther libraries that your application can use in conjunction with the Movie Library include:Video LibraryImageVision LibraryIRIS GL or OpenGLAudio File LibraryThe Movie Library uses the capabilities of the following libraries to play movies:Audio LibraryCompression LibraryDigital Media LibraryIRIS GL or OpenGLYour application need not include the header file for the Digital Media Library, because it is included in movie.h. Normally, your application need not include the header files for the Audio Library, Compression Library, OpenGL, or IRIS GL unless it uses those libraries directly.LBL="28"ID="23635"Getting Started with the Movie LibraryThis chapter explains how to begin developing a Movie Library application. It presents terminology and describes the steps involved in developing, compiling, and debugging a Movie Library application. It also explains how to work with movie and track properties.LBL="" HELPID=""Movie Library BasicsThis section defines basic terms and explains fundamental concepts that you need to know in order to use the Movie Library, including the Movie Library programming model and movie file formats.LBL="" HELPID=""DefinitionsThe definitions in this section provide a foundation for working with the Movie Library.ID="Media6-1GS1"A digital movie is a file that contains tracks of image and audio (optional) data.ID="Media6-1GS2"A track represents a data stream. Each track can contain only one type of medium, either an image sequence or an audio soundtrack. Currently, a maximum of one image track and one audio track is allowed.ID="Media6-1GS3"Just as motion picture film is actually a sequence of individual photographs, the image track of a movie file is actually a sequence of still images, such as illustrations, camera images, or computer graphics, contained within individual image frames of uniform height and width. Some movies have special image frames called keyframes that contain information for a block of frames that is treated as a single unit.ID="Media6-1GS4"The audio track contains digitized audio (samples) of narration, music, sound effects, and so on, that are synchronized to the frames of the image track.An image frame contains a single image, composed of individual pixels. An audio frame is composed of one audio sample for each audio channel.The frame count of a movie is the total number of image frames in that movie. Image frames are numbered from zero to one less than the frame count.ID="Media6-1GS5"ID="Media6-1GS6"IDREF="50446" TYPE="GRAPHIC"Figure 28-1 is a conceptual illustration of somersault.mv, a movie with one image track, a stereo audio track, and 13 frames (only 10 are shown).FILE="Media6-1GS.ras" POSITION="INLINE" SCALE="FALSE"LBL="28-1"Figure 28-1 ID="50446"Typical Movie: somersault.mvThe frame rate of a movie refers to the number of image frames played per unit length of time. A typical frame rate is 15 frames per second (FPS), which means that each frame is displayed for 1/15 of a second.ID="Media6-1GS7"ID="Media6-1GS8"Looping is the process of repeatedly playing a frame, a sequence of frames, or an entire movie. A movie loop is a useful display method for graphical sequences that cannot be rendered in real time or for any sequenced image data. When playing a movie, the movie loop can include the entire movie or a fragment of the movie, but you can only have one loop per movie. You cannot have a track loop. The Movie Library has three loop modes:nonlooping (play once through)ID="Media6-1GS9"looping (continuous play)swinging (repeatedly playing forward then backward)LBL="" HELPID=""Movie Library Programming ModelThe Movie Library programming model consists of:ID="Media6-1GS10"movie instances, which are handles to movie filestracks, which contain audio or imagesparameter-value lists, which contain movie and track propertiesA movie instance is a handle that allows you to read, write, edit, and play a movie file. It contains information about the different tracks (audio and image) in a movie and is identified by a movie ID similar in nature to a file descriptor. An application can create several movie instances at once.ID="Media6-1GS11"LBL="" HELPID=""Movie File FormatsThe Movie Library provides a file-format-independent API and lets you convert movies from one format to another. Currently supported formats are the Silicon Graphics movie format and the Apple Computer QuickTime movie format.ID="Media6-1GS12"LBL="" HELPID=""Silicon Graphics Movie FormatsSilicon Graphics uses a proprietary movie file format that has evolved over time to the current 3.0 version. You can read movies that use previous versions (1.0 and 2.0) of the file formatname='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'for example, movies that were created using the prerelease alphas of the Movie Library or previous releases of Movie Makername='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'but the Movie Library writes only Silicon Graphics version 3.0 movie files.ID="Media6-1GS13"The Silicon Graphics version 3. 0 movie format is a parameterized file format with data that is normally interleaved. It currently consists of a single image track and an optional audio track.LBL="" HELPID=""Apple Computer QuickTime File FormatSee IDREF="12291" TYPE="TITLE"Chapter 31, "Using the Movie Library with QuickTime Movies," for information on working with QuickTime movies and on using the optional Silicon Graphics QuickTime Compressor Library, which provides Movie Library support for both Apple Animation and Apple Video compression.See Inside Macintosh: QuickTime for information about developing QuickTime applications, and see the QuickTime Starter Kit User's Guide for information on how to use the Apple QuickTime utilities on the Macintosh.LBL="" HELPID=""Deciding Which Format to UseThe destination system determines which movie format to choose when writing the final version of a movie file. Use the QuickTime format only if you require cross-platform compatibility between Silicon Graphics and Apple Macintosh computers.LBL="" HELPID=""Developing a Movie Library ApplicationThis section provides a nutshell description of how to develop a Movie Library application, including compiling and debugging instructions.ID="Media6-1GS14"ID="Media6-1GS15"A good way to get started with the Movie Library is to look at the sample programs in the /usr/people/4Dgifts/examples/dmedia/movie directory. They demonstrate creating, editing, playing, and getting information about movies. See IDREF="98659" TYPE="TITLE"Chapter 32, "Using the Movie Library Sample Programs," for complete descriptions of the sample programs.LBL="" HELPID=""Outline for Developing a Movie Library ApplicationFollow these steps to develop a Movie Library application:Open an existing movie file by calling either mvOpenFile(), mvOpenMem(), or mvOpenFD() or create a new movie instance by calling mvCreateFile(), as described in IDREF="61830" TYPE="TITLE"Chapter 29, "File I/O and Editing Movies with the Movie Library."Once you have a movie instance, you can find existing tracks, add tracks to the movie, and delete tracks from the movie. If, in step 1, you created a new movie rather than open an existing movie, you must add an image track to it if one does not already exist.Read and write images and audio from and to the movie tracks using the Movie Library file I/O functions, such as mvReadFrames() and mvInsertFrames() that are described in IDREF="61830" TYPE="TITLE"Chapter 29.Use the Movie Library editing functions to efficiently implement editing tasks such as deleting part of a movie by calling mvDeleteFrames(), and copying data from one movie to another by calling mvPasteFrames(). Editing is described in IDREF="61830" TYPE="TITLE"Chapter 29.To play a movie, configure a playback window and use mvBindWindow() to associate a movie with the playback window. You can play more than one movie simultaneously, in separate windows or in the same window. Playback is described in IDREF="30953" TYPE="TITLE"Chapter 30, "Playing Movies with the Movie Library." You can't edit a movie that you have opened for playback, so write the file before attempting to play it.When you have finished with a movie, call mvClose() to destroy the movie instance.LBL="" HELPID=""Compiling and Linking a Movie Library ApplicationMovie Library applications must include the movie.h header file to obtain definitions for the library. When compiling your Movie Library application, follow these linking requirements in the order specified:ID="Media6-1GS16"-lmovie -ldmedia -lcl -lawareaudiowhere:-lmovielinks with the Movie Library (libmovie)-ldmedialinks with the DM Library (libdmedia), which provides parameter setting and ring buffering capabilities-lcllinks with the Compression Library, which provides compression capability-lawareaudiolinks with the part of the Compression Library that provides Aware audio compression-decompression (codec) support. The part of the CL that provides Aware software codecs requires separate linking to accommodate end-user licensing. Currently, applications are required to link with this library even if it is not used, but licensing is optional for the end user.If your application uses the Movie Library to play movies, you must add the following to the end of the link line:-lgl -laudio -lX11 where:-lgllinks with the IRIS GL-laudiolinks with the Audio Library-lX11links with the X11 libraryLBL="" HELPID=""Debugging a Movie Library ApplicationThe Movie Library has two facilities designed to assist you in debugging your application:ID="Media6-1GS17"ID="Media6-1GS18"a debugging version of the Movie Library, libmovie_d.a, which checks for improper usage of the Movie Library, such as out-of-bounds values and invalid parametersID="Media6-1GS19"environment variables that allow you to emulate certain types of failures such as I/O errors, without actually causing failuresLBL="" HELPID=""Using the Debugging Version of the Movie LibraryThe debugging version of the Movie Library checks for violations by setting assertions that rigidly state the requirements for a parameter or value. One typical assertion, worded in standard English, is: "Check that the current frame number is within the bounds of the number of frames in the movie," which is more precisely stated in the following assertion:ID="Media6-1GS20"ASSERT( 0 <= frame < trackLength, "Frame number out of range")To use the debugging version of the library, link with libmovie_d.a as follows:-lmovie_dand then run your program.ID="Media6-1GS21"Your application will abort with an error message if it fails an assertion. The message explains the situation that caused the error, and by implication or by explicit description, suggests a corrective action.When you have finished debugging your application, you should relink with the nondebugging library, libmovie.a, because the runtime checks imposed by the debugging library cause an undesirable performance and size overhead for a packaged application.LBL="" HELPID=""Emulating I/O FailuresWhen attempting to read data from a device such as a CD-ROM drive or over the network, two types of problems can occur:ID="Media6-1GS22"ID="Media6-1GS23"ID="Media6-1GS24"ID="Media6-1GS25"lost connectionsname='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'either the CD (or other storage medium) is missing or defective or, when reading data over the network, the remote host is unreachableID="Media6-1GS26"delaysname='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'either the seek time is too long or network delays are occurringID="Media6-1GS27"You can test how your application handles these types of I/O failures by setting the following environment variables to emulate failures without causing actual failures:COLUMNS="2"LEFT="0" WIDTH="135"MVPLAYDBG_VIDREADLEFT="140" WIDTH="198"simulates I/O errors when reading videoLEFT="0" WIDTH="135"MVPLAYDBG_AUDREADLEFT="140" WIDTH="198"simulates I/O errors when reading audio LEFT="0" WIDTH="135"MVPLAYDBG_VIDNFSLEFT="140" WIDTH="198"simulates delays when reading videoLEFT="0" WIDTH="135"MVPLAYDBG_AUDNFSLEFT="140" WIDTH="198"simulates delays when reading audioLBL="" HELPID=""Setting and Getting Movie PropertiesMovies have certain inherent properties that provide information about their contents. Global movie properties apply to a movie as a whole; track properties apply only to tracks. Tracks have general properties that apply to all types of tracks, and specific properties that depend on the track medium.ID="Media6-1GS28"ID="Media6-1GS29"ID="Media6-1GS30"ID="Media6-1GS31"ID="Media6-1GS32"Movie and track properties are stored in parameters, some of which you can set, others of which are read-only, because they are computed by the Movie Library from available information. You can also define your own parameters to represent properties not provided by the Movie Library.This section explains how to set and get movie and track parameters using the universal mvSetParams() and mvGetParams() routines; then it explains how to use the Movie Library convenience routines for setting and getting individual movie and track parameters and for setting up defaults.LBL="" HELPID=""Setting and Getting Movie and Track ParametersThe type MVid is used for both movies and for tracks so that universal set and get functions can work on both movies and tracks. The Movie Library also provides convenience routines for setting and getting movie parameters, described in IDREF="99863" TYPE="TITLE""Setting and Getting Global Movie Properties", and setting and getting track parameters, described in IDREF="21924" TYPE="TITLE""Setting and Getting Track Properties".To set or change a group of parameters for a movie or track, make a parameter-value list containing the names of the parameters you want to set and the values you want them to have, then pass this list to ID="Media6-1GS33"mvSetParams(). Its function prototype is:DMstatus mvSetParams ( MVid movieOrTrack, DMparams* params,
                       DMparams* returnParamsSetOrNull)where:movieOrTrackis the movie or the track whose parameters you want to setparamsis a pointer to a parameter-value list that contains the parameters and settings you wantreturnParamsSetOrNullis a pointer to a parameter-value list into which the Movie Library loads those parameters and values that it recognized and was able to set; if returnParamsSetOrNull is NULL, the Movie Library will not return such a listNoteSome parameters cannot be changed and some are not recognized by the Movie Library; verify the results by checking the list returned in returnParamsSetOrNull.   To retrieve a list of the current parameters for a given movie or track, call mvGetParams(). Its function prototype is:ID="Media6-1GS34"DMparams* mvGetParams ( MVid movieOrTrack )mvGetParams() returns the parameter list associated with the given movie or track. For movies, the parameter list contains the movie property settings: file format, looping mode, and so on. For tracks, the parameter list contains information about the format of the data in the track.NoteThe application should not call dmParamsDestroy() on the returned parameter-value list. The Movie Library reuses the same structure for each call to mvGetParams().  See IDREF="30927" TYPE="TITLE""Displaying Movie Parameters" in Chapter 32 for an example program called mvinfo that uses mvGetParams() to extract and print a list of movie and track parameters.LBL="" HELPID=""ID="99863"Setting and Getting Global Movie PropertiesGlobal movie properties apply to the movie as a whole.You can set these global movie properties:ID="Media6-1GS35"commentloop limitloop modetitleThe read-only (computed by the Movie Library) global movie properties are: file format, which is established when the movie is createdoptimizationYou can set and get global movie properties individually, using the Movie Library mvSetParams() and mvGetParams() routines or the DM Library dmSetParams() and dmGetParams() routines.You can make a default parameter-value list to use when creating a movie by calling mvSetMovieDefaults(), as described in IDREF="93494" TYPE="TITLE""Creating a Default Movie Configuration". If you set the default movie parameters by calling mvSetMovieDefaults(), then other parameters, such as the title, require setting only if they differ from the defaults.ID="Media6-1GS36"ID="Media6-1GS37"This section explains how to set and get the global movie properties individually using Movie Library convenience routines that are built on top of the mvSetParams() and mvGetParams() calls.LBL="" HELPID=""Setting and Getting the Movie CommentTo store a comment string in a movie file, call mvSetComment(). Its function prototype is:ID="Media6-1GS38"DMstatus mvSetComment( MVid movie, const char* comment )To retrieve the comment string that is stored in a movie file, call mvGetComment(). Its function prototype is:const char* mvGetComment( MVid movie )LBL="" HELPID=""ID="10648"Setting and Getting the Default Movie Loop ModeThe Silicon Graphics movie file format lets you store a default loop mode setting within a movie file. When opening a movie file, the Movie Library obtains the default loop mode from the movie if it is set; otherwise, it assumes MV_LOOP_NONE is the default loop mode.ID="Media6-1GS39"ID="Media6-1GS40"ID="Media6-1GS41"To store a default loop mode setting in a movie file, call ID="Media6-1GS42"mvSetLoopMode(). Its function prototype is:DMstatus mvSetLoopMode( MVid movie, MVloopmode mode )where:modedetermines the default loop mode:COLUMNS="2"LEFT="0" WIDTH="142"MV_LOOP_NONELEFT="150" WIDTH="118"sets the default mode to 
play the movie onceLEFT="0" WIDTH="142"MV_LOOP_CONTINUOUSLYLEFT="150" WIDTH="118"sets the default mode to 
play the movie 
repeatedlyLEFT="0" WIDTH="142"MV_LOOP_SWINGINGLEFT="150" WIDTH="118"sets the default mode to 
play the movie forward, 
then backward 
repeatedlyYou can set the playback loop mode independently of the default loop mode by using mvSetPlayLoopMode(), as described in IDREF="57199" TYPE="TITLE""Looping" in Chapter 30.To retrieve the loop mode setting that is stored in a movie file, call ID="Media6-1GS43"mvGetLoopMode(). Its function prototype is:ID="Media6-1GS44"MVloopmode mvGetLoopMode( MVid movie )LBL="" HELPID=""Setting and Getting the Default Movie Loop LimitYou can store a default loop limit setting in the movie file that defines the default for the number of movie loops a movie will play if its loop mode is set to MV_LOOP_CONTINUOUSLY or MV_LOOP_SWINGING. To store a default loop limit in a movie file, call ID="Media6-1GS45"mvSetLoopLimit(). Its function prototype is:ID="Media6-1GS46"ID="Media6-1GS47"DMstatus mvSetLoopLimit( MVid movie, int limit )where:limitis the number of movie loops allowed; a value of 0 means that the movie will loop indefinitelyID="Media6-1GS48"ID="Media6-1GS49"To retrieve the default loop limit setting that is stored in a movie file, call ID="Media6-1GS50"mvGetLoopLimit(). Its function prototype is:int mvGetLoopLimit( MVid movie )LBL="" HELPID=""Setting and Getting the Movie TitleTo store a title string in a movie file, call ID="Media6-1GS51"ID="Media6-1GS52"ID="Media6-1GS53"mvSetTitle(). Its function prototype is:DMstatus mvSetTitle( MVid movie, const char* title )To retrieve the title string that is stored in a movie file, call mvGetTitle(). Its function prototype is:ID="Media6-1GS54"ID="Media6-1GS55"const char* mvGetTitle( MVid movie )LBL="" HELPID=""Getting the Movie File FormatTo retrieve the file format of a movie file, call ID="Media6-1GS56"mvGetFileFormat(). Its function prototype is:ID="Media6-1GS57"ID="Media6-1GS58"MVfileformat mvGetFileFormat( MVid movie )mvGetFileFormat() returns the file format of the movie:COLUMNS="2"LEFT="0" WIDTH="120"MV_FORMAT_SGI_1LEFT="125" WIDTH="153"Silicon Graphics version 1 formatLEFT="0" WIDTH="120"MV_FORMAT_SGI_2LEFT="125" WIDTH="153"Silicon Graphics version 2 formatLEFT="0" WIDTH="120"MV_FORMAT_SGI_3LEFT="125" WIDTH="153"Silicon Graphics version 3 formatLEFT="0" WIDTH="120"MV_FORMAT_QTLEFT="125" WIDTH="153"Apple QuickTime formatLBL="" HELPID=""Getting the Movie Optimization SettingTo determine whether a movie is optimized for playback, call ID="Media6-1GS59"mvGetOptimized(). Its function prototype is:ID="Media6-1GS60"DMboolean mvGetOptimized( MVid movie )mvGetOptimized() returns DM_TRUE if the movie is optimized for playback performance and DM_FALSE if it is not. Performing any editing operations on the movie that read, write, insert, or delete frames will disrupt the optimization and clear this flag. See IDREF="59020" TYPE="TITLE""Optimizing a Movie File" in Chapter 29 for more information about movie optimization.ID="Media6-1GS61"LBL="" HELPID=""ID="93494"Creating a Default Movie ConfigurationTo make a default parameter-value list that can be used to create a movie, call ID="Media6-1GS62"mvSetMovieDefaults(), which takes the desired format for the new movie file and sets the rest of the movie parameters (such as loop mode) to their default values. Its function prototype is:DMstatus mvSetMovieDefaults ( DMparams* params,
                              MVfileformat format)formatsets the file format of the movie:COLUMNS="2"LEFT="0" WIDTH="108"MV_FORMAT_SGI_3LEFT="115" WIDTH="153"Silicon Graphics version 3 formatLEFT="0" WIDTH="108"MV_FORMAT_QTLEFT="115" WIDTH="153"Apple QuickTime formatIDREF="10284" TYPE="TABLE"Table 28-1 lists the parameters and values set by ID="Media6-1GS63"mvSetMovieDefaults().COLUMNS="3"LBL="28-1"Table 28-1 ID="10284"Movie DefaultsLEFT="0" WIDTH="110"ParameterLEFT="115" WIDTH="110"Value TypeLEFT="230" WIDTH="110"DefaultLEFT="0" WIDTH="110"MV_COMMENTLEFT="115" WIDTH="110"StringLEFT="230" WIDTH="110"EmptyLEFT="0" WIDTH="110"MV_FILE_FORMATLEFT="115" WIDTH="110"MVfileformatLEFT="230" WIDTH="110"formatLEFT="0" WIDTH="110"MV_LOOP_LIMITLEFT="115" WIDTH="110"IntegerLEFT="230" WIDTH="110"0LEFT="0" WIDTH="110"MV_LOOP_MODELEFT="115" WIDTH="110"MVloopmodeLEFT="230" WIDTH="110"MV_LOOP_NONELEFT="0" WIDTH="110"MV_OPTIMIZEDLEFT="115" WIDTH="110"DMbooleanLEFT="230" WIDTH="110"DM_FALSEIDREF="22730" TYPE="TEXT"Example 28-1 is a code fragment that creates a parameter-value list called params, then initializes params to the movie defaults by calling mvSetMovieDefaults(), and then passes params to mvCreateFile() to configure a new movie file. After the movie has been created, the parameter-value list is destroyed by calling dmParamsDestroy().LBL="28-1"Example 28-1 ID="22730"Creating and Initializing a Default Movie Parameter-value ListMVid movie;
DMparams* params;

if ( dmParamsCreate( &params ) != DM_SUCCESS ) {
    printf( "Out of memory.\n" );
    exit( 1 );
    }
if ( mvSetMovieDefaults(params, MV_FORMAT_SGI_3) != DM_SUCCESS ) {
    printf( "Out of memory.\n" );
    exit( 1 );
    }
if ( mvCreateFile("temp.mv", params, NULL, &movie) != DM_SUCCESS ) {
    printf( "Could not create movie.\n" );
    exit( 1 );

    }
dmParamsDestroy( params );LBL="" HELPID=""Adding Your Own Parameters to the Movie LibraryYou can add your own movie parameters to the Movie Library to represent movie and track properties that do not exist in the parameters provided by the Movie Library. These parameters are global, meaning that any movie opened from within your application will recognize and have access to the parameters defined by the application. User-defined parameters exist as long as the application is running.ID="Media6-1GS64"ID="Media6-1GS65"To add your own parameters to the Movie Library, call ID="Media6-1GS66"mvAddUserParam(). Its function prototype is:DMstatus mvAddUserParam ( const char* paramName )where:paramNameis a string containing 15 or fewer characters that uniquely describes the parameter; for example, use a company abbreviation as a prefix for the parameter nameNoteOnly null-terminated strings of less than 32 K bytes can be used for user-defined parameter-value pairs.  Once the parameters have been added, you can set and get them just like the regular Movie Library parameters. To set the values for the parameters, pass a list of the parameters and values to mvSetParams().ID="Media6-1GS67"You must add the parameters before calling any of the routines for setting and getting their values. If you call mvSetParams() before calling mvAddUserParam(), the Movie Library will not recognize the parameters and they will be ignored.If you open a movie that contains user-defined parameters, you can read those parameters because the Movie Library automatically calls mvAddUserParam() when loading a movie with user-defined parameters. IDREF="78050" TYPE="TEXT"Example 28-2 is a code fragment that adds a user-defined global movie parameter named Newparam to a Movie Library application, then creates a movie that has access to the user-defined parameter.ID="Media6-1GS68"ID="Media6-1GS69"LBL="28-2"Example 28-2 ID="78050"Adding a User-Defined Global Movie Parameter/*
 * Adding a global (movie) user param named Newparam with a value of
 * "Movie type code 1113afq1" to a new movie file.
 */

    DMparams* movieParams;
  
    if ( dmParamsCreate( &movieParams ) != DM_SUCCESS ) {
        fprintf( stderr, "Unable to create default params.\n");
        exit( EXIT_FAILURE );
    }
    if ( mvSetMovieDefaults( movieParams, MV_FORMAT_SGI_3 )
        != DM_SUCCESS ) {
        fprintf( stderr, "Unable to set default params.\n");
        dmParamsDestroy( movieParams );
        exit( EXIT_FAILURE );
    }
    if ( mvAddUserParam( "Newparam" ) != DM_SUCCESS ) {
        fprintf( stderr, "Unable to add user param.\n");
        dmParamsDestroy( movieParams );
        exit( EXIT_FAILURE );
    }
    if ( dmParamsSetString( movieParams, "Newparam",
                           "Movie type code 1113afq1" )
                           != DM_SUCCESS ) {
         fprintf( stderr, "Unable to set user param.\n");
         dmParamsDestroy( movieParams );
         exit( EXIT_FAILURE );
    }
    if ( mvCreateFile( "mymovie", movieParams,
                      NULL, theMovie ) == DM_FAILURE ) {
        fprintf( stderr,
                "Unable to create movie file %s: error =  %s.\n",
                getOutMovieName(),mvGetErrorStr( mvGetErrno() ) );
        dmParamsDestroy( movieParams );
        exit( EXIT_FAILURE );
    }ID="Media6-1GS70"ID="Media6-1GS71"ID="Media6-1GS72"IDREF="44338" TYPE="TEXT"Example 28-3 is a code fragment that adds a user-defined image track parameter named NewImageParam to a Movie Library application, and then adds an image track that has access to the user-defined parameter.LBL="28-3"Example 28-3 ID="44338"Adding a User-Defined Image Track Parameter for a New Track/*
 * Adding a usr param named NewImageParam with a value of
 * "Source code a32bg" to an image track.
 */
    DMparams *imageTrackParams;

    if ( dmParamsCreate( &imageTrackParams ) != DM_SUCCESS ) {
        fprintf( stderr, "Unable to create image track params.\n");
        exit( EXIT_FAILURE );
    }
    if ( dmSetImageDefaults( imageTrackParams, imgWidth, imgHeight,
                            DM_PACKING_RGBX ) != DM_SUCCESS ) {
        fprintf( stderr, "Unable to set image defaults.\n");
        dmParamsDestroy( imageTrackParams );
        exit( EXIT_FAILURE );
    }
    if ( mvAddUserParam( "NewImageParam" ) != DM_SUCCESS ) {
        fprintf( stderr, "Unable to add user param.\n");
        dmParamsDestroy( imageTrackParams );
        exit( EXIT_FAILURE );
    }
    if ( dmParamsSetString( imageTrackParams, "NewImageParam",
                           "Source code a32bg" ) != DM_SUCCESS ) {
        fprintf( stderr, "Unable to set user param.\n");
        dmParamsDestroy( imageTrackParams );
        exit( EXIT_FAILURE );
    }

    /*
     * Add the image track to the movie.
     */
    MVid imageTrack;
    if ( mvAddTrack( movie, DM_IMAGE, imageTrackParams,
                    NULL, &imageTrack ) == DM_FAILURE) {
        fprintf( stderr, "Unable to add image track to movie.\n");
        dmParamsDestroy( imageTrackParams );
        exit( EXIT_FAILURE );
    }
    dmParamsDestroy( imageTrackParams );LBL="" HELPID=""ID="21924"Setting and Getting Track PropertiesThe Movie Library lets you work with the individual image and audio tracks inside movies. A track is an evenly spaced (in time) sequence of frames, where each of the frames is the same size (although the amount of data per frame may vary after compression).This section explains how to use the Movie Library routines for setting and getting both general track properties, which apply to all tracks, and specific track properties, which depend upon the track medium.ID="Media6-1GS73"ID="Media6-1GS74"ID="Media6-1GS75"ID="Media6-1GS76"You can set this general track property for all types of tracks:SMPTE time code stringThe read-only general track properties are:track length (number of frames)track medium (image or audio)LBL="" HELPID=""Setting and Getting General Track PropertiesThis section explains how to set and get the general track properties using the Movie Library convenience routines, which are built on top of the universal set and get routines mvSetParams() and mvGetParams(). You can also set and get general track properties by calling mvSetParams() and mvGetParams(), respectively.LBL="" HELPID=""Setting and Getting SMPTE Time Code Strings Stored in TracksYou can store SMPTE time codes as strings in a track parameter. The time code string does not have any relationship to a timer inside the Movie Library, so you have to obtain the time code from an external source before storing it as a parameter. See IDREF="75968" TYPE="TITLE""Using the SMPTE Time Code Sample Application" in Chapter 32 for sample programs that implement time code capability.ID="Media6-1GS77"ID="Media6-1GS78"ID="Media6-1GS79"ID="Media6-1GS80"To store a SMPTE time code string in a movie track, call ID="Media6-1GS81"mvSetSMPTEStart(). Its function prototype is:DMstatus mvSetSMPTEStart(MVid movie, const char* start_time)where:start_timeis a string representing a SMPTE time code, with two digits each for hour, minute, second, and frame, as in "00:59:30:00"mvSetSMPTEStart() returns either DM_SUCCESS or DM_FAILURE.To retrieve a SMPTE time code string that is stored in a movie track, call ID="Media6-1GS82"mvGetSMPTEStart(), which returns the starting time code as set by mvSetSMPTEStart(). Its function prototype is:const char* mvGetSMPTEStart( MVid track )where:trackis the track for which you want to obtain the SMPTE time code parameterLBL="" HELPID=""Getting the Track LengthTo retrieve the number of frames stored in a track, call mvGetTrackLength(). Its function prototype is:ID="Media6-1GS83"ID="Media6-1GS84"ID="Media6-1GS85"MVframe mvGetTrackLength ( MVid track )LBL="" HELPID=""Getting the Track MediumTo determine the track medium, call mvGetTrackMedium(). Its function prototype is:ID="Media6-1GS86"ID="Media6-1GS87"DMmedium mvGetTrackMedium( MVid track )mvGetTrackMedium() returns either DM_IMAGE or DM_AUDIO.LBL="" HELPID=""ID="94562"Setting and Getting Audio Track PropertiesThe audio track of a movie contains digitized audio samples.ID="Media6-1GS88"ID="Media6-1GS89"ID="Media6-1GS90"When creating an audio track, you can set and get audio properties individually, using the Movie Library mvSetParams() and mvGetParams() routines or the DM Library dmSetParams() and dmGetParams() routines. You can also create default audio parameters for an audio track using the DM Library dmSetAudioDefaults() routine. If you set the audio defaults by calling dmSetAudioDefaults(), then other properties, such as number of channels, require setting only if they differ from the defaults.ID="Media6-1GS91"ID="Media6-1GS92"ID="Media6-1GS93"You can set these properties when creating an audio track:audio sample formatID="Media6-1GS94"audio sample rateID="Media6-1GS95"audio sample width (bytes per sample)number of audio channelsID="Media6-1GS96"The only parameter that you can change for an existing audio track is the default volume (this is not functional in the current version of the Movie Library). The other parameters cannot be changed after a track has been created because they would invalidate the data stored in the track.IDREF="41069" TYPE="TABLE"Table 28-2 lists the audio defaults as set by dmSetAudioDefaults().COLUMNS="3"LBL="28-2"Table 28-2 ID="41069"Audio DefaultsLEFT="0" WIDTH="127"ParameterLEFT="135" WIDTH="75"Value TypeLEFT="215" WIDTH="157"DefaultLEFT="0" WIDTH="127"DM_MEDIUMLEFT="135" WIDTH="75"DMmediumLEFT="215" WIDTH="157"DM_AUDIOLEFT="0" WIDTH="127"DM_AUDIO_WIDTHLEFT="135" WIDTH="75"IntegerLEFT="215" WIDTH="157"widthLEFT="0" WIDTH="127"DM_AUDIO_FORMATLEFT="135" WIDTH="75"DMaudioformatLEFT="215" WIDTH="157"DM_AUDIO_TWOS_COMPLEMENTLEFT="0" WIDTH="127"DM_AUDIO_RATELEFT="135" WIDTH="75"DoubleLEFT="215" WIDTH="157"rateLEFT="0" WIDTH="127"DM_AUDIO_CHANNELSLEFT="135" WIDTH="75"IntegerLEFT="215" WIDTH="157"channelsLEFT="0" WIDTH="127"DM_AUDIO_COMPRESSIONLEFT="135" WIDTH="75"StringLEFT="215" WIDTH="157"DM_AUDIO_UNCOMPRESSEDNoteCurrently, only DM_AUDIO_UNCOMPRESSED is supported for audio compression.  Since the Movie Library guarantees that all audio tracks have all of these parameters defined, the "get" routines in this section do not return errors.LBL="" HELPID=""Setting and Getting the Default Volume of an Audio TrackmvSetDefaultVol() stores a new value, ranging from 0.0 to 1.0, for the default volume parameter. It returns DM_SUCCESS or DM_FAILURE. Its function prototype is:ID="Media6-1GS97"ID="Media6-1GS98"ID="Media6-1GS99"ID="Media6-1GS100"ID="Media6-1GS101"int mvSetDefaultVol( MVid audioTrack double volume)mvGetDefaultVol() returns the default volume setting to use when playing this movie. Its function prototype is:ID="Media6-1GS102"DMstatus mvGetDefaultVol( MVid audioTrack )where:audioTrackis the audio track whose volume you want to set or getNoteThe Movie Library currently ignores the default volume setting.  LBL="" HELPID=""Getting the Audio Sample Width of an Audio TrackThe Movie Library supports 8-bit and 16-bit audio samples.ID="Media6-1GS103"ID="Media6-1GS104"ID="Media6-1GS105"To determine the number of bits used to store each audio sample in an audio track, call mvGetAudioWidth(). Its function prototype is:int mvGetAudioWidth( MVid audioTrack )where:audioTrackis the audio track whose sample width you want to knowLBL="" HELPID=""Getting the Audio Sample Rate of an Audio TrackTo determine the audio sample rate (in samples per second) for an audio track, call mvGetAudioRate(). Its function prototype is:ID="Media6-1GS106"ID="Media6-1GS107"ID="Media6-1GS108"double mvGetAudioRate( MVid audioTrack )where:audioTrackis the audio track whose sample rate you want to knowLBL="" HELPID=""Getting the Number of Audio Channels in an Audio TrackTo determine the number of audio channels in an audio track, call mvGetAudioChannels(). Its function prototype is:ID="Media6-1GS109"ID="Media6-1GS110"ID="Media6-1GS111"int mvGetAudioChannels( MVid audioTrack )mvGetAudioChannels() returns the number of audio channels: 1 for mono; 2 for stereo (the Movie Library does not currently support 4-channel audio tracks). where:audioTrackis the audio track for which you want to know the number of channelsLBL="" HELPID=""Getting the Audio Format of an Audio TrackThe Movie Library supports both unsigned and twos complement audio formats that are directly understood by the Audio Library (AL), in particular the ALwritesamps() call, for Silicon Graphics native movie formats.ID="Media6-1GS112"Audio for QuickTime movies is stored using sampling rates and formats understood by QuickTime on the Apple Macintosh computer. Some of these formats are not directly supported by the AL. See IDREF="12291" TYPE="TITLE"Chapter 31, "Using the Movie Library with QuickTime Movies," for more information on QuickTime audio.To determine the format used to store the audio samples in an audio track, call mvGetAudioFormat(). Its function prototype is:ID="Media6-1GS113"ID="Media6-1GS114"ID="Media6-1GS115"DMaudioformat mvGetAudioFormat( MVid audioTrack )where:audioTrackis the audio track whose format you want to knowmvGetAudioFormat() returns either DM_AUDIO_TWOS_COMPLEMENT or DM_AUDIO_UNSIGNED.LBL="" HELPID=""Getting the Audio Compression Scheme of an Audio TrackTo determine the audio compression scheme for an audio track, call mvGetAudioCompression(). Its function prototype is:ID="Media6-1GS116"ID="Media6-1GS117"ID="Media6-1GS118"const char* mvGetAudioCompression( MVid audioTrack )where:audioTrackis the audio track whose compression you want to knowSince movies do not currently support audio compression, the only valid value is DM_AUDIO_UNCOMPRESSED.LBL="" HELPID=""ID="94575"Setting and Getting Image Track PropertiesThe image track of a movie contains a sequence of image frames. All the images in an image track must have the same frame size (width and height).ID="Media6-1GS119"ID="Media6-1GS120"ID="Media6-1GS121"When creating an image track, you can set and get image properties individually, using the Movie Library mvSetParams() and mvGetParams() routines or the DM Library dmSetParams() and dmGetParams() routines. You can also create default image parameters for an image track using the DM Library dmSetImageDefaults() routine. If you set the image defaults by calling dmSetImageDefaults(), then other properties, such as compression, require setting only if they differ from the defaults.ID="Media6-1GS122"ID="Media6-1GS123"ID="Media6-1GS124"IDREF="54264" TYPE="TABLE"Table 28-3 lists the image defaults as set by dmSetImageDefaults().COLUMNS="3"LBL="28-3"Table 28-3 ID="54264"Image DefaultsLEFT="0" WIDTH="127"ParameterLEFT="135" WIDTH="65"Value TypeLEFT="205" WIDTH="143"ValueLEFT="0" WIDTH="127"DM_MEDIUMLEFT="135" WIDTH="65"DMmediumLEFT="205" WIDTH="143"DM_IMAGELEFT="0" WIDTH="127"DM_IMAGE_WIDTHLEFT="135" WIDTH="65"IntegerLEFT="205" WIDTH="143"widthLEFT="0" WIDTH="127"DM_IMAGE_HEIGHTLEFT="135" WIDTH="65"IntegerLEFT="205" WIDTH="143"heightLEFT="0" WIDTH="127"DM_IMAGE_RATELEFT="135" WIDTH="65"DoubleLEFT="205" WIDTH="143"15.0LEFT="0" WIDTH="127"DM_IMAGE_COMPRESSIONLEFT="135" WIDTH="65"StringLEFT="205" WIDTH="143"DM_IMAGE_UNCOMPRESSEDLEFT="0" WIDTH="127"DM_IMAGE_INTERLACINGLEFT="135" WIDTH="65"DMinterlacingLEFT="205" WIDTH="143"DM_IMAGE_NONINTERLACEDLEFT="0" WIDTH="127"DM_IMAGE_PACKINGLEFT="135" WIDTH="65"DMpackingLEFT="205" WIDTH="143"packingLEFT="0" WIDTH="127"DM_IMAGE_ORIENTATIONLEFT="135" WIDTH="65"DMorientationLEFT="205" WIDTH="143"DM_BOTTOM_TO_TOPThe only parameter that you can change for an existing image track is the image frame rate. The other parameters cannot be changed after a track has been created because they would invalidate the data stored in the track.Since the Movie Library guarantees that all image tracks have all image parameters defined, the "get" routines in this section do not return errors. LBL="" HELPID=""Setting and Getting the Image Frame RateTo change a movie's frame rate by storing a new value for the image rate parameter, call ID="Media6-1GS125"mvSetImageRate(). Its function prototype is:ID="Media6-1GS126"ID="Media6-1GS127"ID="Media6-1GS128"DMstatus mvSetImageRate( MVid imageTrack,                         double framesPerSecond )where:COLUMNS="2"LEFT="0" WIDTH="98"imageTrackLEFT="105" WIDTH="235"is the image track for which you want to set the rateLEFT="0" WIDTH="98"framesPerSecondLEFT="105" WIDTH="235"is the image rate in frames per secondmvSetImageRate() returns DM_SUCCESS if the image rate was successfully changed; otherwise, it returns DM_FAILURE.To determine the image rate in frames per second for a movie, call ID="Media6-1GS129"mvGetImageRate(). Its function prototype is:double mvGetImageRate( MVid imageTrack )where:imageTrackis the image track whose rate you want to knowLBL="" HELPID=""Getting the Image Frame SizeTo get the width (x dimension) in pixels of the images stored in an image track, call mvGetImageWidth(). Its function prototype is:ID="Media6-1GS130"ID="Media6-1GS131"ID="Media6-1GS132"int mvGetImageWidth( MVid imageTrack )where:imageTrackis the image track whose dimensions you want to knowTo get the height (y dimension) in pixels of the images stored in an image track, call mvGetImageHeight(). Its function prototype is:ID="Media6-1GS133"ID="Media6-1GS134"ID="Media6-1GS135"int mvGetImageHeight( MVid imageTrack )where:imageTrackis the image track whose dimensions you want to knowLBL="" HELPID=""Getting the Image OrientationThe ordering of scan lines within an image frame depends upon the source of the data. Bottom-to-top is the default data orientation for Movie Library routines. To find out how pixels are ordered, call mvGetImageOrientation(). Its function prototype is:ID="Media6-1GS136"ID="Media6-1GS137"ID="Media6-1GS138"DMorientation mvGetImageOrientation ( MVid imageTrack)where:imageTrackis the image track whose format you want to knowmvGetImageOrientation() returns the order in which scan lines are stored in the image: DM_BOTTOM_TO_TOP or DM_TOP_TO_BOTTOM.LBL="" HELPID=""Getting the Image FormatThe Movie Library can read images from and write images to a buffer. You can use the IRIS ImageVision Library to read any of these image formats into a buffer:ID="Media6-1GS139"ID="Media6-1GS140"ID="Media6-1GS141"FITSilicon Graphics ImageVision Library tiled image formatID="Media6-1GS142"ID="Media6-1GS143"ID="Media6-1GS144"ID="Media6-1GS145"GIFCompuserveÔ Graphics Image File, a popular 8-bit formatID="Media6-1GS146"PCDKodak Photo CDÔ file format for digital imagesID="Media6-1GS147"ID="Media6-1GS148"SGISilicon Graphics image data formats: .bw, .rgb, .rgba, .sgi, and .screenID="Media6-1GS149"ID="Media6-1GS150"ID="Media6-1GS151"TIFFextended version of Tag Image File Format, Revision 6.0ID="Media6-1GS152"ID="Media6-1GS153"LBL="" HELPID=""Getting the Image Packing FormatThe image packing format describes how pixels are packed within images; for example, the word length used to represent the pixel, and the bit order.ID="Media6-1GS154"ID="Media6-1GS155"ID="Media6-1GS156"ID="Media6-1GS157"Images in a movie file can be stored on disk in any of the image packing formats listed in IDREF="49404" TYPE="TABLE"Table 28-4. When you play a movie whose image track is in a nondefault format, the Movie Library converts the image data to DM_PACKING_RGBX on the fly as it plays the movie.To get the image packing format for a particular image track, call mvGetImagePacking(). Its function prototype is:DMpacking mvGetImagePacking( MVid imageTrack )where:imageTrackis the ID of the image track whose format you want to knowIDREF="49404" TYPE="TABLE"Table 28-4 lists the image packing formats supported by the Movie LibraryCOLUMNS="2"LBL="28-4"Table 28-4 ID="49404"Image Packing FormatsLEFT="0" WIDTH="126"Packing FormatLEFT="135" WIDTH="207"DescriptionLEFT="0" WIDTH="126"DM_PACKING_RGBXLEFT="135" WIDTH="207"R, G, B, and X (don't care) data are packed into a 32-
bit word as 0xXXBBGGRR. This is the default 
format for the Movie Library.LEFT="0" WIDTH="126"DM_PACKING_GRAYSCALELEFT="135" WIDTH="207"Each pixel is an 8-bit luminance value.LEFT="0" WIDTH="126"DM_PACKING_APPLE_16LEFT="135" WIDTH="207"16-bit images are stored with 5 bits each of red, 
green, and blue. The bit layout is:XRRRRRGGGGGBBBBBApple stores images from top to bottom, while SGI 
goes from bottom to top.LEFT="0" WIDTH="126"DM_PACKING_APPLE_32LEFT="135" WIDTH="207"32-bit images are stored with 8 bits each of red, 
green, and blue. The bit layout is: XXXXXXXXRRRRRRRRGGGGGGGGBBBBBBBBLBL="" HELPID=""ID="93555"Getting the Image Compression SchemeCompression settings can result in significant differences in image quality and in playback performance. The most commonly used compression schemes for movies to be played on Silicon Graphics computers are MVC1 and MVC2; QT_VIDEO is frequently used for QuickTime movies to be played on a Macintosh computer.ID="Media6-1GS158"ID="Media6-1GS159"ID="Media6-1GS160"The Movie Library supports image compression through the Compression Library. The compression methods that have been thoroughly tested with the Movie Library are UNCOMPRESSED, MVC1, MVC2, RLE24, and JPEG. You can also use the QuickTime compression schemes, QT_VIDEO and QT_ANIM, in QuickTime and Silicon Graphics movies if you have purchased and installed the Silicon Graphics QuickTime Compressor Library.To get the image compression scheme, call mvGetImageCompression(). Its function prototype is:ID="Media6-1GS161"ID="Media6-1GS162"ID="Media6-1GS163"ID="Media6-1GS164"const char* mvGetImageCompression( MVid imageTrack )where:imageTrackis the ID of the image track whose format you want to knowmvGetImageCompression() returns the name of the compression scheme that is used to compress the images.The most commonly used schemes and the reasons for choosing them are:COLUMNS="2"LEFT="0" WIDTH="105"UNCOMPRESSEDLEFT="110" WIDTH="399"For the best quality in a final movie, all image manipulation and storage should be with 
uncompressed images until the final movie is produced, at which time the images can be 
compressed. Repeatedly compressing, decompressing, and then recompressing images 
reduces the image quality.ID="Media6-1GS165"LEFT="0" WIDTH="105"MVC1LEFT="110" WIDTH="399"This is a good general-purpose compression scheme. It is a color-cell compression 
technique that works well for video, but can cause fuzzy edges in high-contrast animation.ID="Media6-1GS166"LEFT="0" WIDTH="105"MVC2LEFT="110" WIDTH="399"Provides results similar to MVC1 in terms of image quality. MVC2 compresses the data 
more than MVC1, but takes longer to perform the compression. Playback is faster for 
MVC2, because there is less data to read in, and decompression is faster than for MVC1.ID="Media6-1GS167"LEFT="0" WIDTH="105"RLE24LEFT="110" WIDTH="399"This is a lossless compression method that uses run-length encoding (RLE). Run-length 
encoding compresses images by storing a color and its run-length (the number of pixels of 
that color) every time the color changes. It is a good technique for animations where there 
are large areas that have identical colors.ID="Media6-1GS168"LEFT="0" WIDTH="105"JPEGLEFT="110" WIDTH="399"JPEG is designed for still images and is usable, but slow, for video. JPEG is typically used 
to compress each still frame during the writing or editing process, with the intention being 
to apply another type of compression to the final version of the movie or to leave it 
uncompressed. JPEG works better on high-resolution, continuous-tone images such as 
photographs, than on crisp-edged, high-contrast images like line drawings. ID="Media6-1GS169"LEFT="0" WIDTH="105"QT_VIDEOLEFT="110" WIDTH="399"This is the compression used for QuickTime movies that contain video. Like MVC1, 
QT_VIDEO is a color-cell compression technique. It includes temporal compression and is 
good for video and reasonable for animation.ID="Media6-1GS170"LEFT="0" WIDTH="105"QT_ANIMLEFT="110" WIDTH="399"This is a lossy run-length encoding scheme used for QuickTime movies. It also includes 
temporal compression. It has excellent compression ratios for animation that has large 
areas of similar colors. ID="Media6-1GS171"LBL="29"ID="61830"File I/O and Editing Movies with the Movie LibraryThis chapter describes how to set up a Movie Library application and how to use the Movie Library routines for handling file input/output (I/O), editing, compression, and other basic tasks. Playback is discussed in the next chapter.ID="Media6-2IO1"The Movie Library provides these basic file I/O capabilities:creating new movie filesopening an existing movie from a file descriptor, filename, or memoryreading data from and writing data to selected tracks and framesLBL="" HELPID=""ID="75716"Initializing a Movie Library ApplicationThis section explains the basic program setup you will use. To set up a Movie Library application, you need to:open a file descriptor for the movie (if you are using a file descriptor)create a new movie or open an existing movietest a file to determine whether it is a movie fileset and get movie propertiesadd, remove, and find tracksallocate buffers for data passed to and from the Movie LibraryEach of these tasks is described in detail in the subsections that follow. It's a good idea to familiarize yourself with the setup procedures described in this section because you'll be performing most of these tasks in every application that works with movies.Basic routines for creating, verifying, and opening movies are available for each type of interface: filename, file descriptor, and memory-mapped files. Each of these routines features a similar API but has the appropriate arguments for the specified method.ID="Media6-2IO2"IDREF="16132" TYPE="GRAPHIC"Figure 29-1 shows a diagram of the file I/O routines in the Movie Library. FILE="Media6-2IO.cgm" POSITION="INLINE" SCALE="FALSE"LBL="29-1"Figure 29-1 ID="16132"Movie Library File I/O RoutinesID="Media6-2IO3"mvCreateFile(), mvCreateFD(), and mvCreateMem() create a new empty movie, initialized with the given parameters, as set by mvSetMovieDefaults() or by the DM Library routines, and return an identifier for the new movie. Any movie that was already present in the file or memory location will be destroyed.mvIsMovieFile(), mvIsMovieFD(), and mvIsMovieMem() test whether a movie is present in a file or in memory. Only movies in supported formats (Silicon Graphics and QuickTime) are recognized.mvOpenFile(), mvOpenFD(), and mvOpenMem() read an existing movie, create a movie instance in memory that holds information about it, and then return an identifier for the new movie.The following two calls are used when the file I/O or editing is completed:mvWrite()flushes all changes that have been made to a movie and makes sure that they are written to the file, but does not close the filemvClose()flushes all changes that have been made to the movie and makes sure that they are written to the file, and then destroys the movie instanceEach of these calls is described in detail in the sections that follow.LBL="" HELPID=""Using File Descriptors with MoviesFile descriptors can be used to work with movie files on disk, CD-ROM, or DAT, or with embedded or previously opened movie files.ID="Media6-2IO4"ID="Media6-2IO5"ID="Media6-2IO6"ID="Media6-2IO7"NoteThe Movie Library does not support embedded QuickTime files. Use the IRIX open() system call (see the open(2) man page) to open a file descriptor.ID="Media6-2IO8"The movie instance inherits one of three file access modes, which are associated with the file descriptor:O_RDONLYopens a read-only movie fileID="Media6-2IO9"ID="Media6-2IO10"O_WRONLYopens a write-only movie fileID="Media6-2IO11"ID="Media6-2IO12"O_RDWRopens a read-write movie fileID="Media6-2IO13"ID="Media6-2IO14"The file access mode determines which operations are possible for a particular movie. You can't write data to a movie opened as a read-only file. Similarly, you can't play a movie that has been opened for writing.ID="Media6-2IO15"Before opening the movie, the file pointer associated with the file descriptor should be positioned at the beginning of the movie file. If the movie data is embedded within a file, such as an application-specific file format containing a movie as a data chunk, use the IRIX ID="Media6-2IO16"lseek() system call (see the lseek(2) man page) to seek the file descriptor to the beginning of the movie data. For example, a movie file might be embedded in the file of a word-processing program. When the word processing program is ready to access the movie, it seeks to the position of the movie within its file and passes the file descriptor (fd) to the Movie Library.ID="Media6-2IO17"LBL="" HELPID=""Creating a New MovieYou can create a new movie and associate it with a file descriptor, file name, or memory location. There is a function call corresponding to each type.ID="Media6-2IO18"COLUMNS="2"LEFT="0" WIDTH="80"mvCreateFD()LEFT="85" WIDTH="254"creates a movie using a file descriptor returned by open()LEFT="0" WIDTH="80"mvCreateFile()LEFT="85" WIDTH="254"creates a movie using a file nameLEFT="0" WIDTH="80"mvCreateMem()LEFT="85" WIDTH="254"creates a memory-mapped movieThe function prototypes are:DMstatus  mvCreateFD   ( int          fd, 
                         DMparams*    params,
                         DMparams*    returnParamsSetOrNull,
                         MVid*        returnMovie )
DMstatus  mvCreateFile ( const char*  filename,
                         DMparams*    params,
                         DMparams*    returnParamsSetOrNull,
                         MVid*        returnMovie )
DMstatus  mvCreateMem ( void*        pointer, 
                        size_t       size,
                        DMparams*    params,
                        DMparams*    returnParamsSetOrNull,
                        MVid*        returnMovie )where:ID="Media6-2IO19"ID="Media6-2IO20"ID="Media6-2IO21"fdis a file descriptor returned by the open(2) system call that is assigned to the new moviefilenameis a filename assigned to the new moviepointeris a pointer to a memory location that is assigned to the new movie ID="Media6-2IO22"ID="Media6-2IO23"sizeis the size of the block of memory to usename='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'the Movie Library will not read or write beyond this block sizeparamsis a pointer to parameters that describe the movie attributesreturnParamsSetOrNullis a pointer to a parameter-value list into which the Movie Library loads those parameters and values that it recognized and was able to set; if returnParamsSetOrNull is NULL, the Movie Library will not return such a listreturnMovieis a pointer into which the movie identifier is returnedIDREF="46595" TYPE="TEXT"Example 29-1 is a code fragment that shows how to create a movie.ID="Media6-2IO24"LBL="29-1"Example 29-1 ID="46595"Creating a Movie#include <movie.h>

void CreateMovie()
{
    DMparams* params;
    MVid      movie;

    if ( dmParamsCreate( &params ) != DM_SUCCESS ) {
        /* handle error */
    }

    if ( mvSetMovieDefaults( params, MV_FORMAT_SGI_3 )
         != DM_SUCCESS ) {
        /* handle error */
    }

    if ( mvCreateFile( "new-movie", params, NULL, &movie )
         != DM_SUCCESS ) {
        /* handle error */
    }

    /* Add tracks, insert frames, etc. */
}LBL="" HELPID=""Checking for the Presence of a MovieBefore attempting to open an existing movie file from a file descriptor, file name, or memory, your application should check to see whether a movie is present. There is a separate function call to verify the existence of a movie for each source type.ID="Media6-2IO25"COLUMNS="2"LEFT="0" WIDTH="94"mvIsMovieFD()LEFT="100" WIDTH="238"ID="Media6-2IO26"checks for a movie identified by the given file 
descriptorLEFT="0" WIDTH="94"mvIsMovieFile()LEFT="100" WIDTH="238"ID="Media6-2IO27"checks for a movie identified by the given file nameLEFT="0" WIDTH="94"mvIsMovieMem()LEFT="100" WIDTH="238"checks for a movie stored in the given memory 
locationID="Media6-2IO28"Their function prototypes are:DMboolean mvIsMovieFD ( int fd )
DMboolean mvIsMovieFile ( const char* filename )
DMboolean mvIsMovieMem ( void* pointer, size_t size )where: fdis the file descriptor of the movie file you are checking filenameis the name of the movie file you are checkingpointeris a pointer to a memory location where the movie file you are checking is storedsizeis the size of the block of memory to usename='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'the Movie Library will not read or write beyond this block sizeDM_TRUE is returned if the given file is a movie file; otherwise, DM_FALSE is returned.The following code fragment determines whether a given file is a movie file and prints an error message if it is not:if ( !mvIsMovieFile( filename ) ) {
    PrintError ("Not a movie file.");
}LBL="" HELPID=""Opening an Existing MovieMovies can be opened from file descriptors, filenames, or memory. There is a function call corresponding to each source type:ID="Media6-2IO29"COLUMNS="2"LEFT="0" WIDTH="81"mvOpenFD()LEFT="90" WIDTH="251"opens a movie using a file descriptor that has already 
been obtained from another source, such as ID="Media6-2IO30"open(2)LEFT="0" WIDTH="81"mvOpenFile()LEFT="90" WIDTH="251"opens a movie from a file nameID="Media6-2IO31"LEFT="0" WIDTH="81"mvOpenMem()LEFT="90" WIDTH="251"opens a memory-mapped movieID="Media6-2IO32"LBL="" HELPID=""Opening a Movie from a File DescriptorTo open a movie file from a file descriptor, call mvOpenFD(). Its function prototype is:DMstatus mvOpenFD (int fd, MVid* returnMovie)where:ID="Media6-2IO33"fdis a file descriptor that references the movie file you want to openreturnMovieis a pointer into which the movie identifier is returnedLBL="" HELPID=""Opening a Movie from a FilenameTo open a movie from a filename, call mvOpenFile(). Its function prototype is:ID="Media6-2IO34"DMstatus mvOpenFile ( const char* filename, int oflag,
                      MVid* returnMovie )where:ID="Media6-2IO35"filenameis the filename of the movie file you want to openoflagis the file access mode, either O_RDONLY or O_RDWRreturnMovieis a pointer into which the movie identifier is returnedLBL="" HELPID=""Opening Memory-mapped MoviesTo open a movie that resides entirely in memory, beginning at the location pointed to by pointer, call mvOpenMem(). Its function prototype is:DMstatus mvOpenMem ( void* pointer, size_t size, MVid*
                     returnMovie )where:ID="Media6-2IO36"pointeris a pointer to the starting memory location of a moviesizeis the size of the memory buffername='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'the Movie Library will not read or write beyond this block sizereturnMovieis a pointer into which the movie identifier is returnedYour application must allocate and free the memory buffer used by mvOpenMem().ID="Media6-2IO37"LBL="" HELPID=""Adding, Locating, and Deleting Audio and Image TracksThe operations that can be performed on a track include:ID="Media6-2IO38"adding a new trackremoving an existing trackfinding a trackmapping frames between tracks in the same movie or another moviereading and writing data in a trackediting operations (copying from one movie to another)LBL="" HELPID=""Adding an Audio or Image Track to a MovieTo add a track to a movie, call ID="Media6-2IO39"ID="Media6-2IO40"mvAddTrack(). Its function prototype is:DMstatus mvAddTrack ( MVid movie, DMmedium medium, DMparams* params,
                   DMparams* returnParamsSetOrNull, MVid* returnTrack )where:ID="Media6-2IO41"mediumis the type of track, either DM_AUDIO or DM_IMAGEparamsis a pointer to a parameter-value list for configuring the new track, which should be filled in using either dmSetImageDefaults() or dmSetAudioDefaults(), depending on the medium typereturnParamsSetOrNullis a pointer to a parameter-value list into which the Movie Library loads those parameters and values that it recognized and was able to setname='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'if returnParamsSetOrNull is NULL, the Movie Library will not return such a listreturnTrackis a pointer into which the ID of the newly added track is returnedDM_SUCCESS is returned if the new track was created successfully; otherwise DM_FAILURE is returned.The DM_IMAGE_COMPRESSION parameter that is given when creating an image track is important to get right, because different settings can result in large differences in image quality and in playback performance. See IDREF="93555" TYPE="TITLE""Getting the Image Compression Scheme" in Chapter 28 for information on choosing a compression setting when configuring an image track.IDREF="64557" TYPE="TEXT"Example 29-2 shows how to add an audio track to a movie.ID="Media6-2IO42"ID="Media6-2IO43"LBL="29-2"Example 29-2 ID="64557"Adding an Audio Track to a Movie#include <movie.h>

void AddAudioTrack( MVid movie )
{
    DMparams* audioParams;
    MVid      audioTrack;

    if ( dmParamsCreate( &audioParams ) != DM_SUCCESS ) {
        /* handle error */
    }
    if ( dmSetAudioDefaults( audioParams,
                             8,     /* bits per sample */
                             22050, /* sample rate */
                             1,     /* number of channels */
                             ) != DM_SUCCESS ) {
        /* handle error */
    }
    if ( mvAddTrack( movie,
                     DM_AUDIO,
                     audioParams,
                     NULL,
                     &audioTrack ) != DM_SUCCESS ) {
         /* handle error */
    }
    /* Write audio data to track */
    /* with mvInsertFrames(3mv) */
}LBL="" HELPID=""Removing an Audio or Image Track from a MovieTo remove a track from a movie, which deletes all the data from the track, call mvRemoveTrack(). Its function prototype is:DMstatus mvRemoveTrack(MVid movie, MVid track)where:ID="Media6-2IO44"trackis the track you want to removeDM_SUCCESS is returned if the track was removed successfully; otherwise, DM_FAILURE is returned.ID="Media6-2IO45"LBL="" HELPID=""Locating an Existing TrackTo get a handle for a track that already exists, call mvFindTrackByMedium(). Its function prototype is:DMstatus mvFindTrackByMedium ( MVid movie, DMmedium medium,
                               MVid* returnTrack )where:ID="Media6-2IO46"mediumis the medium, either DM_AUDIO or DM_IMAGEreturnTrackis a pointer into which the track identifier is returnedmvFindTrackByMedium() returns DM_SUCCESS if a track of the given medium exists in the movie instance identified by movie; otherwise, it returns DM_FAILURE.ID="Media6-2IO47"LBL="" HELPID=""Mapping Frames from One Track to Another TrackBecause the image and audio tracks are separate, your application must manage the synchronization between tracks when editing movies; for example, when deleting a portion of the image track, the corresponding portion of the audio track must be located and also deleted.ID="Media6-2IO48"ID="Media6-2IO49"IDREF="78198" TYPE="GRAPHIC"Figure 29-2 shows an image track and a corresponding audio track. (The audio track would actually have many more samples per frame, but for clarity, only a few samples are shown.)ID="Media6-2IO50"FILE="29-2.online.bw" POSITION="INLINE" SCALE="FALSE"LBL="29-2"Figure 29-2 ID="78198"Mapping Frames from One Track to AnotherAs shown in IDREF="78198" TYPE="GRAPHIC"Figure 29-2, the frame numbers in one track do not have a one-to-one time correspondence with the frame numbers in another track. When mapping frame numbers from one track to another, the Movie Library chooses the frame that matches the starting time of the given frame. The frame boundaries are not necessarily aligned, so the mapping can differ by one frame, depending on which track you are mapping from. For example, in IDREF="78198" TYPE="GRAPHIC"Figure 29-2, frame 3 in the image track maps to frame 9 of the audio track, but frame 9 of the audio track maps to frame 2 of the image track.Before performing any operation on a frame in one track that affects its corresponding frame in the second track, you must locate the frame number in the second track that corresponds to the frame number in the first track.To locate the frames in one track that correspond (in time) to frames from another track, call mvMapBetweenTracks(). mvMapBetweenTracks() determines which frame number in toTrack corresponds to the frame numbered fromFrameIndex in fromTrack and writes the result into the location pointed to by toFrameIndex. Its function prototype is:DMstatus mvMapBetweenTracks ( MVid fromTrack, MVid toTrack,
                              MVframe fromFrameIndex,
                              MVframe* toFrameIndex )where:fromTrackis the track for which you want to locate a corresponding frame in toTracktoTrackis the track in which to locate the frame number corresponding to the frame numbered fromFrameIndex in fromTrackfromFrameIndexis the frame number in fromTrack for which you want to locate the corresponding frame number in toTracktoFrameIndexis a pointer into which the frame number in toTrack that corresponds to the frame numbered fromFrameIndex in fromTrack is writtenDM_SUCCESS is returned if a corresponding frame was located; otherwise, DM_FAILURE is returned.TipYou can also use mvMapBetweenTracks() to find corresponding frame numbers in tracks from two different movies.LBL="" HELPID=""ID="42732"Editing MoviesThe Movie Library provides these editing operations:ID="Media6-2IO51"optimizing a movieinserting raw image or audio frames from a buffer into a trackreading frames from a track into a bufferdeleting frames from a trackreading and inserting compressed images directlycopying frames from one movie to anotherNoteMovies should not be edited during playback.When you edit a movie, the Movie Library changes pointers to frames rather than operating on the actual frames themselves. After a series of editing calls, the movie frames might not be arranged in the order in which they play, and there are probably empty spaces in the movie. Such a movie does not provide optimum playback, but you can optimize the movie as described in IDREF="59020" TYPE="TITLE""Optimizing a Movie File" before closing it.LBL="" HELPID=""ID="59020"Optimizing a Movie FileTo get the best playback performance from an edited movie, you should optimize the movie file after an editing session. Optimization streamlines the movie file by coalescing the empty space and by flattening the data structure into the most linear structure possible. This is especially helpful for minimizing the excessive seeks that occur during playback that are caused by editing a movie file repeatedly. Optimization does not occur in place; instead, the Movie Library makes a copy of the movie to optimize.To optimize a movie, call mvOptimize(). Its function prototype is:DMstatus mvOptimize ( MVid fromMovie, MVid toMovie )where:ID="Media6-2IO52"fromMovieis the movie you want to optimizetoMovieis a name for the optimized movieLBL="" HELPID=""Using a Buffer for EditingThis section explains how to use a buffer for editing. The routines described in this section work on uncompressed data; there is a similar group of routines described in IDREF="64505" TYPE="TITLE""Reading and Inserting Compressed Images" for working with compressed data.LBL="" HELPID=""Allocating BuffersMemory must be allocated to hold audio or image data that is passed to or obtained from the Movie Library. The buffer that is passed to these routines points to a block of memory that holds an array of frames. Your application is responsible for allocating a buffer large enough to hold the desired number of frames.ID="Media6-2IO53"NoteA playback-only application that does not perform any file I/O operations need not allocate separate memory.Use dmImageFrameSize() to determine the number of bytes needed to hold one frame of raw image data; similarly, use dmAudioFrameSize() to determine the number of bytes needed to hold one frame of raw audio data.Before allocating memory, determine the required buffer size as demonstrated in IDREF="97053" TYPE="TEXT"Example 29-3. Allocate the appropriate amount of memory by using one of the IRIX system calls for memory allocation, such as malloc(), and then check the malloc() return to make sure there was enough memory. See the malloc(3X, 3C) man page for information about memory allocation. See the IRIX Programming Guide for information about using shared memory.ID="Media6-2IO54"ID="Media6-2IO55"ID="Media6-2IO56"To determine the buffer size needed to store the uncompressed frames, multiply the number of frames by the frame size, as shown in IDREF="97053" TYPE="TEXT"Example 29-3.ID="Media6-2IO57"LBL="29-3"Example 29-3 ID="97053"Determining What Size Buffer to Allocatestatic void insertFrames( MVid theEditMovie,
                          MVid theEditTrack,
                          MVid theSourceMovie,
                          MVid theSourceTrack ) 
{
    MVframe editStartFrame;
    MVframe sourceStartFrame;
    MVframe numFrames = getNumEditFrames();
    size_t  insBuffSize;
    void    *insBuff;

    if ( getEditTrackType() == DM_IMAGE ) {
        insBuffSize = numFrames * 
                    dmImageFrameSize( mvGetParams( theSourceTrack ) );
    }

    else if ( getEditTrackType() == DM_AUDIO ) {
        insBuffSize = numFrames * 
                     dmAudioFrameSize( mvGetParams( theSourceTrack ) );
    }

    insBuff = malloc( ( int ) insBuffSize );
    if ( insBuff == NULL ) {
        fprintf( stderr, "%s: Unable to allocate insert buffer.\n",
                getProgramName() );
        exit( EXIT_FAILURE );
    /* insert frames using mvInsertFrames(3mv) */
    }LBL="" HELPID=""Inserting Raw Images and Audio from a Buffer into an Existing TrackYou can insert raw image or audio data into an existing movie trackname='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'the Movie Library compresses the data as it is inserted into the track. To insert frames from a buffer into a track, call mvInsertFrames(). Its function prototype is: ID="Media6-2IO58"ID="Media6-2IO59"DMstatus mvInsertFrames ( MVid track, MVframe frameIndex,
                          MVframe frameCount, size_t bufferSize,
                          void* buffer )where:ID="Media6-2IO60"trackis the track into which you want to insert dataframeIndexis the frame in front of which you want to insert dataframeCountis the number of frames to insertbufferSizeis the size of the bufferbufferis a pointer to a buffer that contains the data you want to insert into the trackWhen you insert frames into an existing track, the new frames are inserted in front of frameIndex. The existing frames immediately following the insertion point, including frame frameIndex, are shifted to the right to make room for the new frames.IDREF="86491" TYPE="GRAPHIC"Figure 29-3 shows two frames (N1 and N2) inserted at frameIndex 5 into a movie with 7 frames. Frames 5 and 6 move to make room for the new frames.FILE="Media6-2IO.cgm3" POSITION="INLINE" SCALE="FALSE"LBL="29-3"Figure 29-3 ID="86491"Inserting Frames into a TrackTo achieve an effect similar to overwriting the existing data, you must first delete the unwanted frames by calling mvDeleteFrames() before inserting the new frames.LBL="" HELPID=""Reading Frames from a Movie into a Buffer for Uncompressed DataTo read a specified number of frames from an existing movie into a buffer that you have allocated for storing movie data, call mvReadFrames(). Its function prototype is:ID="Media6-2IO61"DMstatus mvReadFrames ( MVid track, MVframe frameIndex,
                        MVframe frameCount,
                        size_t bufferSize, void* buffer)where:ID="Media6-2IO62"trackis the track from which you want to read dataframeIndexis the first frame in the sequence that is to be readframeCountis the number of frames of data to readbufferSizeis the size of the buffer, obtained by multiplying the number of frames by the value returned from dmAudioFrameSize() for the audio track or by the value returned from dmImageFrameSize() for the image trackbufferis a pointer to a buffer that you have allocated for storing the dataNoteThe data is decompressed as it is read into the buffer. Use mvReadCompressedImage(), as described in IDREF="64505" TYPE="TITLE""Reading and Inserting Compressed Images" to read compressed image frames directly into a buffer.  LBL="" HELPID=""Deleting Frames from a Movie TrackTo delete frames from a movie track, call mvDeleteFrames():ID="Media6-2IO63"DMstatus mvDeleteFrames ( MVid track, MVframe frameIndex,
                          MVframe frameCount )where:ID="Media6-2IO64"trackis the track from which you want to delete framesframeIndexis the first frame in the sequence that is to be deletedframeCountis the number of frames of data to deleteLBL="" HELPID=""ID="64505"Reading and Inserting Compressed ImagesThe Movie Library has a special group of routines for working with compressed images. Performing editing operations on compressed images takes less disk space and less time than editing full resolution images. These routines operate on one frame of compressed data at a time because the size of compressed data can vary from frame to frame. Use these routines if you want to read or write compressed image data frame-by-frame, such as reading a frame at a time from a Cosmo CompressÔ board into a movie.ID="Media6-2IO65"ID="Media6-2IO66"ID="Media6-2IO67"LBL="" HELPID=""Reading a Compressed Image from a Movie into a BufferTo read a compressed image from an existing movie into a buffer that you have allocated for storing compressed data, call mvReadCompressedImage(). Its function prototype is:DMstatus mvReadCompressedImage ( MVid track,
                                 MVframe frameIndex,
                                 size_t bufferSize,
                                 void* buffer )where:ID="Media6-2IO68"trackis the movie track from which you want to read a compressed image frameframeIndexis the frame number of the image you want to readbufferSizeis the size of the bufferbufferis a pointer to a buffer that you have allocated for storing a compressed image frameTo determine the buffer size (in bytes) needed to hold a compressed image frame, call mvGetCompressedImageSize(). Its function prototype is:size_t mvGetCompressedImageSize( MVid track,
                                 MVframe frameIndex )where:ID="Media6-2IO69"trackis the movie track from which you want to read a compressed image frameframeIndexis the frame whose image size you want to knowmvGetCompressedImageSize() returns the number of bytes that image number frameIndex requires.ID="Media6-2IO70"IDREF="39617" TYPE="TEXT"Example 29-4 reads a compressed image from track into buffer.LBL="29-4"Example 29-4 ID="39617"Reading a Compressed Image from a Movie into a Buffervoid* ReadFirstImage( MVid track )
{
    size_t size   = mvGetCompressedImageSize( track, 0 );
    void*  buffer = malloc( size );
    if ( buffer = NULL ) { /* handle error */}

    if ( mvReadCompressedImage( track,
                                0,
                                size,
                                buffer ) != DM_SUCCESS ) {
        /* handle error */
    }

    return buffer;
}LBL="" HELPID=""Inserting a Compressed Image from a Buffer into an Existing TrackTo insert a compressed image from a buffer into an existing image track, call mvInsertCompressedImage(). Its function prototype is:DMstatus mvInsertCompressedImage ( MVid track,
                                   MVframe frameIndex,
                                   size_t bufferSize,
                                   void* buffer )where:ID="Media6-2IO71"trackis the track into which you want to insert a compressed imageframeIndexis the frame number in front of which the compressed frame is to be insertedbufferSizeis the size of the bufferbufferis a pointer to buffer that contains the compressed image you want to write to the trackWhen you insert a compressed frame into an existing track, the new frame is inserted in front of frame frameIndex. The existing frames immediately following the insertion point, including frame frameIndex, are shifted to the right to make room for the new frame (see IDREF="86491" TYPE="GRAPHIC"Figure 29-3 on IDREF="86491" TYPE="TEXT"page 629). To achieve an effect similar to overwriting the existing data, you must first delete the unwanted frames by calling mvDeleteFrames() before inserting the new frame.ID="Media6-2IO72"LBL="" HELPID=""Copying and Pasting Frames from One Movie into AnotherThe Movie Library has routines that let you copy frames from one movie track to another movie track without using a buffer. The two movies must have the same image frame rate and frame sizename='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'if they do not, an error is generated.ID="Media6-2IO73"ID="Media6-2IO74"These routines also let you work directly with compressed data, without decompressing and recompressing the data. You can copy compressed frames from one movie into another, even if the two movies use different compression schemes.ID="Media6-2IO75"IDREF="80457" TYPE="GRAPHIC"Figure 29-4 shows image frames being pasted from one movie into another.FILE="Media6-2IO.ras4" POSITION="INLINE" SCALE="FALSE"LBL="29-4"Figure 29-4 ID="80457"Pasting Image Frames from One Movie into Another MovieNoteBecause a movie's audio and image tracks are independent, the audio samples associated with the original images do not shift with the image frames (see IDREF="80457" TYPE="GRAPHIC"Figure 29-4). Call mvMapBetweenTracks() to locate the audio associated with the displaced frames, then paste the audio frames in the proper location.  ID="Media6-2IO76"To copy a range of frames from one movie and paste them into another movie without overwriting existing data, call ID="Media6-2IO77"mvPasteFrames(). Its function prototype is:ID="Media6-2IO78"DMstatus mvPasteFrames( MVid fromTrack,
                        MVframe fromFrameIndex,
                        MVframe frameCount,
                        MVid toTrack,
                        MVframe toFrameIndex )where:fromTrackis the source movie track from which frames are copiedfromFrameIndexis the first frame in the sequence of frames to be copied from the source movieframeCountis the number of frames to copy and pastetoTrackis the destination movie track into which the copied frames are insertedtoFrameIndexis the frame in the destination movie in front of which the new frames will be pastedWhen frames are pasted into a non-empty movie, the new frames are inserted in front of frame frameIndex. The existing frames immediately following the insertion point, including frame frameIndex, are shifted to the right to make room for the new frames.To overwrite the existing data, first call mvDeleteFrames() to delete the unwanted frames before calling mvPasteFrames().LBL="" HELPID=""Finalizing Changes and Closing MoviesDuring an editing session, you can flush the changes to the file and make sure they are written into the file by calling mvWrite(). Data in tracks is always written immediately; mvWrite() flushes the header information. Its function prototype is:ID="Media6-2IO79"DMstatus mvWrite( MVid movie )mvWrite() returns DM_SUCCESS if it was able to write the file; otherwise, DM_FAILURE is returned.ID="Media6-2IO80"When you have finished editing a movie, you write and close the file. You may choose to optimize the movie by calling mvOptimize() before closing it.To close a movie file, call mvClose(), which flushes all changes that have been made to the movie and makes sure that they are written to the file, and then destroys the movie instance. Its function prototype is:DMstatus mvClose( MVid movie )
ID="Media6-2IO81"mvClose() returns DM_SUCCESS if it was able to write and close the file; otherwise, DM_FAILURE is returned.ID="Media6-2IO82"LBL="30"ID="30953"Playing Movies with the Movie LibraryThis chapter describes the Movie Library playback facility. It explains how to create and configure a window for playing movies and how to control and synchronize movie playback. The Movie Library can use either OpenGL or IRIS GL rendering to show movies, and you can combine the display of graphics and movies using the techniques described in this chapter.ID="Media6-3PB1"This chapter also explains how to handle movie events in order to respond to user input or to monitor movie playback for system or I/O errors.You can play one or more movies at a time, all in one window or in separate windows. Follow these steps to play movies:ID="Media6-3PB2"If playing more than one movie at a time, set a hint for multiple movie playback by calling mvSetNumMoviesHint().Open the movie(s) that you want to play, using mvOpenFile(), mvOpenFD(), or mvOpenMem().Prepare a window for displaying the movie, as described in IDREF="52491" TYPE="TITLE""Creating and Configuring a Playback Window".Set up event handling, as described in IDREF="31364" TYPE="TITLE""Handling Events".Bind the window to the movie, as described in IDREF="70767" TYPE="TITLE""Binding a Movie to a Window for Playback". This step tells the Movie Library to display your movie inside the window you have prepared.Show the window.Set the movie into motion by calling mvPlay(), as described in IDREF="53016" TYPE="TITLE""Controlling Movie Playback".Process movie and window events as needed, as described in IDREF="31364" TYPE="TITLE""Handling Events".When finished, call mvClose() to destroy the movie instance. You must complete this step before destroying the window in which the movie is playing.LBL="" HELPID=""Opening a Movie for PlaybackIf the application opens more than one movie at a time, set a hint for the Movie Library before opening any movies. ID="Media6-3PB3"ID="Media6-3PB4"The Movie Library uses certain IRIX system facilities such as multiple processes and shared arenas whose size must be configured in advance. The Movie Library chooses defaults for these resources that are suitable for most applications; however, to play several movies at the same time, you must set a hint at initialization time to indicate how many movies are likely to be played so that the proper system resource allocations can be made.To set a hint for the number of movies an application will play, call mvSetNumMoviesHint(). Its function prototype is:DMstatus mvSetNumMoviesHint (int numMovies)where:ID="Media6-3PB5"numMoviesis the number of movies likely to be playedTo retrieve the number of movies at which the hint is currently set, call mvGetNumMoviesHint(). Its function prototype is:ID="Media6-3PB6"int mvGetNumMoviesHint (void)To open a movie for playback, call mvOpenFile(), mvOpenMem(), or mvOpenFD(), as described in IDREF="61830" TYPE="TITLE"Chapter 29, "File I/O and Editing Movies with the Movie Library."These calls return an identifier, movieid, for a movie instance that is used to reference the movie in subsequent Movie Library calls.LBL="" HELPID=""ID="52491"Creating and Configuring a Playback WindowThe Movie Library requires a mixed-model GL/X window (an X window configured for either OpenGL or IRIS GL rendering) in single-buffered RGB mode.ID="Media6-3PB7"LBL="" HELPID=""Creating a Window for OpenGL PlaybackSee the OpenGL Programming Guide for information on using the OpenGL window configuration routines, including: glXgetconfig(3g), glXCreateContext(3g), and glXChooseVisual(3g).LBL="" HELPID=""Creating a Window for IRIS GL Playback There are two methods for creating a GL/X window:Create a mixed-model window using Xlib calls. You can paste the sample code from APP="/usr/sbin/jot /usr/people/4Dgifts/examples/dmedia/movie/common/glxhelper.c" PARMS=""glxhelper.c
 in /usr/people/4Dgifts/examples/dmedia/movie/common/ into your application to create an appropriate window. See Graphics Library Programming Tools and Techniques for details about the calls used in glxhelper.c.If you are using IRIS IM, create a GLXMDraw widget. See the IRIS IM Programming Notes for details.ID="Media6-3PB8"IDREF="33184" TYPE="TEXT"Example 30-1 is a code fragment that shows how to create an IRIS GL playback window. (KeyReleaseMask is needed only if the application needs to listen to keystrokes when the mouse pointer is inside the movie window.)LBL="30-1"Example 30-1 ID="33184"Creating an IRIS GL Playback Window/*********
*
* Open a connection to the X server, and create an X window
* suitable for GL rendering here.
*
*********/

static DMstatus createXWindow( Display **dpy, Window *win,int width, int height )
{
    XSetWindowAttributes childWinAttribs;

    /* 
     *Open a connection to the X server. 
     */

    *dpy = XOpenDisplay( 0 );
    if ( *dpy == NULL ) {
        fprintf( stderr, "%s: Cannot open display.\n", programName );
        return DM_FAILURE;
    }
    childWinAttribs.colormap = DefaultColormap( *dpy, DefaultScreen( *dpy ) );

    /* 
     * Even if we don't use it, childWinAttribs.border_pixel must be something.
     */

    childWinAttribs.border_pixel = 0;

    /* 
     * Create an X window configured for GL rendering, using
     * the helper functions defined in glxhelper.c
     */

    *win = GLXCreateWindow( *dpy, RootWindow( *dpy, DefaultScreen( *dpy ) ),
                           100, 100, width, height, 0, CWColormap|CWBorderPixel,
                           &childWinAttribs, GLXrgbSingleBuffer );
    XSelectInput( *dpy, *win, ExposureMask | StructureNotifyMask | KeyReleaseMask );
    return DM_SUCCESS;
}LBL="" HELPID=""Configuring the Playback DisplayYou can control these settings for the playback display:ID="Media6-3PB9"Backgroundwhat color fills the space between the movie and the window perimeterView sizehow big the movie frame isView offsetwhere the movie frame is relative to the window originYou can call the routines that configure the size, offset, and background color of the playback window whether or not a movie is currently playing within that window. IDREF="76442" TYPE="GRAPHIC"Figure 30-1 diagrams the view settings.FILE="30-1.online.bw" POSITION="INLINE" SCALE="FALSE"LBL="30-1"Figure 30-1 ID="76442"Playback View SettingsLBL="" HELPID=""Setting and Getting the Background ColorThe Movie Library automatically centers the movie in the view you specify and fills the areas within the view that are not part of the movie frame with a background color. The default background color is "SGI light gray," which has the components (170, 170, 170).ID="Media6-3PB10"To set the background color, call mvSetViewBackground(). Its function prototype is:void mvSetViewBackground ( MVid movieid, unsigned short red,
                          unsigned short green, unsigned short blue )where:ID="Media6-3PB11"red, green, blueare color components in the range 0-255To get the background color, call mvGetViewBackground(). Its function prototype is:ID="Media6-3PB12"void mvGetViewBackground ( MVid movieid,
                           unsigned short* redreturn,
                           unsigned short* greenreturn,
                           unsigned short* bluereturn )where:ID="Media6-3PB13"redreturn, greenreturn, bluereturnare pointers into which the values of the color components are returnedLBL="" HELPID=""Setting and Getting the Viewing Area SizeThis section explains how to specify the size of the viewing area used for rendering a movie within a window. The default view size is the width and height of the movie frame, as it is specified in the movie file.ID="Media6-3PB14"The Movie Library zooms the movie frame to fill the specified area as closely as possible, while adhering to the pixel zooming restraints imposed by the IRIS GL or Open GL renderer. Any remaining area between the window and the movie viewing area is filled with the background color that you specified.NoteCurrently, only integer zooming (2name='times' font=symbol charset=fontspecific code=180
	TeX='\times ' descr='[times]', 3name='times' font=symbol charset=fontspecific code=180
	TeX='\times ' descr='[times]', and so on) is supported, regardless of the graphics hardware platform.  ID="Media6-3PB15"To specify the size of the movie viewing area, call mvSetViewSize(). Its function prototype is:ID="Media6-3PB16"void mvSetViewSize ( MVid movieid, int newwidth,
                     int newheight,
                     DMboolean keepaspect )where:ID="Media6-3PB17"newwidthis the requested width, in pixels, of the viewing area newheightis the requested height, in pixels, of the viewing areakeepaspectcontrols whether the aspect ratio is preserved when zooming the movie:DM_TRUE preserves the aspect ratioDM_FALSE does not preserve the aspect ratioTo determine the viewing area size that the Movie Library will actually create for a desired width and height, call mvQueryViewSize(). Its function prototype is:ID="Media6-3PB18"ID="Media6-3PB19"void mvQueryViewSize ( MVid movieid, int width, int height,
                       DMboolean keepaspect,int* widthreturn,
                       int* heightreturn )where:ID="Media6-3PB20"widthis the requested width, in pixels, of the viewing area heightis the requested height, in pixels, of the viewing areakeepaspectcontrols whether the aspect ratio is preserved when zooming the movie:DM_TRUE preserves the aspect ratioDM_FALSE does not preserve the aspect ratiowidthreturnis a pointer into which the actual width, in pixels, of the viewing area is returnedheightreturnis a pointer into which the actual height, in pixels, of the viewing area is returnedIf a movie appears by itself with no other graphics in a GL/X window, you might want to ensure that your code chooses the proper window size by calling mvQueryViewSize() in advance, and using the results to resize the GL/X window to be the same size as the moviename='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'or you can simply choose a background color for the unused portions of the display by calling mvSetViewBackground().ID="Media6-3PB21"If the movie appears in a GL/X window along with other graphic elements, such as a movie that is embedded in a larger display, you might be more concerned about setting an exact view size, so you should set the view size carefully by first calling mvQueryViewSize() and then using the results as inputs to mvSetViewSize(). This makes sure that the Movie Library draws on the display only over regions that the movie frame actually occupies.To get the requested view size, call mvGetViewSize(). Its function prototype is:ID="Media6-3PB22"ID="Media6-3PB23"void mvGetViewSize ( MVid movieid, int *widthreturn,
                     int *heightreturn)where:ID="Media6-3PB24"widthreturnis a pointer into which the width is returnedheightreturnis a pointer into which the height is returnedmvGetViewSize() always returns the last requested size of the movie view, regardless of the movie's actual view size. Thus, you can inspect both the requested size by calling mvGetViewSize(), and the actual frame size by calling mvQueryViewSize().LBL="" HELPID=""Setting and Getting the Viewing Location OffsetThis section explains how to set and get the movie viewing location offset, as measured from the window origin.ID="Media6-3PB25"ID="Media6-3PB26"IRIS GL and the X Window System differ in where they assign the coordinates of the origin (0,0). IRIS GL defines (0,0) as the lower left corner of the screen; the X coordinate system defines (0,0) as the upper left corner of the screen. The default offset is (0, 0) in the IRIS GL coordinate system.ID="Media6-3PB27"ID="Media6-3PB28"ID="Media6-3PB29"ID="Media6-3PB30"To set the viewing location offset, call mvSetViewOffset(). Its function prototype is:void mvSetViewOffset ( MVid movieid, int offsetx,
                       int offsety, DMboolean glcoordsystem )where:ID="Media6-3PB31"offsetxis the offset from the origin in the x (horizontal) dimensionoffsetyis the offset from the origin in the y (vertical) dimensionglcoordsystemis a boolean that you use to specify the screen coordinate system for the y offset If glcoordsystem is DM_TRUE, the Movie Library assumes that you are using the IRIS GL screen coordinate system; if glcoordsystem is DM_FALSE, the Movie Library assumes that you are using the X coordinate system.To determine the offset that the Movie Library will actually produce for a given offset, call mvQueryViewOffset(). Its function prototype is:void mvQueryViewOffset ( MVid movieid,
                         int offsetx,
                         int offsety,
                         int* offsetxreturn,
                         int* offsetyreturn,
                         DMboolean glcoordsystem ) where:ID="Media6-3PB32"offsetxis the requested offset from the origin in the x (horizontal) dimensionoffsetyis the requested offset from the origin in the y (vertical) dimensionoffsetxreturnis a pointer into which the actual offset from the origin in the x (horizontal) dimension is returnedoffsetyreturnis a pointer into which the actual offset from the origin in the y (vertical) dimension is returnedglcoordsystemspecifies the screen coordinate system for the y offset:DM_TRUE for the IRIS GL screen coordinate system;DM_FALSE for the X coordinate system.To get the requested offset value, call mvGetViewOffset(). Its function prototype is:void mvGetViewOffset ( MVid movieid, int* offsetxreturn,
                       int* offsetyreturn,
                       DMboolean glcoordsystem )where:ID="Media6-3PB33"offsetxreturnis a pointer into which the requested offset from the origin in the x (horizontal) dimension is returnedoffsetyreturnis a pointer into which the requested offset from the origin in the y (vertical) dimension is returnedglcoordsystemspecifies the screen coordinate system for the y offset:DM_TRUE for the IRIS GL screen coordinate system;DM_FALSE for the X coordinate system.mvGetViewOffset() always returns the last requested origin of the movie frame, regardless of the movie's actual origin. Thus, you can inspect both the requested origin by calling mvGetViewOffset(), and the actual origin by calling mvQueryViewOffset().LBL="" HELPID=""ID="70767"Binding a Movie to a Window for PlaybackIn order to play a movie, you need to bind it to a window. Binding a movie to a window performs internal initialization that prepares the movie for playback in that window; therefore, you must bind the movie to the window before calling any playback routines on that movie.To bind a movie to an IRIS GL window, call mvBindWindow(). To bind a movie to an OpenGL window, call mvBindOpenGLWindow().The function prototypes for mvBindWindow() and mvBindOpenGLWindow()   are:ID="Media6-3PB34"ID="Media6-3PB35"DMstatus mvBindWindow(MVid movieid, Display* dpy, Window win)
DMstatus mvBindOpenGLWindow(MVid movieid, Display* dpy,                            Window win, GLXContext ctxt)where:ID="Media6-3PB36"dpyis the display on which the movie playswinis the window in which the movie playsctxtis the OpenGL graphics context for the movieFor example, to bind the movie named theMovie to the GL/X window identified by win on the display dpy, using IRIS GL:if ( mvBindWindow( theMovie, *dpy, win ) != DM_SUCCESS ) {
    fprintf( stderr, "%s: Could not bind movie to window.\n",
programName );
}You can bind a movie to only one window at a time. The movie remains bound to that window until you call mvClose() to close the movie. You can call mvBindWindow() or mvBindOpenGLWindow() only once per movie.LBL="" HELPID=""Binding a Window to a Movie with an Audio TrackIf the specified movie has an audio track, mvBindWindow() or mvBindOpenGLWindow() attempts to open an audio port by calling ALopenport() from the Audio Library. If the attempt is successful, an audio port with the name movipid:movieid is created:ID="Media6-3PB37"where:pidis the system process ID that is using the audio portmovieidis the movie ID containing the soundtrack that is being played on the portSee the ALopenport(3A) man page for more information.If it is not possible to open an audio port, either because there are no free audio ports or because your system does not have audio capability, the Movie Library plays the image track without playing the accompanying audio track.ID="Media6-3PB38"NoteAudio is not part of the X protocol, so the audio track will not follow the DISPLAY variable if the movie is played remotely. LBL="" HELPID=""Playing Multiple Movies in the Same WindowTo play more than one movie in the same window, bind the different movies to the same window by calling ID="Media6-3PB39"mvBindWindow() or mvBindOpenGLWindow() with the same dpy and win values for each movieid.ID="Media6-3PB40"ID="Media6-3PB41"IDREF="25792" TYPE="TEXT"Example 30-2 is a code fragment that binds several movies to a window, and then shifts the position of each movie so they do not overlap.LBL="30-2"Example 30-2 ID="25792"Binding a Window for Playing Multiple Moviesstatic DMstatus bindWinToMovies( MVid* movieList, int numMovies,Display* dpy, Window win )
{
    int  i;
    MVid currentMovie;

    for ( i = 0; i < numMovies; i++ ) {
        currentMovie = movieList[ i ];

        /* 
         * Bind the GL window to the movie.  This will cause several
         * movies to play in the same window.
         */
        if ( mvBindWindow( currentMovie, dpy, win ) != DM_SUCCESS ) {
            fprintf( stderr, "%s: Could not bind movie to window.\n",
                    getProgramName() );
            return DM_FAILURE;
        }
    }

    /*
     * Initially, all the movies appear at the same display location.
     * We call a helper function to move them so they do not overlap.
     * The helper function calls mvSetViewOffset(3mv) to accomplish
     * this task.
     */
    if ( setMovieViewOffsets() != DM_SUCCESS ) {
        return DM_FAILURE;
    }

    return DM_SUCCESS;
}
LBL="" HELPID=""ID="53016"Controlling Movie PlaybackThe Movie Library provides calls for controlling movie playback that let you start and stop playback, control audio playback, control the loop mode and loop limit, and scrub to a random frame.LBL="" HELPID=""Starting and Stopping PlaybackTo set a movie into motion, call mvPlay(). Its function prototype is:ID="Media6-3PB42"void mvPlay ( MVid movieid )To halt playback, call mvStop(). Its function prototype is:ID="Media6-3PB43"void mvStop ( MVid movieid )Playing or stopping a movie does not affect any of the other settings such as playback speed or direction; for example, if you stop a movie that is playing backward, then start it again, the movie will continue to play backward.LBL="" HELPID=""Controlling Audio PlaybackIf a movie has an audio track, the Movie Library plays the audio for you automatically if it is able to allocate an audio port.LBL="" HELPID=""Enabling and Muting AudioYou can control audio muting during playback. Muting controls only whether audio is played; it does not alter the audio volume. To control audio muting, call mvSetEnableAudio(). Its function prototype is:void mvSetEnableAudio ( MVid movieid, DMboolean onoff )where:onoffcontrols audio muting: DM_TRUE enables audio, DM_FALSE mutes audioTo retrieve the current mute setting, call mvGetEnableAudio(). Its function prototype is:DMboolean mvGetEnableAudio ( MVid movieid )IDREF="49774" TYPE="TEXT"Example 30-3 is an excerpt from APP="/usr/sbin/jot /usr/people/4Dgifts/examples/dmedia/movie/manymovie/manymovieEvents.c" PARMS=""manymovieEvents.c
, in /usr/people/4Dgifts/examples/dmedia/movie/manymovie, that shows how to use audio muting. See IDREF="61924" TYPE="TITLE""Playing Multiple Movies" in Chapter 32 for a complete description of manymovie.LBL="30-3"Example 30-3 ID="49774"Enabling and Muting Audio Playbackstatic void toggleAudioMuting( MVid *theMovies , int numMoviesInWin )
{
    int i;

    for( i = 0;i < numMoviesInWin; i++ ) {

        printf("%s: %d, toggle mute to ", getProgramName(), theMovies[i]);
        
        if ( mvGetEnableAudio( theMovies[i] ) ) {
            printf( "OFF\n" );
            mvSetEnableAudio( theMovies[i], DM_FALSE );
        } else {
            printf( "ON\n" );
            mvSetEnableAudio( theMovies[i], DM_TRUE );
        }
    }
}LBL="" HELPID=""Guaranteeing the Audio Sample Rate When Playing Multiple MoviesIf you play several movies at the same time, it is possible that not all of them will have the same audio sample rate. The audio hardware uses the same sample rate for all concurrent audio processes; therefore, if you play two (or more) movies that have different audio sample rates at the same time, audio plays at the wrong speed for movies whose sample rate is different than the current audio hardware rate setting. This happens only when playing more than one movie at a time; the audio sample rate is set to the proper value when playing a movie by itself.To guarantee the proper playback rate for a certain movie when it is played in conjunction with other movies, call mvSetPrimaryAudio(). Its function prototype is:void mvSetPrimaryAudio ( MVid movieid )To determine which movie has the primary audio rate control, call mvGetPrimaryAudio(). Its function prototype is:MVid mvGetPrimaryAudio ( void )If you are writing an application that allows users to select and play one movie at a time from among several movies, you want the audio playback to be correct for each movie, even though not all of the movies have the same sample rate. In that case, call mvSetPrimaryAudio() on the selected movie before calling mvPlay().IDREF="92745" TYPE="TEXT"Example 30-4 is an excerpt from APP="/usr/sbin/jot /usr/people/4Dgifts/examples/dmedia/movie/manymovie/manymovieEvents.c" PARMS=""manymovieEvents.c
, in /usr/people/4Dgifts/examples/dmedia/movie/manymovie, which guarantees the proper audio rate for the first movie that the user enters on the command line. See IDREF="61924" TYPE="TITLE""Playing Multiple Movies" in Chapter 32 for a complete description of manymovie.LBL="30-4"Example 30-4 ID="92745"Designating a Movie as the Primary Audio Rate Controllerstatic void playTheMovies( MVid *theMovies, int numMoviesInWin )
{
    int i;

    mvSetPrimaryAudio( theMovies[0] );

    for( i = 0;i < numMoviesInWin; i++ ) {
        printf("%s: %d, play\n", getProgramName(), theMovies[i]);
        mvPlay( theMovies[i] );
    }
}LBL="" HELPID=""ID="57199"LoopingLooping is the process of repeatedly playing movie frames. You can define the starting and ending frame for movie playback, thereby allowing looping on an entire movie or only a fragment of the movie. You can define only one such loop per movie.The playback loop mode, which is independent of the default loop mode that is stored in the movie, controls the playback behavior:NoneThe movie (or fragment) plays once through and then stops.ContinuousThe movie (or fragment) plays repeatedly.SwingingThe movie (or fragment) plays repeatedly back and forth from start to end in a forward direction, then from end to start in a backward direction.If the movie was playing backward when looping began, the movie will continue to loop backward.LBL="" HELPID=""ID="64945"Setting and Getting the Playback Loop ModeTo set the playback loop mode, call mvSetPlayLoopMode(). Its function prototype is:void mvSetPlayLoopMode ( MVid movieid, MVloopmode newloopmode )where:newloopmodeis the loop mode:MV_LOOP_NONE to play once through (default)MV_LOOP_CONTINUOUSLY to play continuouslyMV_LOOP_SWINGING to swing back and forthYou can change the loop mode whether or not the movie is playing. If you change the loop mode from swinging to continuous while the movie is on a backward swing, the movie will play backward continuously.The Silicon Graphics movie file format lets you store a default loop mode setting within the movie file, which is independent of the playback loop mode setting. When opening a movie file, the Movie Library obtains the default loop mode from the movie if it is set; otherwise, it assumes MV_LOOP_NONE is the default loop mode. So, when you first play the movie, it uses the default loop mode until you change the playback loop mode.To change the default loop mode stored in a movie, call mvSetLoopMode(), as described in IDREF="10648" TYPE="TITLE""Setting and Getting the Default Movie Loop Mode".To retrieve the current loop mode setting for a specified movie, call mvGetPlayLoopMode(). Its function prototype is:MVloopmode mvGetPlayLoopMode ( MVid movieid )IDREF="97518" TYPE="TEXT"Example 30-5 is an excerpt from APP="/usr/sbin/jot /usr/people/4Dgifts/examples/dmedia/movie/manymovie/manymovieEvents.c" PARMS=""manymovieEvents.c
, in /usr/people/4Dgifts/examples/dmedia/movie/manymovie, which is described in IDREF="61924" TYPE="TITLE""Playing Multiple Movies" in Chapter 32, that toggles the loop mode for a movie that is playing.LBL="30-5"Example 30-5 Setting and Getting the Loop Mode ID="97518"static void stepToNextLoopState ( MVid *theMovies,
                                  int numMoviesInWin )
{
    int i;
    MVloopmode loopMode;

    for( i = 0;i < numMoviesInWin; i++ ) {

        printf("%s: %d, change loop state to ",
               getProgramName(), theMovies[i]);

        switch ( mvGetPlayLoopMode( theMovies[i] ) ) {
        case MV_LOOP_NONE:
            loopMode = MV_LOOP_CONTINUOUSLY;
            printf( "CONTINUOUS\n" );
            break;
        case MV_LOOP_CONTINUOUSLY:
            loopMode = MV_LOOP_SWINGING;
            printf( "SWINGING\n" );
            break;
        case MV_LOOP_SWINGING:
            loopMode = MV_LOOP_NONE;
            printf( "NONE\n" );
            break;
        }
        mvSetPlayLoopMode( theMovies[i], loopMode );
    }
}LBL="" HELPID=""Counting and Limiting the Number of Loop PlaysThe Movie Library provides a facility for counting the number of times a movie has played. You can use this facility to query how many times the movie has played and to limit the number of times playback can loop.Initialize the counter by calling mvSetPlayLoopCount(), and provide an upper bound for the number of loops by calling mvSetPlayLoopLimit() if you want to limit the number of times the movie is played.You can query for the current loop count and reset it if desired. For example, if your application lets the user stop and restart playback, you can either reset the loop count when playback resumes or continue counting from the number of loops that were already completed when the movie was stopped. In swinging mode, each play of the movie forward or backward counts as one loop.To set the loop count, call mvSetPlayLoopCount(). Its function prototype is:void mvSetPlayLoopCount( MVid movieid, MVframe newloopcount )where:newloopcount is the value to which you want to set the loop countTo retrieve the current loop count for a movie, call mvGetPlayLoopCount(). Its function prototype is:MVframe mvGetPlayLoopCount ( MVid movieid )To limit the number of times a movie can play in continuous or swinging mode, call mvSetPlayLoopLimit(). Its function prototype is:void mvSetPlayLoopLimit ( MVid movieid, MVframe newlooplimit)where:newlooplimitis the number of times to loop or swing. This value must be either an integer or MV_LIMIT_FOREVER to keep playing a movie forever.To retrieve the current loop limit value, call mvGetPlayLoopLimit(). Its function prototype is:MVframe mvGetPlayLoopLimit ( MVid movieid )LBL="" HELPID=""Playing or Looping a Movie FragmentYou can play or loop-play a movie fragment by selecting a start frame and an end frame within the movie. You can define only one such fragment per movie and the fragment must contain at least one frame.The start frame number must be less than or equal to the end frame number; frame 0 (the first frame of the movie) is the default start frame. If the current frame is outside the range specified, the Movie Library treats the movie as having reached end-of-media.To define the start frame for a movie (or fragment), call mvSetStartFrame(). Its function prototype is:void mvSetStartFrame ( MVid movieid, MVframe startframe )where:startframeis the frame number where playback beginsTo define the end frame of a movie (or fragment), call mvSetEndFrame(). Its function prototype is:void mvSetEndFrame ( MVid movieid, MVframe endframe )where:endframeis the frame number where playback endsTo get the frame number where playback begins, call mvGetStartFrame(). Its function prototype is:MVframe mvGetStartFrame ( MVid movieid )To get the frame number where playback ends, call mvGetEndFrame(). Its function prototype is:MVframe mvGetEndFrame ( MVid movieid )LBL="" HELPID=""Scrubbing to a Random Frame During PlaybackThis section explains how to use the Movie Library to respond to an interface such as a scroll bar that lets the user jump to a random location in a movie while it is playing.The Movie Library has two routines for jumping to a particular frame in a movie: mvSetCurrentFrame() and mvScrubCurrentFrame(). Each routine is designed with a specific use in mind. mvScrubCurrentFrame() provides the quickest response by rapidly locating the nearest frame (keyframe for keyframed formats) to the selected frame; mvSetCurrentFrame() provides the most accurate response, by precisely locating and then showing the frame exactly as selected.Their function prototypes are:void mvScrubCurrentFrame( MVid movieid, MVframe newframe )
void mvSetCurrentFrame( MVid movieid, MVframe newframe )where:newframeis the frame to which you want to jumpOne way to use these routines is to call mvScrubCurrentFrame() to make coarse jumps while the user is dragging the scroll bar, then call mvSetCurrentFrame() to go to and display the exact frame when the user releases the mouse button. The triangular pointer in the Movie Player uses this technique.To retrieve the location of the current frame, call mvGetCurrentFrame(). Its function prototype is:MVframe mvGetCurrentFrame ( MVid movieid )LBL="" HELPID=""Synchronizing Movie PlaybackMovies play in real time, slaved to a time base that is expressed in frames per second (FPS). The time base is either synchronized to the audio track or to a software-based timer that you set with mvSetImageRate().LBL="" HELPID=""Getting and Setting the Playback SpeedYou can specify a multiplier for modifying a movie's natural frame rate. The natural frame rate is stored in the movie file and can be retrieved with mvGetImageRate(). The default playback speed is 1.0 times the natural frame rate.To alter the normal playing speed, call mvSetPlaySpeed(), which expresses the new playback speed as a multiple of the movie's natural image frame rate. Its function prototype is:void mvSetPlaySpeed ( MVid movieid, double newplayspeed )where:newplayspeedis an integer (for speeding up playback) or the reciprocal of an integer (for slowing down playback)For example, if you specify a newplayspeed of 2.0, the movie plays twice as fast as it normally would. If you specify a negative value for newplayspeed, the movie plays in reverse. You can measure the resulting frame rate by calling mvGetActualFrameRate(), as described in IDREF="48206" TYPE="TITLE""Measuring the Current Frame Rate".For movies with audio, the Movie Library attempts to resample the audio to match the requested playback speed. This changes the pitch of the audio. The current implementation uses software techniques to perform audio resampling, so you should be aware that changing the speed of playback for a movie with an audio soundtrack consumes more processor time.mvSetPlaySpeed() has no effect if the movie has been set to play every frame with mvSetPlayEveryFrame().To determine the current setting for playback speed, call mvGetPlaySpeed(). Its function prototype is:double mvGetPlaySpeed ( MVid movieid )LBL="" HELPID=""ID="48206"Measuring the Current Frame RateYou can obtain the actual playback frame rate for a movie as it is playing; the Movie Library computes the frame rate over the last second of playback. The current frame rate is the movie's natural frame rate, in frames per second (FPS) modified by the speed factor set by mvSetPlaySpeed().To determine the current frame rate, call mvGetActualFrameRate(). Its function prototype is:DMstatus mvGetActualFrameRate ( MVid movieid,
                                double* ratereturn )where:ratereturnis a pointer into which the actual frame rate is returnedIf your application needs more detailed information about the rate of movie playback, you should check the timestamp information associated with movie frame events as described in IDREF="31364" TYPE="TITLE""Handling Events".LBL="" HELPID=""Setting and Getting a Minimum Playback Speed ThresholdIf the playback rate lags behind the time base, the Movie Library will drop (not display) image frames to stay in sync.You can set a threshold value for the slowest acceptable playback speed and have the Movie Library notify you through the movie event queue if the playback speed falls below this minimum accepted speed by requesting MV_SLOW_PLAY events using mvSetSelectEvents().The threshold value must be a number between 0 and 1, which is a fraction of the movie's current speed. The current speed of a movie is its natural frame rate, as specified in the movie file, multiplied by the speed factor set by mvSetPlaySpeed(). The threshold setting is globalname='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'it applies to all movies in your application. The default threshold is .5, meaning 50% of the current speed.To set a minimum playback rate threshold, call mvSetSlowThreshold(). Its function prototype is:void mvSetSlowThreshold ( double slowthresh )where:slowthreshis the threshold value for minimum playback speedTo retrieve the current minimum playback rate threshold, call mvGetSlowThreshold(). Its function prototype is:double mvGetSlowThreshold ( void )See IDREF="95415" TYPE="TITLE""Checking and Correcting for Slow Playback" for an explanation of how to check for slow playback events and correct for them.LBL="" HELPID=""Forcing Playback of Every FrameSome applications need to display every frame in a movie; for example, visualization applications where the user is looking for data patterns or trends must show every frame to ensure accuracy. The audio track is ignored (the movie plays silently) when you play every frame. In addition, the slow playback threshold is ignored when you play every frame, and you won't receive MV_SLOW_PLAYBACK events either.To force the Movie Library to show every frame in sequence as fast as it can, call mvSetPlayEveryFrame(), passing in a value of DM_TRUE. Its function prototype is:void mvSetPlayEveryFrame( MVid movieid, DMboolean sync )where:syncdetermines whether the Movie Library should play every frame or drop frames to stay in pace with the movie's time base: DM_TRUE plays every frame; DM_FALSE (default) drops frames when it becomes necessary in order to maintain a desired frame rateTo find out if playback is set to play every frame, call mvGetPlayEveryFrame(). Its function prototype is:DMboolean mvGetPlayEveryFrame( MVid movieid )LBL="" HELPID=""Integrating Movies with IRIS GL GraphicsYou can use the Movie Library with the IRIS Graphics Library to combine the display of movies and graphics. You can draw IRIS GL graphics in the same window where a movie is playing, or you can have a movie playing in one window while drawing graphics in another window on the same screen.The Movie Library has two different methods for combining movies and graphicsname='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'which method you use depends on whether the graphics overlap any portion of the movie frame. In one method, the display of movie frames is automatically controlled for you, in the other method, you control the display of movie frames manually from within your application. Frame display is a global setting that applies to all currently open movies.LBL="" HELPID=""Controlling the Frame Display AutomaticallyIf the graphics you want to draw using the IRIS GL do not overlap any portion of the movie display, for example, if your application displays movies and graphics side-by-side in one window or in separate windows, you can use the mvGrabIrisGL() and mvReleaseIrisGL() routines. These routines essentially put movie rendering on hold while the application performs IRIS GL rendering. Their function prototypes are:void mvGrabIrisGL ( void )void  mvReleaseIrisGL ( void )Use mvGrabIrisGL() and mvReleaseIrisGL() to surround the block of code that performs the IRIS GL drawing. Call mvGrabIrisGL() at the beginning of a block of IRIS GL calls, then call mvReleaseIrisGL() when the IRIS GL drawing is completed.IDREF="45092" TYPE="TEXT"Example 30-6 is an excerpt from APP="/usr/sbin/jot /usr/people/4Dgifts/examples/dmedia/movie/moviescreen/moviescreenGl.c" PARMS=""moviescreenGl.c
, in /usr/people/4Dgifts/examples/dmedia/movie/moviescreen, that shows how to use mvGrabIrisGL() and mvReleaseIrisGL(). It uses the IRIS GL bgnpolygon() and endpolygon() routines to draw a box that covers up the distracting visual effects caused by moving the movie around the screen. See IDREF="87810" TYPE="TITLE""Creating a Movie Screensaver Application" in Chapter 32 for more details about the moviescreen program.LBL="30-6"Example 30-6 ID="45092"Using mvGrabIrisGL() and mvReleaseIrisGL() void undrawSaverPicture()
{
    int  width;
    int  height;
    MVid imageTrack;
    MVid theMovie = getMovieID();

    if ( isFullScreen() )
        return;

    /*
     * Determine current size of movie on display
     */

    mvFindTrackByMedium( theMovie, DM_IMAGE, &imageTrack );
    width  = mvGetImageWidth( imageTrack );
    height = mvGetImageHeight( imageTrack );
    width *= getZoom();
    height *= getZoom();
    /*
     * Draw two black boxes to erase trailing garbage as 
     * movie dances around the display.
     */

    mvGrabIrisGL();
    bgnpolygon();
    {
        short vctr[2];
        if ( dx > 0 ) {

            vctr[0] = offsetx - 1;
            vctr[1] = offsety - 1;
            v2s( vctr );

            vctr[0] = offsetx +dx;
            vctr[1] = offsety - 1;
            v2s( vctr );
            vctr[0] = offsetx + dx;
            vctr[1] = offsety + height + 1;
            v2s( vctr );

            vctr[0] = offsetx - 1;
            vctr[1] = offsety + height + 1;
            v2s( vctr );

        } else {

            vctr[0] = offsetx + width;
            vctr[1] = offsety - 1;
            v2s( vctr );

            vctr[0] = offsetx + width + dx + 1;
            vctr[1] = offsety - 1;
            v2s( vctr );

            vctr[0] = offsetx + width + dx + 1;
            vctr[1] = offsety + height + 1;
            v2s( vctr );

            vctr[0] = offsetx + width;
            vctr[1] = offsety + height + 1;
            v2s( vctr );
        }
        if ( dy > 0 ) {

            vctr[0] = offsetx - 1;
            vctr[1] = offsety - 1;
            v2s( vctr );

            vctr[0] = offsetx + width + 1;
            vctr[1] = offsety - 1;
            v2s( vctr );

            vctr[0] = offsetx + width + 1;
            vctr[1] = offsety + dy;
            v2s( vctr );

            vctr[0] = offsetx - 1;
            vctr[1] = offsety + dy;
            v2s( vctr );
        } else {

            vctr[0] = offsetx - 1;
            vctr[1] = offsety + height;
            v2s( vctr );

            vctr[0] = offsetx + width + 1;
            vctr[1] = offsety + height;
            v2s( vctr );

            vctr[0] = offsetx + width + 1;
            vctr[1] = offsety + height + dy + 1;
            v2s( vctr );

            vctr[0] = offsetx - 1;
            vctr[1] = offsety + height + dy + 1;
            v2s( vctr );
        }
    }
    endpolygon();

     mvReleaseIrisGL();
}LBL="" HELPID=""Controlling the Frame Display ManuallyThe disadvantage of the automatic   mvGrabIrisGL() ­ mvReleaseIrisGL() method is that you can't really synchronize the graphics and movie displays, so some things might not display exactly when you want them to.This section describes a method that lets you disable the Movie Library's automatic display so that your application can control the display of movie frames. When you have explicit control over the display of movie frames, you can tell the Movie Library exactly when to show a frame. This technique lets you draw IRIS GL graphics on top of the movie frame.To have your application (rather than the Movie Library) control the display of movie frames, call mvSetFrameDisplay(DM_FALSE). Its function prototype is:void mvSetFrameDisplay ( DMboolean showframes )where:showframesis either DM_TRUE, to allow automatic frame display, or DM_FALSE, to disable automatic frame display for all open moviesTo retrieve the current frame display setting, call mvGetFrameDisplay(). Its function prototype is:DMboolean mvGetFrameDisplay ( void )When you disable the frame display, you have to tell the Movie Library when to display a frame by calling mvShowCurrentFrame() for every frame that you want to display. Its function prototype is:void mvShowCurrentFrame ( MVid movieid )To play a movie with manual frame display, you should include MV_EVENT_FRAME events in your event mask and call mvShowCurrentFrame() in response to those events.Another way to use mvShowCurrentFrame() is to repaint a movie frame in response to an expose event from the window manager. See IDREF="10876" TYPE="TITLE""Handling X Window Events" for more details.You must use one of the two methods described in this section if you want to draw with the IRIS GL while using the Movie Library. The IRIS GL and the Movie Library use separate threads of execution, and the routines discussed in this section supply internal synchronization between the processes. The results of any other method of combining graphics and movies are unpredictable. See Appendix E, "Using Graphics and Share Groups," in the Graphics Library Programming Guide for more information.Hint Advanced programmers might want to turn off the frame display in order to use a double-buffered RGB window. In this case, you have to create a window with the graphics configuration that you want, and you must call swapbuffers() from the GL and mvShowCurrentFrame() from the Movie Library yourself for every frame. This method is only for special cases; it is not the recommended way to combine graphics and movies. IDREF="15003" TYPE="TEXT"Example 30-7 is an excerpt from APP="/usr/sbin/jot /usr/people/4Dgifts/examples/dmedia/movie/moviescreen/moviescreenEvents.c" PARMS=""moviescreenEvents.c
, in /usr/people/4Dgifts/examples/dmedia/movie/moviescreen, that uses mvSetFrameDisplay() to temporarily disable movie display while initialization routines are carried out.LBL="30-7"Example 30-7 ID="15003"Initializing Movie Playbackstatic void initSaverMovie( )
{
    MVid        newMovie;
    char        *movieName;

    movieName = pickMovieAtRandom( );

    if ( mvOpenFile( movieName, O_RDONLY, &newMovie ) ==
                     DM_FAILURE ) {
        printf("%s: Could not open movie %s\n",
               getProgramName(),movieName );
        exit( EXIT_FAILURE );
    }
    setMovieID( newMovie );

    /*
     * Disable movie display temporarily,
     * to avoid visual glitches.
     */

    mvSetFrameDisplay( DM_FALSE );

    setupMovieWindow( newMovie );
    setPlaybackLoopmode( newMovie );
    setVolumeLevel( newMovie );

    /*
     * play the movie
     */

    mvSetCurrentFrame( newMovie, 0 );
    mvSetFrameDisplay( DM_TRUE );
    mvPlay( newMovie );
}LBL="" HELPID=""ID="31364"Handling EventsIf you've used the select() or poll() system calls, or X event handling, the Movie Library event handling routines will be familiar to you. Even if your application has no user interface, it is recommended that you implement movie event handling because it is the best way to find out about system or I/O errors that occur while a movie is playing.The Movie Library has an event queue, similar to the X event queue, that stores movie events such as playing a frame, stopping playback, and errors. One queue stores movie events for all currently open movies by storing the identifier of the movie instance responsible for an event as part of the event.The Movie Library event queue is separate from the X event queue; so if your application handles X events as well as movie events, you'll probably want to create two separate functions, one for handling X events and one for handling movie events, and call them both from main(). To add movie event handling to your application, follow these basic steps:Choose which movie events you want to process by creating an event mask, as described in IDREF="91960" TYPE="TITLE""Preparing an Event Mask".Get the file descriptor associated with the movie event queue, as described in IDREF="68133" TYPE="TITLE""Getting a File Descriptor for the Movie Event Queue".Use one of the following methods to listen for movie and X11 events.If you are using Xlib, prepare a file descriptor set that contains both the Movie Library and X11 file descriptors using the FD_SET macro, and then code a select() loop that listens for movie events and X events, as described in IDREF="69213" TYPE="TITLE""Preparing a File Descriptor Set".If you are using Xt or IRIS IM, pass the Movie Library file descriptor to the X toolkit using XtAppAddInput(3Xt). This call associates an Xt callback function that you have written for your application with the Movie Library file descriptor. You can then process movie events inside this Xt callback, which gets called whenever one or more events appear on the Movie Library event queue.As long as there are events on the queue, keep processing them.These steps are described in detail in the sections that follow.LBL="" HELPID=""ID="91960"Preparing an Event MaskAn event mask specifies which events the application is interested in processing. The event mask is a bitwise or of the events.IDREF="95064" TYPE="TABLE"Table 30-1 lists and describes the Movie Library events.COLUMNS="2"LBL="30-1"Table 30-1 ID="95064"Movie Library EventsLEFT="0" WIDTH="121"EventLEFT="130" WIDTH="211"DescriptionLEFT="0" WIDTH="121"MV_EVENT_FRAMELEFT="130" WIDTH="211"A frame has been played.LEFT="0" WIDTH="121"MV_EVENT_STOPLEFT="130" WIDTH="211"The movie has stopped playing.LEFT="0" WIDTH="121"MV_EVENT_ERRORLEFT="130" WIDTH="211"An error has occurred during playback.LEFT="0" WIDTH="121"MV_EVENT_SLOW_PLAYLEFT="130" WIDTH="211"The last second of playback measured by the Movie 
Library was slower than the threshold set in 
mvSetSlowThreshold().To set an event mask, call mvSetSelectEvents(). Its function prototype is:void mvSetSelectEvents ( MVeventmask eventmask )where:eventmaskis a bitwise OR of one or more of the events in IDREF="95064" TYPE="TABLE"Table 30-1For example, to receive all events, use the following:mvSetSelectEvents( MV_EVENT_MASK_FRAME | MV_EVENT_MASK_STOP |
              MV_EVENT_MASK_ERROR | MV_EVENT_MASK_SLOW_PLAY )To retrieve the current setting of the event mask, call mvGetSelectEvents(). Its function prototype is:MVeventmask mvGetSelectEvents ( void )Movie events are contained in an MVevent structure, which is a union of all of the Movie Library event structures. Each event structure contains fields that store information about the event. Some events have additional fields containing event-specific information.IDREF="26644" TYPE="TABLE"Table 30-2 lists and describes the fields in the movie events.COLUMNS="3"LBL="30-2"Table 30-2 ID="26644"Event Structure FieldsLEFT="0" WIDTH="59"TypeLEFT="65" WIDTH="34"FieldLEFT="105" WIDTH="238"DescriptionLEFT="0" WIDTH="59"MVeventtypeLEFT="65" WIDTH="34"typeLEFT="105" WIDTH="238"Type of event.LEFT="0" WIDTH="59"MVtimeLEFT="65" WIDTH="34"timeLEFT="105" WIDTH="238"X11-style millisecond timestamp indicating when the event 
happened.LEFT="0" WIDTH="59"MVidLEFT="65" WIDTH="34"idLEFT="105" WIDTH="238"Movie instance that produced the event.LEFT="0" WIDTH="59"MVframeLEFT="65" WIDTH="34"frameLEFT="105" WIDTH="238"Current frame number, ranging from zero to one less frame 
than the length of the movie's image track.LEFT="0" WIDTH="59"intLEFT="65" WIDTH="34"errcodeLEFT="105" WIDTH="238"Integer value that represents a Movie Library error code 
that can be retrieved by calling mvGetErrno(). Applies to 
MV_EVENT_ERROR events only. LEFT="0" WIDTH="59"intLEFT="65" WIDTH="34"reasonLEFT="105" WIDTH="238"Integer value that indicates what caused the slow play 
event. Applies to MV_SLOW_PLAYBACK events only and 
is currently unused.See IDREF="19718" TYPE="TITLE""Handling Movie Events" for an explanation of how to handle movie events, and see IDREF="14640" TYPE="TEXT"Example 30-9  for a code fragment that branches depending on the event type and then extracts information from the event fields.LBL="" HELPID=""ID="68133"Getting a File Descriptor for the Movie Event QueueThe Movie Library provides a file descriptor that becomes active when you can read events from the movie event queue. To get a file descriptor for the movie event queue, call mvGetEventFD(). Its function prototype is:DMstatus mvGetEventFD ( int *fdreturn )where:fdreturnis a pointer into which the Movie Library event file descriptor is returnedLBL="" HELPID=""Creating the Event LoopAfter you have set up the event mask and obtained a file descriptor for the movie event queue, you can create the event loop. You'll probably want to define separate window and movie event handling functions and call them from main(). Use one of the following methods to listen for movie and X11 events.If you are using Xlib, prepare a file descriptor set that contains both the Movie Library and X11 file descriptors using the FD_SET macro, and then code a select() loop that listens for movie events and X events, as described in IDREF="69213" TYPE="TITLE""Preparing a File Descriptor Set".If you are using Xt or IRIS IM, pass the Movie Library file descriptor to the X toolkit using XtAppAddInput(3Xt). This call associates an Xt callback function that you have written for your application with the Movie Library file descriptor. You can then process movie events inside this Xt callback, which gets called whenever one or more events appear on the Movie Library event queue.LBL="" HELPID=""ID="19718"Handling Movie EventsIf you are using Xt or IRIS IM, you'll use the Movie Library routines for handling movie events that are described in this section, but you'll get the events from the X11 queue, as described in IDREF="10876" TYPE="TITLE""Handling X Window Events", so skip ahead to IDREF="94613" TYPE="TITLE""Waiting for Movie Events" in this section to learn how to handle movie events.If you are using Xlib, you need to create a loop that uses select() or poll() to wait for Movie Library events using a file descriptor set (fd_set). The basic structure of this loop is:/* wait for libmovie events */
select(...);

/* process all libmovie events on queue before waiting again */
while ( mvPendingEvents() != 0 ) {
    mvNextEvent(&event);
    switch( event ) {
        /* ... */
    }
}LBL="" HELPID=""ID="69213"Preparing a File Descriptor SetFile descriptor sets provide a way for an application to wait for available input from several files at once. The Movie Library event queue is separate from the X11 event queue, so you must obtain the file descriptor for each queue and put them both into a file descriptor set.To get the file descriptor corresponding to the X11 event queue for a specified display, use the Xlib ConnectionNumber(3X) macro.After you have obtained file descriptors for the movie event queue and the X11 event queue, put them into a file descriptor set using the FD_SET macro.IDREF="93840" TYPE="TEXT"Example 30-8 shows how to prepare a file descriptor set.LBL="30-8"Example 30-8 ID="93840"Preparing a File Descriptor Setstatic DMstatus setupFDSet( Display *dpy, int *movieFD,
                           int *xFD, fd_set *theFDSet )
{
 
      if ( mvGetEventFD( movieFD ) != DM_SUCCESS ) {
        fprintf( stderr, "%s: Could not get movie event FD.\n",
            programName );
        return DM_FAILURE;
    }

    *xFD = ConnectionNumber( dpy );

    FD_ZERO( theFDSet );
    FD_SET( *movieFD, theFDSet );
    FD_SET( *xFD, theFDSet );

    return DM_SUCCESS;
}The structure of this code outline and the reasoning behind it are explained in the sections that follow.LBL="" HELPID=""ID="94613"Waiting for Movie EventsWhen creating a movie event loop, it is important that you process all the events on the queue to prevent it from stalling.The Movie Library provides the mvPendingEvents() call to check the queue for events, with which you can create a while loop. Its function prototype is:int mvPendingEvents ( void )You must use the while statement to set up the loop properly; don't use an if statement.When your application wakes up to handle movie events, there may be more than one event in the queue. It is critical that you process all the events in the queue before waiting for more events. As long as events are pending, your application should keep getting events from the queue until it is empty.LBL="" HELPID=""Getting Movie Events from the QueueTo extract the next event from the queue, call mvNextEvent(). Its function prototype is:void mvNextEvent ( MVevent* eventreturn )where:eventreturnis a pointer into which the next event is returnedYou can determine which event is waiting next in the queue, without popping it off the queue, by "peeking" at the next event. To peek at the next event, call mvPeekEvent(). Its function prototype is:void mvPeekEvent ( MVevent* eventreturn )where:eventreturnis a pointer into which the next event is returned without being removed from the queueLBL="" HELPID=""Examining Movie EventsYou can query the fields in the event structure to get specific information about an event, such as the event type. After you know what type of event it is, you can use the structure definitions for the specific event types to extract further information, such as when or why the event occurred.The first field of every event is a type field, which lets you query for the event type without knowing it in advance. After determining the event type, you can examine the fields for each type of event to extract specific information about the event. The typical way to code this is with a switch statement.IDREF="14640" TYPE="TEXT"Example 30-9 is an excerpt from APP="/usr/sbin/jot /usr/people/4Dgifts/examples/dmedia/movie/manymovie/manymovieEvents.c" PARMS=""manymovieEvents.c
, in /usr/people/4Dgifts/examples/dmedia/movie/manymovie, that uses a movie event loop in which a switch statement is used to examine the event type, then extracts event-specific information such as the frame number that caused the event (event.mvframe.id), from the fields in the event structure. (The print statement under MV_EVENT_FRAME is commented out in the sample program.)LBL="30-9"Example 30-9 ID="14640"Handling Movie Frame, Stop, and Error Eventsstatic DMboolean handleMovieEvents( )
{
    MVevent event;
 
    while ( mvPendingEvents() != 0 ) {
        mvNextEvent( &event );
        switch ( event.type ) {
            case MV_EVENT_FRAME:                /* a frame played */

                /* Uncomment and recompile to see which frames are played.
                 * 
                 * 
                 * printf( "%s: Played frame %d of movie %d.\n", 
                 *        getProgramName(), event.mvframe.frame, 
                 *        event.mvframe.id );
                 */

                break;

            case MV_EVENT_STOP:                 /* end of movie */
                printf( "%s: Playback of movie %d stopped.\n",
                       getProgramName(), event.mvstop.id );
                break;

            case MV_EVENT_ERROR:                /* error */
                fprintf( stderr, "%s: Error during playback: %s.\n",
                        getProgramName(), mvGetErrorStr( mvGetErrno() ) );
                return DM_FALSE;
                break;
        }
    }
    return DM_TRUE;
}LBL="" HELPID=""ID="10876"Handling X Window EventsThe method for handling the X events depends on whether you use Xlib or an X toolkit:If you are using Xt or IRIS IM, you need to write Xt callback functions for the GLXMDraw widget that get called when it is resized and when it needs repainting. The X toolkit invokes the Xt callback procedures in your application whenever one or more events appear on the X11 event queue. Call XtAppAddInput(3Xt) to add the Movie Library file descriptor to the X11 file descriptors that invoke your Xt callback function.If you are using Xlib, you need to get the file descriptor corresponding to the X11 event queue for a specified display, by calling the Xlib ConnectionNumber(3X) macro. Put this file descriptor into a file descriptor set along with the Movie Library file descriptor and wait on both of them using select(), as described in IDREF="19718" TYPE="TITLE""Handling Movie Events".The two window events of greatest concern to Movie Library programmers are expose and resize events.If a window containing a movie gets exposed, you must repaint the window; if it gets resized, you must resize and repaint the window.To resize the window identified by win on the display dpy, call mvResizeWindow(). Its function prototype is:void mvResizeWindow ( Display* dpy, Window win )To repaint a window, call mvShowCurrentFrame(). Its function prototype is:void mvShowCurrentFrame ( MVid movieid )IDREF="16052" TYPE="TEXT"Example 30-10 is an excerpt from APP="/usr/sbin/jot /usr/people/4Dgifts/examples/dmedia/movie/misc/simplemovie.c" PARMS=""simplemovie.c
, in /usr/people/4Dgifts/examples/dmedia/movie/misc, that shows how to handle window events from the X11 queue, in particular, expose and resize events. See IDREF="14632" TYPE="TITLE""Creating a Simple Keyboard Interface for Playing Movies" in Chapter 32 for more information about simplemovie.c.LBL="30-10"Example 30-10 ID="16052"Handling X11 Expose and Resize Window Eventsstatic DMboolean handleXEvents( MVid theMovie, Display *dpy, Window win )
{
    XEvent event;

    while ( XPending( dpy ) != 0 ) {

        XNextEvent( dpy, &event );

        switch ( event.type ) {
            case Expose:                     /* repaint display */
                mvShowCurrentFrame( theMovie );
                break;

            case ConfigureNotify:            /* window was resized */
                {
                    XWindowAttributes winAttrs;
                    int actual_width, actual_height;

                    mvResizeWindow( dpy, win );

                    XGetWindowAttributes( dpy, win, &winAttrs );

                    mvQueryViewSize( theMovie, winAttrs.width,
                                     winAttrs.height, DM_TRUE,
                                     &actual_width, &actual_height );
                    printf( "%s: actual width = %d, height = %d\n", 
                           programName, actual_width, actual_height );

                    mvSetViewSize( theMovie, winAttrs.width,
                                  winAttrs.height, DM_TRUE );
                }
                break;
name='hellip' font=symbol charset=fontspecific code=188
        }
    }
 return DM_TRUE;
}LBL="" HELPID=""ID="95415"Checking and Correcting for Slow PlaybackThe Movie Library checks the playback speed once per second and sends a slow play event to the movie event queue if it finds that the last second of playback was too slow. To check for slow playback, you must:set a minimum playback speed thresholdset an event mask to request MV_SLOW_PLAYBACK eventsTo check playback speed more frequently, you can choose to receive frame events and compute the difference between the two timestamps, which is the real time in milliseconds that has elapsed between the two events.Playback may be slow because:the I/O bandwidth from a hardware device is not high enoughthe filesystem is fragmentedthe system CPU cannot decompress data fast enoughthe movie is unoptimized (see mvOptimize(3mv))LBL="31"ID="12291"Using the Movie Library with QuickTime MoviesThis chapter describes how to create, edit, and play uncompressed QuickTime movies on a Silicon Graphics computer using the Movie Library. Silicon Graphics also provides a separate option, the QuickTime Compressor Library, under license from Apple Computer, Inc. for use with the Movie Library, that supports Apple Animation and Apple Video compression for QuickTime movies.NoteTo use the Apple Animation and Apple Video compression discussed in this chapter, you must purchase and install the Silicon Graphics QuickTime Compressor Library. NoteApple QuickTime software for the Macintosh that is mentioned in this chapter is available from Apple Computer, Inc. in the Apple QuickTime Starter Kit. LBL="" HELPID=""QuickTime BasicsThis section presents basic concepts for using QuickTime movies with the Movie Library.QuickTime is an Apple Macintosh system software extension that can be installed in the Macintosh to extend its capabilities so as to allow time-based (audio, video, and animation) data for multimedia applications.QuickTime movies store and play picture tracks and soundtracks independently of each other, analogous to the way the Movie Library stores separate image and audio tracks. You can't work with pictures and sound as separate entities using the QuickTime Starter Kit utilities on the Macintosh, but you can use the Silicon Graphics Movie Library to work with the individual image and audio tracks in a QuickTime movie.LBL="" HELPID=""QuickTime SoundQuickTime movie soundtracks are playable on both Macintosh and Silicon Graphics computers, but each has its own unique audio data format, so audio playback is most efficient when using the native data format and rate for the computer on which the movie is playing.When playing a QuickTime movie soundtrack on a Silicon Graphics computer, the Movie Library chooses the nearest appropriate audio sample rate during playback and file format conversion of nonnative movies, so no performance penalty is incurred for rate conversion. This may change the pitch of some sounds slightly, but usually not enough to cause an audible difference. The Movie Library supports 8-bit and 16-bit uncompressed audio in either two's complement or unsigned format. The native rates for Silicon Graphics audio hardware are 8000, 11025, 16000, 22050, 32000, 44100, and 48000 Hz.When playing a QuickTime movie soundtrack on a Macintosh, the Macintosh QuickTime software converts nonnative audio to the native Macintosh 8-bit format and performs the necessary rate conversions for playing the nonnative soundtrack; but if you use the Movie Library to write audio data to a QuickTime movie that is intended for playback only on the Macintosh, use a format and sampling rate suitable for the Macintosh. QuickTime supports the AIFF sound file format and both signed and unsigned audio, at any rate up to 65 KHz. The native format for Macintosh audio hardware is 8-bit, unsigned audio at either 11127 Hz or 22254 Hz.LBL="" HELPID=""QuickTime CompressionThe Macintosh QuickTime system software extension includes five compressors, two of which are compatible with the Movie Library:Apple None (uncompressed)Apple Photo (JPEG standard)and two of which are compatible with the Movie Library only when you purchase and install the Silicon Graphics QuickTime Compressor Library:Apple AnimationApple VideoYou can use a QuickTime movie saved with either Apple Animation or Apple Video compression with the Movie Library if you have purchased and installed the Silicon Graphics QuickTime Compressor Library; otherwise, you must save the movie without compression or use JPEG.When creating QuickTime movies on a Macintosh for use with the Movie Library, you can select the compression settings using one of the following methods:Use the Macintosh QuickTime Movie Recorder, which gives you two options for compressing a movie:accepting the default setting, "Use Simple Compression," which automatically applies Apple Video compression when you save the movieselecting an appropriate type of compression from the Compression Settings dialog box and then choosing "Compress" from the Movie menu before saving the movieUse the Macintosh QuickTime Movie Converter to change compression settings for existing moviesCompression settings and the applications for which each setting is best suited are summarized in the sections that follow.LBL="" HELPID=""Apple NoneApple None creates an uncompressed movie and can be used to change the number of colors in the images and/or the recording quality. Both the number of colors and the recording quality can affect the size of the movie.To create an uncompressed QuickTime movie on the Macintosh, click on the "Apple None" choice in the QuickTime Compression Settings dialog box.NoteBecause the Macintosh compresses QuickTime movies by default, you must set the compression to Apple None and save the movie again to create an uncompressed movie. LBL="" HELPID=""Apple PhotoApple Photo uses the JPEG standard. The Movie Library supports the JPEG standard through the Compression Library. JPEG is best suited for compressing individual still frames, because decompressing a JPEG image can be a time-consuming task, especially if the decompression is performed in software. JPEG is typically used to compress each still frame during the writing or editing process, with the intention to apply another type of compression to the final version of the movie or to leave it uncompressed. JPEG works better on high-resolution continuous-tone images, such as photographs, than on crisp-edged, high-contrast images like line drawings.LBL="" HELPID=""Apple AnimationApple Animation uses a lossy run-length encoding (RLE) method, which compresses images by storing a color and its run-length (the number of pixels of that color) every time the color changes. Apple Animation is not a true lossless RLE method because it stores colors that are close to the same value as one color. This method is most appropriate for compressing images such as line drawings that have highly contrasting color transitions and few color variations.LBL="" HELPID=""Apple VideoApple Video uses a method developed by Apple Computer whose objective is to decompress and display movie frames as fast as possible. It compresses individual frames and works better on movies recorded from a video source than on animations.NoteThe Apple Video compressor has a restriction that the image width and height be a multiple of 4. Before transferring a movie from a Macintosh to a Silicon Graphics computer, make sure that the image size is a multiple of 4. LBL="" HELPID=""QuickTime Frame Differencing (Keyframes)QuickTime provides a feature called frame differencing, which allows you to save only the changes from one movie frame to the next, rather than store each individual frame for the entire movie. When frame differencing is used, the entire contents of every nth frame are compressed and stored in a keyframe, while only the differences from frame to frame are stored for the frames between keyframes. (On the Macintosh, you enable frame differencing by clicking on the "Key frame every n frames" box in the Motion area of the Compression Settings dialog box.)Reducing the time between keyframes increases the performance when scrubbing to a particular frame (because of the increased likelihood of landing on a keyframe) but increases the size of the movie.You can open and play keyframe movies with the Movie Library. You can't use the mvPasteFrames() editing function to paste frames into the middle of a keyframe movie because the Movie Library does not support the recomputing of frame differencing. For the same reason, you cannot delete frames from a keyframe movie. To create a movie using Apple Animation or Apple Video, you must use mvInsertFrames() to add the frames in order from beginning to end.LBL="" HELPID=""Movie Library QuickTime Compatibility RequirementsQuickTime movies must be single-fork and self-contained in order to be compatible between Apple Macintosh and Silicon Graphics computers. This section describes how to make QuickTime movies that meet these requirements.LBL="" HELPID=""Making a Single-fork MovieFiles on an Apple computer are double-fork, containing both a data fork and a resource fork. Only Apple computers use the double-fork file system; the file system on Silicon Graphics computers does not use double-fork files. Macintosh computers convert a double-fork file into a single-fork file by writing the resource fork into the data fork. You must perform this conversion on the Macintosh for any movie files that you plan on using with the Movie Library before transferring them to the Silicon Graphics computer.To make a single-fork movie on the Macintosh:Click on the "Playable on non-Apple computers" box in the QuickTime Movie Converter Save dialog box (see the QuickTime Starter Kit User's Guide for details).Equivalently, if you are writing an application on a Macintosh using the Apple QuickTime developer kit, call the Movie Toolbox PutMovieIntoDataFork function in that application to write the movie into a single-fork file before using it in a Movie Library application (see Inside Macintosh: QuickTime for details).LBL="" HELPID=""Making a Self-contained MovieApple QuickTime movie utilities offer two ways of saving a QuickTime movie file on the Macintosh: Save normallyMake movie self-containedA QuickTime movie that has been saved on the Macintosh using the "Save normally" feature contains movie synchronization information, such as the location and sequence of frames in the movie, but does not itself contain the actual movie data. The movie data could have been stored in one or more separate files on the disk, on CD-ROM, or on a remote computer accessible from the network. Such a movie is said to have dependencies, because it depends on being able to locate and use other files that make up the movie. You cannot use this type of movie with the Movie Library.Only self-contained (also called "flattened") QuickTime movie files are compatible with the Movie Library. Self-contained QuickTime movies have both the synchronization information and the movie data in one file.You can use the Macintosh Movie Info window to determine whether an existing movie is self-contained. If the movie is self-contained, nothing will appear in the box under "Uses data from one other file." If anything does appear in this box, you need to convert the movie to a self-contained file.You must convert any movie files that you plan on using with the Movie Library to self-contained movies on the Macintosh before transferring them to the Silicon Graphics computer, using one of the following methods:Click on "Make movie self-contained" from the Save dialog box that appears when you select "Save" or "Save as" from the Macintosh File menu of the QuickTime Movie Player or Movie Recorder. Because the Macintosh default is "Save normally," you must specify "Make movie self-contained" when saving any QuickTime movie that you plan on using with the Movie Library. To convert an existing QuickTime movie, use the Macintosh QuickTime Movie Converter to open the movie, select "Make movie self-contained," and then save the movie.Equivalently, if you are writing a Macintosh application using the Apple QuickTime developer kit, call the Movie Toolbox FlattenMovieData function in that application to make the movie file self-contained before using it in a Movie Library application.LBL="" HELPID=""Transferring Files Between Macintosh and Silicon Graphics ComputersYou can transfer QuickTime movies between an Apple Macintosh computer and a Silicon Graphics computer by using a floppy disk or by using a network file transfer application.LBL="" HELPID=""Transferring Files from Floppy DiskIf you have an Indigo or an Indy with a floptical drive, you can read files from a floppy disk that was formatted on a Macintosh computer.LBL="" HELPID=""Transferring Files Over a NetworkTo transfer QuickTime movies between an Apple Macintosh computer and a Silicon Graphics computer over a network, use either ftp or the Apple Computer AppleShare network software. When using ftp, specify "binary" to ensure complete data transfer.LBL="" HELPID=""Adding QuickTime Capability to Your Movie Library ApplicationOnce you know how to program a Movie Library application, adding QuickTime capability is as easy as adding two lines to your code. Only three steps are needed to create an application that can use QuickTime movies (as long as they meet the compatibility criteria):including qt.h, the QuickTime header filecalling mvInitQuickTime() to initialize the Movie Library for QuickTime moviesThis step also installs the QuickTime codecs in the Movie Library if you have purchased the QuickTime Compressor Library.linking with the QuickTime Compressor Library by putting -lqt before -lmovie on the link lineQuickTime versions of many of the Movie Library sample programs are provided in the createmovieqt and miscqt directories under /usr/people/4Dgifts/examples/dmedia/movie.LBL="" HELPID=""Using the QuickTime Compressor LibraryIf you have purchased and installed the Silicon Graphics QuickTime Compressor Library, codecs for Apple Video and Apple Animation are installed into the Movie Library and are available for your application to use on either QuickTime or Silicon Graphics movies.LBL="" HELPID=""Creating a QuickTime MovieYou can use the Movie Library to create a QuickTime movie that is playable on both Silicon Graphics and Apple computers. Use the same steps as creating a Silicon Graphics movie, but specify QuickTime format, as in the createmovieqt.c sample program.IDREF="19705" TYPE="TEXT"Example 31-1 highlights the changes that were made to the createmovie sample program to turn it into createmovieqt, which is an identical program that offers QuickTime compatibility. These excerpts are from createmovieqtArgs.c++ in the /usr/people/4Dgifts/examples/dmedia/movie/createmovieqt directory.LBL="31-1"Example 31-1 ID="19705"Creating QuickTime Movies with the Movie Library#include "createmovieArgs.h"
#include <movie.h>
#include <qt.h>
#include <audiofile.h>
#include <getopt.h>
#include <string.h>
#include <assert.h>
#include <il/ilImage.h>
#include <il/ilGenericImgFile.h>


typedef enum _compScheme
{
    unknownComp,
    none,
    mvc1,
    mvc2,
    rle,
    jpeg,
    rgb8,
    qtvideo,
    qtanim
} compScheme;

name='hellip' font=symbol charset=fontspecific code=188
static compScheme   compressionScheme = qtvideo;
name='hellip' font=symbol charset=fontspecific code=188
static MVfileformat movieFormat       = MV_FORMAT_QT;
name='hellip' font=symbol charset=fontspecific code=188
static void      setMovieFormat( char *formatArg );
name='hellip' font=symbol charset=fontspecific code=188

static DMboolean badCompressionScheme( void );

name='hellip' font=symbol charset=fontspecific code=188
while(( ch = getopt( argc, argv, "f:c:l:r:s:p:o:" ) ) != -1) {
        switch ( ch ) {
            case 'f':
                setMovieFormat( optarg );
                break;
name='hellip' font=symbol charset=fontspecific code=188

    if ( badCompressionScheme() ) {
        fprintf( stderr, "%s: Compression %s"
                 "unavailable for QuickTime.\n",
                  programName, getCompressionScheme() );
        exit( EXIT_FAILURE );
    }
name='hellip' font=symbol charset=fontspecific code=188

    mvInitQuickTime();

name='hellip' font=symbol charset=fontspecific code=188

static void usage( void )
{
    name='hellip' font=symbol charset=fontspecific code=188
    fprintf( stderr, "[-f format] [-s xsize,ysize]"
                    "[-o outMovie] \n" );
    fprintf( stderr, "[-p paramType,userParam,userParamVal]"
                    " file name='hellip' font=symbol charset=fontspecific code=188\n" );
    fprintf( stderr, "\n");
    fprintf( stderr, "\"compression\" = none, mvc1, mvc2,"
            " rle, jpeg, " );
    fprintf( stderr, "8rgb, qtvideo, or qtanim.\n" );
    fprintf( stderr, "Default compression scheme qtvideo.\n" );
    fprintf( stderr, "\"format\" = sgi or qt,"
             " the format of the new movie.");
    fprintf( stderr, "The default is qt.\n" );
    name='hellip' font=symbol charset=fontspecific code=188
}

name='hellip' font=symbol charset=fontspecific code=188
static void setMovieFormat( char *formatArg )
{
    if ( ( strcmp( formatArg, "sgi" ) == 0 ) || 
        ( strcmp( formatArg, "SGI" ) == 0 ) ) {
        movieFormat = MV_FORMAT_SGI_3;
    }
    else if ( ( strcmp( formatArg, "qt" ) == 0 ) ||
             ( strcmp( formatArg, "QT" ) == 0 ) ) {
        movieFormat = MV_FORMAT_QT;
    }
    else {
        fprintf( stderr, "%s: Unknown movie format %s.\n",
                programName, formatArg );
        usage();
    }
}
name='hellip' font=symbol charset=fontspecific code=188

static DMboolean badCompressionScheme( void )
{
    if ( ( getMovieFormat() == MV_FORMAT_QT ) &&
        ( ( compressionScheme != none ) && 
         ( compressionScheme != jpeg ) && 
         ( compressionScheme != qtanim ) && 
         ( compressionScheme != qtvideo ) ) ) {
        return( DM_TRUE );
    }
    return( DM_FALSE );
}
name='hellip' font=symbol charset=fontspecific code=188

static void setCompressionScheme( char *compressArg )
{
    if ( strcmp( compressArg, "none" ) == 0 ) {
        compressionScheme = none;
    }
    else if ( strcmp( compressArg, "mvc1" ) == 0 ) {
        compressionScheme = mvc1;
    }
    else if ( strcmp( compressArg, "mvc2" ) == 0 ) {
        compressionScheme = mvc2;
    }
    else if ( strcmp( compressArg, "rle" ) == 0 ) {
        compressionScheme = rle;
    }
    else if ( strcmp( compressArg, "jpeg" ) == 0 ) {
        compressionScheme = jpeg;
    }
    else if ( strcmp( compressArg, "8rgb" ) == 0 ) {
        compressionScheme = rgb8;
    }
    else if ( strcmp( compressArg, "qtvideo" ) == 0 ) {
        compressionScheme = qtvideo;
    }
    else if ( strcmp( compressArg, "qtanim" ) == 0 ) {
        compressionScheme = qtanim;
    }
    else {
        fprintf( stderr, "%s: Unknown compress scheme %s.\n",
                programName, compressArg );
        usage();
    }
}
name='hellip' font=symbol charset=fontspecific code=188

schar *getCompressionScheme( void )
{
    switch( compressionScheme ) {
        case none:
            return( DM_IMAGE_UNCOMPRESSED );
        case mvc1:
            return( DM_IMAGE_MVC1 );
        case mvc2:
            return( DM_IMAGE_MVC2 );
        case jpeg:
            return( DM_IMAGE_JPEG );
        case rle:
            return( DM_IMAGE_RLE );
        case rgb8:
            return( DM_IMAGE_UNCOMPRESSED );
        case qtvideo:
            return( DM_IMAGE_QT_VIDEO );
        case qtanim:
            return( DM_IMAGE_QT_ANIM );
        case unknownComp:
            assert( DM_FALSE );
            break;
    }
}LBL="" HELPID=""Reading Existing QuickTime MoviesThe Movie Library can read uncompressed QuickTime movies in 16- and 32-bit depths, which correspond to the "Thousands" and "Millions" of colors choices in the QuickTime compression settings on the Macintosh.To open an uncompressed QuickTime movie, call mvOpenFile(). To read the data, call mvReadFrames(), which puts the image data into your buffer. LBL="" HELPID=""Converting QuickTime Image Data to Silicon Graphics Image FormatIf you're working with an uncompressed QuickTime movie, you need to convert the QuickTime picture data to a Silicon Graphics image data format such as RGBX.You can then invert it and transform it into RGBX, as shown in IDREF="16780" TYPE="TEXT"Example 31-2.IDREF="16780" TYPE="TEXT"Example 31-2 contains a listing of createmovieConvert.c, which is located in the /usr/people/4Dgifts/examples/dmedia/movie/createmovieqt directory.LBL="31-2"Example 31-2 ID="16780"Converting QuickTime Picture Data to RGBX Format/*****************************************************************************
 *
 * File:        createmovieConvert.c
 *
 * Description: Part of createmovie. Used only in conjunction with the
 *              SGI QuickTime Library. Code for converting between
 *              QuickTime and SGI rgb image data.
 *
 *****************************************************************************/

#include <sys/types.h>
#include "createmovieConvert.h"

/*
 * Forward declarations for functions local to this module.
 */

static void InvertImage32( void* buffer, int width, int height );
static void InvertImage16( void* buffer, int width, int height );


/********
 *
 * Apple16ToRGBX
 *
 * 16-bit images are stored with 5 bits each of red, green, and blue.
 * The bit layout is: 
 *
 *             XRRRRRGG GGGBBBBB
 *
 * The bit layout for SGI 32-bit RGBX images is: 
 *
 *             XXXXXXXX BBBBBBBB GGGGGGGG RRRRRRRR
 *
 * Apple stores images from top to bottom, while SGI goes from bottom to top.
 *
 ********/
void Apple16ToRGBX( int width, int height, void* from, void* to)
{
    static unsigned char Apple16Table[32];

    unsigned short* src  = ( unsigned short* ) from;
    __uint32_t*     dst  = ( __uint32_t* )     to;
    size_t          size = ( ( size_t ) width ) * ( ( size_t ) height );
    size_t          i;

    for ( i = 0;  i < 32;  i++ ) {
        Apple16Table[i] = ( ( i << 3 ) | ( i >> 2 ) );
    }

    for ( i = 0;  i < size;  i++ ) {
        unsigned short bits = src[i];
        unsigned char  red   = Apple16Table[( ( 0x1F << 10 ) & bits ) >> 10];
        unsigned char  green = Apple16Table[( ( 0x1F <<  5 ) & bits ) >>  5];
        unsigned char  blue  = Apple16Table[( ( 0x1F <<  0 ) & bits ) >>  0];
        dst[i] = ( blue << 16 ) | ( green << 8 ) | ( red << 0 );
    }
    
    InvertImage32( to, width, height );
}

/********
 * RGBXToApple16
 ********/

void RGBXToApple16( int width, int height, void* from, void* to)
{
    __uint32_t*     src  = ( __uint32_t* )     from;
    unsigned short* dst  = ( unsigned short* ) to;
    size_t          size = ( ( size_t ) width ) * ( ( size_t ) height );
    size_t          i;
    
    for ( i = 0;  i < size;  i++ ) {
        __uint32_t bits      = src[i];
        unsigned char  red   = ( ( 0x1F <<  3 ) & bits ) >>  3;
        unsigned char  green = ( ( 0x1F << 11 ) & bits ) >> 11;
        unsigned char  blue  = ( ( 0x1F << 19 ) & bits ) >> 19;
        dst[i] = ( blue << 0 ) | ( green << 5 ) | ( red << 10 );
    }
    
    InvertImage16( to, width, height );
}
/********
 *
 * Apple32ToRGBX
 *
 * 32-bit images are stored with 8 bits each of red, green, and blue.
 * The bit layout is: 
 *
 *         XXXXXXXX RRRRRRRR GGGGGGGG BBBBBBBB
 *
 * The bit layout for SGI 32-bit RGBX images is: 
 *
 *         XXXXXXXX BBBBBBBB GGGGGGGG RRRRRRRR
 *
 * Apple stores images from top to bottom, while SGI goes from bottom to top.
 *
 ********/

void Apple32ToRGBX( int width, int height, void* from, void* to)
{
    __uint32_t*     src  = ( __uint32_t* ) from;
    __uint32_t*     dst  = ( __uint32_t* ) to;
    size_t          size = ( ( size_t ) width ) * ( ( size_t ) height );
    size_t          i;

    for ( i = 0;  i < size;  i++ ) {
        __uint32_t bits = src[i];
        dst[i] = ( ( bits & ( 0xFF << 16 ) ) >> 16 ) |
                 ( ( bits & ( 0xFF <<  8 ) ) >>  0 ) |
                 ( ( bits & ( 0xFF <<  0 ) ) << 16 );
    }
  
    InvertImage32( to, width, height );
}
/********
 * RGBXToApple32
 * Apple packs the colors in a different order than SGI does:
 ********/

void RGBXToApple32( int width, int height, void* from, void* to)
{
    __uint32_t*     src  = ( __uint32_t* ) from;
    __uint32_t*     dst  = ( __uint32_t* ) to;
    size_t          size = ( ( size_t ) width ) * ( ( size_t ) height );
    size_t          i;

    for ( i = 0;  i < size;  i++ ) {
        __uint32_t bits = src[i];
        dst[i] = ( ( bits & ( 0xFF << 16 ) ) >> 16 ) |
                 ( ( bits & ( 0xFF <<  8 ) ) >>  0 ) |
                 ( ( bits & ( 0xFF <<  0 ) ) << 16 );
    }
  
    InvertImage32( to, width, height );
}

/********
 * InvertImage32 
 * Inverts a 32-bit image.
 ********/

static void InvertImage32( void* buffer, int width, int height )
{
    __uint32_t* buff = ( __uint32_t* ) buffer;
    
    int x;
    int y1;
    for ( x = 0;  x < width;  x++ ) {
        for ( y1 = 0;  y1 < height/2; y1++ ) {
            int y2 = height - y1 - 1;
            int index1 = x + y1 * width;
            int index2 = x + y2 * width;
            
            __uint32_t t = buff[index1];
            buff[index1] = buff[index2];
            buff[index2] = t;
        }
    }
}
/********
 * InvertImage16
 * Inverts a 16-bit image.
 ********/

static void InvertImage16( void* buffer, int width, int height )
{
    unsigned short* buff = ( unsigned short* ) buffer;
    
    int x;
    int y1;
    for ( x = 0;  x < width;  x++ ) {
        for ( y1 = 0;  y1 < height/2; y1++ ) {
            int y2 = height - y1 - 1;
            int index1 = x + y1 * width;
            int index2 = x + y2 * width;
            
            unsigned short t = buff[index1];
            buff[index1]     = buff[index2];
            buff[index2]     = t;
        }
    }
}
LBL="32"ID="98659"Using the Movie Library Sample ProgramsA comprehensive set of sample programs is provided with the Movie Library. You can use sample programs to learn about the Movie Library or as skeleton code for building your own application. This chapter describes the Movie Library sample programs.LBL="" HELPID=""About the Sample ProgramsSample programs demonstrating how to use the Movie Library for creating, editing, and playing movies and displaying their parameters are included in the /usr/people/4Dgifts/examples/libmovie directory. Sample movies, somersault.mv and sampleQT.mv are also provided.The sample programs are organized into directories according to purpose; most programs reside in a directory having the same name:commoncontains source code to a helper function called glxhelper, which creates an X window suitable for GL drawingname='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'it uses no libmovie functions but is used by the simplemovie, manymovie, and moviescreen sample programscreatemovieis a movie-making application with a command-line interfacecreatemovieqtis a movie-making application with a command-line interface for making QuickTime movieseditmovieis a movie-editing application with a command-line interface for editing QuickTime moviesmanymovieis a movie-playing application that allows several movies to play at the same timemisccontains four programs:aud-to-movie, which adds (or replaces) a movie audio trackimg-to-movie, which converts a sequenced image file into a moviemvinfo, which displays the parameters of a movie filesimplemovie, which provides a keyboard interface for playing a moviemiscqtcontains two programs:mvinfoqt, which displays the parameters of a movie filesimplemovieqt, which provides a keyboard interface for playing a QuickTime movieThe Movie Library sample programs have been put through an extensive code review and testing process by the engineers who created the Movie Library. The sample programs demonstrate how to write solid code with the Movie Library and provide many basic movie application features that you may want to incorporate into your own code.Some of the guidelines followed in creating these sample programs:global functions are put in header files to increase modularityforward declarations are provided for local functionsvariables are lowercasesymbols are uppercasefunctions begin with lowercase and have uppercase letters at word breaksextensive comments appear both at the beginning of each program or module and in the body of the codeIDREF="94412" TYPE="GRAPHIC"Figure 32-1 shows the format of the introductory comments that appear in all the Movie Library sample programs. Introductory comments document the name of the file, its usage, a brief description of what the program does, and a complete listing of the functions used in the program.FILE="32-1.online.bw" POSITION="INLINE" SCALE="FALSE"LBL="32-1"Figure 32-1 ID="94412"Comments in Movie Library Sample Programs: createmovie.c++IDREF="82940" TYPE="GRAPHIC"Figure 32-2 shows the body of createmovie.c++, showing the modularity of this programname='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'main() contains only 5 lines that call other modules.FILE="32-2.online.bw" POSITION="INLINE" SCALE="FALSE"LBL="32-2"Figure 32-2 ID="82940"Modularity of Movie Library Sample Programs: createmovie.c++LBL="" HELPID=""ID="52131"Creating MoviesThis section describes two sample programs that demonstrate how to use the Movie Library to create movies:createmoviewhich creates a movie from any combination of image files, audio files, or other movie files, letting you specify the compression scheme, loop mode, frame rate and size, and the output movie filename from the command lineimg-to-moviewhich creates a movie file from an image filecreatemovie [-ccompression] [-lloopMode] [-rframeRate] [-sxsize,ysize]                           [-pparamtype, userParam, userParamVal] [-ooutMovie] file[name='hellip' font=symbol charset=fontspecific code=188]where:-c compressionspecifies the image compression scheme, where you enter one of the following for compression:COLUMNS="2"LEFT="0" WIDTH="71"jpegLEFT="80" WIDTH="178"DM_IMAGE_JPEGLEFT="0" WIDTH="71"mvc1 (default)LEFT="80" WIDTH="178"DM_IMAGE_MVC1LEFT="0" WIDTH="71"mvc2LEFT="80" WIDTH="178"DM_IMAGE_MVC2LEFT="0" WIDTH="71"noneLEFT="80" WIDTH="178"DM_IMAGE_UNCOMPRESSEDLEFT="0" WIDTH="71"rleLEFT="80" WIDTH="178"DM_IMAGE_RLELEFT="0" WIDTH="71"8rgbLEFT="80" WIDTH="178"DM_IMAGE_UNCOMPRESSED-l loopModespecifies the loop mode, where you enter one of the following for loopMode: once, loop, or swing-r frameRatespecifies the frame rate in frames per secondIf one or more movie files are included in the file source material, outmovie will have the same frame rate as the first movie file on the command line; otherwise, if the frame rate is not set explicitly, the default is 15.0 frames per second.-s xsize,ysizespecifies the frame size, where:xsize is the horizontal dimensionysize is the vertical dimensionThe default frame size is that of the first file on the command line containing frame size information, which might be either an image or a movie file.You can set the frame size explicitly, which enlarges or reduces the images as required. Aspect ratios are preserved when scaling images, so that the resulting images might be letter-boxed if the aspect ratio of the source image is different from the aspect ratio specified by the given xsize and ysize.-p paramtype, userParam, userParamValadds a user-defined parameter of type paramtype, named userParam, whose value is userParamVal-o outmoviespecifies the name of the output moviefileincludes one or more image, audio, or movie filesYou can also use createmovie to convert a movie file from one compression scheme to another, by creating a new movie in the specified compression scheme from the old movie.createmovie contains the following files:COLUMNS="2"LEFT="0" WIDTH="117"createmovieArgs.c++LEFT="125" WIDTH="234"contains command line processing functions, access 
to user preferences, and creation and access to the 
filenames entered on the command lineLEFT="0" WIDTH="117"createmovieArgs.hLEFT="125" WIDTH="234"is the external interface to createmovieArgs.cLEFT="0" WIDTH="117"createmovieConvert.cLEFT="125" WIDTH="234"is used only in conjunction with the Silicon Graphics 
QuickTime Compressor Library to convert between 
QuickTime and rgb image dataLEFT="0" WIDTH="117"createmovieConvert.hLEFT="125" WIDTH="234"is the external interface to createmovieqtConvert.c++LEFT="0" WIDTH="117"createmovieFiles.c++LEFT="125" WIDTH="234"puts files into a new movieLEFT="0" WIDTH="117"createmovieFiles.hLEFT="125" WIDTH="234"has external declarations for createmovieFiles.c++LEFT="0" WIDTH="117"createmovieInit.c++LEFT="125" WIDTH="234"initializes a new movie fileLEFT="0" WIDTH="117"createmovieInit.hLEFT="125" WIDTH="234"has external declarations for createmovieInit.c++LEFT="0" WIDTH="117"createmovieResize.c++LEFT="125" WIDTH="234"performs filtering operations on image framesLEFT="0" WIDTH="117"createmovieResize.hLEFT="125" WIDTH="234"has external declarations for createmovieResize.c++IDREF="99405" TYPE="GRAPHIC"Figure 32-3 shows a call graph for createmovie. The call graph shows the overall structure of the program, indicating which modules call which functions. Some calls are omitted for the sake of clarity.FILE="Media6-5EX.ras3" POSITION="INLINE" SCALE="FALSE"LBL="32-3"Figure 32-3 ID="99405"Call Graph for createmovieLBL="" HELPID=""Creating a Movie from a Sequence of Images The img-to-movie.c program in the /usr/people/4Dgifts/examples/libmovie/misc directory creates a movie file from an image file. The image file can have a single image, in which case the movie file has only a single frame, or a sequence of images. The resulting movie file has a frame rate of 15.0 frames per second and uses MVC1 compression.To run the program, enter:img-to-movieimagefilenewmoviefilewhere:imagefileis the name of the image file you want to make into a movienewmoviefileis the name of the new movieLBL="" HELPID=""Adding or Replacing a Movie Audio TrackThe aud-to-movie.c program in the /usr/people/4Dgifts/examples/libmovie/misc directory adds an audio track to a silent movie or replaces the movie's existing audio track. The audio track can be an AIFF file or any audio file that is readable by libaudiofile.To run the program, enter:aud-to-movieaudiofilemoviefilewhere:audiofileis the name of the audio track you want to put in the moviemoviefileis the name of the movieLBL="" HELPID=""Editing MoviesThe /usr/people/4Dgifts/examples/libmovie/editmovie directory contains editmovie.c, a simple command line program for editing movies, and its associated files. The editing operations available are insert, delete, and paste. Only one editing operation can be selected at a time.To run the program, enter:editmovie-eeditMovie,trackType,firstEditFrame,numFrames[-d] [-ssourceMovie,firstSrcFrame [-i] [-p] [-m] [-ooutMovie]where:-eis followed by four arguments:COLUMNS="2"LEFT="0" WIDTH="72"editMovieLEFT="80" WIDTH="189"the name of the movie to be editedLEFT="0" WIDTH="72"trackTypeLEFT="80" WIDTH="189"the type of track to edit, either image or 
audioLEFT="0" WIDTH="72"firstEditFrameLEFT="80" WIDTH="189"the first frame to be editedLEFT="0" WIDTH="72"numFramesLEFT="80" WIDTH="189"the number of frames to be edited-ddeletes frames from editMovie-sspecifies both the source movie and first frame to copy-iinserts frames from sourceMovie into editMovie-ppastes frames from sourceMovie into editMovie-mperforms editing on a memory-resident copy of the movie-ooptimizes the edited movie for playback and places it in outMovieeditmovie has the following requirements: numFrames must be > 0firstSrcFrame must be name='ge' font=symbol charset=fontspecific code=179 
	TeX='\geq ' descr='[ge]' 0when inserting (-i) or pasting (-p), the -s option must be supplied in order to specify the movie to copy from and the first frame to copywhen inserting (-i) or pasting (-p), the sum of numFrames and firstSrcFrame must not exceed the last frame number in sourceMovie.when deleting (-d), the sum of numFrames and firstEditFrame must not exceed the last frame number in editMovieeditmovie contains the following files:editmovie.c a simple command line movie editoreditmovieArgs.c a command-line-parsing module for editmovieeditmovieArgs.h declarations for external functions in editmovieArgs.ceditmovieEdit.ccontains editTheMovie(), which is the only external function, and its supporting functions editmovieEdit.hexternal interface to editmovieEdit.c, which actually performs the editing for editmovieIDREF="29308" TYPE="GRAPHIC"Figure 32-4 shows the call graph for editmovie.cFILE="editfinal.bw" POSITION="INLINE" SCALE="FALSE"LBL="32-4"Figure 32-4 ID="29308"Call Graph for editmovie.c LBL="" HELPID=""ID="30927"Displaying Movie Parametersmvinfo displays information about a movie, including its image track and audio track (if present) parameters. To run the program, enter:mvinfomoviefileLBL="" HELPID=""Playing MoviesThis section describes three sample programs that play movies:simplemovieimplements a keyboard interface for playing moviesmanymovieplays up to four movies simultaneouslymoviescreenimplements a movie-based screen-saver applicationLBL="" HELPID=""ID="14632"Creating a Simple Keyboard Interface for Playing Moviessimplemovie has a keyboard interface for playing a movie. To run the program, enter:simplemovie moviefileThe keyboard commands are: <3>loop 3 times<b> or <B>play backward<e> or <E>play every frame<f> or <F>play fast<h> or <H>play slow<l> or <L>toggle loop state<m> or <M>toggle audio muting<p> or <P>play the movie <q> or <Q>quit simplemovie<r> or <R>rewind the movie<s> or <S>stop playbackLBL="" HELPID=""ID="61924"Playing Multiple Movies manymovie is a command line program for playing up to 4 movies simultaneously. The movies can have different frame sizes.NoteThe 4 movie limitation is not related to the Movie Library; rather, it is the result of the very simple layout scheme used in manymovie for placing all movies in one window.   To run the program, enter:manymovie [-one] moviefile1 [moviefile2name='hellip' font=symbol charset=fontspecific code=188]By default, each of the movies appears in a separate window. When each movie is played in a separate window, the keyboard commands apply only to the current window, except for the quit command, which always exits from manymovie.To play all movies in a single window, use the -one command line option. If the -one option is used, all keyboard commands apply simultaneously to all movies.manymovie uses the following keyboard interface:<l> or <L>changes the looping state<m> or <M>toggles audio muting<p> or <P>plays the movie<q> or <Q>quits manymovie<r> or <R>rewinds to the beginning of the movie<s> or <S>stops the moviemanymovie contains the following files:COLUMNS="2"LEFT="0" WIDTH="93"manymovie.cLEFT="100" WIDTH="239"is the main program, which plays several movies at 
once.LEFT="0" WIDTH="93"manymovieArgs.cLEFT="100" WIDTH="239"contains the code for processing command-line 
arguments, creating and accessing the movie list, and 
recording the number of movieLEFT="0" WIDTH="93"manymovieArgs.hLEFT="100" WIDTH="239"contains the external interface to the command-line 
argument processing codeLEFT="0" WIDTH="93"manymovieEvents.cLEFT="100" WIDTH="239"contains X and Movie event handling codeLEFT="0" WIDTH="93"manymovieEvents.hLEFT="100" WIDTH="239"is the external interface to manymovieEvents.c and 
handles X and Movie events for manymovieLEFT="0" WIDTH="93"manymovieWin.cLEFT="100" WIDTH="239"contains code for creating X windows suitable for 
playing moviesLEFT="0" WIDTH="93"manymovieWin.hLEFT="100" WIDTH="239"is the external interface to manymovieWin.c, creates 
X windows suitable for GL rendering for all the 
movies, opens the movies and the windows, provides 
access to the X DisplayLBL="" HELPID=""ID="87810"Creating a Movie Screensaver Applicationmoviescreen is a screensaver application that plays movies. moviescreen does not save screens by itself; rather, it is designed to run under haven(1), a wrapper for IRIS GL-based screensavers, which is available on Silicon Graphics computers. Normally, the screen turns black and the movie begins playing silently, slowly drifting around the screen as it plays.To run the program as a screensaver under haven(1), enter:haven [-n |-o] [< moviescreen [-f] [-s] [-vvolume] [-zzoom] [-lloopmode] moviefile1 [moviefile2name='hellip' font=symbol charset=fontspecific code=188]>|-k] where:-fenables fullscreen playback-sturns on sound for the movie (by default, screensaver movies play silently)-v volumesets the playback volume if the -s option is usedEnter a value from 0 to 255 for volume.-z zoomzooms the movie larger for playbackEnter an integer value for zoom. Zooming takes effect only if the -f option, fullscreen playback, is not used.-l loopmodesets the loop mode.To loop continuously (default), set loopmode to 0. To swing, set it to 1. moviescreen contains the following files:COLUMNS="2"LEFT="0" WIDTH="102"moviescreen.cLEFT="110" WIDTH="230"is the main program, which uses a movie as a 
screensaverLEFT="0" WIDTH="102"moviescreenArgs.cLEFT="110" WIDTH="230"contains code for helper functions used by 
moviescreen for accessing/manipulating data 
entered by the user via command-line arguments; 
maintains this information as static variables that 
are restricted to this moduleLEFT="0" WIDTH="102"moviescreenArgs.hLEFT="110" WIDTH="230"contains functions for processing the command-
line arguments and accessing variables set 
therefrom, including movie namesLEFT="0" WIDTH="102"moviescreenEvents.cLEFT="110" WIDTH="230"contains code to start, perform, and end screen 
saving, depending on reception of XEventsLEFT="0" WIDTH="102"moviescreenEvents.hLEFT="110" WIDTH="230"contains code to wait for an XEvent and initiate or 
terminate screen saving as appropriateLEFT="0" WIDTH="102"moviescreenGl.cLEFT="110" WIDTH="230"contains code used by moviescreen for determining 
the movie window position and erasing the movie 
window as it moves via GL drawingLEFT="0" WIDTH="102"moviescreenGl.hLEFT="110" WIDTH="230"contains functions for controlling positioning and 
erasing (via GL drawing) of the movie windowLEFT="0" WIDTH="102"moviescreenWin.cLEFT="110" WIDTH="230"contains code for creating and accessing an X 
window suitable for GL rendering that is used by 
moviescreenLEFT="0" WIDTH="102"moviescreenWin.hLEFT="110" WIDTH="230"contains functions to create and access a mixed-
model GL windowIDREF="71327" TYPE="GRAPHIC"Figure 32-5 shows the call graph for moviescreen.cFILE="screensave.bw" POSITION="INLINE" SCALE="FALSE"LBL="32-5"Figure 32-5 ID="71327"Call Graph for moviescreen.c LBL="" HELPID=""ID="75968"ID="94736"Using the SMPTE Time Code Sample ApplicationIncluded with the Movie Library sample code is a group of utility routines in a sample application for handling SMPTE time codes. These routines work with the movie frame that is closest to the given time code rather than providing exact time code precision. Time code type is indicated by the timetype argument, which supports the four basic types of time codes listed in IDREF="37670" TYPE="TABLE"Table 32-1.COLUMNS="2"LBL="32-1"Table 32-1 ID="37670"SMPTE Time Code TypesLEFT="0" WIDTH="108"Time Code TypeLEFT="115" WIDTH="234"MeaningLEFT="0" WIDTH="108"MV_TIME_SMPTE_24LEFT="115" WIDTH="234"24 frames/second (motion pictures)LEFT="0" WIDTH="108"MV_TIME_SMPTE_25LEFT="115" WIDTH="234"25 frames/second (PAL video)LEFT="0" WIDTH="108"MV_TIME_SMPTE_30LEFT="115" WIDTH="234"30 frames/second (NTSC)LEFT="0" WIDTH="108"MV_TIME_SMPTE_D30LEFT="115" WIDTH="234"30 frame/second drop (NTSC 29.97)NoteCurrently, only MV_TIME_SMPTE_30 is supported. LBL="" HELPID=""Converting a SMPTE Time Code String to a Frame NumberTo access the frame closest to a given SMPTE code, pass the SMPTE code as a string to mvStringToFrame(), which converts the given string to a frame number in the specified movie. Its function prototype is:extern DMstatus mvStringToFrame ( MVid        movieid,
                                  char        *timestring,
                                  MVtimetype  timetype,
                                  MVframe     *framereturn )where:timestringis the SMPTE time code stringtimetypeis the SMPTE code typeframereturnis a pointer to the frame number that is to be returnedSpecify the time code string in the format "HH:MM:SS:FF" where HH signifies two digits that represent hours, and similarly, MM minutes, SS seconds, and FF the frame count for the current second. The converter assumes that incomplete time strings are specified from the least-significant unit up; that is, a value of "02" means SMPTE frame code 02 of the current second. "03:02" means second 03, frame 02 of the current minute, and so on. The frame number is between 0 and one less than the total number of frames in the movie.LBL="" HELPID=""Converting a Frame Number to a SMPTE Time Code StringTo obtain a SMTPE-style time code string for a given frame number, call mvFrameToString() with the frame number for which you need the SMPTE code. Its function prototype is:extern DMstatus mvFrameToString ( MVid        movieid,
                                  MVframe     frame,
                                  MVtimetype  timetype,
                                  char        *timereturn )where:frameis the frame number for which you want to get the time codetimetypeis the SMPTE code typetimereturnis a pointer into which the time code string is returnedThe SMPTE code is returned as a string that lists the hour, minute, second, and frame count for the given frame number.LBL="" HELPID=""Converting a Time Specification to a Frame NumberTo convert a fully formed time code to the nearest corresponding frame in the specified movie, call mvTimeToFrame(). Its function prototype is:extern DMstatus mvTimeToFrame ( MVid        movieid,
                                int         hour,
                                int         minute,
                                int         second,
                                MVframe     framecnt,
                                MVtimetype  timetype,
                                MVframe     *framereturn)where:houris the hour field of the time codeminuteis the minute field of the time codesecondis the second field of the time codeframecntis the frame field of the time codetimetypeis a time type from IDREF="37670" TYPE="TABLE"Table 32-1framereturnis a pointer into which the frame number is returnedLBL="" HELPID=""Converting a Frame Number to a Time CodeTo obtain a time code corresponding to a particular frame, call mvFrameToTime() with the index of the frame for which you want the time code. Its function prototype is:DMstatus mvFrameToTime    ( MVid        movieid,
                            MVframe     frame,
                            MVtimetype  timetype,
                            int         *hourreturn,
                            int         *minutereturn,
                            int         *secondreturn,
                            MVframe     *framecntreturn)where:frameis the frame number for which you want to get the time codetimetypeis a time type from IDREF="37670" TYPE="TABLE"Table 32-1hourreturnis a pointer into which the time code hour is returnedminutereturnis a pointer into which the time code minutes are returnedsecondreturnis a pointer into which the time code seconds are returnedframecntreturnis a pointer into which the time code frame is returnedLBL="A"ID="91848"Audio SpecificationsThis appendix describes the audio hardware specifications for the Indigo and Indigo2 workstations. It also contains some video specifications for IndigoVideo.ID="MediaA-1A1"LBL="" HELPID=""Indigo Workstation Audio Hardware SpecificationsThis section contains the specifications for the Indigo audio hardware.Unless otherwise stated, all audio hardware parameters are measured under the following conditions:analog input signal levels are +7 dB re: 1Vrmsdigital input signal levels are 100% of full scale48 kHz sample rateinput source impedances are 600 name='OHgr' font=symbol charset=fontspecific code=87
	TeX='\Omega '   descr='[OHgr]'output destination impedances are 5 kname='OHgr' font=symbol charset=fontspecific code=87
	TeX='\Omega '   descr='[OHgr]'measurement bandwidth is 10 to 30 kHz unweightedAll measurement results are typical. All connectors are single-ended 3.5-mm stereo phono plugs.LBL="" HELPID=""Indigo Analog Audio I/OThe following specifications describe the analog audio I/O.LBL="" HELPID=""Analog Stereo Line-level InputsImpedance: 5 kname='OHgr' font=symbol charset=fontspecific code=87
	TeX='\Omega '   descr='[OHgr]' nominalAmplitude at full scale: 1 Vpp to 10 VppLevel control: two independent, digitally-controlled analog attenuators Frequency response: 20 Hz­20 kHz name='plusmn' font=symbol charset=fontspecific code=177 
	TeX='\pm ' descr='[plusmn]'0.25 dB Total Harmonic Distortion + Noise (THD+N): < 0.003% @ 1 kHz, < 0.005% 20 Hz­20 kHz Residual noise: -90 dB unweighted, -93 dB A-weighted (re: full scale)Interchannel isolation: -76 dB @ 1 kHz, -68 dB @ 10 kHz,-77 dB @ 20 kHzAnalog-to-Digital Converter resolution: 16-bit Delta-Sigma LBL="" HELPID=""Analog Mono Microphone InputImpedance: 2 kname='OHgr' font=symbol charset=fontspecific code=87
	TeX='\Omega '   descr='[OHgr]'Amplitude at full scale: 0.25 Vpp to 2.5 VppPower supply: +3 Vdc @ 1 mALBL="" HELPID=""MicrophoneType: omnidirectional electret condenser (powered by system)Output level: 65 dB \xb1 4 dB @ 1 kHz (+0 dB = 1 V/0.1 pa)Frequency response: 40 Hz­18 kHz \xb1 2 dBLBL="" HELPID=""Analog Stereo Line-level OutputsImpedance: 600 name='OHgr' font=symbol charset=fontspecific code=87
	TeX='\Omega '   descr='[OHgr]' nominalAmplitude at full scale: 6.0 VppFrequency response: 20 Hz­20 kHz +0 dB, -0.8 dB Total Harmonic Distortion + Noise (THD+N): <0.005% @ 1 kHz,<0.02% 20 Hz­20 kHz Residual noise: -85 dB unweighted, -92 dB A-weighted (re: full scale)Interchannel isolation: -76 dB @ 1 kHz, -66 dB @ 10 kHz,-61 dB @ 20 kHzDigital output filter data resolution: 16-bit input, 18-bit output,8name='times' font=symbol charset=fontspecific code=180
	TeX='\times ' descr='[times]' oversamplingDigital-to-Analog Converter resolution: 18 bitsDigital-to-Analog Converter sample rate: 8name='times' font=symbol charset=fontspecific code=180
	TeX='\times ' descr='[times]' oversamplingLBL="" HELPID=""Analog Stereo Headphone Outputs/Mono Internal SpeakerHeadphone output impedance: 16 name='OHgr' font=symbol charset=fontspecific code=87
	TeX='\Omega '   descr='[OHgr]'Headphone level: 200 mW into 32 name='OHgr' font=symbol charset=fontspecific code=87
	TeX='\Omega '   descr='[OHgr]' loadHeadphone and speaker level control: two independent, digitally-controlled analog attenuatorsSpeaker: 2.6" diameter dynamicSpeaker sound pressure level: 88 dB/WSpeaker output level: 3 Watts max, 1.5 Watts nominalLBL="" HELPID=""Indigo Digital Audio I/OThe following specifications describe the digital audio serial I/O.LBL="" HELPID=""Digital Coaxial Serial InputImpedance: 75 name='OHgr' font=symbol charset=fontspecific code=87
	TeX='\Omega '   descr='[OHgr]', transformer-coupled Level: 1 Vpp Sample rates: 30 kHz to 50 kHz Resolution: supports up to 24 bits per sample Coding: AES-3, IEC-958LBL="" HELPID=""Digital Coaxial Serial OutputImpedance: 75 name='OHgr' font=symbol charset=fontspecific code=87
	TeX='\Omega '   descr='[OHgr]', transformer-coupledLevel: 1 Vpp into 75 name='OHgr' font=symbol charset=fontspecific code=87
	TeX='\Omega '   descr='[OHgr]' loadSample rates: 32, 44.1, 48 kHz, and divisorsResolution: supports up to 24 bits per sampleCoding: AES-3, IEC-958LBL="" HELPID=""Indigo Dedicated Real-time ProcessorThe dedicated real-time processor in the Indigo workstation has the following characteristics:Processor: 20 MHz Motorola DSP56001Native word length: 24 bits, fixed pointDSP RAM: 32K word SRAMLBL="" HELPID=""Indigo2 Workstation Audio Hardware SpecificationsThis section contains the specifications for the Indigo2 audio hardware.ID="MediaA-1A2"LBL="" HELPID=""ID="53697"Indigo2 Analog Stereo Line-level InputsImpedance: 20 kname='OHgr' font=symbol charset=fontspecific code=87
	TeX='\Omega '   descr='[OHgr]' nominalAmplitude at full scale: 0.63Vpp to 8.4VppLevel control: analog gain control internal to CODECsFrequency response: 20Hz­20 kHz \xb1  0.81 dBTotal Harmonic Distortion + Noise (THD+N): < 0.006% @ 1 kHz, < 0.007% 20 to 20 kHzResidual noise: -86 dB unweighted, -88 dB A-weighted (re: Full Scale)Interchannel isolation: -82 dB@1 kHz, -72 dB@10 kHz, -67 dB@20 kHzAnalog-to-Digital Converter resolution: 16-bitLBL="" HELPID=""Indigo2 Stereo Microphone InputImpedance: 1.5 kname='OHgr' font=symbol charset=fontspecific code=87
	TeX='\Omega '   descr='[OHgr]'Amplitude at full scale: 0.063Vpp to 0.84VppPower supply: +3Vdc @ 1mAIn 4-channel mode, the microphone input connector can be configured as a line-level input. When configured this way, it has the characteristics described in IDREF="53697" TYPE="TITLE""Indigo2 Analog Stereo Line-level Inputs"LBL="" HELPID=""ID="10765"Indigo2 Analog Stereo Line-level OutputsImpedance: 600 name='OHgr' font=symbol charset=fontspecific code=87
	TeX='\Omega '   descr='[OHgr]' nominalAmplitude at full scale: 4.7 VppFrequency response: 20 Hz­20 kHz \xb1  1.2 dBTotal Harmonic Distortion + Noise (THD+N): <0.02% 20 to 20 kHzResidual noise: -81 dB unweighted, -85 dB A-weighted (re: full scale)Interchannel isolation:-80 dB@1 kHz, -75 dB@10 kHz, -71 dB@20 kHzDigital-to-Analog converter resolution: 16-bitLBL="" HELPID=""Indigo2 Analog Stereo Headphone Output/Mono Internal SpeakerHeadphone output impedance: 10 name='OHgr' font=symbol charset=fontspecific code=87
	TeX='\Omega '   descr='[OHgr]'Headphone level: 57mW into 32 name='OHgr' font=symbol charset=fontspecific code=87
	TeX='\Omega '   descr='[OHgr]' loadHeadphone and speaker level control: two independent, digitally-controlled analog attenuatorsSpeaker: 70mm by 40mmSpeaker sound pressure level: 80 dB @1W, 1meterSpeaker output level: 5W max, 2W nominalIn 4-channel mode, the headphone output connector is configured as a line-level output. When configured this way, it has the characteristics described in the IDREF="10765" TYPE="TITLE""Indigo2 Analog Stereo Line-level Outputs"LBL="" HELPID=""Indigo2 Digital Audio I/OThe following specifications describe the digital audio serial I/O.LBL="" HELPID=""Digital Coaxial Serial InputImpedance: 75 name='OHgr' font=symbol charset=fontspecific code=87
	TeX='\Omega '   descr='[OHgr]', transformer-coupled Level: 1 Vpp Sample rates: 30 kHz to 50 kHz Resolution: supports up to 24 bits per sample Coding: AES-3, IEC-958LBL="" HELPID=""Digital Coaxial Serial OutputImpedance: 75 name='OHgr' font=symbol charset=fontspecific code=87
	TeX='\Omega '   descr='[OHgr]', transformer-coupledLevel: 1 Vpp into 75 name='OHgr' font=symbol charset=fontspecific code=87
	TeX='\Omega '   descr='[OHgr]' loadSample rates: 32, 44.1, 48 kHz, and divisorsResolution: supports up to 24 bits per sampleCoding: AES-3, IEC-958LBL="B"ID="63476"Aware Scalable Audio Compression SoftwareThis appendix describes built-in licensable compression software from Aware, Inc. Developers are encouraged to integrate Aware's audio compression products to add value to their applications that incorporate audio processing. Doing so requires that you add Network License SystemID="MediaA-2B1"Ô (NetLSÔ) licensing support to your application, which is described in IDREF="83855" TYPE="TITLE""Installing a NetLS Nodelocked License".Aware's compression software can be accessed through the following Silicon Graphics digital media development libraries:Audio File Library (AF)Compression Library (CL)The AF and CL parameters to access Aware compression are provided in this appendix.LBL="" HELPID=""ID="48343"Introduction to Aware Audio Compression SoftwareAware offers audio compression software engines that enable users of Silicon Graphics workstations to reduce the storage size and transmission bandwidth required for audio data. These products are compatible with third-party applications that incorporate audio in the Silicon Graphics digital media computing environment. Scalable operation provides the ability to control processor loading for audio playback concurrently with other computational tasks.Aware's software compression engines include:Aware AudioPublisherÔ MPEG and MultiRate audio codecProvides advanced psychoacoustic processing for compression of CD-quality sound in authoring, publishing, and large audio database applications. Compatible with MPEG-audio standard.Aware AudioProducerÔ MultiRate audio codecProvides a compression solution for studio and audio production applications requiring lossless and near-lossless coding and using low processor loading. Includes AudioProducer and AudioPlayback engines.Aware AudioPlaybackÔ audio decoderProvides a decoding solution for applications requiring playback-only of compressed audio in multi-tasking processing environments.Aware markets its audio compression software to end users. Once an end user licenses and activates Aware's audio compression engines in his or her workstation, the engines are available as a system resource and can work with any application that calls Aware's compression engines.Portions of this software are© Copyright 1993, Aware, Inc.name='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'All Rights Reserved.The Aware Software is proprietary and confidential. You may interface to the Aware Software by using the Audio File (AF) Library and the Compression Library (CL) supplied on this software release. You or any other user may gain authorization to execute the functions of the Aware Software (which includes a keylock mechanism) only by purchasing a usage license from Aware, Inc. Unauthorized use of the Aware Software is expressly forbidden. To obtain a usage license from Aware, contact:ID="MediaA-2B2"Aware, Inc.One Memorial DriveCambridge, MA 02142phone: (617) 577-1700fax: (617) 577-1710email: sales@aware.comAsk for audio products.LBL="" HELPID=""Aware Software Products Features and ApplicationsThis section describes the applications and features of Silicon Graphics digital media-compatible products from Aware.LBL="" HELPID=""Aware Products Available in IRIS Digital Media LibrariesThe IRIS digital media libraries contain three software products that end users can license by contacting Aware (see IDREF="48343" TYPE="TITLE""Introduction to Aware Audio Compression Software") to obtain a license password:Aware AudioPublisherThe Aware AudioPublisher provides advanced psychoacoustic processing for compression of high-quality audio (up to 48 kHz sampling rate) in studio, authoring, and archiving applications.ID="MediaA-2B3"AudioPublisher supports the MPEG (Moving Pictures Experts Group) audio standard format. The encoding process invokes advanced psychoacoustic modeling to achieve high compression ratios.Encoding bit-rates range from 32 Kbits/second to 448 Kbits per second (corresponding to compression ratios as high as 48:1). AudioPublisher also includes Aware's AudioProducer and AudioPlayback software.Aware AudioProducerThe Aware AudioProducer provides a compression solution for audio production applications requiring lossless (perfectly invertible) and near-lossless coding at low computational complexity.ID="MediaA-2B4"Lossless mode reduces storage requirements by 2:1 to 3:1. Near-lossless operation achieves even greater storage savings, while retaining a 90 dB signal-to-noise ratio. Real-time encoding of audio at 8, 11.025, 16, 22.05, and 44.1 kHz sampling rates is supported. Scalable encoding and decoding operation provides control of processor loading during playback. AudioProducer includes Aware's AudioPlayback software.Aware AudioPlaybackThe Aware AudioPlayback engine provides a decoding solution for applications requiring decoding of compressed audio in multi-tasking processing environments. The decoder runs in real time, at sampling rates of up to 48 kHz. Installation of the decoder enables real-time playback of compressed audio that was encoded with the Aware AudioPublisher or with the Aware AudioProducer. Scalable decoding operation provides the ability to control processor loading used for playback, and it enables special functions such as fast playback to let you speed-search through compressed audio libraries.ID="MediaA-2B5"LBL="" HELPID=""Other Digital Media Compatible Aware Audio ProductsThese audio productivity software products are compatible with Aware's audio compression engines in the Silicon Graphics digital media environment. Contact Aware for more information on these products.LBL="" HELPID=""Aware AudioSuite ToolsThe Aware AudioSuiteÔ provides these graphical user interface (GUI) software applications for the end user that work with Aware's licensable software audio compression engines on Silicon Graphics platforms:ID="MediaA-2B6"COLUMNS="2"LEFT="0" WIDTH="72"AuditionÔLEFT="80" WIDTH="226"provides compressed file playback and extractionLEFT="0" WIDTH="72"ArchiverÔLEFT="80" WIDTH="226"provides file-to-file compression batch processingLEFT="0" WIDTH="72"PsycoderÔLEFT="80" WIDTH="226"provides optimal compression parameter selectionLBL="" HELPID=""Aware Speed-of-Sound Library, Volume I, Sound EffectsThe Aware Speed-of-SoundÔ Library is a single CD-ROM that puts more than 1,000 digitally recorded sound effects at your fingertips. Cars, trains, planes, birds, bees, laughing, crying, water, and windname='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'all the effects you need to make your creative vision come alive. Each sound on this disk was professionally recorded at the 16-bit, 44.1 kHz CD-audio format. But unlike standard CDs that contain a maximum of 72 minutes of sound, the Aware Speed-of-Sound Library offers over seven hours of effects, along with the interactive ID="MediaA-2B7"BrowsFXÔ librarian tool that makes finding and auditioning any sound a snap.LBL="" HELPID=""Aware AudioSuite PackageThis comprehensive package includes the Speed-of-Sound Library,Volume I: SFX, plus all audio compression engines and GUIs.LBL="" HELPID=""Accessing Aware Audio Compression from the Audio File LibraryThis section describes the data formats, defaults, and parameters for accessing the Aware audio compression software engines from the Audio File Library.ID="MediaA-2B8"LBL="" HELPID=""Valid Audio Input DataAudio input data can be accepted in any of these formats:16-bit two's complement samplessingle or dual channelsampling rates for MPEG must be 32 kHz, 44.1 kHz, or 48 kHz.sampling rates for MultiRate must be 8 kHz, 11.025 kHz, 16 kHz, 22.05 kHz, 32 kHz, 44.1 kHz, or 48 kHz. ID="MediaA-2B9"LBL="" HELPID=""Compression DefaultsThe simplest method of invoking Aware compression is to use this command to pick from one of four default settings:AFinitcompression ( AFfilesetup setup, long track,                    long compression_scheme)The defaults for the Aware compression parameters are:AF_COMPRESSION_AWARE_DEFAULT_MPEG_IMPEG layer 1, joint-stereo, fixed rate at 192 Kbps/channelAF_COMPRESSION_AWARE_DEFAULT_MPEG_IIMPEG layer 2, joint-stereo, fixed rate at 128 Kbps/channelAF_COMPRESSION_AWARE_DEFAULT_MULTIRATEMultiRate, near-lossless ID="MediaA-2B10"AF_COMPRESSION_AWARE_DEFAULT_LOSSLESSMultiRate, losslessLBL="" HELPID=""Compression Custom ConfigurationTo access individual compression parameters, use the AF library call:AFinitcompressionparams ( AFfilesetup setup, long track,              long scheme,AUpvlist pvbuffer, long buffersize)The compression scheme should be passed as one of:AF_COMPRESSION_AWARE_MPEGAF_COMPRESSION_AWARE_MULTIRATEID="MediaA-2B11"AF_COMPRESSION_AWARE_DEFAULT_MPEG_IAF_COMPRESSION_AWARE_DEFAULT_MPEG_IIAF_COMPRESSION_AWARE_DEFAULT_MULTIRATEAF_COMPRESSION_AWARE_DEFAULT_LOSSLESSThe parameters and values passed in the AUpvlist structure include:AF_AWARE_PARAM_LAYER(valid only for algorithm AF_AWARE_MPEG) selects which MPEG layer:AF_AWARE_LAYER_IAF_AWARE_LAYER_II (default)NoteAF_AWARE_PARAM_CHANNEL_POLICY chooses how multiple channels should be treated (these settings are equivalent for single channel input).AF_AWARE_STEREO indicates that the channels are part of a single multichannel signal, such as quadraphonic, and so on.AF_AWARE_JOINT_STEREO (default for MPEG)indicates that the algorithm may attempt to exploit redundancy between channels for greater coding gain. Not valid for AF_AWARE_MULTIRATE.AF_AWARE_INDEPENDENT (default for MultiRate)indicates that the separate channels are unrelated and should be processed separately, such as multilingual sound tracks. AF_AWARE_PARAM_BITRATE_TARGETspecifies the desired bitrate for all channels of compressed data, in bits per second. Note that for some schemes such as MPEG's maxrate (not yet implemented) this is treated as an upper limit, whereas for MPEG's fixrate, this is strictly achieved as a constant rate. This parameter is not used for Aware's MultiRate algorithm.The following is a list of valid bitrates for MPEG:Layer 1: 32000, 64000, 96000, 128000, 160000, 192000, 224000, 256000, 288000, 320000, 352000, 384000, 416000, and 448000.Layer 2: 32000, 48000, 56000, 64000, 80000, 96000, 112000, 128000, 160000, 192000, 224000, 256000, 320000, and 384000.Default value is 192 Kbps/channel for layer 1 and 128 Kbps/channel for layer 2.AF_AWARE_PARAM_BITRATE_POLICYselects variants for interpreting AF_AWARE_PARAM_BITRATE_TARGET. The valid values depend on the compression type:Aware MPEG (AF_COMPRESSION_AWARE_MPEG) uses either:AF_AWARE_FIXED_RATE (default)fixed bitrate per second, where the compression ratio is set by AF_AWARE_PARAM_BITRATE_TARGET orAF_AWARE_CONST_QUALlets the bitrate be driven by the psychoacoustic model. Enough bits are assigned so that a constant noise-to-mask ratio is attained. See AF_AWARE_CONST_QUAL_NMR.Aware MultiRate (AF_COMPRESSION_AWARE_MULTIRATE) uses either:ID="MediaA-2B12"AF_AWARE_LOSSLESSsupplies enough bits to provide for perfect reconstruction. Compression ratios are typically between 2:1 and 3:1.orAF_AWARE_CONST_QUALenough bits are assigned so that the signal is 90+ dB above the quantization noise. Compression ratios are typically between 2.5:1 and 4:1.AF_AWARE_CONST_QUAL_NMRnot used in the Aware MultiRate algorithm. For AF_COMPRESSION_AWARE_MPEG with AF_AWARE_CONST_QUAL, it sets the constant quality mode noise-to-mask ratio in dB. Zero yields a theoretical psychoacoustically imperceptible compression. Positive values provide more compression and noise becomes audible. Negative values cause less compression and less perceptible noise. The type is AU_PVTYPE_DOUBLE.LBL="" HELPID=""Accessing Aware Audio Compression from the Compression LibraryThe Aware audio compression engines are already installed in the Silicon Graphics Compression Library uniform interface to video and audio compression routines. You can incorporate these routines into your applications and set up license querying to enable end users who choose to do so to license the routines. This section describes how to use the Compression Library interface to the Aware engines.ID="MediaA-2B13"LBL="" HELPID=""Compression SchemesAware currently provides two distinct compression schemes under the CL:CL_AWARE_MPEG_AUDIOISO/MPEG-audio standard algorithm with layers I and II.CL_AWARE_MULTIRATEAware MultiRate I proprietary lossless or low-distortion algorithm. ID="MediaA-2B14"ID="MediaA-2B15"Both scheme identifiers are defined in cl.h, as are all the identifiers in this section with the prefix CL_. Identifiers prefixed AWCMP_ are defined in awareAudio.h.LBL="" HELPID=""Using Compression Library ParametersThe CL is controlled by a wide range of parameters. Some parameters have multiple usesname='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'others are appropriate only in certain circumstances. This list explains the use of all the parameters of relevance to the Aware schemes:CL_ORIGINAL_FORMATindicates the format of the components of the original uncompressed audio. Default is CL_STEREO_INTERLEAVED, legal alternative is CL_MONO.CL_COMPONENTSindicates the number of channels in the original uncompressed audio. Only single and dual channel signals are currently supported.CL_BITS_PER_COMPONENTinforms the scheme of the format of the audio samples. They are assumed to be two's complement linear ints with the specified number of bits. The default is 16, which is also currently the only legal value.CL_FRAME_RATEtells the scheme the sampling rate of the uncompressed audio. Default is 44100.0 Hz. Legal values for scheme CL_AWARE_MPEG are 32000.0, 44100.0, and 48000.0. Legal values for scheme CL_AWARE_MULTIRATE are 8000.0, 11025.0, 16000.0, 22050.0, 32000.0, 44100.0, and 48000.0 Hz.ID="MediaA-2B16"CL_SPEED during decompression, this parameter controls the computational complexity by controlling several factors: sample rate decimation, combining of stereo channels, filter shape (CL_AWARE_MPEG only), and lossy decode (CL_AWARE_MULTIRATE only). Acceptable values range from 1.0 to 100.0. CL_SPEED has no effect during compression. NoteChanging this parameter may change the sampling rate, the total number of sample frames available, and the sample frames per compressed block. After changing this parameter, you must call clGetParams() to find out the new CL_FRAME_RATE and CL_BLOCK_SIZE.CL_CHANNEL_POLICYselects the treatment of stereo signals:AWCMP_STEREOstereo channels are coded separately.AWCMP_JOINT_STEREOstereo redundancy is exploited by coding high frequency sub-bands together.AWCMP_INDEPENDENTchannels are unrelated and are coded separately, such as multilingual sound tracks.NoteScheme CL_AWARE_MPEG_AUDIO supports all three modes; CL_AWARE_MULTIRATE supports only AWCMP_STEREO and AWCMP_INDEPENDENT.CL_BITRATE_POLICYProvides for different modes of bit assignment: ID="MediaA-2B17"AWCMP_FIXED_RATE  fixed bitrate for CL_AWARE_MPEG_AUDIO only. See CL_BITRATE_TARGET. This is the default for CL_AWARE_MPEG_AUDIO. AWCMP_CONST_QUAL  bitrate is allowed to vary to satisfy MPEG psychoacoustic model or MultiRate data requirements (90+ dB). This is the default for CL_AWARE_MULTIRATE. See CL_NOISE_MARGIN. AWCMP_LOSSLESS  valid for CL_AWARE_MULTIRATE only. Data is coded losslessly.CL_NOISE_MARGINused in conjunction with CL_BITRATE_POLICY's AWCMP_CONST_QUAL. Provides for specification of the noise-to-mask ratio in MPEG psychoacoustics. Zero yields a theoretical psychoacoustically imperceptible compression. Positive values provide more compression and noise becomes audible. Negative values cause less compression and less perceptible noise. For the ID="MediaA-2B18"MultiRate algorithm, this parameter sets the noise-floor at -90 dB below the signal level. This is the only valid MultiRate setting at this time.CL_LAYER(MPEG only) selects the MPEG layer. Default is AWCMP_MPEG_LAYER_II for layer II, other legal value is AWCMP_MPEG_LAYER_I for layer I.CL_BITRATE_MODEprovides for different modes of operation such as fixed rate or constant quality; however, the only legal values at present are AWCMP_FIXED_RATE for MPEG and AWCMP_CONST_QUAL or AWCMP_LOSSLESS for MultiRate.CL_BITRATE_TARGETused in conjunction with CL_BITRATE_POLICY's AWCMP_FIXED_RATE (MPEG). Determines the output data rate in bits per second. The allowed values are:MPEG layer I: 32000, 64000, 96000, 128000, 160000, 192000, 224000256000, 288000, 320000, 352000, 384000, 416000, 448000.MPEG layer II: 32000, 48000, 56000, 64000, 80000, 96000, 112000,128000, 160000, 192000, 224000, 256000, 320000, 384000.CL_COMPRESSED_BUFFER_SIZEWhen a scheme is configured, this field can be queried to find the maximum compressed size of a block of CL_BLOCK_SIZE samples. This is useful for allocating buffers to hold this compressed data. If the application sets this parameter (to inform the CL of the size of an externally allocated buffer), the compression code does not change that value, even if its configuration is modified.The following parameters are read-only:CL_BLOCK_SIZEThe scheme writes the number of uncompressed sample frames to compress at a time by the scheme. It is strongly advised that data be passed to clCompress() in blocks that are exact multiples of this size; otherwise, the algorithm will defer to compression of partial frames, which may cause unexpected problems. CL_COMPRESSION_RATIOThe scheme writes a compression ratio into this parameter. Parameter CL_EXACT_COMPRESSION_RATIO will be true if this is an exact compression ratio and false if it is a worst-case estimate.CL_EXACT_COMPRESSION_RATIOIndicates whether the compression ration in CL_EXACT_COMPRESSION_RATIO is exact (TRUE), or a worst-case estimate (FALSE). This parameter is always true for pure MPEG, because it is a fixed-bitrate scheme, and false otherwise; therefore, this parameter is currently of interest only for the MPEG scheme.CL_FRAME_TYPEindicates the status of the next frame to be processed: CL_KEYFRAME, meaning that the frame is the first frame in a compressed block, which is what it should be if all calls are in BLOCK_SIZE blocks.CL_PREDICTED is the frame type for all other alignments.LBL="" HELPID=""Usage HintsYou'll get the best compression results if you keep these hints in mind:Always compress blocks of CL_BLOCKSIZE sample frames.The first block that comes out of an instance of a compression scheme has extra configuration information attached in front of it. On decompression, this should be fetched first (you will have to query its maximum size), then fed to clReadHeader(). Thereafter, the CL_COMPRESSED_BUFFER_SIZE can be read and used to allocate a proper data buffer, if so desired.No provision has been made for alignment of delay as a result of filter latency through the Compression Library (which can be a few hundred samples depending on scheme), or for zero-padding the final frame to ensure that the filter is flushed. These must be handled by the application, if required.LBL="" HELPID=""Aware Audio Compression Software SpecificationsThis section lists the specifications for the Aware audio compression software.ID="MediaA-2B19"Compression AlgorithmsISO/MPEG Layer I and II low bitrate psychoacoustic compressionAware MultiRate I high resolution audio compressionLibrary SupportSilicon Graphics Audio File Library (AF)Silicon Graphics Compression Library (CL)Encoder InputFormat: 16-bit linear audio samples, mono or stereoSample Rates:COLUMNS="2"LEFT="0" WIDTH="95"MPEG EncoderLEFT="100" WIDTH="147"48, 44.1, 32 kHzLEFT="0" WIDTH="95"MultiRate EncoderLEFT="100" WIDTH="147"48, 44.1, 32, 22.05, 16, 11.025 kHzID="MediaA-2B20"Decoder OutputFormat:16 bit linear audio samples, mono or stereoSample Rates: 48, 44.1, 32, 24, 22.05, 16, 11.025 (MPEG, MultiRate Decoders)Compression Ratios:2:1 to 3:1typical MultiRate lossless (perfectly invertible) mode2.5:1 to 4:1typical MultiRate near-lossless (90+ dB signal to noise) mode2.2:1 to 48:1 selectable, MPEG Layer I and IIBitrates (Kbits/sec):MPEG Layer I32, 64, 96, 128, 160, 192, 224, 256, 288, 320, 352, 384, 416, 448MPEG Layer II32, 48, 56, 64, 80, 96, 112, 128, 160, 192, 224, 256, 320, 384 Channel ProcessingCOLUMNS="2"LEFT="0" WIDTH="104"MPEG Layer I and IILEFT="110" WIDTH="175"mono, stereo, joint stereo, dual channelLEFT="0" WIDTH="104"MultiRateLEFT="110" WIDTH="175"mono, stereoScalable ProcessingControlled reduction of CPU usage by parametric control of frequency response, signal to noise ratio, and mono decoding of stereo.IDREF="29989" TYPE="TABLE"Table B-1 lists the compression algorithms that are built in to each Aware software compression engine product.COLUMNS="2"LBL="B-1"Table B-1 ID="29989"Built-in Algorithms for Aware Audio Software Compression EnginesLEFT="0" WIDTH="144"ProductLEFT="150" WIDTH="189"Compression Algorithms InstalledLEFT="0" WIDTH="144"AudioPublisherLEFT="150" WIDTH="189"ISO/MPEG Layer I and II EncoderLEFT="0" WIDTH="144"LEFT="150" WIDTH="189"Aware MultiRate I EncoderLEFT="0" WIDTH="144"LEFT="150" WIDTH="189"ISO/MPEG Layer I and II DecoderLEFT="0" WIDTH="144"LEFT="150" WIDTH="189"Aware MultiRate DecoderLEFT="0" WIDTH="144"AudioProducerLEFT="150" WIDTH="189"Aware MultiRate I EncoderLEFT="0" WIDTH="144"LEFT="150" WIDTH="189"ISO/MPEG Layer I and II DecoderLEFT="0" WIDTH="144"LEFT="150" WIDTH="189"Aware MultiRate DecoderLEFT="0" WIDTH="144"AudioPlaybackLEFT="150" WIDTH="189" ISO/MPEG Layer I and II DecoderLEFT="0" WIDTH="144"LEFT="150" WIDTH="189"Aware MultiRate DecoderLBL="" HELPID=""ID="83855"Installing a NetLS Nodelocked LicenseA product that is licensed with NetLS can use either a nodelocked license or a concurrent access license. On the ID="MediaA-2B21"Silicon Graphics Vendor and License Information sheet, in the licensetype field, it lists either "Concurrent Access" or "NodeLocked" specifying the type of license you have been issued.This section documents nodelocked licenses. With a nodelocked license, the product can be run only on the machine that has the nodelocked license installed. To enable a nodelocked license, the "vendor ID" and "product password" from the ID="MediaA-2B22"Silicon Graphics Vendor and License Information sheet are entered into the file /usr/netls/nodelock. After that, a product should have permission to run on the machine with the ID="MediaA-2B23"nodelock file.To install a nodelocked license:Verify that the product you want to license is already installed, by entering:ID="MediaA-2B24"% /usr/sbin/versionsFor IRIX release 5.0 or earlier, netls_eoe and nck subsystems do not need to be installed. For IRIX release 5.0.1 or later, it is recommended that netls_eoe be installed, in order to create the link from /usr/netls/nodelock to /var/netls/nodelock. This link is needed by some NetLS licensed products that are not Silicon Graphics products. The llb, glb, and netls flags, which you can set with the /etc/chkconfig command, do not affect whether a nodelocked license will work, but it is a good idea to set these flags correctly anyway. If this machine is not being used as a NetLS or glb database server, chkconfig these flags off. To change the flags, enter this sequence of commands:# su
# /etc/chkconfig glb off# /etc/chkconfig llb off# /etc/chkconfig netls offEdit the nodelock file. For IRIX release 5.0 or earlier, the nodelock file is /usr/netls/nodelock. For IRIX release 5.0.1 or later, the nodelock file is /var/netls/nodelock. A license in the nodelock file takes up one line and looks something like this:549db468491e.02.c0.1a.3d.52.00.00.00 4cb3cwxxy29awcv9998xawhere the first string is a vendor ID and the second string is the product password. Comment lines are ignored. A comment line is a line whose first character is a "#".If a temporary nodelocked license is already installed for the product for which you are adding a longer term license, comment out that temporary license line by putting a "#" in front of the line. If no other products are nodelocked and other licenses exist in this file, comment all of the licenses out by putting a "#"character in front of them.Continue editing the nodelock file to add the vendor ID and the product password for the product. Put them on the same line in the nodelock file with the vendor ID first, followed by a space, then the product password.It is strongly recommended to add a comment line before the license that describes what product the license is for and when it expires. See the online NetLS Administration Guide for more information and troubleshooting guidelines.ID="27729"Glossaryactive videoThe portion of the video signal containing the chrominance or luminance information; all video lines not occurring in the vertical blanking signal containing the chrominance or luminance information. See also chrominance, composite video, horizontal blanking, luminance, and video waveform. aliasingOne of several types of digital video artifact appearing as jagged edges. Aliasing results when an image is sampled that contains frequency components above the Nyquist limit for the sampling rate. See also Nyquist limit.alphaSee alpha value.alpha blendingOverlaying one image on another so that some of the underlying image may or may not be visible. See also key.alpha planeA bank of memory that stores alpha values; the values are 8 bits per pixel.alpha registerRegisters that stores an alpha value. alpha valueThe component of a pixel that specifies the pixel's opacity, translucency, or transparency. The alpha component is typically output as a separate component signal. antialiasingFiltering or blending lines of video to smooth the appearance of jagged edges in order to reduce the visibility of aliasing.APLAverage Picture Level, with respect to blanking, during active picture time, expressed as a percentage of the difference between the blanking and reference white levels. See also blanking level. artifactIn video systems, an unnatural or artificial effect that occurs when the system reproduces an image; examples are aliasing, pixellation, and contouring. aspect ratioThe ratio of the width to the height of an electronic image. For example, the standard aspect ratio for television is 4:3. back porchThe portion of the horizontal pedestal that follows the horizontal synchronizing pulse. In a composite signal, the color burst is located on the back porch, but is absent on a YUV or GBR signal. See also blanking level, video waveform.BetacamA component videotape format developed by Sony that uses a Y/R-Y/B-Y video signal and 1/2-inch tape.Betacam formatAdvanced form (Superior Performance) of Betacam using special metal tape and offering longer recording time (90 minutes instead of 30 minutes) and superior performance.bit mapA region of memory that contains the pixels representing an image. The pixels are arranged in the sequence in which they are normally scanned to display the image. bitplaneOne of a group of memory arrays for storing an image in bitmap format on a workstation. The workstation reads the bitplanes in parallel to re-create the image in real time.black burstActive video signal that has only black in it. The black portion of the video signal, containing color burst. See also color burst.black levelIn the active video portion of the video waveform, the voltage level that defines black. See also horizontal blanking and video waveform.blanking levelThe signal level at the beginning and end of the horizontal and vertical blanking intervals, typically representing zero output (0 IRE). See also video waveform and IRE units.blendTo combine proportional amounts of a 3D graphic over a clip frame by frame, pixel by pixel, with the alpha determining how they are combined. See also key, frame, and alpha.breezewayIn the horizontal blanking part of the video signal, the portion between the end of the horizontal sync pulse and the beginning of the color burst. See also horizontal blanking and video waveform.broad pulsesVertical synchronizing pulses in the center of the vertical interval. These pulses are long enough to be distinguished from other pulses in the signal; they are the part of the signal actually detected by vertical sync separators.Bruch blankingIn PAL signals, a four-field burst blanking sequence used to ensure that burst phase is the same at the end of each vertical interval.burst, burst flagSee color burst.burst lockThe ability of the output subcarrier to be locked to input subcarrier, or of output to be genlocked to an input burst.burst phaseIn the RS-170A standard, burst phase is at field 1, line 10; in the European PAL standards, it is at field 1, line 1. Both define a continuous burst waveform to be in phase with the leading edge of sync at these points in the video timing. See also vertical blanking interval and video waveform.B-Y (B minus Y) signalOne of the color difference signals used on the NTSC and PAL systems, obtained by subtracting luminance (Y) from the blue camera signal (B). This signal drives the horizontal axis of a vectorscope. Color mixture is close to blue; phase is 180 degrees opposite of color sync burst; bandwidth is 0.0 to 0.5MHz. See also luminance, R-Y signal, Y signal, and Y/R-Y/B-Y.C signalChrominance; the color portion of the signal. For example, the Y/C video format used for S-VHS has separate Y (luminance) and C (chrominance) signals. See also chrominance.CAVComponent Analog Video; a generic term for all analog component video formats, which keep luminance and chrominance information separate. D1 is a digital version of this signal. See also component video.C formatType C, or one-inch reel-to-reel videotape machine; an analog composite recording format still used in some broadcast and postproduction applications.CCIR 601The digital interface standard developed by the CCIR (Comite' Consultatif International de Radiodiffusion, International Radio Consultative Committee) based on component color encoding, in which the luminance and chrominance (color difference) sampling frequencies are related in the ratio 4:2:2: four samples of luminance (spread across four pixels), two samples of CR color difference, and two samples of CB color difference. The standard, which is also referred to as 4:2:2, sets parameters for both 525-line and 625-line systems.chromaSee chrominance.chroma keyingOverlaying one video source on another by choosing a key color. For example, if chroma keying is on blue, video source A might show through video source B everywhere the color blue appears in video source B. A common example is the TV weather reporter standing in front of the satellite weather map. The weather reporter, wearing any color but blue, stands in front of a blue background; keying on blue shows the satellite picture everywhere blue appears. Because there is no blue on the weatherperson, he or she appears to be standing in front of the weather map.chroma signalA 3.58MHz (NTSC) or 4.43MHz (PAL) subcarrier signal for color in television. SECAM uses two frequency-modulated color subcarriers transmitted on alternate horizontal lines; SCR is 4.406MHz and SCB is 4.250MHz.chrominance In an image reproduction system, a separate signal that contains the color information. Black, white, and all shades of gray have no chrominance and contain only the luminance (brightness) portion of the signal. However, all colors have both chrominance and luminance.Chrominance is derived from the I and Q signals in the NTSC television system and the U and V signals in the PAL television system. See also luminance.chrominance signalAlso called the chroma, or C, signal. The high-frequency portion of the video signal (3.58MHz for NTSC, 4.43MHz for PAL) color subcarrier with quadrature modulation by I (R-Y) and Q (B-Y) color video signals. The amplitude of the C signal is saturation; the phase angle is hue. See also color subcarrier, hue, and saturation.clientIn the context of the Video Library, an application that has connected to the video daemon to perform video requests. clipSegment of video, audio, or both. An image is a clip that is one frame long.color barsA test pattern used by video engineers to determine the quality of a video signal, developed by the Society of Television and Motion Picture Engineers (SMPTE). The test pattern consists of equal-width bars representing black, white, red, green, blue, and combinations of two of the three RGB values: yellow, cyan, and magenta. These colors are usually shown at 75% of their pure values. IDREF="33270" TYPE="GRAPHIC"Figure Gl-1 diagrams the color bars.
FILE="Gl-1.online.bw" POSITION="INLINE" SCALE="FALSE"LBL="Gl-1"Figure Gl-1 ID="33270"SMPTE Color Bars (75%)color burstAlso called burst and burst flag. The segment of the horizontal blanking portion of the video signal that is used as a reference for decoding color information in the active video part of the signal. The color burst is required for synchronizing the phase of 3.58MHz oscillator in the television receiver for correct hues in the chrominance signal.In composite video, the image color is determined by the phase relationship of the color subcarrier to the color burst. The color burst sync is 8 to 11 cycles of 3.58MHz color subcarrier transmitted on the back porch of every horizontal pulse; The hue of the color sync phase is yellow-green.IDREF="15407" TYPE="GRAPHIC"Figure Gl-2IDREF="15407" TYPE="TEXT"diagrams the relationship of the color burst and the chrominance signal. See also color subcarrier and video waveform.
FILE="Gl-2.online.bw" POSITION="INLINE" SCALE="FALSE"LBL="Gl-2"Figure Gl-2 ID="15407"Color Burst and Chrominance Signalcolor difference signalsSignals used by color television systems to convey color information so that the signals go to zero when the picture contains no color; for example, unmodulated R-Y and B-Y, I and Q, U, and V.color-frame sequenceIn NTSC and S-Video, a two-frame sequence that must elapse before the same relationship between line pairs of video and frame sync repeats itself. In PAL, the color-frame sequence consists of four frames.color spaceA space defined by three color components, such as R, G, and B. color subcarrierA portion of the active portion of a composite video signal that carries color information, referenced to the color burst. The color subcarrier's amplitude determines saturation; its phase angle determines hue. Hue and saturation are derived with respect to the color burst. Its frequency is defined as 3.58MHz in NTSC and 4.43MHz in PAL. See also color burst.complementary colorOpposite hue and phase angle from a primary color. Cyan, magenta, and yellow are complementary colors for red, green, and blue, respectively.comb filteringProcess that improves the accuracy of extracting color and brightness portions of the signal from a composite video source.component videoA color encoding method for the three color signalsname='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'R, G, and B; Y, I, and Q; or Y, U, and Vname='mdash' font=symbol charset=fontspecific code=190 
			descr='[mdash]'that make up a color image. See also RGB, YIQ, and YUV.component video signalsA video signal in which luminance and chrominance are send as separate components, for example:
RGB (basic signals generated from a camera)YIQ (used by the NTSC broadcasting standard)Y/R-Y/B-Y (used by Betacam and M-II recording formats and SECAM broadcasting standard)YUV (subset of Y/R-Y/B-Y used by the PAL broadcasting standard)Separating these components yields a signal with a higher color bandwidth than that of composite video.IDREF="91447" TYPE="GRAPHIC"Figure Gl-3 depicts video signals for one horizontal scan of a color-bar test pattern. The RGB signals change in relation to the individual colors in the test pattern. When a secondary color is generated, a combination of the RGB signals occurs. Since only the primary and secondary colors are being displayed at 100% saturation, the R, G, and B waveforms are simply on or off. For more complex patterns of color, the individual R, G, and B signals would be varying amplitudes in the percentages needed to express that particular color. See also composite video, RGB, YUV, Y/R-Y/B-Y, and YIQ.
FILE="Gl-3.online.bw" POSITION="INLINE" SCALE="FALSE"LBL="Gl-3"Figure Gl-3 ID="91447"Component Video SignalscompositingCombining graphics with another image. composite videoA color encoding method or a video signal that contains all of the color, brightness, and synchronizing information in one signal. The chief composite television standard signals are NTSC, PAL, and SECAM. See also NTSC, PAL, and SECAM.cross-chrominance, cross-luminanceAlso known as cross-color, hanging dots, dot crawl; moving colors on stationary objects. This undesirable artifact is caused by high bandwidth luminance information being misinterpreted as color information. Hanging dots are a byproduct of the comb filters (used to help separate the color and brightness information) found in most modern television receivers. This artifact can be reduced or eliminated by using S-Video or a component video format.cross-fadeA type of transition in which one video clip is faded down while another is faded up.D1Digital recording technique for component video; also known as CCIR 601, 4:2:2. D1 is the best choice for high-end production work where many generations of video are needed. D1 can be an 8-bit or 10-bit signal. See also CCIR 601.D2Digital recording technique for composite video. As with analog composite, the luminance and chrominance information is sent as one signal. A D2 VTR offers higher resolution and can make multiple generation copies without noticeable quality loss, because it samples an analog composite video signal at four times the subcarrier (using linear quantization), representing the samples as 8-bit digital words. D2 is not compatible with D1.D3, DXDeveloped by Panasonic, a 1/2-inch tape version of D2. More often called DX.decoderHardware or software that converts, or decodes, a composite video signal into the various components of the signal. For example, to grab a frame of composite video from a VHS tapedeck and store it as an RGB file, it would have to be decoded first. Several Silicon Graphics video options have on-board decoders. ditheringApproximating a signal value on a chroma-limited display device by producing a matrix of color values that fool human perception into believing that the signal value is being reproduced accurately. For example, dithering is used to display a true-color image on a display capable of rendering only 256 unique colors, such as IndigoVideo images on a Starter Graphics display.drainIn the context of the Video Library, a target or consumer of video signals.editingThe process in which data is examined, created, and modified. In video, the part of the postproduction process in which the finished videotape is derived from raw video footage. Animation is a subset of editing. encoderDevice that combines the R, G, and B primary color video signals into hue and saturation for the C portion of a composite signal. Several Silicon Graphics video options have on-board encoders.equalizing pulsePulse of one half the width of the horizontal sync pulse, transmitted at twice the rate of the horizontal sync pulse, during the portions of the vertical blanking interval immediately before and after the vertical sync pulse. The equalizing pulse makes the vertical deflection start at the same time in each interval, and also keeps the horizontal sweep circuits in step during the portions of the vertical blanking interval immediately before and after the vertical sync pulse.eventExceptional or noteworthy condition produced during video processing, such as loss of sync, dropping of frames or fields, and synchronization with other applications.exclusive useA term applied to usage of the video data stream and controls on a pathway. A pathway in exclusive-use mode is available for writing of controls only to the client that requested the exclusive use, yet any application may read the controls on that pathway.fadeTo modify the opacity and/or volume of a clip. A faded-up clip is unaffected, a clip faded down to 50% has 50% less opacity or volume, and a faded-down clip is completely transparent of turned off.fieldOne of two (or more) equal parts of information in which a frame is divided in interlace scanning. A vertical scan of a frame carrying only its odd-numbered or its even-numbered lines. The odd field and even field make up the complete frame. See also frame and interlace.field averagingA filter that corrects flicker by averaging pixel values across successive fields. See also flicker.field blankingThe blanking signals at the end of each field, used to make the vertical retrace invisible. Also called vertical blanking; see vertical blanking and vertical blanking interval.filterTo process a clip with spatial or frequency domain methods. Each pixel is modified by using information from neighboring (or all) pixels of the image. Filter functions include blur (low-pass) and crisp (high-pass).flickerThe effect caused by a one-pixel-deep line in a high-resolution graphics frame that is output to a low-resolution monitor, because the line is in only one of the alternating fields that make up the frame. This effect can be filtered out by field averaging. See also field and frame.frameThe result of a complete scanning of one image. In television, the odd field (all the odd lines of the frame) and the even field (all the even lines of the frame) make up the frame. In motion video, the image is scanned repeatedly, making a series of frames. freeze, freeze-frameA condition on the digitized video signal where the digitizing is stopped and the contents of the signal appear frozen on the display or in the buffer. Sometimes used to capture the video data for processing or storage.frequencySignal cycles per second. frequency interlacePlacing of harmonic frequencies of C signal midway between harmonics of horizontal scanning frequency Fh. Accomplished by making color subcarrier frequency exactly 3.579545MHz. This frequency is an odd multiple of H/2.front porchThe portion of the video signal between the end of active video and the falling edge of sync. See also back porch, horizontal blanking, and video waveform.G-Y signalColor mixture close to green, with a bandwidth 0.0MHz to 0.5MHz. Usually formed by combining B-Y and R-Y video signals.gamma correctionCorrection of gray-scale inconsistency. The brightness characteristic of a CRT is not linear with respect to voltage; the voltage-to-intensity characteristic is usually close to a power of 2.2. If left uncorrected, the resulting display has too much contrast and detail in black regions is not reproduced.To correct this inconsistency, a correction factor using the 2.2 root of the input signal is included, so that equal steps of brightness or intensity at the input are reproduced with equal steps of intensity at the display. genlockingSynchronizing with another video signal serving as a master timing source. The master timing source can be a composite video signal, a video signal with no active video (only sync information), or, for video studio, a device called house sync. When there is no master sync available, VideoFramer, for example, can be set to "free run" (or "stand-alone") mode, so that it becomes the master timing device to which other devices sync. See also line lock.gray-scaleMonochrome or black-and-white, as in a monitor that does not display color.H rateNumber of complete horizontal lines, including trace and retrace, scanned per second.HDTVHigh-definition television. Though there is more than one proposal for a broadcast standard for HDTV, most currently available equipment is based on the 1125/60 standard, that is, 1125 lines of video, with a refresh rate of 60Hz, 2:1 interlacing (same as NTSC and PAL), and aspect ratio of 16:9 (1920 x 1035 viewable resolution), trilevel sync, and 30MHz RGB and luminance bandwidth.Hi-8mmAn 8mm recording format developed by Sony; accepts composite and S-Video signals.horizontal blanking The period when the electron beam is turned off, beginning when each scan line finishes its horizontal path (scan) across the screen (see IDREF="54577" TYPE="GRAPHIC"Figure Gl-4).
FILE="Gl-4.online.bw" POSITION="INLINE" SCALE="FALSE"LBL="Gl-4"Figure Gl-4 ID="54577"Horizontal Blanking horizontal blanking intervalAlso known as the horizontal retrace interval, the period when a scanning process is moving from the end of one horizontal line to the start of the next line. This portion of the signal is used to carry information other than video information. See also video waveform.
FILE="Gl-5.online.bw" POSITION="INLINE" SCALE="FALSE"LBL="Gl-5"Figure Gl-5 Horizontal Blanking Intervalhorizontal driveThe portion of the horizontal blanking part of the video signal composed of the sync pulse together with the front porch and breezeway; that is, horizontal blanking minus the color burst. See also video waveform. horizontal syncThe lowest portion of the horizontal blanking part of the video signal, it provides a pulse for synchronizing video input with output. Also known as h sync. See also horizontal blanking and video waveform. HSISee hue-saturation-intensity.HSVHue-saturation-value; see hue-saturation-intensity.hueThe designation of a color in the spectrum, such as cyan, blue, magenta. Sometimes called tint on NTSC television receivers. The varying phase angles in the 3.58MHz (NTSC) or 4.43MHz (PAL) C signal indicate the different hues in the picture information.hue-saturation-intensity A tri-stimulus color system based on the parameters of hue, saturation, and intensity (luminance). Also referred to as HSI or HSV.I signalColor video signal transmitted as amplitude modulation of the 3.58MHz C signal (NTSC). The hue axis is orange and cyan. This signal is the only color video signal with a bandwidth of 0 to 1.3MHz.image planeSee bitplane.image processingManipulating an image by changing its color, brightness, shape, or size.interlaceA technique that uses more than one vertical scan to reproduce a complete image. In television, the 2:1 interlace used yields two vertical scans (fields) per frame: the first field consists of the odd lines of the frame, the other of the even lines. See also field and frame.IRE unitsA scale for measuring analog video signal levels, normally starting at the bottom of the horizontal sync pulse and extending to the top of peak white. Blanking level is 0 IRE units and peak white level is 100 IRE units (700mv). An IRE unit equals 7.14mv (+100 IRE to -40 IRE = 1v). IRE stands for Institute of Radio Engineers, a forerunner of the IEEE. keyingCombining proportional amounts of two frames, pixel by pixel, with optional opacity. This process resembles taking two panes of glass with images on them and placing one pane on top of the other. The opacity of the top pane determines the parts of the bottom pane that show. Usually, keying is a real-time continuous process, as in the "over the shoulder" graphics in TV news programs. The alpha component of each pixel, which defines its opacity, determines how the images are combined. Combining images based on the alpha component is often called alpha keying or luma keying. See also compositing and mixing.leading edge of syncThe portion of the video waveform after active video, between the sync threshold and the sync pulse. See also video waveform.levelSignal amplitude. lineThe result of a single pass of the sensor from left to right across the image.line blankingThe blanking signal at the end of each horizontal scanning line, used to make the horizontal retrace invisible. Also called horizontal blanking.line frequencyThe number of horizontal scans per second, normally 15,734.26 times per second for NTSC color systems. The line frequency for the PAL 625/50H. system is 15,625 times per second.line lockInput timing that is derived from the horizontal sync signal, also implying that the system clock (the clock being used to sample the incoming video) is an integer multiple of the horizontal frequency and that it is locked in phase to the horizontal sync signal. See also at video waveform.linear matrix transformationThe process of combining a group of signals through addition or subtraction; for example, RGB signals into luminance and chrominance signals. live videoVideo being delivered at a nominal frame rate appropriate to the format.lumaSee luminance.luminanceThe video signal that describes the amount of light in each pixel. Luminance is a weighted sum of the R, G, and B signals. See also chrominance and Y signal.mapNumerical lookup of pixel data that modifies each pixel without using neighboring pixels. This large category of video editing functions includes clip/gain, solarization, and histogram equalization.MII (M2)A second-generation recording format based on a version of the Y/R-Y/B-Y video signal. Developed by Panasonic, MII is also marketed by other video manufacturers. Though similar to Betacam, it is nonetheless incompatible.matrix transformationThe process of converting analog color signals from one tristimulus format to another, for example, RGB to YUV. See also tristimulus color system.mixingIn video editing, combining two clips frame by frame, pixel by pixel. Usually, a linear interpolation between the pixels in each clip is used, with which one can, for example, perform a cross-fade. Other operations include averaging, adding, differencing, maximum (non-additive mix), minimum, and equivalence (white where equal, else black). See also compositing and keying.multiburstA test pattern consisting of sets of vertical lines with closer and closer spacing; used for testing horizontal resolution of a video system. NTSCA color television standard or timing format encoding all of the color, brightness, and synchronizing information in one signal. Used in North America, most of South America, and most of the Far East, this standard is named after the National Television Systems Committee, the standardizing body that created this system in the U.S. in 1953. NTSC employs a total of 525 horizontal lines per frame, with two fields per frame of 262.5 lines each. Each field refreshes at 60Hz (actually 59.94Hz). Nyquist limitThe highest frequency of input signal that can be correctly sampled without aliasing. The Nyquist limit is equal to half of the sampling frequency. offsetIn the context of a video signal, the relative coordinates from the upper left corner of the video image where signal sampling begins.overscanTo scan a little beyond the display raster area of the monitor so that the edges of the raster are not visible. Television is overscanned; computer displays are underscanned.PAL A color television standard or timing format developed in West Germany and used by most other countries in Europe, including the United Kingdom but excluding France, as well as Australia and parts of the Far East. PAL employs a total of 625 horizontal lines per frame, with two fields per frame of 312.5 lines per frame. Each field refreshes at 50Hz. PAL encodes color differently from NTSC. PAL stands for Phase Alternation Line or Phase Alternated by Line, by which this system attempts to correct some of the color inaccuracies in NTSC. See also NTSC and SECAM.pathwayIn the Video Library, a connection of sources and drains that provide useful processing of video signals. Pathways have controls and video streams. Pathways can be locked for exclusive use, and are the target of events generated during video processing. See also exclusive use and event.pedestalSee setup; see also video waveform.pixelPicture element; the smallest addressable spatial element of the computer graphics screen. A digital image address, or the smallest reproducible element in analog video. A pixel can be monochrome, gray-scale, or color, and can have an alpha component to determine opacity or transparency. Pixels are referred to as having a color component and an alpha component, even if the color component is gray-scale or monochrome. pixel mapA two-dimensional piece of memory, any number of bits deep. See also bitmap. postproductionThe processes that occur before release of the finished video product, including editing, painting (2D graphics), production, and 3D graphics production.primary colorsRed, green, and blue. Opposite voltage polarities are the complementary colors cyan, magenta, and yellow.Q signalThe color video signal that modulates 3.58MHz C signal in quadrature with the I signal. Hues are green and magenta. Bandwidth is 0.0MHz to 0.5MHz. See also C signal, I signal, YC, and YIQ.quantization errorThe magnitude of the error introduced in a signal when the actual signal is between levels, resulting from subdividing a video signal into distinct increments, such as levels from 0 to 255.rasterThe scanning pattern for television display; a series of horizontal lines, usually left to right, top to bottom. In NTSC and PAL systems, the first and last lines are half lines.raster operation, raster opA logical or arithmetic operation on a pixel value. registrationThe process of causing two frames to coincide exactly. In component video cameras or displays, the process of causing the three color images to coincide exactly, so that no color fringes are visible.resolutionNumber of horizontal lines in a television display standard; the higher the number, the greater a system's ability to reproduce fine detail. RGBRed, green, blue; the basic component set used by graphics systems and some video cameras, in which a separate signal is used for each primary color. RGB formatThe technical specification for NTSC color television. Often (incorrectly) used to refer to an RGB signal that is being sent at NTSC composite timings, for example, a Silicon Graphics computer set to output 640 x 480. The timing would be correct to display on a television, but the signal would still be split into red, green and blue components. This component signal would have to go through an encoder to yield a composite signal (RS-170A format) suitable for display on a television receiver.R-Y (R minus Y) signalA color difference signal obtained by subtracting the luminance signal from the red camera signal. It is plotted on the 90 to 270 degree axis of a vector diagram. The R-Y signal drives the vertical axis of a vectorscope. The color mixture is close to red. Phase is in quadrature with B-Y; bandwidth is 0.0MHz to 0.5MHz. See also luminance, B-Y (B minus Y) signal, Y/R-Y/B-Y, and vectorscope.sampleTo read the value of a signal at evenly spaced points in time; to convert representational data to sampled data (that is, synthesizing and rendering). sampling rate, sample rateNumber of samples per second.saturationColor intensity; zero saturation is white (no color) and maximum saturation is the deepest or most intense color possible for that hue. Different saturation values are varying peak-to-peak amplitudes in the 3.58MHz modulated C signal. In signal terms, saturation is determined by the ratio between luminance level and chrominance amplitude. See also hue.scalingTo change the size of an image. scanTo convert an image to an electrical signal by moving a sensing point across the image, usually left to right, top to bottom.SECAMSequentiel Couleur avec Memoire, the color television system developed in France and used there as well as in eastern Europe, the Near East and Mideast, and parts of Africa and the Caribbean. setup The difference between the blackest level displayed on the receiver and the blanking level (see IDREF="24946" TYPE="GRAPHIC"Figure Gl-6). A black level that is elevated to 7.5 IRE instead of being left at 0.0 IRE, the same as the lowest level for active video. Because the video level is known, this part of the signal is used for black-level clamping circuit operation. Setup is typically used in the NTSC video format and is typically not used in the PAL video format; it was originally introduced to simplify the design of early television receivers, which had trouble distinguishing between video black levels and horizontal blanking. Also called pedestal. IDREF="24946" TYPE="GRAPHIC"Figure Gl-6 shows waveform displays of a signal with and without setup. See also video waveform.
FILE="Gl-6.online.bw" POSITION="INLINE" SCALE="FALSE"LBL="Gl-6"Figure Gl-6 ID="24946"Waveform Monitor Readings with and without Setup smearAn artifact usually caused by mid-frequency distortions in an analog system that results in the vertical edges of the picture spreading horizontally.SMPTE time codeA signal specified by the Society of Motion Picture and Television Engineers for facilitating videotape editing; this signal uniquely identifies each frame of the video signal. Program originators use vertical blanking interval lines 12 through 14 to store a code identifying program material, time, frame number, and other production information (see IDREF="59888" TYPE="GRAPHIC"Figure Gl-7).
FILE="Gl-7.online.bw" POSITION="INLINE" SCALE="FALSE"LBL="Gl-7"Figure Gl-7 ID="59888"SMPTE Time CodesourceIn the context of the Video Library, a provider of video input signals.subcarrierA portion of a video signal that carries a specific signal, such as color. See color subcarrier.subpixelA unit derived from a pixel by using a filter for sizing and positioning.S-VHS, S-VideoVideo format in which the Y (luminance) and C (chrominance) portions of the signal are kept separate. Also known as YC.sync informationThe part of the television video signal that ensures that the display scanning is synchronized with the broadcast scanning. See also video waveform.sync pulseA vertical or horizontal pulse (or both) that determines the display timing of a video signal. Composite sync has both horizontal and vertical sync pulses, as well as equalization pulses. The equalization pulses are part of the interlacing process.sync tipThe lowest part of the horizontal blanking interval, used for synchronization. See also video waveform.synchronizeTo perform time shifting so that things line up. texturingApplying images to three-dimensional objects to give additional realism to displayed renderings.terminationTo send a signal through a transmission line accurately, there must be an impedance at the end which matches the impedance of the source and of the line itself. Amplitude errors, frequency response, and pulse distortions and reflections (ghosting) occur on a line without proper termination. Video is a 75Ohm system; therefore a 75Ohm terminator of .5% to .25% accuracy must be installed at the end of the signal path.thresholdIn a digital circuit, the signal level that is specified as the division point between levels used to represent different digital values; for example, the sync threshold is the level at which the leading edge of sync begins. See also video waveform.time-base errorsAnalog artifacts caused by nonuniform motion of videotape or of the tape head drum. Time-base errors usually cause horizontal display problems, such as horizontal jitter.time codeSee SMPTE time code.time-delay equalizationFrame-by-frame alignment of all video inputs to one sync pulse, so that all frames start at the same time. This alignment is necessary because cable length differences cause unequal delays. See time-base errors.transcoderA device that converts a component video signal to a different component video signal, for example, RGB to Y/R-Y/B-Y, or D1 to RGB.transducerA microphone, video camera, or other device that can convert sounds or images to electrical signals.transformThe geometric perspective transformation of 3-D graphics models and planar images. tristimulus color systemA system of transmitting and reproducing images that uses three color signals, for example, RGB, YIQ, and YUV.U signalOne of the chrominance signals of the PAL color television system, along with V. Sometimes referred to as B-Y, but U becomes B-Y only after a weighting factor of 0.493 is applied. The weighting is required to reduce peak modulation in the composite signal.U-MaticSony trademark of its 3/4-inch composite videotape format. SP U-Matic is an improved version using metal tape.underscanTo scan a television screen so that the edges of the raster are visible. See also overscan.V signalOne of the chrominance signals of the PAL color television system, along with U. Sometimes referred to as R-Y, but V becomes R-Y only after a weighting factor of 0.877 is applied. The weighting is required to reduce peak modulation in the composite signal.vectorscopeA specialized oscilloscope that demodulates the video signal and presents a display of R-Y versus B-Y for NTSC (V and U for PAL). Video engineers use vectorscopes to measure the amplitude (gain) and phase angle (vector) of the primary (red, green, and blue) and the secondary (yellow, cyan, and magenta) color components of a television signal.vertical blanking The portion of the video signal that is blanked so that the vertical retrace of the beam is not visible. vertical blanking intervalThe blanking portion at the beginning of each field. It contains the equalizing pulses, the vertical sync pulses, and vertical interval test signals (VITS). Also the period when a scanning process is moving from the lowest horizontal line back to the top horizontal line. video levelVideo signal amplitude.video outputSee drain.video signalThe electrical signal produced by a scanning image sensor. videotape formatsIDREF="51580" TYPE="TABLE"Table Gl-1 lists major videotape formats.COLUMNS="5"LBL="Gl-1"Table Gl-1 ID="51580"Videotape FormatsLEFT="0" WIDTH="49"ElectronicsLEFT="55" WIDTH="94"ConsumerLEFT="155" WIDTH="128"ProfessionalLEFT="290" WIDTH="151"BroadcastLEFT="450" WIDTH="79"PostproductionLEFT="0" WIDTH="49"AnalogLEFT="55" WIDTH="94"VHS cassetteLEFT="155" WIDTH="128"U-Matic (SP) cassette, 3/4-inchLEFT="290" WIDTH="151"Type C reel-to-reel, 1-inch compositeLEFT="450" WIDTH="79"LEFT="0" WIDTH="49"LEFT="55" WIDTH="94"S-VHSLEFT="155" WIDTH="128"LEFT="290" WIDTH="151"Type B (Europe), compositeLEFT="450" WIDTH="79"LEFT="0" WIDTH="49"LEFT="55" WIDTH="94"S-Video (YC-358)LEFT="155" WIDTH="128"S-Video (YC-358)LEFT="290" WIDTH="151"LEFT="450" WIDTH="79"LEFT="0" WIDTH="49"LEFT="55" WIDTH="94"Beta8mmHi-8mm (YC)LEFT="155" WIDTH="128"Hi-8mm (YC)LEFT="290" WIDTH="151"Betacam (component)Type MII (component)LEFT="450" WIDTH="79"LEFT="0" WIDTH="49"DigitalLEFT="55" WIDTH="94"LEFT="155" WIDTH="128"LEFT="290" WIDTH="151"LEFT="450" WIDTH="79"D1 525/625 (YUV)D2 525 (NTSC)D2 625 (PAL)video waveform The main components of the video waveform are the active video portion and the horizontal blanking portion. Certain video waveforms carry information during the horizontal blanking interval.IDREF="16680" TYPE="GRAPHIC"Figure Gl-8 and IDREF="30932" TYPE="GRAPHIC"Figure Gl-9 diagram a typical red or blue signal, which carries no information during the horizontal blanking interval, and a typical Y or green-plus-sync signal, which carries a sync pulse.
FILE="Gl-8.online.bw" POSITION="INLINE" SCALE="FALSE"LBL="Gl-8"Figure Gl-8 ID="16680"Red or Blue SignalFILE="Gl-9.online.bw" POSITION="INLINE" SCALE="FALSE"LBL="Gl-9"Figure Gl-9 ID="30932"Y or Green Plus Sync SignalIDREF="23868" TYPE="GRAPHIC"Figure Gl-10 and IDREF="35903" TYPE="GRAPHIC"Figure Gl-11 show the video waveform and its components for composite video in more detail. The figures show the composite video waveform with and without setup, respectively.IDREF="23868" TYPE="GRAPHIC"Figure Gl-10 shows a composite video signal with setup.
FILE="Gl-10.online.bw" POSITION="INLINE" SCALE="FALSE"LBL="Gl-10"Figure Gl-10 ID="23868"Video Waveform: Composite Video Signal With Setup (Typical NTSC)IDREF="35903" TYPE="GRAPHIC"Figure Gl-11 shows a composite video signal without setup.FILE="Gl-11.online.bw" POSITION="INLINE" SCALE="FALSE"LBL="Gl-11"Figure Gl-11 ID="35903"Video Waveform: Composite Video Signal (Typical PAL)white levelIn the active video portion of the video waveform, the 1.0-volt (100 IRE) level. See also video waveform.Y signalLuminance, corresponding to the brightness of an image. See also luminance and Y/R-Y/B-Y.YC A color space (color component encoding format) based on YIQ or YUV. Y is luminance, but the two chroma signals (I and Q or U and V) are combined into a composite chroma called C, resulting in a two-wire signal. C is derived from I and Q as follows: C - I cos(2\xb9 fsct) + Q sin(2\xb9 fsct)where fsc is the subcarrier frequency. YC-358 is the NTSC version of this luminance/chrominance format; YC-443 is the PAL version. Both are referred to as S-Video formats.YIQA color space (color component encoding format) used in decoding, in which Y is the luminance signal and I and Q are the chrominance signals. The two chrominance signals I and Q (in-phase and quadrature, respectively) are two-phase amplitude-modulated; the I component modulates the subcarrier at an angle of 0 degrees and the Q component modulates it at 90 degrees. The color burst is at 33 degrees relative to the Q signal.The amplitude of the color subcarrier represents the saturation values of the image; the phase of the color subcarrier represents the hue value of the image.Y = 0.299R + 0.587G + 0.114BI = 0.596R - 0.275G - 0.321BQ = 0.212R - 0.523G + 0.311BY/R-Y/B-YA name for the YUV color component encoding format that summarizes how the chrominance components are derived. Y is the luminance signal and R-Y and B-Y are the chrominance signals. R-Y (red minus Y) and B-Y (blue minus Y) are the color differences or chrominance components. The color difference signals R-Y and B-Y are derived as follows:Y = 0.299R + 0.587 + 0.114BY/R-Y/B-Y has many variations, just as NTSC and PAL do. All component and composite color encoding formats are derived from RGB without scan standards being changed. The matrix (amount of red, green, and blue) values and scale (amplitude) factors can differ from one component format to another (YUV, Y/R-Y, B-Y, SMPTE Y/R-Y, B-Y).YUVA color space (color component encoding format) used by the PAL video standard, in which Y is the luminance signal and U and V are the chrominance signals. The two chrominance signals U and V are two-phase amplitude-modulated. The U component modulates the subcarrier at an angle of 0 degree, but the V component modulates it at 90 degrees or 180 degrees on alternate lines. The color burst is also line-alternated at +135 and -135 degrees relative to the U signal. The YUV matrix multiplier derives colors from RGB via the following formula:Y = .299R + .587 G + .114 BCR = R - YCB = B - YIn this formula, Y represents luminance; red and blue are derived from it: CR denotes red and (V), CB denotes blue. V corresponds to CR; U corresponds to CB c. The U and V signals are carried on the same bandwidth. This system is sometimes referred to as Y/R-Y/B-Y. The name for this color encoding method is YUV, despite the fact that the order of the signals according to the formula is YVU.4-channel audiocablingIDREF="Media2-2HW18"4-channel Audio I/O Interfaceconfiguring hardwareIDREF="Media2-4AL136"Querying and Controlling the Global Audio Device StateframesillustratedIDREF="Media2-4AL17"Digital Audio Sample FramesIndigo2IDREF="Media2-2HW13"Indigo2 and Indy Audio I/O InterfaceIndyIDREF="Media2-2HW11"Indigo2 and Indy Audio System ArchitectureinputIDREF="Media2-4AL109"Reading Samples from an Input ALportoutputIDREF="Media2-4AL115"Writing Samples to an Output ALportqueryingIDREF="Media2-4AL160"Determining Whether 4-channel Capability Exists8mm videoIDREF="Media3-1VB56"Videotape FormatsaddingmovieparametersIDREF="Media6-1GS64"Adding Your Own Parameters to the Movie Library parameters:exampleIDREF="Media6-1GS68"Adding Your Own Parameters to the Movie Libraryparameters:exampleIDREF="Media6-1GS70"Adding Your Own Parameters to the Movie LibrarytracksIDREF="Media6-2IO40"Adding an Audio or Image Track to a Moviemovie tracksexampleIDREF="Media6-2IO42"Adding an Audio or Image Track to a Movieadding algorithms to the Compression LibraryIDREF="Media5-3UP7"Adding Custom Algorithms to the Compression LibraryADPCMIDREF="Media2-5AF72"Initializing Audio Track CompressionAESchannel status bytesIDREF="Media2-5AF66"Initializing AES DatajacksIDREF="Media2-2HW6"Indigo Audio I/O InterfaceIDREF="Media2-5AF63"Initializing AES DataresolutionsIDREF="Media2-4AL25"Digital Audio Input and Output Sample ResolutionsIDREF="Media2-4AL21"Digital Audio Sample FormatsstandardIDREF="Media2-5AF64"Initializing AES DataAES3-1985 (ANSI S4.40-1985)IDREF="Media2-5AF64"Initializing AES DataAF LibraryIDREF="Media2-5AF3"Programming with the Audio File LibraryAFfilehandleIDREF="Media2-5AF35"Creating an Audio File SetupchunksIDREF="Media2-5AF9"About Audio FilesIDREF="Media2-5AF32"AIFF-C and the AF Library APIcompilingIDREF="Media2-3SW51"Compiling and Linking an Audio Applicationerror handlingIDREF="Media2-5AF7"Handling Audio File Library ErrorsexampleIDREF="Media2-5AF123"Sample Audio File Programfile formatsIDREF="Media2-5AF51"Initializing Audio File FormatIDREF="Media2-5AF4"Programming with the Audio File LibrarytasksIDREF="Media2-5AF5"Programming with the Audio File LibraryAF_FILE_AIFFIDREF="Media2-5AF53"Initializing Audio File FormatAF_FILE_AIFFCIDREF="Media2-5AF52"Initializing Audio File FormatAFfilehandleIDREF="Media2-5AF35"Creating an Audio File SetupAFfilesetupIDREF="Media2-5AF37"Creating an Audio File SetupIDREF="Media2-5AF34"Creating an Audio File SetupcreatingIDREF="Media2-5AF36"Creating an Audio File SetupdefaultsIDREF="Media2-5AF38"Creating an Audio File SetupfreeingIDREF="Media2-5AF47"Creating an Audio File SetupparametersIDREF="Media2-5AF37"Creating an Audio File SetupsettingIDREF="Media2-5AF50"Initializing Audio File FormatAFLibraryAFfilesetupIDREF="Media2-5AF34"Creating an Audio File SetupAIFFIDREF="Media2-5AF28"AIFF-C and the AF Library APIAIFF-CIDREF="Media2-5AF29"AIFF-C and the AF Library APIIDREF="Media2-5AF8"About Audio FilesIDREF="Media2-5AF25"AIFF-C and the AF Library APIchunksIDREF="Media2-5AF31"AIFF-C and the AF Library APIALconfigsIDREF="Media2-4AL38"Using ALconfig Structures to Configure ALportscloningIDREF="Media2-4AL81"Retrieving the Setup of an Existing ALportcreatingIDREF="Media2-4AL47"Creating a New ALconfigdefaultIDREF="Media2-4AL40"Using ALconfig Structures to Configure ALportsdefinedIDREF="Media2-4AL8"Audio Library Programming ModelfreeingIDREF="Media2-4AL84"Retrieving the Setup of an Existing ALportALerrfuncIDREF="Media2-4AL32"Handling Audio Library Errorsalgorithm-independent compressionIDREF="Media5-0ZIN11"Compression Library Featuresalgorithmsadding to the Compression LibraryIDREF="Media5-3UP1"Customizing the Compression LibraryCompression LibraryIDREF="Media5-2AP1"Using Compression Library Algorithms and ParametersallocatingIDREF="Media2-5AF80"Initializing Miscellaneous DatabuffersaudioIDREF="Media1-2DM39"Determining the Buffer Size Needed to Store an Audio FrameimageIDREF="Media1-2DM44"Setting Image Defaultsparameter-value listsIDREF="Media1-2DM26"Creating and Destroying Parameter-value ListsALportsIDREF="Media2-4AL36"About ALportsallocating and initializingIDREF="Media2-4AL86"Opening and Closing Audio Portsclosing and deallocatingIDREF="Media2-4AL90"Opening and Closing Audio PortsconfiguringIDREF="Media2-4AL39"Using ALconfig Structures to Configure ALportsexampleIDREF="Media2-4AL46"Using ALconfig Structures to Configure ALportscountingIDREF="Media2-4AL132"Querying and Controlling the Global Audio Device StatedefinedIDREF="Media2-4AL5"Audio Library Programming ModelfeaturesIDREF="Media2-4AL37"About ALportsopening and closingIDREF="Media2-4AL85"Opening and Closing Audio PortsexampleIDREF="Media2-4AL91"Opening and Closing Audio Portsstatic settingsIDREF="Media2-4AL45"Using ALconfig Structures to Configure ALportsanalog-to-digital (A/D) convertersIDREF="Media2-4AL24"Digital Audio Input and Output Sample ResolutionsanimationIDREF="Media5-0ZIN7"Compression Library ApplicationsANSI CIDREF="Media2-3SW69"Compiling and Linking an Audio ApplicationAPICompression LibraryIDREF="Media5-1GS1"Getting Started with the Compression LibraryApple Computer, Inc.AIFF formatIDREF="Media2-5AF26"AIFF-C and the AF Library APIapplicationsMovie LibraryIDREF="Media6-0ZIN2"Movie Library ApplicationsarenassharedIDREF="Media2-4AL185"Using Shared Arenas and SemaphoresexampleIDREF="Media2-4AL198"Using Shared Arenas and SemaphoresassertionsDM LibraryIDREF="Media1-2DM21"Debugging a Digital Media Library ApplicationMovie LibraryIDREF="Media6-1GS20"Using the Debugging Version of the Movie LibraryattenuationaudioIDREF="Media2-4AL128"Querying and Controlling the Global Audio Device StateIDREF="Media2-4AL138"Querying and Controlling the Global Audio Device Stateaudio4-channelcablingIDREF="Media2-2HW18"4-channel Audio I/O Interfaceadding a movie soundtrackIDREF="Media6-2IO43"Adding an Audio or Image Track to a MovieattenuationIDREF="Media2-4AL128"Querying and Controlling the Global Audio Device StateIDREF="Media2-4AL138"Querying and Controlling the Global Audio Device StatebreakscauseIDREF="Media2-4AL93"Using Audio Sample QueuesIDREF="Media2-4AL56"Setting and Getting the Sample Queue Size for an ALconfigtroubleshootingIDREF="Media2-4AL118"Detecting Errors in the Audio Streambuffer sizeIDREF="Media1-2DM39"Determining the Buffer Size Needed to Store an Audio FrameCD Audio LibraryIDREF="Media2-6CD2"Programming with the CD Audio Librarychangingglobal stateIDREF="Media2-4AL154"Modifying the Values of the Global Parameterschannels for moviesIDREF="Media6-1GS109"Getting the Number of Audio Channels in an Audio TrackclippingIDREF="Media2-3SW15"How Outputs from Multiple Audio Applications Are CombinedcompressionAware, Inc.IDREF="Media2-3SW39"Aware Audio Compression Software and Audio Productscompression for moviesIDREF="Media6-1GS116"Getting the Audio Compression Scheme of an Audio TrackconcurrentIDREF="Media2-3SW24"Programming Guidelines for Managing System-Wide ResourcesIDREF="Media2-3SW4"About Shared System-Wide ResourcesIDREF="Media2-3SW12"How Audio Applications Share Audio System Resourcesconfidence testsIDREF="Media2-3SW35"Graphical User Interface Audio ToolsconfigurationsIDREF="Media2-4AL38"Using ALconfig Structures to Configure ALportscloningIDREF="Media2-4AL81"Retrieving the Setup of an Existing ALportconnectionsIDREF="Media2-4AL35"Initializing an Audio Library ApplicationIDREF="Media2-4AL7"Audio Library Programming ModelconversionsIDREF="Media2-4AL26"Digital Audio Input and Output Sample Resolutionscore global parametersIDREF="Media2-4AL126"Querying and Controlling the Global Audio Device StatedefaultsIDREF="Media1-2DM36"Setting Audio Defaultsfloating point rangeIDREF="Media2-4AL78"Getting and Setting the Floating Point Data RangeI/O rateIDREF="Media2-3SW20"How Global Audio Settings Are Established and MaintainedmovieIDREF="Media6-1GS91"Setting and Getting Audio Track PropertiesportIDREF="Media2-4AL40"Using ALconfig Structures to Configure ALportsdevicesIDREF="Media2-4AL4"Audio Library Programming ModeldigitizingIDREF="Media2-4AL9"Digital Audio Data RepresentationdiscontinuitiesIDREF="Media2-4AL119"Detecting Errors in the Audio Streamdisk spaceIDREF="Media2-2HW21"Disk SpacedistortionIDREF="Media2-4AL155"Modifying the Values of the Global Parameterserror handlersIDREF="Media2-4AL30"Handling Audio Library ErrorsexamplesIDREF="Media2-3SW36"Online Source Code Examplesfile descriptorsIDREF="Media2-4AL167"Multiplexing Synchronous I/Ofill pointsIDREF="Media2-4AL175"Setting and Getting the Fill Point for a QueueillustratedIDREF="Media2-4AL179"Setting and Getting the Fill Point for a QueueformatsIDREF="Media2-4AL20"Digital Audio Sample Formatsformats for moviesIDREF="Media6-1GS113"Getting the Audio Format of an Audio TrackframesIDREF="Media2-4AL15"Digital Audio Sample FramesillustratedIDREF="Media2-4AL18"Digital Audio Sample Framesgetting file descriptorsIDREF="Media2-4AL169"Getting a File Descriptor for an ALportglobal stateIDREF="Media2-3SW22"How Global Audio Settings Are Established and MaintainedIDREF="Media2-4AL124"Querying and Controlling the Global Audio Device Statehardware specificationsIDREF="MediaA-1A1"Audio SpecificationsIDREF="MediaA-1A2"Indigo2 Workstation Audio Hardware SpecificationsI/OerrorsIDREF="Media2-4AL117"Detecting Errors in the Audio StreammutiplexingIDREF="Media2-4AL166"Multiplexing Synchronous I/OiconsIDREF="Media2-2HW5"Indigo Audio I/O InterfaceIndigofeaturesIDREF="Media2-2HW2"Indigo Audio FeatureshardwareIDREF="Media2-2HW1"Indigo Audio System ArchitecturejacksIDREF="Media2-2HW4"Indigo Audio I/O InterfaceIndigo2featuresIDREF="Media2-2HW8"Indigo2 and Indy Audio System ArchitecturehardwareIDREF="Media2-2HW7"Indigo2 and Indy Audio System ArchitecturejacksIDREF="Media2-2HW12"Indigo2 and Indy Audio I/O InterfaceIDREF="Media2-2HW14"Indigo2 and Indy Audio I/O InterfaceIndyfeaturesIDREF="Media2-2HW9"Indigo2 and Indy Audio System ArchitecturehardwareIDREF="Media2-2HW9"Indigo2 and Indy Audio System ArchitecturejacksIDREF="Media2-2HW17"Indy Workstation Layoutvolume buttonsIDREF="Media2-2HW16"Indy Workstation LayoutinputIDREF="Media2-4AL108"Reading Samples from an Input ALport4-channelIDREF="Media2-4AL109"Reading Samples from an Input ALportconversionsIDREF="Media2-4AL110"Reading Samples from an Input ALportinterleavingIDREF="Media2-4AL16"Digital Audio Sample FrameslibrariesIDREF="Media1-1ML4"About the Digital Audio and MIDI LibrarieslimitingIDREF="Media2-3SW16"How Outputs from Multiple Audio Applications Are Combinedmemory requirementsIDREF="Media2-2HW19"Recommendations for Audio Development System ConfigurationsmonitoringIDREF="Media2-4AL133"Querying and Controlling the Global Audio Device Statemoviedefaults:volumeIDREF="Media6-1GS98"Setting and Getting the Default Volume of an Audio Tracknative formatsIDREF="Media6-1GS112"Getting the Audio Format of an Audio TrackpropertiesIDREF="Media6-1GS88"Setting and Getting Audio Track Propertiestrack propertiesIDREF="Media6-1GS94"Setting and Getting Audio Track Propertiesnative formatsIDREF="Media2-4AL22"Digital Audio Input and Output Sample Resolutionsnonblocking I/OIDREF="Media2-4AL101"Monitoring the Audio Sample Queue Status to Provide Nonblocking I/ONyquist TheoremIDREF="Media2-4AL10"Digital Audio Data RepresentationoutputIDREF="Media2-4AL112"Writing Samples to an Output ALportconversionsIDREF="Media2-4AL116"Writing Samples to an Output ALportoverflowIDREF="Media2-4AL105"More Methods for Working with Queuesoverflow and underflowIDREF="Media2-4AL123"Detecting Errors in the Audio StreamillustratedIDREF="Media2-4AL94"Using Audio Sample Queuesparameter-value bufferIDREF="Media2-4AL140"Techniques for Working with Global ParametersparametersIDREF="Media1-2DM34"Audio Parameterscurrent valueIDREF="Media2-4AL151"Getting Current Parameter Settingsgetting and settingIDREF="Media2-4AL141"Techniques for Working with Global ParametersnamesIDREF="Media2-4AL148"Getting the Names Corresponding to the Global ParameterssettingIDREF="Media2-4AL143"Getting a List of Available ParametersIDREF="Media2-4AL153"Modifying the Values of the Global Parametersspecial featuresIDREF="Media2-4AL135"Querying and Controlling the Global Audio Device StateperformancetuningIDREF="Media2-4AL120"Detecting Errors in the Audio StreamportsIDREF="Media2-4AL36"About ALportsallocating and initializingIDREF="Media2-4AL86"Opening and Closing Audio PortschannelsIDREF="Media2-4AL50"Setting and Getting the Number of Channels for an ALconfigclosing and deallocatingIDREF="Media2-4AL90"Opening and Closing Audio PortsconfiguringIDREF="Media2-4AL39"Using ALconfig Structures to Configure ALportscountingIDREF="Media2-4AL132"Querying and Controlling the Global Audio Device StatedefaultIDREF="Media2-4AL40"Using ALconfig Structures to Configure ALportsdefinedIDREF="Media2-4AL7"Audio Library Programming ModelexampleIDREF="Media2-4AL46"Using ALconfig Structures to Configure ALportsformatsIDREF="Media2-4AL63"Setting and Getting the Sample Data Format for an ALconfigmoviesIDREF="Media6-3PB38"Binding a Window to a Movie with an Audio TracknamesIDREF="Media2-4AL88"Opening and Closing Audio Portsopening and closingIDREF="Media2-4AL85"Opening and Closing Audio Portsopening and closing:exampleIDREF="Media2-4AL91"Opening and Closing Audio Portsqueue sizeIDREF="Media2-4AL60"Setting and Getting the Sample Queue Size for an ALconfigstatic settingsIDREF="Media2-4AL45"Using ALconfig Structures to Configure ALportsprecisionIDREF="Media2-4AL69"Setting and Getting the Integer Sample Width for an ALconfigprioritizingIDREF="Media2-4AL181"Using Scheduling Control to Give Audio High PrioritypriorityIDREF="Media2-3SW7"How Audio Applications Share CPU Resourcesprocess controlIDREF="Media2-4AL184"Preventing Memory SwapoutprogrammingguidelinesIDREF="Media2-3SW23"Programming Guidelines for Managing System-Wide ResourcesqualityIDREF="Media2-4AL13"Digital Audio Sample Ratesquantization stepsIDREF="Media2-4AL79"Getting and Setting the Floating Point Data Rangequerying4-channelIDREF="Media2-4AL160"Determining Whether 4-channel Capability Existsconcurrent processesIDREF="Media2-4AL157"Determining Whether Other Audio Applications Are RunningI/O ratesIDREF="Media2-4AL158"Determining the Input and Output Ratessupported featuresIDREF="Media2-4AL125"Querying and Controlling the Global Audio Device StatequeuesIDREF="Media2-4AL92"Using Audio Sample QueuesillustratedIDREF="Media2-4AL94"Using Audio Sample QueuessizeIDREF="Media2-4AL55"Setting and Getting the Sample Queue Size for an ALconfigIDREF="Media2-4AL98"Monitoring the Audio Sample Queue Status to Provide Nonblocking I/Osize limitsIDREF="Media2-4AL58"Setting and Getting the Sample Queue Size for an ALconfigstatusIDREF="Media2-4AL95"Monitoring the Audio Sample Queue Status to Provide Nonblocking I/OthresholdsIDREF="Media2-4AL174"Setting and Getting the Fill Point for a Queuereading and writing dataIDREF="Media2-4AL106"Reading and Writing Samplesreal-time programmingIDREF="Media2-4AL164"Real-time Programming Techniques for AudioreferencesIDREF="Media2-1GS2"Introduction to Digital Audio and MIDI ProgrammingresolutionsIDREF="Media2-4AL73"Setting and Getting the Integer Sample Width for an ALconfigIDREF="Media2-4AL23"Digital Audio Input and Output Sample Resolutionssample rate for moviesIDREF="Media6-1GS106"Getting the Audio Sample Rate of an Audio Tracksample width for moviesIDREF="Media6-1GS103"Getting the Audio Sample Width of an Audio Tracksample widthsgetting and settingIDREF="Media2-4AL68"Setting and Getting the Integer Sample Width for an ALconfigsamplersIDREF="Media2-5AF18"Instrument Configurations and LoopssamplingIDREF="Media2-4AL9"Digital Audio Data Representationsampling from CDexampleIDREF="Media2-6CD95"CD Sample Programsampling ratesIDREF="Media2-2HW3"Indigo Audio Featuresscheduling controlIDREF="Media2-4AL181"Using Scheduling Control to Give Audio High Priorityselecting inputs and outputsIDREF="Media2-4AL127"Querying and Controlling the Global Audio Device StatesilencecauseIDREF="Media2-4AL93"Using Audio Sample QueuestroubleshootingIDREF="Media2-4AL118"Detecting Errors in the Audio Streamsystem softwareIDREF="Media2-3SW1"Digital Audio System Softwarethird-party softwareIDREF="Media2-3SW38"Third-party Audio Software and Sound LibrariesthresholdIDREF="Media2-4AL174"Setting and Getting the Fill Point for a Queuetime required for output IDREF="Media2-4AL113"Writing Samples to an Output ALporttoolsIDREF="Media2-3SW25"Graphical User Interface Audio ToolsunderflowIDREF="Media2-4AL104"More Methods for Working with Queueswriting samplesIDREF="Media2-4AL112"Writing Samples to an Output ALportAudio Engineering Society. See AESIDREF="Media2-2HW6"Indigo Audio I/O InterfaceAudio File (AF) Library. See AF LibraryIDREF="Media2-5AF1"Programming with the Audio File LibraryAudio File LibrarypurposeIDREF="Media1-1ML6"About the Digital Audio and MIDI LibrariesAudio File Library, libaudiofileaccessing Aware compression fromIDREF="MediaA-2B8"Accessing Aware Audio Compression from the Audio File Libraryaudio filesAESIDREF="Media2-5AF93"Getting AES DataAES channel status bytesIDREF="Media2-5AF66"Initializing AES DataAIFF-CIDREF="Media2-5AF8"About Audio FileschannelsIDREF="Media2-5AF67"Initializing AES DataIDREF="Media2-5AF59"Initializing Audio Track ChannelsclosingIDREF="Media2-5AF84"Closing and Updating FilescompressionIDREF="Media2-5AF71"Initializing Audio Track CompressionIDREF="Media2-5AF94"Getting Audio Track CompressionIDREF="Media2-5AF70"Initializing Audio Track CompressionIDREF="Media2-5AF29"AIFF-C and the AF Library APInameIDREF="Media2-5AF96"Getting Audio Track Compressionconfiguring tracksIDREF="Media2-5AF54"Initializing Audio Track DatadefaultsIDREF="Media2-5AF38"Creating an Audio File SetupeditingsoundeditorIDREF="Media2-3SW32"Graphical User Interface Audio ToolsemphasisIDREF="Media2-5AF60"Initializing AES Datafile descriptorsIDREF="Media2-5AF82"Getting an IRIX File Descriptor for an Audio FileIDREF="Media2-5AF83"Getting an IRIX File Descriptor for an Audio Filefile formatsIDREF="Media2-5AF87"Getting Audio File FormatIDREF="Media2-5AF88"Getting Audio File FormatformatsIDREF="Media2-5AF90"Getting Audio Track Sample Format and Sample WidthIDREF="Media2-5AF10"About Audio FilesIDREF="Media2-5AF91"Getting Audio Track Sample Format and Sample WidthIDREF="Media2-5AF57"Initializing Audio Track Sample Format and Sample Widthframe countsIDREF="Media2-5AF97"Getting Audio Track Sample Frame CountframesIDREF="Media2-5AF103"Reading Audio Frames from an Audio TrackIDREF="Media2-5AF15"Audio Tracks, Sample Frames, and Track MarkersinitializingformatIDREF="Media2-5AF49"Initializing Audio File Formatinitializing tracksIDREF="Media2-5AF55"Initializing Audio Track Datainstrument configurationsIDREF="Media2-5AF75"Initializing Instrument DataIDREF="Media2-5AF105"Reading and Writing Instrument ConfigurationsIDREF="Media2-5AF17"Instrument Configurations and LoopsdefinedIDREF="Media2-5AF13"About Audio FilesIDsIDREF="Media2-5AF106"Getting and Setting Instrument ParametersparametersIDREF="Media2-5AF107"Getting and Setting Instrument ParameterswritingIDREF="Media2-5AF108"Getting and Setting Instrument ParametersloopsIDREF="Media2-5AF22"Instrument Configurations and LoopsIDREF="Media2-5AF76"Initializing Instrument DataendingIDREF="Media2-5AF113"Getting and Setting Loop InformationIDREF="Media2-5AF114"Getting and Setting Loop Informationgetting and settingIDREF="Media2-5AF109"Getting and Setting Loop InformationmodeIDREF="Media2-5AF110"Getting and Setting Loop InformationstartIDREF="Media2-5AF111"Getting and Setting Loop InformationmarkersIDREF="Media2-5AF12"About Audio FilesIDREF="Media2-5AF98"Getting and Setting Audio Track MarkersIDsIDREF="Media2-5AF73"Initializing Audio Track MarkersnamesIDREF="Media2-5AF99"Getting and Setting Audio Track MarkerspositionIDREF="Media2-5AF100"Getting and Setting Audio Track MarkerstrackIDREF="Media2-5AF16"Audio Tracks, Sample Frames, and Track Markersmiscellaneous chunksIDREF="Media2-5AF115"Handling Miscellaneous Data ChunksIDREF="Media2-5AF80"Initializing Miscellaneous DataIDsIDREF="Media2-5AF77"Initializing Miscellaneous DataparametersIDREF="Media2-5AF116"Getting Miscellaneous Data ParametersreadingIDREF="Media2-5AF119"Reading, Writing, and Seeking Miscellaneous DataseekingIDREF="Media2-5AF121"Reading, Writing, and Seeking Miscellaneous DatasizeIDREF="Media2-5AF118"Getting Miscellaneous Data ParameterstypesIDREF="Media2-5AF117"Getting Miscellaneous Data ParametersIDREF="Media2-5AF78"Initializing Miscellaneous DataIDREF="Media2-5AF79"Initializing Miscellaneous DatawritingIDREF="Media2-5AF120"Reading, Writing, and Seeking Miscellaneous DataopeningIDREF="Media2-5AF81"Opening an Audio FilepreviewingIDREF="Media2-3SW34"Graphical User Interface Audio Toolsreading and writingIDREF="Media2-5AF86"Reading and Writing Audio Track Informationsampling rateIDREF="Media2-5AF89"Getting Audio Track Sample RateseekingIDREF="Media2-5AF102"Seeking to a Position in an Audio File TracksetupIDREF="Media2-5AF6"Audio File Library Programming ModeltracksIDREF="Media2-5AF14"Audio Tracks, Sample Frames, and Track MarkersdefinedIDREF="Media2-5AF11"About Audio FilesupdatingIDREF="Media2-5AF85"Closing and Updating FileswritingIDREF="Media2-5AF85"Closing and Updating FilesIDREF="Media2-5AF104"Writing Audio Frames to an Audio Trackaudio I/OIDREF="Media2-4AL106"Reading and Writing SamplesAudio LibraryIDREF="Media2-4AL1"Programming with the Audio LibraryALconfigsIDREF="Media2-4AL8"Audio Library Programming ModelALportsIDREF="Media2-4AL5"Audio Library Programming ModelcompilingIDREF="Media2-3SW49"Compiling and Linking an Audio Applicationdata typesIDREF="Media2-4AL72"Setting and Getting the Integer Sample Width for an ALconfigerror handlingIDREF="Media2-4AL27"Handling Audio Library ErrorsexamplesIDREF="Media2-4AL156"Sample Code for Querying Features and ValuesIDREF="Media2-4AL195"Using Shared Arenas and SemaphoresfeaturesIDREF="Media2-4AL2"Audio Library FeaturesinitializingIDREF="Media2-4AL34"Initializing an Audio Library ApplicationprogrammingmodelIDREF="Media2-4AL3"Audio Library Programming ModeloutlineIDREF="Media2-4AL33"Audio Library Application Programming ConceptspurposeIDREF="Media1-1ML5"About the Digital Audio and MIDI LibrariesAudio Utility LibrarypurposeIDREF="Media1-1ML7"About the Digital Audio and MIDI Libraries Awarecompression softwareIDREF="Media5-0ZIN2"Introduction to the Compression LibraryAwareaudio compressionaccessing from the ALIDREF="MediaA-2B8"Accessing Aware Audio Compression from the Audio File Library accessing from the CLIDREF="MediaA-2B13"Accessing Aware Audio Compression from the Compression Librarysoftware enginesIDREF="MediaA-2B1"Aware Scalable Audio Compression Softwareaudio compression for multimedia applicationsIDREF="Media5-2AP6"Choosing a Compression Library Algorithmaudio librariesIDREF="Media2-2HW23"PeripheralsAudioPlaybackIDREF="MediaA-2B5"Aware Products Available in IRIS Digital Media LibrariesAudioProducerIDREF="MediaA-2B4"Aware Products Available in IRIS Digital Media LibrariesAudioPublisherIDREF="MediaA-2B3"Aware Products Available in IRIS Digital Media LibrariesAudioSuiteIDREF="MediaA-2B6"Aware AudioSuite Toolscompression software specificationsIDREF="MediaA-2B19"Aware Audio Compression Software SpecificationsMPEG noise-to-mask ratioIDREF="MediaA-2B18"Using Compression Library ParametersMultiRateIDREF="MediaA-2B11"Compression Custom ConfigurationIDREF="MediaA-2B14"Compression SchemesIDREF="MediaA-2B10"Compression DefaultsIDREF="MediaA-2B15"Compression SchemesIDREF="MediaA-2B12"Compression Custom ConfigurationIDREF="MediaA-2B16"Using Compression Library ParametersIDREF="MediaA-2B9"Valid Audio Input DataIDREF="MediaA-2B17"Using Compression Library ParametersMultiRate specificationsIDREF="MediaA-2B20"Aware Audio Compression Software Specificationsobtaining software licensesIDREF="MediaA-2B2"Introduction to Aware Audio Compression SoftwareSpeed-of-Sound libraryIDREF="MediaA-2B7"Aware Speed-of-Sound Library, Volume I, Sound EffectsAware, Inc.audio compression softwareIDREF="Media2-3SW39"Aware Audio Compression Software and Audio ProductscontactingIDREF="Media2-3SW42"Aware Audio Compression Software and Audio ProductsB-Y video signalIDREF="Media3-1VB26"YUVBetacamIDREF="Media3-1VB58"Videotape FormatsIDREF="Media3-1VB20"YUVBetacam SPIDREF="Media3-1VB59"Videotape Formatsbindingmovies to windowsIDREF="Media6-3PB34"Binding a Movie to a Window for Playbackaudio considerationsIDREF="Media6-3PB37"Binding a Window to a Movie with an Audio Trackmultiple moviesIDREF="Media6-3PB40"Playing Multiple Movies in the Same WindowblockingaudiopreventingIDREF="Media2-4AL101"Monitoring the Audio Sample Queue Status to Provide Nonblocking I/OblocksCDIDREF="Media2-6CD56"Seeking to a CD LocationseekingIDREF="Media2-6CD57"Seeking to a CD Location­cckrIDREF="Media2-3SW71"Compiling and Linking an Audio Application­laudioIDREF="Media2-3SW50"Compiling and Linking an Audio Application­laudiofileIDREF="Media2-3SW52"Compiling and Linking an Audio Application­lcdaudioIDREF="Media2-3SW59"Compiling and Linking an Audio Application­ldataudioIDREF="Media2-3SW65"Compiling and Linking an Audio Application-laudioutilIDREF="Media2-3SW53"Compiling and Linking an Audio Application-ldmediaIDREF="Media1-2DM20"Compiling and Linking a Digital Media Library Application-ldmedia_dIDREF="Media1-2DM22"Debugging a Digital Media Library Application-libmediadIDREF="Media2-3SW60"Compiling and Linking an Audio Application-lmIDREF="Media2-3SW54"Compiling and Linking an Audio ApplicationAFclosefile()IDREF="Media2-5AF84"Closing and Updating FilesAFfreefilesetup()IDREF="Media2-5AF46"Creating an Audio File SetupAFgetaeschanneldata()IDREF="Media2-5AF93"Getting AES DataAFgetchannels()IDREF="Media2-5AF92"Getting Audio Track ChannelsAFgetcompressionIDREF="Media2-5AF94"Getting Audio Track CompressionAFgetcompressionname()IDREF="Media2-5AF96"Getting Audio Track CompressionAFgetcompressionparams()IDREF="Media2-5AF95"Getting Audio Track CompressionAFgetfd()IDREF="Media2-5AF83"Getting an IRIX File Descriptor for an Audio FileAFgetfilefmt()IDREF="Media2-5AF87"Getting Audio File FormatAFgetframecnt()IDREF="Media2-5AF97"Getting Audio Track Sample Frame CountAFgetinstids()IDREF="Media2-5AF106"Getting and Setting Instrument ParametersAFgetinstparamlong()IDREF="Media2-5AF107"Getting and Setting Instrument ParametersAFgetloopend()IDREF="Media2-5AF113"Getting and Setting Loop InformationAFgetloopmode()IDREF="Media2-5AF109"Getting and Setting Loop InformationAFgetloopstart()IDREF="Media2-5AF111"Getting and Setting Loop InformationAFgetmarkids()IDREF="Media2-5AF98"Getting and Setting Audio Track MarkersAFgetmarkname()IDREF="Media2-5AF99"Getting and Setting Audio Track MarkersAFgetmarkpos()IDREF="Media2-5AF100"Getting and Setting Audio Track MarkersAFgetmiscids()IDREF="Media2-5AF116"Getting Miscellaneous Data ParametersAFgetmiscsize()IDREF="Media2-5AF118"Getting Miscellaneous Data ParametersAFgetmisctype()IDREF="Media2-5AF117"Getting Miscellaneous Data ParametersAFgetrate()IDREF="Media2-5AF89"Getting Audio Track Sample RateAFgetsampfmt()IDREF="Media2-5AF90"Getting Audio Track Sample Format and Sample WidthIDREF="Media2-5AF91"Getting Audio Track Sample Format and Sample WidthAFidentifyfd()IDREF="Media2-5AF88"Getting Audio File FormatAFinitaeschanneldata()IDREF="Media2-5AF65"Initializing AES DataAFinitchannels()IDREF="Media2-5AF58"Initializing Audio Track ChannelsAFinitcompression()IDREF="Media2-5AF71"Initializing Audio Track CompressionIDREF="Media2-5AF68"Initializing Audio Track CompressionAFinitcompressionparams()IDREF="Media2-5AF69"Initializing Audio Track CompressionAFinitfilefmt()IDREF="Media2-5AF48"Initializing Audio File FormatAFinitinstids()IDREF="Media2-5AF75"Initializing Instrument DataAFinitloopids()IDREF="Media2-5AF76"Initializing Instrument DataAFinitmarkids()IDREF="Media2-5AF73"Initializing Audio Track MarkersAFinitmarknameIDREF="Media2-5AF74"Initializing Audio Track MarkersAFinitmiscids()IDREF="Media2-5AF77"Initializing Miscellaneous DataAFinitmiscsize()IDREF="Media2-5AF80"Initializing Miscellaneous DataAFinitmisctype()IDREF="Media2-5AF78"Initializing Miscellaneous DataAFinitrateIDREF="Media2-5AF56"Initializing Audio Track Sample RateAFinitsampfmt()IDREF="Media2-5AF57"Initializing Audio Track Sample Format and Sample WidthAFnewfilesetup()IDREF="Media2-5AF36"Creating an Audio File SetupAFopenfd()IDREF="Media2-5AF82"Getting an IRIX File Descriptor for an Audio FileAFopenfile()IDREF="Media2-5AF81"Opening an Audio FileAFreadframes()IDREF="Media2-5AF103"Reading Audio Frames from an Audio TrackAFreadmisc()IDREF="Media2-5AF119"Reading, Writing, and Seeking Miscellaneous DataAFseekframe()IDREF="Media2-5AF102"Seeking to a Position in an Audio File TrackAFseekmisc()IDREF="Media2-5AF121"Reading, Writing, and Seeking Miscellaneous DataAFsetaeschanneldata()IDREF="Media2-5AF67"Initializing AES DataAFseterrorhandler()IDREF="Media2-5AF7"Handling Audio File Library ErrorsAFsetinstparamlong()IDREF="Media2-5AF108"Getting and Setting Instrument ParametersAFsetloopend()IDREF="Media2-5AF114"Getting and Setting Loop InformationAFsetloopmode()IDREF="Media2-5AF110"Getting and Setting Loop InformationAFsetloopstart()IDREF="Media2-5AF112"Getting and Setting Loop InformationAFsetmarkpos()IDREF="Media2-5AF101"Getting and Setting Audio Track MarkersAFsyncfile()IDREF="Media2-5AF85"Closing and Updating FilesAFwriteframes()IDREF="Media2-5AF104"Writing Audio Frames to an Audio TrackAFwritemisc()IDREF="Media2-5AF120"Reading, Writing, and Seeking Miscellaneous DataALcloseport()IDREF="Media2-4AL89"Opening and Closing Audio PortsALfreeconfig()IDREF="Media2-4AL83"Retrieving the Setup of an Existing ALportALgetchannels()IDREF="Media2-4AL54"Setting and Getting the Number of Channels for an ALconfigALgetconfig()IDREF="Media2-4AL82"Retrieving the Setup of an Existing ALportALgetdefault()IDREF="Media2-4AL146"Getting the Defaults of Global ParametersALgetfd()IDREF="Media2-4AL168"Getting a File Descriptor for an ALportALgetfillable()IDREF="Media2-4AL99"Monitoring the Audio Sample Queue Status to Provide Nonblocking I/OALgetfilled()IDREF="Media2-4AL102"Monitoring the Audio Sample Queue Status to Provide Nonblocking I/OIDREF="Media2-4AL100"Monitoring the Audio Sample Queue Status to Provide Nonblocking I/OALgetfillpoint()IDREF="Media2-4AL178"Setting and Getting the Fill Point for a QueueALgetfloatmaxIDREF="Media2-4AL80"Getting and Setting the Floating Point Data RangeALgetminmax()IDREF="Media2-4AL144"Getting the Bounds of Global ParametersALgetname()IDREF="Media2-4AL149"Getting the Names Corresponding to the Global ParametersALgetparams()IDREF="Media2-4AL150"Getting Current Parameter SettingsALgetqueuesize()IDREF="Media2-4AL62"Setting and Getting the Sample Queue Size for an ALconfigALgetsampfmt()IDREF="Media2-4AL67"Setting and Getting the Sample Data Format for an ALconfigALgetstatus()IDREF="Media2-4AL103"More Methods for Working with QueuesIDREF="Media2-4AL121"Detecting Errors in the Audio StreamALgetwidth()IDREF="Media2-4AL75"Setting and Getting the Integer Sample Width for an ALconfigALnewconfig()IDREF="Media2-4AL48"Creating a New ALconfigALopenport()IDREF="Media2-4AL87"Opening and Closing Audio PortsALqueryparams()IDREF="Media2-4AL142"Getting a List of Available ParametersALreadsamps()IDREF="Media2-4AL107"Reading Samples from an Input ALportconversionsIDREF="Media2-4AL111"Reading Samples from an Input ALportfill pointsIDREF="Media2-4AL176"Setting and Getting the Fill Point for a QueueALsetchannels()IDREF="Media2-4AL51"Setting and Getting the Number of Channels for an ALconfigerrors and returnsIDREF="Media2-4AL52"Setting and Getting the Number of Channels for an ALconfigALseterrorhandler()IDREF="Media2-4AL31"Handling Audio Library ErrorsALsetfillpoint()IDREF="Media2-4AL173"Setting and Getting the Fill Point for a QueueALsetparams()IDREF="Media2-4AL143"Getting a List of Available ParametersIDREF="Media2-4AL152"Modifying the Values of the Global ParametersALsetqueuesize()IDREF="Media2-4AL61"Setting and Getting the Sample Queue Size for an ALconfigALsetsampfmt()IDREF="Media2-4AL65"Setting and Getting the Sample Data Format for an ALconfigALsetwidth()IDREF="Media2-4AL74"Setting and Getting the Integer Sample Width for an ALconfigALwritesamps()IDREF="Media2-4AL114"Writing Samples to an Output ALportfill pointsIDREF="Media2-4AL177"Setting and Getting the Fill Point for a QueueCDallowremoval()IDREF="Media2-6CD34"Controlling the CD-ROM Drive CaddyCDatomsf()IDREF="Media2-6CD44"Getting CD Locations from Calculations Internal to Your ApplicationIDREF="Media2-6CD46"Getting CD Locations from Calculations Internal to Your ApplicationCDbestreadsize()IDREF="Media2-6CD68"Reading Audio Data from the CD-ROM DriveCDclose()IDREF="Media2-6CD29"Opening and Closing the CD-ROM DeviceCDcreateparser()IDREF="Media2-6CD74"Allocating and Initializing the CD ParserCDdeleteparser()IDREF="Media2-6CD86"Freeing the Memory Allocated for the Parser CDeject()IDREF="Media2-6CD30"Controlling the CD-ROM Drive CaddyCDframetomsf()IDREF="Media2-6CD47"Getting CD Locations from Calculations Internal to Your ApplicationCDgetstatus()IDREF="Media2-6CD89"Communicating CD Status to the End UserIDREF="Media2-6CD51"Getting the Current CD Location CDgettrackinfo()IDREF="Media2-6CD88"Communicating CD Status to the End UserCDmsftoframe()IDREF="Media2-6CD42"Getting CD Locations from Calculations Internal to Your ApplicationIDREF="Media2-6CD45"Getting CD Locations from Calculations Internal to Your ApplicationCDparseframe()IDREF="Media2-6CD84"Parsing CD Frames CDplay()IDREF="Media2-6CD59"Playing an Audio CD from the CD-ROM DriveCDplayabs()IDREF="Media2-6CD65"Playing an Audio CD from the CD-ROM DriveCDplaytrack()IDREF="Media2-6CD64"Playing an Audio CD from the CD-ROM DriveCDplaytrackabs()IDREF="Media2-6CD66"Playing an Audio CD from the CD-ROM DriveCDpreventremoval()IDREF="Media2-6CD33"Controlling the CD-ROM Drive CaddyCDreadda()IDREF="Media2-6CD70"Reading Audio Data from the CD-ROM DriveCDremovecallback()IDREF="Media2-6CD82"Deleting and Changing a CD Parser CallbackCDresetparser()IDREF="Media2-6CD76"Allocating and Initializing the CD ParserCDsbtoa()IDREF="Media2-6CD90"Communicating CD Status to the End UserCDseek()IDREF="Media2-6CD54"Seeking to a CD LocationCDseekblock()IDREF="Media2-6CD55"Seeking to a CD LocationCDseektrackIDREF="Media2-6CD53"Seeking to a CD LocationCDstop()IDREF="Media2-6CD63"Playing an Audio CD from the CD-ROM DriveCDtctoframe()IDREF="Media2-6CD43"Getting CD Locations from Calculations Internal to Your ApplicationIDREF="Media2-6CD48"Getting CD Locations from Calculations Internal to Your ApplicationCDtimetoa()IDREF="Media2-6CD93"Communicating CD Status to the End UserCDtogglepause()IDREF="Media2-6CD61"Playing an Audio CD from the CD-ROM Driveclose()IDREF="Media2-7DAT20"Opening and Closing the DAT Device for AudiodmAudioFrameSize()for movie buffersIDREF="Media6-2IO55"Allocating BuffersdmImageFrameSize()for movie buffersIDREF="Media6-2IO54"Allocating BuffersdmParamsCopyAllElems()IDREF="Media1-2DM70"Copying the Contents of One Parameter-value List into AnotherdmParamsCopyElem()IDREF="Media1-2DM73"Copying an Individual Parameter Value from One List into AnotherdmParamsCreate()IDREF="Media1-2DM28"Creating and Destroying Parameter-value ListsdmParamsGetElem()IDREF="Media1-2DM75"Determining the Name of a Given ParameterdmParamsGetElemType()IDREF="Media1-2DM77"Determining the Data Type of a Given ParameterdmParamsGetEnum()IDREF="Media1-2DM61"Setting and Getting Individual Parameter ValuesdmParamsGetFloat()IDREF="Media1-2DM63"Setting and Getting Individual Parameter ValuesdmParamsGetFract()IDREF="Media1-2DM64"Setting and Getting Individual Parameter ValuesdmParamsGetInt()IDREF="Media1-2DM60"Setting and Getting Individual Parameter ValuesdmParamsGetNumElems()IDREF="Media1-2DM69"Determining the Number of Elements in a Parameter-value ListdmParamsGetParams()IDREF="Media1-2DM65"Setting and Getting Individual Parameter ValuesdmParamsGetString()IDREF="Media1-2DM62"Setting and Getting Individual Parameter ValuesdmParamsGetTocEntry()IDREF="Media1-2DM66"Setting and Getting Individual Parameter ValuesdmParamsIsPresent()IDREF="Media1-2DM78"Determining if a Given Parameter ExistsdmParamsRemoveElem()IDREF="Media1-2DM81"Removing an Element from a Parameter-value ListdmParamsSetEnum()IDREF="Media1-2DM52"Setting and Getting Individual Parameter ValuesdmParamsSetFloat()IDREF="Media1-2DM51"Setting and Getting Individual Parameter ValuesIDREF="Media1-2DM53"Setting and Getting Individual Parameter ValuesdmParamsSetFract()IDREF="Media1-2DM54"Setting and Getting Individual Parameter ValuesdmParamsSetInt()IDREF="Media1-2DM50"Setting and Getting Individual Parameter ValuesdmParamsSetParams()IDREF="Media1-2DM55"Setting and Getting Individual Parameter ValuesdmParamsSetString()IDREF="Media1-2DM56"Setting and Getting Individual Parameter ValuesdmParamsSetTocEntry()IDREF="Media1-2DM57"Setting and Getting Individual Parameter ValuesdmSetAudioDefaults()IDREF="Media1-2DM38"Setting Audio DefaultsdmSetAudioDefaultsIDREF="Media6-1GS96"Setting and Getting Audio Track PropertiesdmSetImageDefaults()IDREF="Media1-2DM43"Setting Image DefaultsDTaddcallback()IDREF="Media2-7DAT55"Adding and Removing DAT Parser Callbacks DTatohmsf()IDREF="Media2-7DAT32"Getting DAT Locations from Calculations Internal to Your ApplicationDTcreateparser()IDREF="Media2-7DAT52"Allocating and Initializing the DAT ParserDTdeleteparser()IDREF="Media2-7DAT60"Freeing the Memory Reserved for the DAT Parser DTframetohmsf()IDREF="Media2-7DAT33"Getting DAT Locations from Calculations Internal to Your ApplicationDThmsftoframe()IDREF="Media2-7DAT31"Getting DAT Locations from Calculations Internal to Your ApplicationDTparseframe()IDREF="Media2-7DAT58"Parsing DAT Frames DTpnotodec()IDREF="Media2-7DAT65"Communicating DAT Status to the End User DTremovecallback()IDREF="Media2-7DAT56"Deleting or Changing a DAT Parser Callback DTresetparser()IDREF="Media2-7DAT53"Allocating and Initializing the DAT ParserDTsbtoa()IDREF="Media2-7DAT63"Communicating DAT Status to the End User DTtctoframe()IDREF="Media2-7DAT34"Getting DAT Locations from Calculations Internal to Your ApplicationIDREF="Media2-7DAT30"Getting DAT Locations from Calculations Internal to Your ApplicationDTtimetoa()IDREF="Media2-7DAT64"Communicating DAT Status to the End User ioctl()IDREF="Media2-7DAT21"Opening and Closing the DAT Device for AudioIDREF="Media2-7DAT26"Navigating through a DATIDREF="Media2-7DAT36"Seeking to a DAT Locationmalloc()IDREF="Media2-3SW67"Compiling and Linking an Audio Applicationfor movie buffersIDREF="Media6-2IO56"Allocating BuffersmvAddTrack()IDREF="Media6-2IO41"Adding an Audio or Image Track to a MoviemvAddUserParam()IDREF="Media6-1GS66"Adding Your Own Parameters to the Movie LibrarymvBindWindow()IDREF="Media6-3PB36"Binding a Movie to a Window for PlaybackmvClose()IDREF="Media6-2IO81"Finalizing Changes and Closing MoviesmvCreateFD()IDREF="Media6-2IO20"Creating a New MoviemvCreateFile()IDREF="Media6-2IO21"Creating a New MoviemvCreateMem()IDREF="Media6-2IO19"Creating a New MoviemvDeleteFramesIDREF="Media6-2IO64"Deleting Frames from a Movie TrackmvFindTrackByMedium()IDREF="Media6-2IO46"Locating an Existing TrackmvGetAudioChannels()IDREF="Media6-1GS111"Getting the Number of Audio Channels in an Audio TrackmvGetAudioCompression()IDREF="Media6-1GS118"Getting the Audio Compression Scheme of an Audio TrackmvGetAudioFormat()IDREF="Media6-1GS115"Getting the Audio Format of an Audio TrackmvGetAudioRate()IDREF="Media6-1GS108"Getting the Audio Sample Rate of an Audio TrackmvGetAudioWidth()IDREF="Media6-1GS105"Getting the Audio Sample Width of an Audio TrackmvGetCompressedImageSizeIDREF="Media6-2IO69"Reading a Compressed Image from a Movie into a BuffermvGetDefaultVol()IDREF="Media6-1GS102"Setting and Getting the Default Volume of an Audio TrackmvGetFileFormatIDREF="Media6-1GS56"Getting the Movie File FormatmvGetImageCompression()IDREF="Media6-1GS164"Getting the Image Compression SchememvGetImageFormat()IDREF="Media6-1GS140"Getting the Image FormatmvGetImageHeight()IDREF="Media6-1GS134"Getting the Image Frame SizemvGetImageOrientation()IDREF="Media6-1GS137"Getting the Image OrientationmvGetImagePacking()IDREF="Media6-1GS156"Getting the Image Packing FormatmvGetImageRateIDREF="Media6-1GS129"Setting and Getting the Image Frame RatemvGetImageWidth()IDREF="Media6-1GS131"Getting the Image Frame SizemvGetLoopLimit()IDREF="Media6-1GS50"Setting and Getting the Default Movie Loop LimitmvGetLoopMode()IDREF="Media6-1GS44"Setting and Getting the Default Movie Loop ModemvGetNumMoviesHint()IDREF="Media6-3PB6"Opening a Movie for PlaybackmvGetOptimized()IDREF="Media6-1GS61"Getting the Movie Optimization SettingmvGetParams()IDREF="Media6-1GS34"Setting and Getting Movie and Track ParametersmvGetSMPTEStart()IDREF="Media6-1GS82"Setting and Getting SMPTE Time Code Strings Stored in TracksmvGetTitle()IDREF="Media6-1GS55"Setting and Getting the Movie TitlemvGetTrackLength()IDREF="Media6-1GS83"Getting the Track LengthmvGetTrackMedium()IDREF="Media6-1GS86"Getting the Track MediummvGetViewBackground()IDREF="Media6-3PB13"Setting and Getting the Background ColormvGetViewOffset()IDREF="Media6-3PB33"Setting and Getting the Viewing Location OffsetmvGetViewSize()IDREF="Media6-3PB24"Setting and Getting the Viewing Area SizemvInsertCompressedImage()IDREF="Media6-2IO71"Inserting a Compressed Image from a Buffer into an Existing TrackmvInsertFramesIDREF="Media6-2IO60"Inserting Raw Images and Audio from a Buffer into an Existing TrackmvIsMovieFD()IDREF="Media6-2IO26"Checking for the Presence of a MoviemvIsMovieFile()IDREF="Media6-2IO27"Checking for the Presence of a MoviemvIsMovieMem()IDREF="Media6-2IO28"Checking for the Presence of a MoviemvMapBetweenTracksIDREF="Media6-2IO48"Mapping Frames from One Track to Another TrackIDREF="Media6-2IO76"Copying and Pasting Frames from One Movie into Anotherwhen pasting movie framesIDREF="Media6-2IO77"Copying and Pasting Frames from One Movie into AnothermvOpenFD()IDREF="Media6-2IO30"Opening an Existing MovieIDREF="Media6-2IO33"Opening a Movie from a File DescriptormvOpenFile()IDREF="Media6-2IO31"Opening an Existing MovieIDREF="Media6-2IO35"Opening a Movie from a FilenamemvOpenMem()IDREF="Media6-2IO36"Opening Memory-mapped MoviesIDREF="Media6-2IO32"Opening an Existing MoviemvOptimize()IDREF="Media6-2IO52"Optimizing a Movie FilemvPasteFramesIDREF="Media6-2IO78"Copying and Pasting Frames from One Movie into AnothermvPlay()IDREF="Media6-3PB42"Starting and Stopping PlaybackmvQueryViewOffsetIDREF="Media6-3PB32"Setting and Getting the Viewing Location OffsetmvQueryViewSize()IDREF="Media6-3PB20"Setting and Getting the Viewing Area SizemvReadCompressedImageIDREF="Media6-2IO68"Reading a Compressed Image from a Movie into a BuffermvReadFrames()IDREF="Media6-2IO62"Reading Frames from a Movie into a Buffer for Uncompressed DatamvRemoveTrack()IDREF="Media6-2IO44"Removing an Audio or Image Track from a MoviemvSetDefaultVol()IDREF="Media6-1GS101"Setting and Getting the Default Volume of an Audio TrackmvSetImageRate()IDREF="Media6-1GS125"Setting and Getting the Image Frame RatemvSetLoopLimit()IDREF="Media6-1GS45"Setting and Getting the Default Movie Loop LimitmvSetLoopMode()IDREF="Media6-1GS42"Setting and Getting the Default Movie Loop ModemvSetMovieDefaults()IDREF="Media6-1GS62"Creating a Default Movie ConfigurationmvSetNumMoviesHint()IDREF="Media6-3PB5"Opening a Movie for PlaybackmvSetParams()IDREF="Media6-1GS33"Setting and Getting Movie and Track Parametersfor setting user parametersIDREF="Media6-1GS67"Adding Your Own Parameters to the Movie LibrarymvSetSMPTEStart()IDREF="Media6-1GS81"Setting and Getting SMPTE Time Code Strings Stored in TracksmvSetTitle()IDREF="Media6-1GS53"Setting and Getting the Movie TitlemvSetViewBackground()IDREF="Media6-3PB11"Setting and Getting the Background ColormvSetViewOffsetIDREF="Media6-3PB31"Setting and Getting the Viewing Location OffsetmvSetViewSize()IDREF="Media6-3PB17"Setting and Getting the Viewing Area SizemvStop()IDREF="Media6-3PB43"Starting and Stopping PlaybackmvWriteIDREF="Media6-2IO80"Finalizing Changes and Closing Moviesopen()IDREF="Media2-7DAT19"Opening and Closing the DAT Device for Audiooserror()IDREF="Media2-4AL29"Handling Audio Library Errorspoll()IDREF="Media2-4AL162"Real-time Programming Techniques for AudioIDREF="Media2-4AL170"Getting a File Descriptor for an ALportprctl()IDREF="Media2-4AL183"Preventing Memory Swapoutread()IDREF="Media2-7DAT40"Reading Audio Data from the DAT Drive schedctl()IDREF="Media2-4AL182"Using Scheduling Control to Give Audio High Priorityselect()IDREF="Media2-4AL161"Real-time Programming Techniques for AudioaudioillustratedIDREF="Media2-4AL180"Setting and Getting the Fill Point for a Queuefor multiplexing audio I/OIDREF="Media2-4AL165"Multiplexing Synchronous I/Osproc()exampleIDREF="Media2-4AL197"Using Shared Arenas and Semaphoresusinit()IDREF="Media2-4AL189"Using Shared Arenas and Semaphoresuspsema()IDREF="Media2-4AL193"Using Shared Arenas and Semaphoresusvsema()IDREF="Media2-4AL194"Using Shared Arenas and Semaphoresbrightness. See luminanceIDREF="Media3-1VB22"YUVbroadcast videoformatsIDREF="Media3-1VB66"Videotape FormatsstandardsIDREF="Media3-1VB6"Broadcast Standardsbuffered interface of the Compression LibraryIDREF="Media5-1GS4"Buffered Access APIbuffering interface of the Compression LIbraryIDREF="Media5-1GS32"Using the Buffering Interfacebuffersallocating for moviesexampleIDREF="Media6-2IO57"Allocating BuffersaudiosizeIDREF="Media1-2DM39"Determining the Buffer Size Needed to Store an Audio FrameimagesizeIDREF="Media1-2DM46"Determining the Buffer Size Needed to Store an Image Frameinternal versus externalIDREF="Media5-1GS41"Creating a Buffermanaging when adding algorithmsIDREF="Media5-3UP8"Managing Buffers for Added AlgorithmsmoviesallocatingIDREF="Media6-2IO53"Allocating Buffersnon-blocking playbackIDREF="Media5-1GS63"Creating a Nonblocking Buffered Playback Applicationnon-blocking recording applicationIDREF="Media5-1GS70"Creating a Nonblocking Buffered Record Applicationplayback applicationIDREF="Media5-1GS56"Creating a Basic Buffered Playback Applicationrecord applicationIDREF="Media5-1GS64"Creating a Buffered Record ApplicationringIDREF="Media5-1GS34"Using the Buffering Interface.bw images in moviesIDREF="Media6-1GS149"Getting the Image Formatbyte orderingDATsIDREF="Media2-7DAT9"DAT Frames, Samples, and SubcodesC++referencesIDREF="Media2-1GS5"Introduction to Digital Audio and MIDI ProgrammingcallbacksCDIDREF="Media2-6CD77"Defining Callbacks for the CD ParserIDREF="Media2-6CD71"Reading Audio Data from the CD-ROM DriveaddingIDREF="Media2-6CD78"Adding Callbacks to the CD Parser removingIDREF="Media2-6CD81"Deleting and Changing a CD Parser CallbackDAT parserIDREF="Media2-7DAT54"Adding and Removing DAT Parser Callbacks capacityCDsIDREF="Media2-6CD21"CD Tracks, Indices, and Time Codescatalog numbersDATIDREF="Media2-7DAT14"DAT Frames, Samples, and SubcodesCCIR 601 video standardIDREF="Media3-1VB27"YUVCCITT /TSB G.711 A-lawIDREF="Media5-0ZIN23"Audio AlgorithmsCCITT/TSB G.711 mu-lawIDREF="Media5-0ZIN22"Audio AlgorithmsCD Audio LibraryIDREF="Media2-6CD1"Programming with the CD Audio LibrarycompilingIDREF="Media2-3SW58"Compiling and Linking an Audio ApplicationexampleIDREF="Media2-6CD97"CD Sample ProgramfeaturesIDREF="Media2-6CD3"CD Audio Library BasicspurposeIDREF="Media1-1ML8"About the Digital Audio and MIDI LibrariesCD-ROMmoviesIDREF="Media6-2IO7"Using File Descriptors with Moviestesting movie I/OIDREF="Media6-1GS26"Emulating I/O FailuresCD-ROM drivesconfidence testsIDREF="Media2-3SW35"Graphical User Interface Audio ToolscontrollingIDREF="Media2-6CD28"Opening and Closing the CD-ROM Deviceplaying audio CDsIDREF="Media2-6CD58"Using the CD-ROM DrivestatusIDREF="Media2-6CD87"Communicating CD Status to the End UserCDDA_DATASIZEIDREF="Media2-6CD13"CD Frames, Samples, and SubcodesCDFRAMEIDREF="Media2-6CD8"CD Frames, Samples, and SubcodesCDsaudio librariesIDREF="Media2-2HW23"PeripheralsblocksIDREF="Media2-6CD56"Seeking to a CD Locationbyte orderingIDREF="Media2-6CD12"CD Frames, Samples, and SubcodescallbacksIDREF="Media2-6CD77"Defining Callbacks for the CD ParserIDREF="Media2-6CD71"Reading Audio Data from the CD-ROM DrivecapacityIDREF="Media2-6CD21"CD Tracks, Indices, and Time CodesconversionsISRC to ASCIIIDREF="Media2-6CD91"Communicating CD Status to the End UserlocationsIDREF="Media2-6CD40"Getting CD Locations from the End Usertime codesIDREF="Media2-6CD94"Communicating CD Status to the End UserejectingIDREF="Media2-6CD31"Controlling the CD-ROM Drive Caddyframe countsIDREF="Media2-6CD41"Getting CD Locations from Calculations Internal to Your ApplicationframesIDREF="Media2-6CD6"CD Frames, Samples, and SubcodesISRCIDREF="Media2-6CD18"CD Frames, Samples, and Subcodeslead-in trackIDREF="Media2-6CD15"CD Frames, Samples, and SubcodeslocationsIDREF="Media2-6CD37"Navigating through a CDconversionsIDREF="Media2-6CD40"Getting CD Locations from the End UsercurrentIDREF="Media2-6CD49"Getting the Current CD Location formatsIDREF="Media2-6CD38"Navigating through a CDnavigatingIDREF="Media2-6CD35"Navigating through a CDparserIDREF="Media2-6CD72"Controlling the CD ParserbasicsIDREF="Media2-6CD27"CD ParsercallbacksIDREF="Media2-6CD80"Adding Callbacks to the CD Parser freeingIDREF="Media2-6CD85"Freeing the Memory Allocated for the Parser initializingIDREF="Media2-6CD73"Allocating and Initializing the CD ParserresettingIDREF="Media2-6CD75"Allocating and Initializing the CD ParserparsingIDREF="Media2-6CD72"Controlling the CD ParserpausingIDREF="Media2-6CD62"Playing an Audio CD from the CD-ROM DriveplayingcdmanIDREF="Media2-3SW28"Graphical User Interface Audio Toolspreemphasis in AES bytesIDREF="Media2-5AF61"Initializing AES Datapreventing ejectionIDREF="Media2-6CD32"Controlling the CD-ROM Drive CaddyreadingIDREF="Media2-6CD69"Reading Audio Data from the CD-ROM Driverecording to DATIDREF="Media2-5AF62"Initializing AES Datasample rateIDREF="Media2-6CD12"CD Frames, Samples, and SubcodessamplesIDREF="Media2-6CD5"CD Frames, Samples, and SubcodesillustratedIDREF="Media2-6CD11"CD Frames, Samples, and SubcodesseekingIDREF="Media2-6CD36"Navigating through a CDblocksIDREF="Media2-6CD57"Seeking to a CD LocationtracksIDREF="Media2-6CD52"Seeking to a CD LocationstatusIDREF="Media2-6CD50"Getting the Current CD Location subcodesIDREF="Media2-6CD10"CD Frames, Samples, and SubcodesmodesIDREF="Media2-6CD14"CD Frames, Samples, and SubcodessubcodeQIDREF="Media2-6CD9"CD Frames, Samples, and SubcodesIDREF="Media2-6CD19"CD Frames, Samples, and Subcodestable of contentsIDREF="Media2-6CD16"CD Frames, Samples, and Subcodestime codesIDREF="Media2-6CD25"CD Tracks, Indices, and Time CodestracksIDREF="Media2-6CD22"CD Tracks, Indices, and Time CodesplayingIDREF="Media2-6CD67"Playing an Audio CD from the CD-ROM DriveCHALLENGEIDREF="Media3-2GS3"How the VL Works with Hardwarechangingaudioglobal stateIDREF="Media2-4AL154"Modifying the Values of the Global ParameterschannelsaudioconfiguringIDREF="Media2-4AL50"Setting and Getting the Number of Channels for an ALconfigdefaultsIDREF="Media2-4AL42"Using ALconfig Structures to Configure ALportsaudio filesIDREF="Media2-5AF59"Initializing Audio Track ChannelsDATIDREF="Media2-7DAT12"DAT Frames, Samples, and Subcodescheckingaudioparameter rangesIDREF="Media2-4AL145"Getting the Bounds of Global ParametersparametersIDREF="Media1-2DM79"Determining if a Given Parameter ExistschunksAF LibraryIDREF="Media2-5AF32"AIFF-C and the AF Library APIIDREF="Media2-5AF9"About Audio FilesAIFF-CIDREF="Media2-5AF31"AIFF-C and the AF Library APImiscellaneousIDREF="Media2-5AF79"Initializing Miscellaneous DataCL_BEST_FITIDREF="Media5-0ZIN30"Video Data FormatsCL_GRAYSCALEIDREF="Media5-0ZIN35"Video Data FormatsCL_MVC1 in an exampleIDREF="Media5-1GS19"Closing a CompressorIDREF="Media5-1GS31"Closing a DecompressorCL_ORIENTATIONIDREF="Media5-0ZIN25"Image Data FormatsCL_RGBIDREF="Media5-0ZIN33"Video Data FormatsCL_RGB332IDREF="Media5-0ZIN34"Video Data FormatsCL_RGBAIDREF="Media5-0ZIN31"Video Data FormatsCL_RGBXIDREF="Media5-0ZIN32"Video Data FormatsCL_YIDREF="Media5-0ZIN36"Video Data FormatsCL_YCbCrIDREF="Media5-0ZIN38"Video Data FormatsCL_YUVIDREF="Media5-0ZIN37"Video Data FormatsCL_YUV422IDREF="Media5-0ZIN39"Video Data FormatsCL_YUV422DCIDREF="Media5-0ZIN40"Video Data FormatsclAddAlgorithm()IDREF="Media5-3UP2"Adding Custom Algorithms to the Compression LibraryclAddParam()IDREF="Media5-3UP12"Adding Custom Parameters to the Compression LibraryclCloseCompressor()IDREF="Media5-1GS18"Closing a CompressorclCloseDecompressor()IDREF="Media5-1GS30"Closing a DecompressorclCompress()IDREF="Media5-1GS66"Creating a Buffered Record ApplicationIDREF="Media5-1GS73"Creating a Nonblocking Buffered Record ApplicationIDREF="Media5-1GS16"Compressing FramesIDREF="Media5-1GS15"Compressing FramesclCompressImage()IDREF="Media5-1GS9"Using the Still Image InterfaceclCreateBuf()IDREF="Media5-1GS42"Creating a BufferIDREF="Media5-1GS25"Getting Stream InformationclDecompress()IDREF="Media5-1GS29"Decompressing FramesIDREF="Media5-1GS28"Decompressing FramesIDREF="Media5-1GS58"Creating a Basic Buffered Playback ApplicationclDecompressImage()IDREF="Media5-1GS10"Using the Still Image InterfaceclDestroyBufIDREF="Media5-1GS43"Creating a BufferclDone(IDREF="Media5-3UP10"Reading Data Across Buffer DiscontinuitiesclDoneUpdatingHead()IDREF="Media5-1GS50"Managing BuffersIDREF="Media5-1GS59"Creating a Basic Buffered Playback ApplicationclGetAlgorithmName()IDREF="Media5-2AP16"Getting an Algorithm Scheme or NameclGetUnique()IDREF="Media5-3UP4"Adding Custom Algorithms to the Compression LibraryclicksaudioIDREF="Media2-4AL56"Setting and Getting the Sample Queue Size for an ALconfigclientsVideo Library (VL)IDREF="Media3-2GS8"Device ManagementclippingaudioIDREF="Media2-3SW15"How Outputs from Multiple Audio Applications Are CombinedcloningALconfigsIDREF="Media2-4AL81"Retrieving the Setup of an Existing ALportclOpenCompressor()IDREF="Media5-1GS14"Opening a CompressorclOpenDecompressor()IDREF="Media5-1GS27"Opening a DecompressorclosingCD-ROM deviceIDREF="Media2-6CD29"Opening and Closing the CD-ROM DevicemoviesIDREF="Media6-2IO82"Finalizing Changes and Closing MoviesclQuery()IDREF="Media5-1GS62"Creating a Basic Buffered Playback ApplicationIDREF="Media5-1GS69"Creating a Buffered Record ApplicationclQueryAlgorithms()IDREF="Media5-2AP13"Getting a List of AlgorithmsclQueryBufferHdl()IDREF="Media5-1GS44"Creating a BufferclQueryFree()IDREF="Media5-1GS46"Managing BuffersIDREF="Media5-1GS52"Producing and Consuming Data in BuffersclQueryHandle()IDREF="Media5-1GS45"Creating a BufferclQueryLicense()IDREF="Media5-2AP18"Getting License InformationclQueryMaxHeaderSize()IDREF="Media5-1GS24"Getting Stream InformationclQueryScheme()IDREF="Media5-1GS21"Getting Stream InformationclQuerySchemeFromHandle()IDREF="Media5-2AP14"Getting an Algorithm Scheme or NameclQuerySchemeFromName()IDREF="Media5-2AP15"Getting an Algorithm Scheme or NameclQueryValid()IDREF="Media5-1GS60"Creating a Basic Buffered Playback ApplicationIDREF="Media5-1GS71"Creating a Nonblocking Buffered Record ApplicationIDREF="Media5-1GS48"Managing BuffersIDREF="Media5-1GS67"Creating a Buffered Record ApplicationIDREF="Media5-1GS54"Producing and Consuming Data in BuffersclReadData()IDREF="Media5-3UP9"Reading Data Across Buffer DiscontinuitiesclReadHeader()IDREF="Media5-1GS23"Getting Stream InformationclSetMax()IDREF="Media5-3UP6"Adding Custom Algorithms to the Compression LibraryclSetMin()IDREF="Media5-3UP5"Adding Custom Algorithms to the Compression LibraryclSetUnique()IDREF="Media5-3UP3"Adding Custom Algorithms to the Compression LibraryclUpdateHead()IDREF="Media5-1GS57"Creating a Basic Buffered Playback ApplicationIDREF="Media5-1GS47"Managing BuffersIDREF="Media5-1GS65"Creating a Buffered Record ApplicationclUpdateTail(IDREF="Media5-1GS68"Creating a Buffered Record ApplicationclUpdateTail()IDREF="Media5-1GS61"Creating a Basic Buffered Playback ApplicationIDREF="Media5-1GS49"Managing BufferscodecsAware, Inc.IDREF="Media2-3SW40"Aware Audio Compression Software and Audio ProductscodesCDIDREF="Media2-6CD18"CD Frames, Samples, and SubcodescolorencodingillustratedIDREF="Media3-1VB40"Composite Videosync burstIDREF="Media3-1VB42"Video Signalscolor space conversionIDREF="Media5-0ZIN27"Video Data Formats formats not requiringIDREF="Media5-0ZIN41"Video Data Formatscombiningmovies and graphicsview sizeIDREF="Media6-3PB21"Setting and Getting the Viewing Area SizecommentsmovieIDREF="Media6-1GS38"Setting and Getting the Movie CommentcommunicatingDAT statusIDREF="Media2-7DAT61"Communicating DAT Status to the End User communicationsinterprocessIDREF="Media2-4AL200"Using Shared Arenas and SemaphoresIDREF="Media2-4AL188"Using Shared Arenas and Semaphorescompensatingfor CD preemphasisIDREF="Media2-5AF61"Initializing AES DatacompilingAF LibraryIDREF="Media2-3SW51"Compiling and Linking an Audio ApplicationAudio LibraryIDREF="Media2-3SW49"Compiling and Linking an Audio ApplicationCD Audio LibraryIDREF="Media2-3SW58"Compiling and Linking an Audio ApplicationDAT Audio LibraryIDREF="Media2-3SW64"Compiling and Linking an Audio ApplicationDM LibraryIDREF="Media1-2DM19"Compiling and Linking a Digital Media Library ApplicationMovie LibraryIDREF="Media6-1GS16"Compiling and Linking a Movie Library ApplicationcompositevideoillustratedIDREF="Media3-1VB43"Video Signalscomposite videoIDREF="Media3-1VB37"Composite Videocompressed imagesinserting into moviesIDREF="Media6-2IO72"Inserting a Compressed Image from a Buffer into an Existing TrackCOMPRESSED_BUFFER_SIZEIDREF="Media5-1GS17"Compressing Framescompressionaudio filesIDREF="Media2-5AF29"AIFF-C and the AF Library APIIDREF="Media2-5AF94"Getting Audio Track CompressionIDREF="Media2-5AF70"Initializing Audio Track CompressiondefaultIDREF="Media2-5AF41"Creating an Audio File SetupAwareIDREF="Media5-0ZIN2"Introduction to the Compression Librarycomputer versus camera imagesIDREF="Media5-0ZIN14"Lossy versus Lossless Compression MethodsdefinitionIDREF="Media5-0ZIN3"Overview of the Compression Librarygettingmovie image settingIDREF="Media6-1GS163"Getting the Image Compression Schemehardware accelerationIDREF="Media5-1GS5"Buffered Access APIimageIDREF="Media5-1GS2"Still Image APIIDREF="Media5-1GS8"Using the Still Image InterfaceJPEGmoviesIDREF="Media6-1GS169"Getting the Image Compression Schememovie imagesIDREF="Media6-1GS160"Getting the Image Compression Schememultiprocessing exampleIDREF="Media5-1GS75"Creating Buffered Multiprocess Record and Play ApplicationsmultithreadingIDREF="Media5-1GS33"Using the Buffering InterfaceMVC1IDREF="Media6-1GS166"Getting the Image Compression SchemeIDREF="Media5-0ZIN19"Movie AlgorithmsMVC2IDREF="Media6-1GS167"Getting the Image Compression SchemeperformanceIDREF="Media5-2AP11"Choosing a Compression Library AlgorithmQT_ANIMIDREF="Media6-1GS171"Getting the Image Compression SchemeQT_VIDEOIDREF="Media6-1GS170"Getting the Image Compression SchemeRLE24RLE24 compressionIDREF="Media6-1GS168"Getting the Image Compression Schemeserver-client environmentIDREF="Media5-0ZIN9"Compression Library Applications Compression Libraryadding parametersIDREF="Media5-3UP11"Adding Custom Parameters to the Compression Libraryalgorithm performance statisticsIDREF="Media5-2AP11"Choosing a Compression Library AlgorithmalgorithmsIDREF="Media5-2AP1"Using Compression Library Algorithms and ParametersAPIIDREF="Media5-1GS1"Getting Started with the Compression Library data formatsIDREF="Media5-0ZIN24"Compression Library Data Formatserror handlingIDREF="Media5-1GS7"About File I/O and Error Handling file I/OIDREF="Media5-1GS6"About File I/O and Error HandlingCompression LibraryIDREF="Media5-0ZIN1"Introduction to the Compression Libraryaccessing Aware audio compressionIDREF="MediaA-2B13"Accessing Aware Audio Compression from the Compression Libraryadding algorithmsIDREF="Media5-3UP1"Customizing the Compression Libraryalgorithm independenceIDREF="Media5-2AP12"Querying Compression Library AlgorithmsIDREF="Media5-0ZIN11"Compression Library FeaturesapplicationsIDREF="Media5-0ZIN4"Compression Library Applicationsbuffered interfaceIDREF="Media5-1GS32"Using the Buffering InterfaceIDREF="Media5-1GS4"Buffered Access APIexample of adding algorithmsIDREF="Media5-3UP7"Adding Custom Algorithms to the Compression LibraryfeaturesIDREF="Media5-0ZIN10"Compression Library FeaturesparametersIDREF="Media5-2AP20"Using the Compression Library ParameterspurposeIDREF="Media1-1ML19"About the Compression Librarysequential interfaceIDREF="Media5-1GS12"Using the Sequential Frame InterfaceIDREF="Media5-1GS3"Sequential Access APIstandardsIDREF="Media5-0ZIN15"Compression Standardsstill-frame interfaceIDREF="Media5-1GS8"Using the Still Image InterfaceIDREF="Media5-1GS2"Still Image APIcompressorIDREF="Media5-1GS13"Compressing a Sequence of FramesCompuserve GIF images in moviesIDREF="Media6-1GS146"Getting the Image FormatcomputersmusicreferencesIDREF="Media2-1GS4"Introduction to Digital Audio and MIDI ProgrammingconcurrentaudioIDREF="Media2-3SW12"How Audio Applications Share Audio System ResourcesIDREF="Media2-3SW4"About Shared System-Wide ResourcesIDREF="Media2-3SW24"Programming Guidelines for Managing System-Wide Resourcesconcurrent audio processesqueryingIDREF="Media2-4AL157"Determining Whether Other Audio Applications Are Runningconfidence testsIDREF="Media2-3SW35"Graphical User Interface Audio ToolsconfigurationsaudiocloningIDREF="Media2-4AL81"Retrieving the Setup of an Existing ALportaudio defaultIDREF="Media1-2DM37"Setting Audio Defaultsaudio filesdefaultsIDREF="Media2-5AF38"Creating an Audio File SetupdefaultIDREF="Media1-2DM33"Creating Default Audio and Image Configurationsimage defaultIDREF="Media1-2DM44"Setting Image DefaultsinstrumentIDREF="Media2-5AF17"Instrument Configurations and LoopsconfiguringALportsIDREF="Media2-4AL39"Using ALconfig Structures to Configure ALportsexampleIDREF="Media2-4AL46"Using ALconfig Structures to Configure ALportsaudio4-channel modeIDREF="Media2-4AL136"Querying and Controlling the Global Audio Device Statequeue sizeIDREF="Media2-4AL60"Setting and Getting the Sample Queue Size for an ALconfigaudio file tracksIDREF="Media2-5AF54"Initializing Audio Track DataMIDIIDREF="Media2-8MI9"Connecting Devices to MIDI I/O Interfacesmovieaudio tracksIDREF="Media6-1GS93"Setting and Getting Audio Track Propertiesimage tracksIDREF="Media6-1GS124"Setting and Getting Image Track Propertiesplayback windowIDREF="Media6-3PB9"Configuring the Playback Displayparameter-value listsIDREF="Media1-2DM32"Creating Default Audio and Image ConfigurationsconnectionsaudioIDREF="Media2-4AL35"Initializing an Audio Library ApplicationIDREF="Media2-4AL7"Audio Library Programming Modelconsumervideo formatsIDREF="Media3-1VB46"Videotape FormatsconsumingIDREF="Media5-1GS38"Using the Buffering InterfaceIDREF="Media5-1GS53"Producing and Consuming Data in BufferscontactingAware, inc.IDREF="Media2-3SW42"Aware Audio Compression Software and Audio ProductsProsonusIDREF="Media2-3SW47"The Prosonus Sound LibrarycontrollersMIDIIDREF="Media2-8MI7"Configuring Your System for MIDI DevelopmentcontrollingDAT drivesIDREF="Media2-7DAT22"Opening and Closing the DAT Device for Audioheadphone and speaker volumeIDREF="Media2-4AL130"Querying and Controlling the Global Audio Device StateIndy audio volumeIDREF="Media2-2HW15"Indy Workstation LayoutconversionsaudioIDREF="Media2-4AL26"Digital Audio Input and Output Sample ResolutionsinputIDREF="Media2-4AL110"Reading Samples from an Input ALportoutputIDREF="Media2-4AL116"Writing Samples to an Output ALportCDISRC to ASCIIIDREF="Media2-6CD91"Communicating CD Status to the End UserlocationsIDREF="Media2-6CD40"Getting CD Locations from the End Usertime codesIDREF="Media2-6CD94"Communicating CD Status to the End UserDATsIDREF="Media2-7DAT62"Communicating DAT Status to the End User IDREF="Media2-7DAT29"Getting DAT Locations from Calculations Internal to Your ApplicationMIDIIDREF="Media2-8MI4"Configuring Your System for MIDI Developmentcoordinate systemsmoviesIDREF="Media6-3PB28"Setting and Getting the Viewing Location Offsetcopyingmovie framesIDREF="Media6-2IO74"Copying and Pasting Frames from One Movie into Anotherparameter-value listsIDREF="Media1-2DM71"Copying the Contents of One Parameter-value List into AnotherparametersIDREF="Media1-2DM72"Copying an Individual Parameter Value from One List into Anothercopyrightsaudio librariesaudio:copyrightsIDREF="Media2-3SW48"The Prosonus Sound LibraryCosmo Compresscapturing input formoviesIDREF="Media6-2IO67"Reading and Inserting Compressed ImagescountingALportsIDREF="Media2-4AL132"Querying and Controlling the Global Audio Device Stateaudio file framesIDREF="Media2-5AF97"Getting Audio Track Sample Frame Countparameter-value list entriesIDREF="Media1-2DM68"Determining the Number of Elements in a Parameter-value ListCPU resourcesIDREF="Media2-3SW5"How Audio Applications Share CPU ResourcescreatingALconfigsIDREF="Media2-4AL47"Creating a New ALconfigmemory-mapped moviesIDREF="Media6-2IO23"Creating a New Moviemovie playback windowIDREF="Media6-3PB7"Creating and Configuring a Playback WindowexampleIDREF="Media6-3PB8"Creating a Window for IRIS GL Playback moviesIDREF="Media6-2IO18"Creating a New MovieexampleIDREF="Media6-2IO24"Creating a New Movieparameter-value listsIDREF="Media1-2DM27"Creating and Destroying Parameter-value Listscritical regions of memoryIDREF="Media2-4AL191"Using Shared Arenas and SemaphoresD1IDREF="Media3-1VB21"YUVD1 525 (YUV)IDREF="Media3-1VB62"Videotape FormatsD1 625 (YUV)IDREF="Media3-1VB63"Videotape FormatsD2 525 (digital NTSC)IDREF="Media3-1VB13"Broadcast StandardsD2 525 (NTSC)IDREF="Media3-1VB64"Videotape FormatsD2 625 (digital PAL)IDREF="Media3-1VB14"Broadcast StandardsD2 625 (PAL)IDREF="Media3-1VB65"Videotape FormatsdaemonvideoIDREF="Media3-2GS6"Video Daemondaemonsmedia libraryIDREF="Media2-3SW63"Compiling and Linking an Audio ApplicationDAT Audio LibraryIDREF="Media2-7DAT1"Programming with the DAT Audio LibrarycompilingIDREF="Media2-3SW64"Compiling and Linking an Audio ApplicationpurposeIDREF="Media1-1ML9"About the Digital Audio and MIDI LibrariesDAT drivesaudio modeIDREF="Media2-7DAT23"Opening and Closing the DAT Device for Audioconfidence testsIDREF="Media2-3SW35"Graphical User Interface Audio ToolscontrollingIDREF="Media2-7DAT22"Opening and Closing the DAT Device for Audioplaying and recording DATsIDREF="Media2-7DAT37"Using the DAT DriveworkaroundIDREF="Media2-7DAT47"Recording Digital Audio over Digital Data Storage (DDS) TapesDAT parserIDREF="Media2-7DAT50"Controlling the DAT Parserallocating and initializingIDREF="Media2-7DAT51"Allocating and Initializing the DAT ParserbasicsIDREF="Media2-7DAT17"DAT ParserfreeingIDREF="Media2-7DAT59"Freeing the Memory Reserved for the DAT Parser datadependenciesIDREF="Media2-4AL190"Using Shared Arenas and Semaphorestwo's complementIDREF="Media2-4AL64"Setting and Getting the Sample Data Format for an ALconfigsample widthsIDREF="Media2-4AL71"Setting and Getting the Integer Sample Width for an ALconfigdata formatsCompression LibraryIDREF="Media5-0ZIN24"Compression Library Data Formatsdata structuresAF LibraryIDREF="Media2-5AF34"Creating an Audio File SetupAudio LibraryIDREF="Media2-4AL6"Audio Library Programming ModelCD Audio LibraryCDFRAMEIDREF="Media2-6CD7"CD Frames, Samples, and SubcodescdtimecodeIDREF="Media2-6CD24"CD Tracks, Indices, and Time CodessubcodeQIDREF="Media2-6CD20"CD Frames, Samples, and SubcodesDAT Audio LibraryDTFRAMEIDREF="Media2-7DAT7"DAT Frames, Samples, and Subcodesdata typesAudio LibraryIDREF="Media2-4AL72"Setting and Getting the Integer Sample Width for an ALconfigDATsbyte orderingIDREF="Media2-7DAT9"DAT Frames, Samples, and SubcodesconversionsIDREF="Media2-7DAT62"Communicating DAT Status to the End User device driverIDREF="Media2-7DAT18"Opening and Closing the DAT Device for AudioIDREF="Media2-7DAT3"DAT Audio Library BasicsexampleIDREF="Media2-7DAT66"DAT Sample Programframe countsIDREF="Media2-7DAT29"Getting DAT Locations from Calculations Internal to Your ApplicationframesIDREF="Media2-7DAT4"DAT Frames, Samples, and Subcodeslead-inIDREF="Media2-7DAT45"Recording the DAT Lead-in AreanavigatingIDREF="Media2-7DAT25"Navigating through a DATnonaudio informationIDREF="Media2-7DAT5"DAT Frames, Samples, and SubcodesparsingIDREF="Media2-7DAT49"Controlling the DAT ParsercallbacksIDREF="Media2-7DAT54"Adding and Removing DAT Parser Callbacks framesIDREF="Media2-7DAT57"Parsing DAT Frames playingIDREF="Media2-7DAT38"Playing a Tape in the DAT DrivedatmanIDREF="Media2-3SW30"Graphical User Interface Audio Toolsplaying and recordingIDREF="Media2-7DAT37"Using the DAT DrivereadingIDREF="Media2-7DAT41"Reading Audio Data from the DAT Drive recordingIDREF="Media2-7DAT39"Making DAT Recordings for Playback on the DAT DriveexamplesIDREF="Media2-7DAT48"Example Programs Demonstrating DAT Recordingrecording from CDIDREF="Media2-5AF62"Initializing AES DatasamplesIDREF="Media2-7DAT8"DAT Frames, Samples, and SubcodesseekingIDREF="Media2-7DAT35"Seeking to a DAT Locationend user locationsIDREF="Media2-7DAT27"Getting DAT Locations from the End UserstatusingIDREF="Media2-7DAT61"Communicating DAT Status to the End User subcodesIDREF="Media2-7DAT10"DAT Frames, Samples, and SubcodessubdivisionsIDREF="Media2-7DAT15"DAT Audio Program Numbers and Indicestime codesIDREF="Media2-7DAT28"Getting DAT Locations from the End UserIDREF="Media2-7DAT16"DAT Run Time, Absolute Time, and Program Timechecking and settingIDREF="Media2-7DAT44"Writing Audio Data to the DAT DrivetimestampsIDREF="Media2-7DAT43"Writing Audio Data to the DAT DrivewritingIDREF="Media2-7DAT42"Writing Audio Data to the DAT DrivedebuggingDM LibraryIDREF="Media1-2DM21"Debugging a Digital Media Library ApplicationMovie LibraryIDREF="Media6-1GS18"Debugging a Movie Library ApplicationMovie Library I/OIDREF="Media6-1GS25"Emulating I/O FailuresdecompressorIDREF="Media5-1GS20"Decompressing a Sequence of FramesdefaultsaudioIDREF="Media1-2DM36"Setting Audio DefaultschannelsIDREF="Media2-4AL42"Using ALconfig Structures to Configure ALportsfloating point rangeIDREF="Media2-4AL78"Getting and Setting the Floating Point Data RangegettingIDREF="Media2-4AL147"Getting the Defaults of Global ParametersI/O rateIDREF="Media2-3SW20"How Global Audio Settings Are Established and MaintainedportsIDREF="Media2-4AL40"Using ALconfig Structures to Configure ALportsaudio filesIDREF="Media2-5AF38"Creating an Audio File SetupconfiguringIDREF="Media1-2DM33"Creating Default Audio and Image ConfigurationsimagesIDREF="Media1-2DM42"Setting Image DefaultsinstrumentsIDREF="Media2-5AF45"Creating an Audio File SetupmovieaudioIDREF="Media6-1GS91"Setting and Getting Audio Track Propertiesaudio volumeIDREF="Media6-1GS97"Setting and Getting the Default Volume of an Audio TrackglobalIDREF="Media6-1GS63"Creating a Default Movie ConfigurationimagesIDREF="Media6-1GS122"Setting and Getting Image Track Propertiesloop limitIDREF="Media6-1GS46"Setting and Getting the Default Movie Loop Limitloop modeIDREF="Media6-1GS39"Setting and Getting the Default Movie Loop ModedefinitionsMovie LibraryIDREF="Media6-1GS1"DefinitionsdelayaudioIDREF="Media2-4AL113"Writing Samples to an Output ALportdelaysMovie Library I/OIDREF="Media6-1GS27"Emulating I/O Failuresdeletingmovie framesIDREF="Media6-2IO63"Deleting Frames from a Movie Trackmovie tracksIDREF="Media6-2IO45"Removing an Audio or Image Track from a MovieparametersIDREF="Media1-2DM80"Removing an Element from a Parameter-value ListdependenciesdataIDREF="Media2-4AL190"Using Shared Arenas and SemaphoresdetuningIDREF="Media2-5AF19"Instrument Configurations and Loopsdevice driversDATIDREF="Media2-7DAT3"DAT Audio Library BasicsIDREF="Media2-7DAT18"Opening and Closing the DAT Device for AudiodevicesaudioIDREF="Media2-4AL4"Audio Library Programming Modelgetting defaultsIDREF="Media2-4AL147"Getting the Defaults of Global ParametersCD-ROMcontrollingIDREF="Media2-6CD28"Opening and Closing the CD-ROM DeviceMIDIIDREF="Media2-8MI6"Configuring Your System for MIDI DevelopmentSCSIIDREF="Media2-3SW3"Digital Audio System Software OverviewlibraryIDREF="Media2-3SW62"Compiling and Linking an Audio Applicationvideomanaging IDREF="Media3-2GS7"Device ManagementdigitalaudioratesIDREF="Media2-4AL139"Querying and Controlling the Global Audio Device StateDigital Data Storage (DDS) tapesIDREF="Media2-7DAT46"Recording Digital Audio over Digital Data Storage (DDS) Tapesdigital mediaparameter typesIDREF="Media1-2DM8"Digital Media Type Definitionstype definitionsIDREF="Media1-2DM7"Digital Media Type DefinitionsDigital Media (DM) Library. See DM LibraryIDREF="Media1-2DM1"Programming with the Digital Media Librarydigital videoformatsIDREF="Media3-1VB61"Videotape FormatsYUV (CCIR 601)IDREF="Media3-1VB27"YUVdigital video formatsIDREF="Media3-1VB15"Broadcast Standardsdigital video recordingIDREF="Media3-1VB12"Broadcast StandardsdigitizingaudioIDREF="Media2-4AL9"Digital Audio Data RepresentationdiscontinuitiesaudioIDREF="Media2-4AL119"Detecting Errors in the Audio StreamcauseIDREF="Media2-4AL93"Using Audio Sample Queuesdisk spaceaudioIDREF="Media2-2HW21"Disk SpacedistortionaudioIDREF="Media2-4AL155"Modifying the Values of the Global ParametersDM LibraryIDREF="Media1-2DM3"Digital Media Library BasicsassertionsIDREF="Media1-2DM21"Debugging a Digital Media Library Applicationcompiling and linkingIDREF="Media1-2DM19"Compiling and Linking a Digital Media Library ApplicationdebuggingIDREF="Media1-2DM21"Debugging a Digital Media Library ApplicationfeaturesIDREF="Media1-2DM4"Digital Media Library Basicsgetting and setting parametersIDREF="Media1-2DM49"Setting and Getting Individual Parameter ValuesexampleIDREF="Media1-2DM67"Setting and Getting Individual Parameter Valuesheader filesIDREF="Media1-2DM13"Compiling and Linking a Digital Media Library Applicationinclude filesIDREF="Media1-2DM14"Compiling and Linking a Digital Media Library ApplicationinitializingIDREF="Media1-2DM23"Initializing a Digital Media Applicationparameter-value listsIDREF="Media1-2DM24"Initializing a Digital Media ApplicationdefinedIDREF="Media1-2DM10"Digital Media ParametersexampleIDREF="Media1-2DM83"Removing an Element from a Parameter-value ListpurposeIDREF="Media1-1ML2"About the Digital Media Librarysupported librariesIDREF="Media1-2DM2"Digital Media Library Basicstype definitionsIDREF="Media1-2DM7"Digital Media Type DefinitionsDM_MEDIUMIDREF="Media1-2DM12"Digital Media ParametersdrainsvideoIDREF="Media3-2GS20"VL Architectural Model of Video Devicesdynamically tuning audio applicationsIDREF="Media2-4AL120"Detecting Errors in the Audio StreamEA IFF 85IDREF="Media2-5AF33"AIFF-C and the AF Library APIEA IFF 85 standardIDREF="Media2-5AF27"AIFF-C and the AF Library APIeditingmoviesIDREF="Media6-2IO51"Editing Moviesejecting CDsIDREF="Media2-6CD31"Controlling the CD-ROM Drive Caddyembedded moviesIDREF="Media6-2IO6"Using File Descriptors with MoviesopeningIDREF="Media6-2IO7"Using File Descriptors with MoviesseekingIDREF="Media6-2IO17"Using File Descriptors with Moviesemphasisaudio filesIDREF="Media2-5AF60"Initializing AES DataemulatingMovie Library I/O errorsIDREF="Media6-1GS23"Emulating I/O FailuresencodingvideoillustratedIDREF="Media3-1VB40"Composite Videoend usersCD controlIDREF="Media2-6CD39"Getting CD Locations from the End Userenvironment variablesMovie LibraryIDREF="Media6-1GS22"Emulating I/O FailuresequationsYIQIDREF="Media3-1VB30"YIQYUVIDREF="Media3-1VB25"YUVerror handlersaudioIDREF="Media2-4AL30"Handling Audio Library Errorserror handlingAudio LibraryIDREF="Media2-4AL27"Handling Audio Library ErrorsCompression LibraryIDREF="Media5-1GS7"About File I/O and Error HandlingerrorsAF LibraryIDREF="Media2-5AF7"Handling Audio File Library Errorsallocating audio configurationsIDREF="Media2-4AL49"Creating a New ALconfigaudiochannelsIDREF="Media2-4AL53"Setting and Getting the Number of Channels for an ALconfigaudio I/OIDREF="Media2-4AL117"Detecting Errors in the Audio StreamtypesIDREF="Media2-4AL122"Detecting Errors in the Audio StreameventsVideo Library (VL)IDREF="Media3-2GS2"Getting Started with the Video Libraryevents, handlingGL eventsIDREF="Media4-5EH1"IRIS GL Event HandlingexamplesaudioIDREF="Media2-3SW36"Online Source Code Examplesexternal bufferIDREF="Media5-1GS41"Creating a BufferfailuresmovieemulatingIDREF="Media6-1GS24"Emulating I/O FailuresfeaturesALportsIDREF="Media2-4AL37"About ALportsAudio LibraryIDREF="Media2-4AL2"Audio Library FeaturesCD Audio LibraryIDREF="Media2-6CD3"CD Audio Library BasicsDM LibraryIDREF="Media1-2DM4"Digital Media Library BasicsIndigoaudioIDREF="Media2-2HW2"Indigo Audio FeaturesMovie LibraryIDREF="Media6-0ZIN1"Movie Library FeaturesVLIDREF="Media1-1ML16"About the Video Libraryfeatures of the Compression LibraryIDREF="Media5-0ZIN10"Compression Library FeaturesfieldsvideoIDREF="Media3-1VB2"Interlacingfile descriptorsaudioIDREF="Media2-4AL167"Multiplexing Synchronous I/OgettingIDREF="Media2-4AL169"Getting a File Descriptor for an ALportmoviesIDREF="Media6-2IO5"Using File Descriptors with Moviesfile formatsAF LibraryIDREF="Media2-5AF51"Initializing Audio File FormatAIFFIDREF="Media2-5AF26"AIFF-C and the AF Library APIaudioIDREF="Media2-5AF10"About Audio Filesaudio filesdefaultIDREF="Media2-5AF39"Creating an Audio File SetupinitializingIDREF="Media2-5AF49"Initializing Audio File FormatEA IFF 85IDREF="Media2-5AF27"AIFF-C and the AF Library APImovieIDREF="Media6-1GS12"Movie File FormatsgettingIDREF="Media6-1GS58"Getting the Movie File FormatSiliconGraphicsIDREF="Media6-1GS13"Silicon Graphics Movie Formatsfile I/OMovie LibraryIDREF="Media6-2IO1"File I/O and Editing Movies with the Movie LibraryillustratedIDREF="Media6-2IO3"Initializing a Movie Library ApplicationoverviewIDREF="Media6-2IO2"Initializing a Movie Library Applicationfile I/O in the Compression LibraryIDREF="Media5-1GS6"About File I/O and Error Handlingfilesaccess mode for moviesIDREF="Media6-2IO15"Using File Descriptors with MoviesAIFF-CIDREF="Media2-5AF8"About Audio FilesformatsAF LibraryIDREF="Media2-5AF4"Programming with the Audio File Libraryfill pointsaudioIDREF="Media2-4AL175"Setting and Getting the Fill Point for a QueueillustratedIDREF="Media2-4AL179"Setting and Getting the Fill Point for a QueuefillableIDREF="Media2-4AL97"Monitoring the Audio Sample Queue Status to Provide Nonblocking I/OfilledIDREF="Media2-4AL96"Monitoring the Audio Sample Queue Status to Provide Nonblocking I/Ofindingmovie tracksIDREF="Media6-2IO47"Locating an Existing TrackFIT images in moviesIDREF="Media6-1GS142"Getting the Image FormatflagsAES channel status bytesIDREF="Media2-5AF66"Initializing AES DataPOLLINIDREF="Media2-4AL171"Getting a File Descriptor for an ALportPOLLOUTIDREF="Media2-4AL172"Getting a File Descriptor for an ALportfloating pointaudio formatsIDREF="Media2-4AL66"Setting and Getting the Sample Data Format for an ALconfigfloating point rangegetting and settingIDREF="Media2-4AL76"Getting and Setting the Floating Point Data Rangeflushingmovie editsIDREF="Media6-2IO79"Finalizing Changes and Closing MoviesformatsaudioIDREF="Media2-4AL20"Digital Audio Sample FormatsdefaultIDREF="Media2-4AL43"Using ALconfig Structures to Configure ALportsfloating pointIDREF="Media2-4AL66"Setting and Getting the Sample Data Format for an ALconfiggetting and settingIDREF="Media2-4AL63"Setting and Getting the Sample Data Format for an ALconfignativeIDREF="Media2-4AL22"Digital Audio Input and Output Sample Resolutionstwo's complementIDREF="Media2-4AL64"Setting and Getting the Sample Data Format for an ALconfigbroadcast videoIDREF="Media3-1VB66"Videotape FormatsCDlocationsIDREF="Media2-6CD38"Navigating through a CDconsumer videoIDREF="Media3-1VB46"Videotape Formatsdigital videoIDREF="Media3-1VB61"Videotape FormatsmoviesIDREF="Media6-1GS12"Movie File Formatsparameter-value listsIDREF="Media1-2DM11"Digital Media ParametersvideoIDREF="Media3-1VB7"Broadcast StandardsvideotapeIDREF="Media3-1VB44"Videotape Formatsframe countdefinedIDREF="Media6-1GS5"Definitionsframe countsaudio filesIDREF="Media2-5AF97"Getting Audio Track Sample Frame CountCDIDREF="Media2-6CD41"Getting CD Locations from Calculations Internal to Your ApplicationDATsIDREF="Media2-7DAT29"Getting DAT Locations from Calculations Internal to Your Applicationframe ratedefinedIDREF="Media6-1GS7"DefinitionsvideoIDREF="Media3-1VB3"InterlacingframesaudioIDREF="Media2-4AL15"Digital Audio Sample FramesillustratedIDREF="Media2-4AL18"Digital Audio Sample Framesaudio fileIDREF="Media2-5AF15"Audio Tracks, Sample Frames, and Track Markersaudio filesIDREF="Media2-5AF103"Reading Audio Frames from an Audio TrackCDIDREF="Media2-6CD6"CD Frames, Samples, and SubcodesDATIDREF="Media2-7DAT4"DAT Frames, Samples, and Subcodesdeleting from moviesIDREF="Media6-2IO63"Deleting Frames from a Movie TrackillustratedIDREF="Media6-1GS6"DefinitionsnumberingIDREF="Media6-1GS5"DefinitionsvideoIDREF="Media3-1VB2"InterlacingfreeingAFfilesetupIDREF="Media2-5AF47"Creating an Audio File SetupALconfigsIDREF="Media2-4AL84"Retrieving the Setup of an Existing ALportCD parserIDREF="Media2-6CD85"Freeing the Memory Allocated for the Parser DAT parserIDREF="Media2-7DAT59"Freeing the Memory Reserved for the DAT Parser parameter-value listsIDREF="Media1-2DM30"Creating and Destroying Parameter-value Listsfull-scale audio rangesIDREF="Media2-4AL70"Setting and Getting the Integer Sample Width for an ALconfiggainIDREF="Media2-5AF21"Instrument Configurations and LoopsGalileo VideoIDREF="Media1-1ML15"About the Video Librarygettingfile descriptorsaudioIDREF="Media2-4AL169"Getting a File Descriptor for an ALportmovieaudio track propertiesIDREF="Media6-1GS89"Setting and Getting Audio Track Propertiesaudio track properties:audio channelsIDREF="Media6-1GS110"Getting the Number of Audio Channels in an Audio Trackaudio track properties:audio compressionIDREF="Media6-1GS117"Getting the Audio Compression Scheme of an Audio Trackaudio track properties:audio formatIDREF="Media6-1GS114"Getting the Audio Format of an Audio Trackaudio track properties:audio sample rateIDREF="Media6-1GS107"Getting the Audio Sample Rate of an Audio Trackaudio track properties:audio sample widthIDREF="Media6-1GS104"Getting the Audio Sample Width of an Audio Trackaudio track properties:default volumeIDREF="Media6-1GS100"Setting and Getting the Default Volume of an Audio Trackbackground colorIDREF="Media6-3PB12"Setting and Getting the Background Colorfile formatIDREF="Media6-1GS58"Getting the Movie File Formatgeneral track propertiesIDREF="Media6-1GS74"Setting and Getting Track Propertiesgeneral track properties:lengthIDREF="Media6-1GS84"Getting the Track Lengthgeneral track properties:mediumIDREF="Media6-1GS87"Getting the Track Mediumgeneral track properties:SMPTE time code stringIDREF="Media6-1GS79"Setting and Getting SMPTE Time Code Strings Stored in Tracksglobal propertiesIDREF="Media6-1GS37"Setting and Getting Global Movie Propertiesglobal properties:file formatIDREF="Media6-1GS57"Getting the Movie File Formatglobal properties:loop limitIDREF="Media6-1GS49"Setting and Getting the Default Movie Loop Limitglobal properties:loop modeIDREF="Media6-1GS43"Setting and Getting the Default Movie Loop Modeglobal properties:optimizationIDREF="Media6-1GS60"Getting the Movie Optimization Settingglobal properties:titleIDREF="Media6-1GS52"Setting and Getting the Movie Titleimage track propertiesIDREF="Media6-1GS120"Setting and Getting Image Track Propertiesimage track properties:frame rateIDREF="Media6-1GS127"Setting and Getting the Image Frame Rateimage track properties:image formatIDREF="Media6-1GS141"Getting the Image Formatimage track properties:image heightIDREF="Media6-1GS135"Getting the Image Frame Sizeimage track properties:image orientationIDREF="Media6-1GS138"Getting the Image Orientationimage track properties:image packing formatIDREF="Media6-1GS157"Getting the Image Packing Formatimage track properties:image widthIDREF="Media6-1GS132"Getting the Image Frame SizeparametersIDREF="Media6-1GS30"Setting and Getting Movie Propertiesview sizeIDREF="Media6-3PB23"Setting and Getting the Viewing Area SizeparametersIDREF="Media1-2DM59"Setting and Getting Individual Parameter ValuesnameIDREF="Media1-2DM74"Determining the Name of a Given ParametertotalIDREF="Media1-2DM68"Determining the Number of Elements in a Parameter-value ListtypeIDREF="Media1-2DM76"Determining the Data Type of a Given ParameterGIF images in moviesIDREF="Media6-1GS146"Getting the Image FormatGLevents, handlingIDREF="Media4-5EH1"IRIS GL Event Handlingglobal audio device stateIDREF="Media2-4AL124"Querying and Controlling the Global Audio Device Stateglobal audio parametersIDREF="Media2-4AL126"Querying and Controlling the Global Audio Device Stateglobal audio stateIDREF="Media2-3SW22"How Global Audio Settings Are Established and MaintainedchangingIDREF="Media2-4AL154"Modifying the Values of the Global Parametersglobal movie propertiesIDREF="Media6-1GS35"Setting and Getting Global Movie PropertiesGraphics Library, recommended readingIDREF="Media0-5ATB1"References for Using Digital Media with Other LibrariesgrayscaleIDREF="Media5-2AP8"Choosing a Compression Library Algorithmguaranteeing movie view sizeIDREF="Media6-3PB19"Setting and Getting the Viewing Area SizehandlesAFfilehandleIDREF="Media2-5AF35"Creating an Audio File SetupALconfigsIDREF="Media2-4AL47"Creating a New ALconfigCDFRAMEIDREF="Media2-6CD8"CD Frames, Samples, and SubcodesDAT frameIDREF="Media2-7DAT6"DAT Frames, Samples, and Subcodesparameter-value listsIDREF="Media1-2DM29"Creating and Destroying Parameter-value ListshardwareIndigoaudioIDREF="Media2-2HW1"Indigo Audio System Architecturehardware accelerationcompressionIDREF="Media5-1GS5"Buffered Access APIHeadIDREF="Media5-1GS35"Using the Buffering InterfaceheaderreadingIDREF="Media5-1GS22"Getting Stream InformationstructureIDREF="Media5-1GS26"Getting Stream Informationheader filesDM LibraryIDREF="Media1-2DM13"Compiling and Linking a Digital Media Library Applicationdm_params.hIDREF="Media1-2DM9"Digital Media Type Definitionsdmedia.hIDREF="Media1-2DM6"Digital Media Type DefinitionsVideo Library (VL)IDREF="Media3-2GS17"Library and Header Filesheadphonescontrolling volumeIDREF="Media2-4AL130"Querying and Controlling the Global Audio Device StatemutingIDREF="Media2-4AL134"Querying and Controlling the Global Audio Device Statehertz (Hz)IDREF="Media2-4AL12"Digital Audio Sample RatesHi-8mmIDREF="Media3-1VB57"Videotape Formatshintsmultiple movie playbackIDREF="Media6-3PB3"Opening a Movie for PlaybackI/OaudioIDREF="Media2-4AL106"Reading and Writing Sampleserror parametersIDREF="Media2-4AL122"Detecting Errors in the Audio StreamerrorsIDREF="Media2-4AL117"Detecting Errors in the Audio Streammeasuring and matching sample ratesIDREF="Media2-4AL129"Querying and Controlling the Global Audio Device StatemultiplexingIDREF="Media2-4AL166"Multiplexing Synchronous I/OnonblockingIDREF="Media2-4AL101"Monitoring the Audio Sample Queue Status to Provide Nonblocking I/Oquerying ratesIDREF="Media2-4AL158"Determining the Input and Output RatesMIDIIDREF="Media2-8MI15"Opening and Closing MIDI PortsMovie LibrarytestingIDREF="Media6-1GS23"Emulating I/O FailuresiconsaudioIDREF="Media2-2HW5"Indigo Audio I/O InterfaceIEEEdouble-precision floating point dataIDREF="Media2-4AL66"Setting and Getting the Sample Data Format for an ALconfigimagecompressionIDREF="Media5-1GS8"Using the Still Image Interfaceimage compressionIDREF="Media5-1GS2"Still Image APIimagesbuffer sizeIDREF="Media1-2DM46"Determining the Buffer Size Needed to Store an Image Framecompressedediting in moviesIDREF="Media6-2IO66"Reading and Inserting Compressed Imagesinserting into moviesIDREF="Media6-2IO72"Inserting a Compressed Image from a Buffer into an Existing Trackcompression in moviesIDREF="Media6-1GS159"Getting the Image Compression SchemedefaultsIDREF="Media1-2DM42"Setting Image DefaultsFIT in moviesIDREF="Media6-1GS142"Getting the Image Formatformat for moviesIDREF="Media6-1GS139"Getting the Image FormatformatsSGIIDREF="Media6-1GS149"Getting the Image Formatgettingcompression for moviesIDREF="Media6-1GS162"Getting the Image Compression Schemeheight for moviesIDREF="Media6-1GS133"Getting the Image Frame Sizeinserting into moviesIDREF="Media6-2IO59"Inserting Raw Images and Audio from a Buffer into an Existing TrackKodak Photo CD in moviesIDREF="Media6-1GS148"Getting the Image FormatmoviedefaultsIDREF="Media6-1GS122"Setting and Getting Image Track Propertiesmovie frame rateIDREF="Media6-1GS126"Setting and Getting the Image Frame Rateorientation for moviesIDREF="Media6-1GS136"Getting the Image Orientationpacking format for moviesIDREF="Media6-1GS155"Getting the Image Packing FormatparametersIDREF="Media1-2DM41"Image Parametersproperties in moviesIDREF="Media6-1GS119"Setting and Getting Image Track PropertiesTIFF in moviesIDREF="Media6-1GS153"Getting the Image Formatwidth for moviesIDREF="Media6-1GS130"Getting the Image Frame SizeImageVision Libraryreading and writing movie imagesIDREF="Media6-1GS144"Getting the Image Formatinclude filesDM LibraryIDREF="Media1-2DM14"Compiling and Linking a Digital Media Library ApplicationindexesCD tracksIDREF="Media2-6CD23"CD Tracks, Indices, and Time CodesIndigoaudiofeaturesIDREF="Media2-2HW2"Indigo Audio FeatureshardwareIDREF="Media2-2HW1"Indigo Audio System ArchitecturejacksIDREF="Media2-2HW4"Indigo Audio I/O Interfacequeue size limitsIDREF="Media2-4AL59"Setting and Getting the Sample Queue Size for an ALconfigMIDI portsIDREF="Media2-8MI8"Connecting Devices to MIDI I/O InterfacesIndigo2audio4-channelIDREF="Media2-2HW13"Indigo2 and Indy Audio I/O InterfacefeaturesIDREF="Media2-2HW8"Indigo2 and Indy Audio System ArchitecturehardwareIDREF="Media2-2HW7"Indigo2 and Indy Audio System ArchitecturejacksIDREF="Media2-2HW14"Indigo2 and Indy Audio I/O Interfacequeue size limitsIDREF="Media2-4AL59"Setting and Getting the Sample Queue Size for an ALconfigMIDI portsIDREF="Media2-8MI10"Connecting Devices to MIDI I/O InterfacesIndigo2 VideoIDREF="Media1-1ML13"About the Video LibraryIndigoVideo LibrarypurposeIDREF="Media1-1ML18"About the IndigoVideo LibraryIndyaudio4-channelIDREF="Media2-2HW11"Indigo2 and Indy Audio System ArchitecturefeaturesIDREF="Media2-2HW9"Indigo2 and Indy Audio System ArchitecturehardwareIDREF="Media2-2HW9"Indigo2 and Indy Audio System ArchitecturejacksIDREF="Media2-2HW17"Indy Workstation Layoutqueue size limitsIDREF="Media2-4AL59"Setting and Getting the Sample Queue Size for an ALconfigvolume buttonsIDREF="Media2-2HW16"Indy Workstation LayoutMIDI portsIDREF="Media2-8MI11"Connecting Devices to MIDI I/O InterfacesmutingIDREF="Media2-2HW15"Indy Workstation LayoutIndy VideoIDREF="Media1-1ML14"About the Video LibraryinitializingAudio LibraryIDREF="Media2-4AL34"Initializing an Audio Library ApplicationCD parserIDREF="Media2-6CD73"Allocating and Initializing the CD ParserDM LibraryIDREF="Media1-2DM23"Initializing a Digital Media ApplicationMIDIIDREF="Media2-8MI12"Configuring Serial Ports for MIDI WIth the Port Setup ToolinputaudioIDREF="Media2-4AL108"Reading Samples from an Input ALport4-channelIDREF="Media2-4AL109"Reading Samples from an Input ALportconversionsIDREF="Media2-4AL110"Reading Samples from an Input ALportsourceIDREF="Media2-4AL127"Querying and Controlling the Global Audio Device Stateinsertingmovie dataIDREF="Media6-2IO58"Inserting Raw Images and Audio from a Buffer into an Existing TrackinstancesmovieIDREF="Media6-1GS11"Movie Library Programming Modelinstrument configurationsIDREF="Media2-5AF105"Reading and Writing Instrument Configurationsaudio filesIDREF="Media2-5AF17"Instrument Configurations and LoopsdefaultIDREF="Media2-5AF43"Creating an Audio File SetupdefinedIDREF="Media2-5AF13"About Audio FilesparametersIDREF="Media2-5AF45"Creating an Audio File SetupinstrumentssamplesIDREF="Media2-3SW46"The Prosonus Sound LibraryinterlacingvideoIDREF="Media3-1VB1"InterlacinginterleavingaudioIDREF="Media2-4AL16"Digital Audio Sample Framesinternal bufferIDREF="Media5-1GS40"Creating a Bufferinternal formatIDREF="Media5-0ZIN28"Video Data FormatsInternational Standard Recording Code (ISRC)IDREF="Media2-6CD17"CD Frames, Samples, and Subcodesconverting to ASCIIIDREF="Media2-6CD92"Communicating CD Status to the End Userinterprocess communicationIDREF="Media2-4AL188"Using Shared Arenas and SemaphoresexampleIDREF="Media2-4AL200"Using Shared Arenas and Semaphoresintializingaudio filesIDREF="Media2-5AF6"Audio File Library Programming ModelIRIS Digital Media Development EnvironmentIDREF="Media1-1ML1"Programming with the IRIS Digital Media Development EnvironmentIRIS GLscreen originIDREF="Media6-3PB30"Setting and Getting the Viewing Location OffsetIRIS Media Librariesdigital audio librariesIDREF="Media2-1GS1"Introduction to Digital Audio and MIDI ProgrammingIRIXfile systemIDREF="Media2-3SW2"Digital Audio System Software Overviewreal-time programmingIDREF="Media2-4AL163"Real-time Programming Techniques for AudioIRIX kernelIDREF="Media2-3SW6"How Audio Applications Share CPU Resources4DgiftsIDREF="Media2-3SW37"Online Source Code ExamplesapanelIDREF="Media2-3SW17"How Outputs from Multiple Audio Applications Are CombinedIDREF="Media2-3SW21"How Global Audio Settings Are Established and MaintainedIDREF="Media2-3SW26"Graphical User Interface Audio ToolsIDREF="Media2-3SW8"How Audio Applications Share Audio System ResourcesAudio Interchange File Format with Compression. See AIFF-CIDREF="Media2-5AF30"AIFF-C and the AF Library APIcdmanIDREF="Media2-3SW27"Graphical User Interface Audio Toolscdsample.cIDREF="Media2-6CD96"CD Sample ProgramdatmanIDREF="Media2-3SW29"Graphical User Interface Audio Toolsdatplay.cIDREF="Media2-7DAT66"DAT Sample Programdm_audio.hIDREF="Media1-2DM16"Compiling and Linking a Digital Media Library Applicationdm_image.hIDREF="Media1-2DM17"Compiling and Linking a Digital Media Library Applicationdm_params.hIDREF="Media1-2DM18"Compiling and Linking a Digital Media Library ApplicationIDREF="Media1-2DM9"Digital Media Type Definitionsdmedia.hIDREF="Media1-2DM15"Compiling and Linking a Digital Media Library ApplicationIDREF="Media1-2DM5"Digital Media Type Definitionslibaudiofile.so. See AF LibraryIDREF="Media2-5AF2"Programming with the Audio File Librarylibaudioutil.aIDREF="Media2-3SW57"Compiling and Linking an Audio Applicationlibcdaudio. See CD Audio LibraryIDREF="Media2-6CD2"Programming with the CD Audio Librarylibcl. See Compression LibraryIDREF="Media1-1ML20"About the Compression Librarylibdataudio. See DAT Audio LibraryIDREF="Media2-7DAT2"Programming with the DAT Audio Librarylibdmedia.so. SeeDM LibraryIDREF="Media1-1ML3"About the Digital Media Librarylibds.aIDREF="Media2-3SW61"Compiling and Linking an Audio Applicationlibm.aIDREF="Media2-3SW55"Compiling and Linking an Audio Applicationlibmalloc.aIDREF="Media2-3SW68"Compiling and Linking an Audio Applicationlibmidi. See MIDI LibraryIDREF="Media2-8MI2"Programming with the MIDI Librarylibmovie. See Movie LibraryIDREF="Media1-1ML22"About the Movie Librarylibmovie_d.aIDREF="Media6-1GS19"Debugging a Movie Library Applicationlseek(2)IDREF="Media6-2IO16"Using File Descriptors with MoviesmemtovidIDREF="Media3-2GS16"Generic Video Toolsopen(2)IDREF="Media6-2IO8"Using File Descriptors with MoviessoundeditorIDREF="Media2-3SW18"How Outputs from Multiple Audio Applications Are CombinedIDREF="Media2-3SW9"How Audio Applications Share Audio System ResourcesIDREF="Media2-3SW31"Graphical User Interface Audio ToolsIDREF="Media2-3SW13"How Outputs from Multiple Audio Applications Are CombinedsoundfilerIDREF="Media2-3SW14"How Outputs from Multiple Audio Applications Are CombinedIDREF="Media2-3SW10"How Audio Applications Share Audio System ResourcesIDREF="Media2-3SW19"How Outputs from Multiple Audio Applications Are CombinedIDREF="Media2-3SW33"Graphical User Interface Audio ToolsstderrIDREF="Media2-4AL28"Handling Audio Library ErrorsvcpIDREF="Media3-2GS11"Generic Video ToolsvideodIDREF="Media3-2GS6"Video DaemonvideopanelIDREF="Media3-2GS10"Generic Video ToolsvidtomemIDREF="Media3-2GS15"Generic Video ToolsvlinfoIDREF="Media3-2GS14"Generic Video ToolsjacksAESIDREF="Media2-2HW6"Indigo Audio I/O InterfaceIndigoaudioIDREF="Media2-2HW4"Indigo Audio I/O InterfaceIndigo2audioIDREF="Media2-2HW12"Indigo2 and Indy Audio I/O InterfaceIDREF="Media2-2HW14"Indigo2 and Indy Audio I/O InterfaceJPEGIDREF="Media5-0ZIN16"Still Image AlgorithmsIDREF="Media5-1GS11"Using the Still Image InterfacemoviesIDREF="Media6-1GS169"Getting the Image Compression Schemekey velocityIDREF="Media2-5AF20"Instrument Configurations and LoopskeyframesIDREF="Media5-0ZIN26"Video Data FormatsdefinedIDREF="Media6-1GS4"DefinitionsKodak Photo CD images in moviesIDREF="Media6-1GS147"Getting the Image FormatLaunchword/usr/sbin/jot /usr/people/4Dgifts/examples/dmedia/audio/ratequery.cIDREF="Media2-4AL159"Determining the Input and Output Rateslead-inDATsIDREF="Media2-7DAT45"Recording the DAT Lead-in ArealibrariesAudio File LibraryIDREF="Media1-1ML6"About the Digital Audio and MIDI LibrariesAudio LibraryIDREF="Media1-1ML5"About the Digital Audio and MIDI LibrariesIDREF="Media2-4AL1"Programming with the Audio LibraryAudio Utility LibraryIDREF="Media1-1ML7"About the Digital Audio and MIDI LibrariesCD audioIDREF="Media2-2HW23"PeripheralsCD Audio LibraryIDREF="Media2-6CD2"Programming with the CD Audio LibraryIDREF="Media1-1ML8"About the Digital Audio and MIDI LibrariesCompression LibraryIDREF="Media1-1ML19"About the Compression LibraryDAT Audio LibraryIDREF="Media2-7DAT1"Programming with the DAT Audio LibraryIDREF="Media1-1ML9"About the Digital Audio and MIDI LibrariesDM LibraryIDREF="Media1-2DM3"Digital Media Library BasicsIDREF="Media1-1ML2"About the Digital Media LibraryIndigoVideo LibraryIDREF="Media1-1ML18"About the IndigoVideo LibrarymathIDREF="Media2-3SW56"Compiling and Linking an Audio ApplicationMIDIIDREF="Media1-1ML4"About the Digital Audio and MIDI LibrariesIDREF="Media2-8MI1"Programming with the MIDI LibraryMovie LibraryIDREF="Media1-1ML21"About the Movie LibraryProsonusIDREF="Media2-3SW43"The Prosonus Sound LibraryVideo LibraryIDREF="Media1-1ML10"About the Video LibraryVideo Library (VL)IDREF="Media3-2GS1"Getting Started with the Video LibrarylicenseinstallingIDREF="MediaA-2B24"Installing a NetLS Nodelocked LicensenodelockedIDREF="MediaA-2B22"Installing a NetLS Nodelocked Licensequerying forIDREF="Media5-2AP19"Getting License Informationlicensing Aware compression softwareIDREF="MediaA-2B2"Introduction to Aware Audio Compression SoftwarelimitingaudioIDREF="Media2-3SW16"How Outputs from Multiple Audio Applications Are Combinedlimitsaudiofloating point rangeIDREF="Media2-4AL76"Getting and Setting the Floating Point Data RangegettingIDREF="Media2-4AL145"Getting the Bounds of Global Parametersqueue sizeIDREF="Media2-4AL58"Setting and Getting the Sample Queue Size for an ALconfigCD tracksIDREF="Media2-6CD21"CD Tracks, Indices, and Time Codeslinear pulse code modulation (PCM)IDREF="Media2-4AL19"Digital Audio Sample FormatslinkingMovie LibraryIDREF="Media6-1GS16"Compiling and Linking a Movie Library Applicationdebugging versionIDREF="Media6-1GS21"Using the Debugging Version of the Movie Librarynon-ANSI-compliant codeIDREF="Media2-3SW70"Compiling and Linking an Audio Applicationlocatingmovie tracksIDREF="Media6-2IO47"Locating an Existing TracklocationsCDIDREF="Media2-6CD37"Navigating through a CDcurrentIDREF="Media2-6CD49"Getting the Current CD Location formatsIDREF="Media2-6CD38"Navigating through a CDlockingarenasIDREF="Media2-4AL187"Using Shared Arenas and SemaphoresCD-ROM ejectIDREF="Media2-6CD32"Controlling the CD-ROM Drive CaddyloopingmodesdefinedIDREF="Media6-1GS9"DefinitionsmoviesIDREF="Media6-1GS8"Definitionsloopsaudio filesIDREF="Media2-5AF22"Instrument Configurations and LoopsdefaultIDREF="Media2-5AF44"Creating an Audio File SetupdefaultlimitIDREF="Media6-1GS46"Setting and Getting the Default Movie Loop LimitmodeIDREF="Media6-1GS40"Setting and Getting the Default Movie Loop ModelosslessdefinitionIDREF="Media5-0ZIN12"Lossy versus Lossless Compression MethodslossydefinitionIDREF="Media5-0ZIN13"Lossy versus Lossless Compression MethodsluminanceIDREF="Media3-1VB22"YUVM(agnetic) T(ape) I/O C(ontrol) T(ape) OP(eration). See MTIOCTOPIDREF="Media2-7DAT67"Playing a DATmalloc()IDREF="Media5-2AP17"Getting an Algorithm Scheme or Namemanagingvideo devicesIDREF="Media3-2GS7"Device Managementmappingmovie tracksIDREF="Media6-2IO49"Mapping Frames from One Track to Another TrackillustratedIDREF="Media6-2IO50"Mapping Frames from One Track to Another Trackmarkersaudio file tracksIDREF="Media2-5AF16"Audio Tracks, Sample Frames, and Track Markersaudio filesIDREF="Media2-5AF98"Getting and Setting Audio Track MarkersIDREF="Media2-5AF12"About Audio FilesdefaultIDREF="Media2-5AF42"Creating an Audio File SetuploopsIDREF="Media2-5AF22"Instrument Configurations and LoopsnamesIDREF="Media2-5AF99"Getting and Setting Audio Track MarkerspositionIDREF="Media2-5AF100"Getting and Setting Audio Track Markersmatchingaudio I/O ratesIDREF="Media2-4AL129"Querying and Controlling the Global Audio Device Statemath libraryIDREF="Media2-3SW56"Compiling and Linking an Audio Applicationmeasuringaudiodigital I/O ratesIDREF="Media2-4AL139"Querying and Controlling the Global Audio Device Statesample ratesIDREF="Media2-4AL129"Querying and Controlling the Global Audio Device Statemediatype definitionsIDREF="Media1-2DM7"Digital Media Type DefinitionstypesIDREF="Media1-2DM12"Digital Media Parametersmedia library daemonIDREF="Media2-3SW63"Compiling and Linking an Audio Applicationmemoryaudio requirementsIDREF="Media2-2HW19"Recommendations for Audio Development System Configurationscritical regionsIDREF="Media2-4AL191"Using Shared Arenas and Semaphoresmemory requirementsdigital audioIDREF="Media2-2HW20"Memorymemory-mapped moviescreatingIDREF="Media6-2IO22"Creating a New MovieopeningIDREF="Media6-2IO37"Opening Memory-mapped MoviesmicrophonesIDREF="Media2-4AL137"Querying and Controlling the Global Audio Device StateresolutionIDREF="Media2-4AL24"Digital Audio Input and Output Sample ResolutionsstereoIDREF="Media2-2HW10"Indigo2 and Indy Audio System ArchitectureIDREF="Media2-4AL137"Querying and Controlling the Global Audio Device StateMIDIIDREF="Media2-8MI8"Connecting Devices to MIDI I/O InterfacesC++ referencesIDREF="Media2-1GS6"Introduction to Digital Audio and MIDI ProgrammingconfiguringIDREF="Media2-8MI9"Connecting Devices to MIDI I/O InterfacescontrollersIDREF="Media2-8MI7"Configuring Your System for MIDI DevelopmentconversionsIDREF="Media2-8MI4"Configuring Your System for MIDI DevelopmentdevicesIDREF="Media2-8MI6"Configuring Your System for MIDI DevelopmentI/OIDREF="Media2-8MI15"Opening and Closing MIDI PortspatchbaysIDREF="Media2-8MI5"Configuring Your System for MIDI DevelopmentperipheralsIDREF="Media2-8MI3"Configuring Your System for MIDI DevelopmentportsIDREF="Media2-8MI14"Opening and Closing MIDI PortsreferencesIDREF="Media2-1GS3"Introduction to Digital Audio and MIDI Programmingserial portsIndigo2IDREF="Media2-8MI10"Connecting Devices to MIDI I/O InterfacesIndyIDREF="Media2-8MI11"Connecting Devices to MIDI I/O Interfacesstarting and stoppingIDREF="Media2-8MI12"Configuring Serial Ports for MIDI WIth the Port Setup TooltimestampingIDREF="Media2-8MI16"About MIDI EventstimestampsIDREF="Media2-8MI13"MIDI Library BasicsMIDI LibraryIDREF="Media2-8MI1"Programming with the MIDI LibraryIDREF="Media1-1ML4"About the Digital Audio and MIDI LibrariesMII videoIDREF="Media3-1VB60"Videotape Formatsmiscellaneous chunksIDREF="Media2-5AF115"Handling Miscellaneous Data ChunksmodesIDREF="Media2-4AL137"Querying and Controlling the Global Audio Device StateDAT driveIDREF="Media2-7DAT23"Opening and Closing the DAT Device for AudiomicrophoneIDREF="Media2-4AL137"Querying and Controlling the Global Audio Device StatemonitorSilicon GraphicsIDREF="Media3-1VB5"InterlacingmonitoringaudioIDREF="Media2-4AL133"Querying and Controlling the Global Audio Device StateMotif, recommended readingIDREF="Media0-5ATB2"References for Adding a User Interface to Your Program moviesQT_ANIM compressionIDREF="Media6-1GS171"Getting the Image Compression Schemeread-onlyIDREF="Media6-2IO9"Using File Descriptors with Movieswrite-onlyIDREF="Media6-2IO11"Using File Descriptors with Moviesmovie editingIDREF="Media5-0ZIN8"Compression Library Applicationsmovie framesdeletingIDREF="Media6-2IO63"Deleting Frames from a Movie TrackMovie Libraryadding user parametersIDREF="Media6-1GS65"Adding Your Own Parameters to the Movie LibraryexampleIDREF="Media6-1GS71"Adding Your Own Parameters to the Movie LibraryIDREF="Media6-1GS69"Adding Your Own Parameters to the Movie LibraryapplicationsIDREF="Media6-0ZIN2"Movie Library ApplicationsassertionsIDREF="Media6-1GS20"Using the Debugging Version of the Movie Librarycompiling and linkingIDREF="Media6-1GS16"Compiling and Linking a Movie Library Applicationdebugging versionIDREF="Media6-1GS21"Using the Debugging Version of the Movie LibrarydebuggingIDREF="Media6-1GS17"Debugging a Movie Library ApplicationdefinitionsIDREF="Media6-1GS1"Definitionsenvironment variablesIDREF="Media6-1GS22"Emulating I/O FailuresfeaturesIDREF="Media6-0ZIN1"Movie Library Featuresfile formatsIDREF="Media6-1GS13"Silicon Graphics Movie Formatsfile I/OIDREF="Media6-2IO1"File I/O and Editing Movies with the Movie LibraryillustratedIDREF="Media6-2IO3"Initializing a Movie Library ApplicationoverviewIDREF="Media6-2IO2"Initializing a Movie Library Applicationprogramming guidelinesIDREF="Media6-1GS14"Developing a Movie Library Applicationprogramming modelIDREF="Media6-1GS10"Movie Library Programming ModelpurposeIDREF="Media1-1ML21"About the Movie LibraryMovie Makerfile formatsIDREF="Media6-1GS13"Silicon Graphics Movie FormatsmoviesaddingtracksIDREF="Media6-2IO39"Adding an Audio or Image Track to a Movieadding tracksexampleIDREF="Media6-2IO42"Adding an Audio or Image Track to a Movieallocating buffersIDREF="Media6-2IO53"Allocating BuffersexampleIDREF="Media6-2IO57"Allocating BuffersaudiochannelsIDREF="Media6-1GS109"Getting the Number of Audio Channels in an Audio TrackcompressionIDREF="Media6-1GS116"Getting the Audio Compression Scheme of an Audio TrackformatsIDREF="Media6-1GS113"Getting the Audio Format of an Audio Tracknative formatsIDREF="Media6-1GS112"Getting the Audio Format of an Audio TrackportsIDREF="Media6-3PB38"Binding a Window to a Movie with an Audio Tracksample rateIDREF="Media6-1GS106"Getting the Audio Sample Rate of an Audio Tracksample widthIDREF="Media6-1GS103"Getting the Audio Sample Width of an Audio Trackbackground colorIDREF="Media6-3PB10"Setting and Getting the Background Colorbinding to windowsIDREF="Media6-3PB35"Binding a Movie to a Window for Playbackmultiple moviesIDREF="Media6-3PB41"Playing Multiple Movies in the Same Windowcapturing input from Cosmo CompressIDREF="Media6-2IO67"Reading and Inserting Compressed ImagesclosingIDREF="Media6-2IO82"Finalizing Changes and Closing MoviescommentsIDREF="Media6-1GS38"Setting and Getting the Movie Commentcopying and pastingIDREF="Media6-2IO73"Copying and Pasting Frames from One Movie into AnotherillustratedIDREF="Media6-2IO75"Copying and Pasting Frames from One Movie into AnothercreatingIDREF="Media6-2IO18"Creating a New MovieexampleIDREF="Media6-2IO24"Creating a New MoviedefaultsIDREF="Media6-1GS63"Creating a Default Movie ConfigurationaudioIDREF="Media6-1GS91"Setting and Getting Audio Track Propertiesaudio volumeIDREF="Media6-1GS98"Setting and Getting the Default Volume of an Audio TrackimageIDREF="Media6-1GS122"Setting and Getting Image Track Propertiesloop limitIDREF="Media6-1GS47"Setting and Getting the Default Movie Loop Limitloop modeIDREF="Media6-1GS40"Setting and Getting the Default Movie Loop ModedefinedIDREF="Media6-1GS2"DefinitionseditingIDREF="Media6-2IO51"Editing Moviescompressed imagesIDREF="Media6-2IO65"Reading and Inserting Compressed ImagesembeddedIDREF="Media6-2IO6"Using File Descriptors with Moviesfile access modeIDREF="Media6-2IO15"Using File Descriptors with Moviesfinding tracksIDREF="Media6-2IO47"Locating an Existing TrackFIT imagesIDREF="Media6-1GS143"Getting the Image Formatflushing editsIDREF="Media6-2IO79"Finalizing Changes and Closing MoviesformatsIDREF="Media6-1GS12"Movie File Formatsframe rateIDREF="Media6-1GS126"Setting and Getting the Image Frame Rategettingimage compressionIDREF="Media6-1GS161"Getting the Image Compression Schemeglobal propertiesIDREF="Media6-1GS35"Setting and Getting Global Movie PropertiesillustratedIDREF="Media6-1GS6"Definitionsimage compressionIDREF="Media6-1GS158"Getting the Image Compression Schemeimage formatIDREF="Media6-1GS139"Getting the Image Formatimage heightIDREF="Media6-1GS133"Getting the Image Frame Sizeimage orientationIDREF="Media6-1GS136"Getting the Image Orientationimage packing formatIDREF="Media6-1GS154"Getting the Image Packing Formatimage widthIDREF="Media6-1GS130"Getting the Image Frame Sizeinsertingcompressed imagesIDREF="Media6-2IO72"Inserting a Compressed Image from a Buffer into an Existing Tracktrack dataIDREF="Media6-2IO58"Inserting Raw Images and Audio from a Buffer into an Existing TrackinstancesIDREF="Media6-1GS11"Movie Library Programming ModelJPEGIDREF="Media6-1GS169"Getting the Image Compression SchemekeyframesIDREF="Media6-1GS4"Definitionsloop modesdefinedIDREF="Media6-1GS9"DefinitionsloopingdefinedIDREF="Media6-1GS8"Definitionsmapping tracksIDREF="Media6-2IO49"Mapping Frames from One Track to Another TrackillustratedIDREF="Media6-2IO50"Mapping Frames from One Track to Another Trackmultiple movie playback hintIDREF="Media6-3PB4"Opening a Movie for PlaybackopeningIDREF="Media6-2IO29"Opening an Existing Moviefrom file descriptorsIDREF="Media6-2IO4"Using File Descriptors with Moviesfrom filenames:filenames:for opening moviesIDREF="Media6-2IO34"Opening a Movie from a Filenamefrom memoryIDREF="Media6-2IO37"Opening Memory-mapped MoviesoptimizationgettingIDREF="Media6-1GS59"Getting the Movie Optimization Settingparameterssetting and gettingIDREF="Media6-1GS31"Setting and Getting Movie PropertiesPCD imagesIDREF="Media6-1GS147"Getting the Image FormatplaybackcontrollingIDREF="Media6-3PB42"Starting and Stopping Playbackplayback windowIDREF="Media6-3PB7"Creating and Configuring a Playback WindowconfigurationIDREF="Media6-3PB9"Configuring the Playback DisplayexampleIDREF="Media6-3PB8"Creating a Window for IRIS GL Playback playingIDREF="Media6-3PB1"Playing Movies with the Movie LibraryoutlinedIDREF="Media6-3PB2"Playing Movies with the Movie LibrarypropertiesIDREF="Media6-1GS32"Setting and Getting Movie PropertiesQT_VIDEO compressionIDREF="Media6-1GS170"Getting the Image Compression Schemeread-writeIDREF="Media6-2IO13"Using File Descriptors with Moviesreading and writing from ImageVision LibraryIDREF="Media6-1GS145"Getting the Image Formatreading compressed imagesIDREF="Media6-2IO70"Reading a Compressed Image from a Movie into a Bufferreading framesstoring:movie frames in a bufferIDREF="Media6-2IO61"Reading Frames from a Movie into a Buffer for Uncompressed Dataremoving tracksIDREF="Media6-2IO45"Removing an Audio or Image Track from a MovieseekingIDREF="Media6-2IO17"Using File Descriptors with MoviesSGI image formatsIDREF="Media6-1GS151"Getting the Image FormatSilicon Graphics format definedIDREF="Media6-1GS13"Silicon Graphics Movie FormatsSMPTE time codesIDREF="Media6-1GS78"Setting and Getting SMPTE Time Code Strings Stored in TracksTIFF imagesIDREF="Media6-1GS152"Getting the Image FormattitlesIDREF="Media6-1GS54"Setting and Getting the Movie Titletrack operationsIDREF="Media6-2IO38"Adding, Locating, and Deleting Audio and Image TracksuncompressedIDREF="Media6-1GS165"Getting the Image Compression SchemeverifyingIDREF="Media6-2IO25"Checking for the Presence of a MovieviewoffsetIDREF="Media6-3PB25"Setting and Getting the Viewing Location OffsetsizeIDREF="Media6-3PB14"Setting and Getting the Viewing Area Size size:gettingIDREF="Media6-3PB22"Setting and Getting the Viewing Area Sizesize:guaranteeingIDREF="Media6-3PB18"Setting and Getting the Viewing Area SizezoomingIDREF="Media6-3PB15"Setting and Getting the Viewing Area SizeMPEGIDREF="Media5-2AP3"Choosing a Compression Library AlgorithmIDREF="Media5-0ZIN17"Movie AlgorithmsAware noise-to-mask ratioIDREF="MediaA-2B18"Using Compression Library ParametersMTIOCTOPIDREF="Media2-7DAT24"Opening and Closing the DAT Device for Audiomultimedia applications, choosing a compression methodIDREF="Media5-2AP2"Choosing a Compression Library Algorithmmultiplexingsynchronous audio I/OIDREF="Media2-4AL166"Multiplexing Synchronous I/Omultiprocessing compressionIDREF="Media5-1GS33"Using the Buffering InterfaceexampleIDREF="Media5-1GS75"Creating Buffered Multiprocess Record and Play ApplicationsMultiRateAware, Inc.IDREF="Media2-3SW41"Aware Audio Compression Software and Audio ProductsMultiRate Aware compression algorithmIDREF="MediaA-2B9"Valid Audio Input DataIDREF="MediaA-2B10"Compression DefaultsIDREF="MediaA-2B11"Compression Custom ConfigurationIDREF="MediaA-2B12"Compression Custom ConfigurationIDREF="MediaA-2B14"Compression SchemesIDREF="MediaA-2B15"Compression SchemesIDREF="MediaA-2B16"Using Compression Library ParametersIDREF="MediaA-2B17"Using Compression Library ParametersspecificationsIDREF="MediaA-2B20"Aware Audio Compression Software SpecificationsmusicIDREF="Media2-3SW44"The Prosonus Sound LibraryreferencesIDREF="Media2-1GS4"Introduction to Digital Audio and MIDI Programmingmusic-quality audioIDREF="Media2-4AL14"Digital Audio Sample Ratesmutingheadphones and speakersIDREF="Media2-4AL134"Querying and Controlling the Global Audio Device StateIndyIDREF="Media2-2HW15"Indy Workstation LayoutMVC1IDREF="Media5-2AP9"Choosing a Compression Library AlgorithmIDREF="Media5-0ZIN18"Movie AlgorithmsIDREF="Media5-2AP4"Choosing a Compression Library AlgorithmMVC1 compressionIDREF="Media6-1GS166"Getting the Image Compression SchemeIDREF="Media5-0ZIN19"Movie AlgorithmsMVC2 compressionIDREF="Media6-1GS167"Getting the Image Compression Schemenamesaudio portsIDREF="Media2-4AL88"Opening and Closing Audio PortsparametersIDREF="Media1-2DM74"Determining the Name of a Given ParameternavigatingCDsIDREF="Media2-6CD35"Navigating through a CDDATsIDREF="Media2-7DAT25"Navigating through a DATNetLSIDREF="MediaA-2B21"Installing a NetLS Nodelocked Licensenodelock fileIDREF="MediaA-2B23"Installing a NetLS Nodelocked Licensenodelocked licensesIDREF="MediaA-2B22"Installing a NetLS Nodelocked Licensenoise-to-mask ratioAware MPEGIDREF="MediaA-2B18"Using Compression Library Parametersnon-ANSI-compliant codeIDREF="Media2-3SW70"Compiling and Linking an Audio Applicationnonblockingaudio I/OIDREF="Media2-4AL101"Monitoring the Audio Sample Queue Status to Provide Nonblocking I/ONTSCIDREF="Media3-1VB38"Composite VideoIDREF="Media3-1VB8"Broadcast Standardsdigital recordingIDREF="Media3-1VB12"Broadcast StandardsillustratedIDREF="Media3-1VB4"InterlacingresolutionIDREF="Media3-1VB11"Broadcast StandardsYIQ encodingIDREF="Media3-1VB29"YIQNyquist TheoremIDREF="Media2-4AL10"Digital Audio Data RepresentationO_RDONLYIDREF="Media6-2IO10"Using File Descriptors with MoviesO_RDWRIDREF="Media6-2IO14"Using File Descriptors with MoviesO_WRONLYIDREF="Media6-2IO12"Using File Descriptors with Moviesoffsetsmovie viewIDREF="Media6-3PB27"Setting and Getting the Viewing Location Offsetopeningmemory-mapped moviesIDREF="Media6-2IO37"Opening Memory-mapped MoviesmoviesIDREF="Media6-2IO29"Opening an Existing MovieoptimizationmoviegettingIDREF="Media6-1GS59"Getting the Movie Optimization SettingoriginscreenIRIS GLIDREF="Media6-3PB30"Setting and Getting the Viewing Location OffsetX Window SystemIDREF="Media6-3PB29"Setting and Getting the Viewing Location Offsetoriginal formatIDREF="Media5-0ZIN29"Video Data FormatsoutputaudioIDREF="Media2-4AL112"Writing Samples to an Output ALportconversionsIDREF="Media2-4AL116"Writing Samples to an Output ALportoverflowaudioIDREF="Media2-4AL105"More Methods for Working with QueuesPALIDREF="Media3-1VB9"Broadcast StandardsIDREF="Media3-1VB39"Composite Videodigital recordingIDREF="Media3-1VB12"Broadcast StandardsillustratedIDREF="Media3-1VB4"InterlacingresolutionIDREF="Media3-1VB11"Broadcast StandardsYUVIDREF="Media3-1VB19"YUVparameter-value bufferaudioIDREF="Media2-4AL140"Techniques for Working with Global Parametersparameter-value listsconfiguringIDREF="Media1-2DM32"Creating Default Audio and Image ConfigurationsaudioIDREF="Media1-2DM35"Setting Audio DefaultsimageIDREF="Media1-2DM45"Setting Image DefaultscopyingIDREF="Media1-2DM71"Copying the Contents of One Parameter-value List into Anothercreating and destroyingIDREF="Media1-2DM25"Creating and Destroying Parameter-value ListsexampleIDREF="Media1-2DM31"Creating and Destroying Parameter-value ListsdefinedIDREF="Media1-2DM10"Digital Media ParametersdestroyingIDREF="Media1-2DM30"Creating and Destroying Parameter-value ListsDMIDREF="Media1-2DM24"Initializing a Digital Media ApplicationexampleIDREF="Media1-2DM83"Removing an Element from a Parameter-value ListformatsIDREF="Media1-2DM11"Digital Media Parametersgetting and setting valuesIDREF="Media1-2DM48"Setting and Getting Individual Parameter Valuesnumber of elementsIDREF="Media1-2DM68"Determining the Number of Elements in a Parameter-value Listremoving parametersIDREF="Media1-2DM82"Removing an Element from a Parameter-value ListparametersIDREF="Media2-5AF37"Creating an Audio File Setupadding to the Compression LibraryIDREF="Media5-3UP11"Adding Custom Parameters to the Compression LibraryaudioIDREF="Media1-2DM34"Audio Parameterscurrent valueIDREF="Media2-4AL151"Getting Current Parameter Settingsgetting and settingIDREF="Media2-4AL141"Techniques for Working with Global ParametersI/O errorsIDREF="Media2-4AL122"Detecting Errors in the Audio StreamnamesIDREF="Media2-4AL148"Getting the Names Corresponding to the Global ParameterssettingIDREF="Media2-4AL153"Modifying the Values of the Global Parameterssystem-dependentIDREF="Media2-4AL135"Querying and Controlling the Global Audio Device Stateaudio core globalIDREF="Media2-4AL126"Querying and Controlling the Global Audio Device Stateaudio tracksIDREF="Media2-5AF55"Initializing Audio Track DatacheckingIDREF="Media1-2DM79"Determining if a Given Parameter Existschecking boundsIDREF="Media2-4AL145"Getting the Bounds of Global ParametersCompression LibraryIDREF="Media5-2AP20"Using the Compression Library Parameterscopying from parameter-value listsIDREF="Media1-2DM72"Copying an Individual Parameter Value from One List into AnotherdeletingIDREF="Media1-2DM80"Removing an Element from a Parameter-value ListgettingtypeIDREF="Media1-2DM76"Determining the Data Type of a Given Parametergetting and settingIDREF="Media1-2DM58"Setting and Getting Individual Parameter ValuesimagesIDREF="Media1-2DM41"Image ParametersinstrumentsIDREF="Media2-5AF45"Creating an Audio File SetupmovieaddingIDREF="Media6-1GS65"Adding Your Own Parameters to the Movie LibraryIDREF="Media6-1GS69"Adding Your Own Parameters to the Movie LibraryIDREF="Media6-1GS72"Adding Your Own Parameters to the Movie Librarysetting and gettingIDREF="Media6-1GS29"Setting and Getting Movie PropertiesnamesIDREF="Media1-2DM74"Determining the Name of a Given ParameterremovingIDREF="Media1-2DM80"Removing an Element from a Parameter-value ListparserCDbasicsIDREF="Media2-6CD27"CD ParsercallbacksIDREF="Media2-6CD80"Adding Callbacks to the CD Parser framesIDREF="Media2-6CD83"Parsing CD Frames freeingIDREF="Media2-6CD85"Freeing the Memory Allocated for the Parser initializingIDREF="Media2-6CD73"Allocating and Initializing the CD ParserDATIDREF="Media2-7DAT50"Controlling the DAT Parserallocating and initializingIDREF="Media2-7DAT51"Allocating and Initializing the DAT ParserbasicsIDREF="Media2-7DAT17"DAT ParserfreeingIDREF="Media2-7DAT59"Freeing the Memory Reserved for the DAT Parser parsingCDsIDREF="Media2-6CD72"Controlling the CD ParsercallbacksIDREF="Media2-6CD79"Adding Callbacks to the CD Parser framesIDREF="Media2-6CD83"Parsing CD Frames DATsIDREF="Media2-7DAT49"Controlling the DAT ParserframesIDREF="Media2-7DAT57"Parsing DAT Frames pastingmovie framesIDREF="Media6-2IO74"Copying and Pasting Frames from One Movie into AnotherillustratedIDREF="Media6-2IO75"Copying and Pasting Frames from One Movie into AnotherpatchbaysMIDIIDREF="Media2-8MI5"Configuring Your System for MIDI DevelopmentpathsvideoblendingIDREF="Media3-2GS22"VL Architectural Model of Video DevicesVLdefinedIDREF="Media3-2GS18"VL Architectural Model of Video DevicesillustratedIDREF="Media3-2GS21"VL Architectural Model of Video DevicespausingCDsIDREF="Media2-6CD62"Playing an Audio CD from the CD-ROM DrivePCD images in moviesIDREF="Media6-1GS148"Getting the Image Formatperformance tuningaudioIDREF="Media2-4AL164"Real-time Programming Techniques for AudioperipheralsaudioIDREF="Media2-2HW22"PeripheralsMIDIIDREF="Media2-8MI3"Configuring Your System for MIDI DevelopmentplaybackmoviescontrollingIDREF="Media6-3PB42"Starting and Stopping Playback non-blockingIDREF="Media5-1GS63"Creating a Nonblocking Buffered Playback ApplicationplayingCDsIDREF="Media2-6CD60"Playing an Audio CD from the CD-ROM DrivetracksIDREF="Media2-6CD67"Playing an Audio CD from the CD-ROM DriveDATsIDREF="Media2-7DAT38"Playing a Tape in the DAT DriveexampleIDREF="Media2-7DAT66"DAT Sample ProgrammoviesIDREF="Media6-3PB1"Playing Movies with the Movie LibraryoutlinedIDREF="Media6-3PB2"Playing Movies with the Movie Librarymultiple movies in one windowIDREF="Media6-3PB39"Playing Multiple Movies in the Same WindowPOLLINIDREF="Media2-4AL171"Getting a File Descriptor for an ALportpollingexampleIDREF="Media2-4AL201"Using Shared Arenas and SemaphoresPOLLOUTIDREF="Media2-4AL172"Getting a File Descriptor for an ALportpopsaudioIDREF="Media2-4AL56"Setting and Getting the Sample Queue Size for an ALconfigportsaudioIDREF="Media2-4AL36"About ALportsallocating and initializingIDREF="Media2-4AL86"Opening and Closing Audio Portsclosing and deallocatingIDREF="Media2-4AL90"Opening and Closing Audio PortsconfiguringIDREF="Media2-4AL39"Using ALconfig Structures to Configure ALportsdefaultsIDREF="Media2-4AL40"Using ALconfig Structures to Configure ALportsdefinedIDREF="Media2-4AL7"Audio Library Programming ModelexampleIDREF="Media2-4AL46"Using ALconfig Structures to Configure ALportsformatsIDREF="Media2-4AL63"Setting and Getting the Sample Data Format for an ALconfignamesIDREF="Media2-4AL88"Opening and Closing Audio Portsopening and closingIDREF="Media2-4AL85"Opening and Closing Audio Portsopening and closing:exampleIDREF="Media2-4AL91"Opening and Closing Audio Portsstatic settingsIDREF="Media2-4AL45"Using ALconfig Structures to Configure ALportsMIDIIDREF="Media2-8MI14"Opening and Closing MIDI Portspostproductionvideo formatsIDREF="Media3-1VB48"Videotape FormatsprecisionaudioIDREF="Media2-4AL69"Setting and Getting the Integer Sample Width for an ALconfigpreviewingaudio filesIDREF="Media2-3SW34"Graphical User Interface Audio ToolsprioritizingaudioIDREF="Media2-4AL181"Using Scheduling Control to Give Audio High PrioritypriorityaudioIDREF="Media2-3SW7"How Audio Applications Share CPU Resourcesprocess controlsaudioIDREF="Media2-4AL184"Preventing Memory SwapoutprocessesaudioconcurrentIDREF="Media2-4AL157"Determining Whether Other Audio Applications Are RunningproducingIDREF="Media5-1GS37"Using the Buffering InterfaceIDREF="Media5-1GS51"Producing and Consuming Data in Buffersprofessionalvideo formatsIDREF="Media3-1VB47"Videotape FormatsprogrammingguidelinesaudioIDREF="Media2-3SW23"Programming Guidelines for Managing System-Wide ResourcesMovie LibraryIDREF="Media6-1GS15"Developing a Movie Library ApplicationmodelsAudio LibraryIDREF="Media2-4AL3"Audio Library Programming ModelMovie LibraryIDREF="Media6-1GS10"Movie Library Programming ModeloutlinesAudio LibraryIDREF="Media2-4AL33"Audio Library Application Programming Conceptsvideo hardwareIDREF="Media1-1ML11"About the Video LibrarypropertiesmovieIDREF="Media6-1GS32"Setting and Getting Movie PropertiesglobalIDREF="Media6-1GS35"Setting and Getting Global Movie PropertiesProsonusaudio librariesIDREF="Media2-2HW23"PeripheralscontactingIDREF="Media2-3SW47"The Prosonus Sound Librarysound librariesIDREF="Media2-3SW43"The Prosonus Sound Libraryqdevice()IDREF="Media4-5EH2"IRIS GL Event Handlingin sample programIDREF="Media4-5EH3"IRIS GL Event HandlingQT_ANIM compressionIDREF="Media6-1GS171"Getting the Image Compression SchemeQT_VIDEO compressionIDREF="Media6-1GS170"Getting the Image Compression Schemequantization stepsaudioIDREF="Media2-4AL79"Getting and Setting the Floating Point Data Rangequeryingaudio4-channelIDREF="Media2-4AL160"Determining Whether 4-channel Capability Existsconcurrent processesIDREF="Media2-4AL157"Determining Whether Other Audio Applications Are RunningI/O ratesIDREF="Media2-4AL158"Determining the Input and Output Ratessupported featuresIDREF="Media2-4AL125"Querying and Controlling the Global Audio Device Statequerying algorithmsIDREF="Media5-2AP12"Querying Compression Library Algorithmsquerying for a licenseIDREF="Media5-2AP19"Getting License InformationqueuesaudioIDREF="Media2-4AL92"Using Audio Sample QueuesdefaultsIDREF="Media2-4AL41"Using ALconfig Structures to Configure ALportsillustratedIDREF="Media2-4AL94"Using Audio Sample QueuessizeIDREF="Media2-4AL98"Monitoring the Audio Sample Queue Status to Provide Nonblocking I/OIDREF="Media2-4AL55"Setting and Getting the Sample Queue Size for an ALconfigsize limitsIDREF="Media2-4AL58"Setting and Getting the Sample Queue Size for an ALconfigstatusIDREF="Media2-4AL95"Monitoring the Audio Sample Queue Status to Provide Nonblocking I/OthresholdsIDREF="Media2-4AL174"Setting and Getting the Fill Point for a QueueR-Y video signalIDREF="Media3-1VB26"YUVrangesaudiofloating pointIDREF="Media2-4AL76"Getting and Setting the Floating Point Data Rangefull-scaleIDREF="Media2-4AL70"Setting and Getting the Integer Sample Width for an ALconfigread-only moviesIDREF="Media6-2IO9"Using File Descriptors with Moviesread-write moviesIDREF="Media6-2IO13"Using File Descriptors with Moviesreadingaudio dataIDREF="Media2-4AL108"Reading Samples from an Input ALportaudio from CDsIDREF="Media2-6CD69"Reading Audio Data from the CD-ROM Drivecompressed movie imagesIDREF="Media6-2IO70"Reading a Compressed Image from a Movie into a BufferDATsIDREF="Media2-7DAT41"Reading Audio Data from the DAT Drive real-time programmingaudioIDREF="Media2-4AL164"Real-time Programming Techniques for AudioexampleIDREF="Media2-4AL196"Using Shared Arenas and Semaphoresrecordexample.cIDREF="Media2-5AF122"Sample Audio File ProgramrecordingDATsIDREF="Media2-7DAT39"Making DAT Recordings for Playback on the DAT DriveexamplesIDREF="Media2-7DAT48"Example Programs Demonstrating DAT Recordingdigital videoIDREF="Media3-1VB12"Broadcast Standardsusing buffers for non-blocking compressionIDREF="Media5-1GS70"Creating a Nonblocking Buffered Record Applicationusing buffers to compress forIDREF="Media5-1GS64"Creating a Buffered Record Applicationreel-to-reel (Type C) videotapeIDREF="Media3-1VB51"Videotape FormatsreferencesaudioIDREF="Media2-1GS2"Introduction to Digital Audio and MIDI ProgrammingMIDIIDREF="Media2-1GS3"Introduction to Digital Audio and MIDI ProgrammingMIDI C++IDREF="Media2-1GS6"Introduction to Digital Audio and MIDI Programmingrelease loopsaudio filesIDREF="Media2-5AF24"Instrument Configurations and LoopsremovingCD callbacksIDREF="Media2-6CD81"Deleting and Changing a CD Parser Callbackmovie framesIDREF="Media6-2IO63"Deleting Frames from a Movie Trackmovie tracksIDREF="Media6-2IO45"Removing an Audio or Image Track from a MovieparametersIDREF="Media1-2DM80"Removing an Element from a Parameter-value ListresettingCD parserIDREF="Media2-6CD75"Allocating and Initializing the CD ParserresolutionvideoIDREF="Media3-1VB11"Broadcast StandardsresolutionsAESIDREF="Media2-4AL21"Digital Audio Sample FormatsaudioIDREF="Media2-4AL73"Setting and Getting the Integer Sample Width for an ALconfigIDREF="Media2-4AL23"Digital Audio Input and Output Sample ResolutionsRGBIDREF="Media3-1VB17"RGB32-bitIDREF="Media4-1GS2"32-bit RGB8-bitIDREF="Media4-1GS3"8-bit RGB.rgb images in moviesIDREF="Media6-1GS150"Getting the Image Format.rgba images in moviesIDREF="Media6-1GS150"Getting the Image Formatring bufferIDREF="Media5-1GS39"Using the Buffering Interfacering buffersIDREF="Media5-1GS34"Using the Buffering InterfaceRLEIDREF="Media5-0ZIN20"Movie AlgorithmsRTR1IDREF="Media5-2AP5"Choosing a Compression Library AlgorithmIDREF="Media5-0ZIN21"Movie AlgorithmsIDREF="Media5-2AP10"Choosing a Compression Library Algorithmrun-length encodingmoviesIDREF="Media6-1GS168"Getting the Image Compression SchemeS-VHSIDREF="Media3-1VB36"YC, YC-358, YC-443, or S-VideoIDREF="Media3-1VB52"Videotape FormatsS-VideoIDREF="Media3-1VB34"YC, YC-358, YC-443, or S-VideoIDREF="Media3-1VB54"Videotape FormatsIDREF="Media3-1VB55"Videotape Formatssample widthsaudiodefaultIDREF="Media2-4AL44"Using ALconfig Structures to Configure ALportsgetting and settingIDREF="Media2-4AL68"Setting and Getting the Integer Sample Width for an ALconfigsamplersaudioIDREF="Media2-5AF18"Instrument Configurations and LoopssamplesCDIDREF="Media2-6CD5"CD Frames, Samples, and SubcodesillustratedIDREF="Media2-6CD11"CD Frames, Samples, and SubcodesDATIDREF="Media2-7DAT8"DAT Frames, Samples, and SubcodesinstrumentsIDREF="Media2-3SW46"The Prosonus Sound LibrarysamplingaudioIDREF="Media2-4AL9"Digital Audio Data Representationsampling ratesaudioIDREF="Media2-2HW3"Indigo Audio FeaturesIDREF="Media2-4AL11"Digital Audio Sample RatesmeasuringIDREF="Media2-4AL129"Querying and Controlling the Global Audio Device Stateaudio filesIDREF="Media2-5AF89"Getting Audio Track Sample RateCDIDREF="Media2-6CD12"CD Frames, Samples, and SubcodesDATIDREF="Media2-7DAT11"DAT Frames, Samples, and Subcodesscalingaudio floating point rangesIDREF="Media2-4AL77"Getting and Setting the Floating Point Data RangeschedulingaudioIDREF="Media2-4AL181"Using Scheduling Control to Give Audio High PriorityscreenoriginIRIS GLIDREF="Media6-3PB30"Setting and Getting the Viewing Location OffsetX Window SystemIDREF="Media6-3PB28"Setting and Getting the Viewing Location OffsetSCSI devicesIDREF="Media2-3SW3"Digital Audio System Software OverviewlibraryIDREF="Media2-3SW62"Compiling and Linking an Audio ApplicationSECAMIDREF="Media3-1VB10"Broadcast Standardsseekingaudio filesIDREF="Media2-5AF102"Seeking to a Position in an Audio File Trackmiscellaneous chunksIDREF="Media2-5AF121"Reading, Writing, and Seeking Miscellaneous DataCDIDREF="Media2-6CD36"Navigating through a CDblocksIDREF="Media2-6CD57"Seeking to a CD LocationdefinedIDREF="Media2-6CD26"CD Seeking, Reading, and PlayingtracksIDREF="Media2-6CD52"Seeking to a CD LocationDATsIDREF="Media2-7DAT35"Seeking to a DAT LocationmoviesIDREF="Media6-2IO17"Using File Descriptors with Moviesselectingaudio I/O sourcesIDREF="Media2-4AL127"Querying and Controlling the Global Audio Device StatesemaphoresIDREF="Media2-4AL188"Using Shared Arenas and SemaphoresexampleIDREF="Media2-4AL199"Using Shared Arenas and Semaphoressequential interface of the Compression LibraryIDREF="Media5-1GS3"Sequential Access APIserial portsIndigoIDREF="Media2-8MI8"Connecting Devices to MIDI I/O Interfacesserver-client environment, CLIDREF="Media5-0ZIN9"Compression Library ApplicationssettingaudioparametersIDREF="Media2-4AL153"Modifying the Values of the Global Parametersaudio defaultsIDREF="Media1-2DM36"Setting Audio DefaultsexampleIDREF="Media1-2DM40"Determining the Buffer Size Needed to Store an Audio Frameaudio file formatIDREF="Media2-5AF50"Initializing Audio File Formataudio file parametersIDREF="Media2-5AF55"Initializing Audio Track Dataaudio fill pointsIDREF="Media2-4AL175"Setting and Getting the Fill Point for a Queueimage defaultsIDREF="Media1-2DM42"Setting Image DefaultsexampleIDREF="Media1-2DM47"Determining the Buffer Size Needed to Store an Image Framemovieaudio track propertiesIDREF="Media6-1GS90"Setting and Getting Audio Track Propertiesaudio track properties defaultsIDREF="Media6-1GS92"Setting and Getting Audio Track Propertiesaudio track properties:default volumeIDREF="Media6-1GS99"Setting and Getting the Default Volume of an Audio Trackgeneral track propertiesIDREF="Media6-1GS75"Setting and Getting Track Propertiesgeneral track properties:SMPTE time code stringIDREF="Media6-1GS80"Setting and Getting SMPTE Time Code Strings Stored in Tracksglobal propertiesIDREF="Media6-1GS36"Setting and Getting Global Movie Propertiesglobal properties:loop limitIDREF="Media6-1GS48"Setting and Getting the Default Movie Loop Limitglobal properties:loop modeIDREF="Media6-1GS41"Setting and Getting the Default Movie Loop Modeglobal properties:titleIDREF="Media6-1GS51"Setting and Getting the Movie Titleimage track propertiesIDREF="Media6-1GS121"Setting and Getting Image Track Propertiesimage track properties:defaultsIDREF="Media6-1GS123"Setting and Getting Image Track Propertiesimage track properties:frame rateIDREF="Media6-1GS128"Setting and Getting the Image Frame RateparametersIDREF="Media6-1GS28"Setting and Getting Movie Propertiesview sizeIDREF="Media6-3PB16"Setting and Getting the Viewing Area Sizemultiple movie playback hintIDREF="Media6-3PB4"Opening a Movie for PlaybackparametersIDREF="Media1-2DM58"Setting and Getting Individual Parameter Valuesby copyingIDREF="Media1-2DM72"Copying an Individual Parameter Value from One List into Another.sgi images in moviesIDREF="Media6-1GS151"Getting the Image FormatsharedarenasIDREF="Media2-4AL185"Using Shared Arenas and SemaphoresexampleIDREF="Media2-4AL198"Using Shared Arenas and Semaphoresaudio resourcesIDREF="Media2-3SW4"About Shared System-Wide ResourcesillustratedIDREF="Media2-3SW11"How Audio Applications Share Audio System ResourcesGLIDREF="Media2-3SW66"Compiling and Linking an Audio Applicationprocess synchronizationIDREF="Media2-4AL186"Using Shared Arenas and Semaphoressignalcomposite videoillustratedIDREF="Media3-1VB43"Video SignalsvideoIDREF="Media3-1VB26"YUVSilicon Graphicsnoninterlaced monitorIDREF="Media3-1VB5"InterlacingSirius VideoIDREF="Media1-1ML12"About the Video LibraryIDREF="Media3-2GS4"How the VL Works with HardwaresizingaudiobuffersIDREF="Media1-2DM39"Determining the Buffer Size Needed to Store an Audio FramequeuesIDREF="Media2-4AL55"Setting and Getting the Sample Queue Size for an ALconfigimagesbuffersIDREF="Media1-2DM46"Determining the Buffer Size Needed to Store an Image FrameSMPTE time codesmoviesIDREF="Media6-1GS77"Setting and Getting SMPTE Time Code Strings Stored in Trackssound effectsIDREF="Media2-3SW45"The Prosonus Sound LibrarysoundtracksfindingIDREF="Media6-2IO47"Locating an Existing TrackremovingIDREF="Media6-2IO45"Removing an Audio or Image Track from a MoviesourcesvideoIDREF="Media3-2GS19"VL Architectural Model of Video Devicesspeakerscontrolling volumeIDREF="Media2-4AL131"Querying and Controlling the Global Audio Device StatemutingIDREF="Media2-4AL134"Querying and Controlling the Global Audio Device StatespecificationsAES3-1985IDREF="Media2-5AF64"Initializing AES Dataaudio hardwareIDREF="MediaA-1A1"Audio SpecificationsIDREF="MediaA-1A2"Indigo2 Workstation Audio Hardware SpecificationsAware audio compression softwareIDREF="MediaA-2B19"Aware Audio Compression Software SpecificationsEA IFF 85IDREF="Media2-5AF33"AIFF-C and the AF Library APIspinlocksIDREF="Media2-4AL192"Using Shared Arenas and Semaphoressproc()IDREF="Media5-1GS74"Creating Buffered Multiprocess Record and Play ApplicationsIDREF="Media5-1GS72"Creating a Nonblocking Buffered Record ApplicationstandardsAES3-1985IDREF="Media2-5AF64"Initializing AES DataCCIR 601IDREF="Media3-1VB27"YUVcompressionIDREF="Media5-0ZIN15"Compression Standardsvideo broadcastIDREF="Media3-1VB6"Broadcast StandardsstateaudioIDREF="Media2-3SW22"How Global Audio Settings Are Established and MaintainedstatusCD-ROM drivesIDREF="Media2-6CD87"Communicating CD Status to the End UserCDsIDREF="Media2-6CD50"Getting the Current CD Location statusingDATsIDREF="Media2-7DAT61"Communicating DAT Status to the End User stereoaudio framesillustratedIDREF="Media2-4AL17"Digital Audio Sample FramesmicrophoneIDREF="Media2-2HW10"Indigo2 and Indy Audio System ArchitecturemicrophonesIDREF="Media2-4AL137"Querying and Controlling the Global Audio Device Statestereo framesCDIDREF="Media2-6CD4"CD Frames, Samples, and Subcodesstoringcompressed movie framesIDREF="Media6-2IO70"Reading a Compressed Image from a Movie into a BufferstructuresAF LibraryIDREF="Media2-5AF34"Creating an Audio File SetupCDFRAMEIDREF="Media2-6CD8"CD Frames, Samples, and SubcodessubcodeQIDREF="Media2-6CD9"CD Frames, Samples, and SubcodesCDIDREF="Media2-6CD19"CD Frames, Samples, and SubcodessubcodesCDIDREF="Media2-6CD10"CD Frames, Samples, and SubcodesmodesIDREF="Media2-6CD14"CD Frames, Samples, and SubcodesDATIDREF="Media2-7DAT10"DAT Frames, Samples, and SubcodessubdivisionsCD tracksIDREF="Media2-6CD23"CD Tracks, Indices, and Time CodesDATIDREF="Media2-7DAT15"DAT Audio Program Numbers and Indicessustain loopsaudio filesIDREF="Media2-5AF23"Instrument Configurations and LoopsSV_IN_REPLACEin sample programIDREF="Media4-5EH8"IRIS GL Event HandlingSvActiveAttributein sample programIDREF="Media4-5EH6"IRIS GL Event HandlingsvBindGLWindow()in sample programIDREF="Media4-5EH7"IRIS GL Event HandlingSvVideoPreemptedin sample programIDREF="Media4-5EH5"IRIS GL Event Handlingswitching4-channel audioIDREF="Media2-2HW13"Indigo2 and Indy Audio I/O Interfacesync burstIDREF="Media3-1VB42"Video Signalssynchronizingmovie tracksIDREF="Media6-2IO49"Mapping Frames from One Track to Another TrackillustratedIDREF="Media6-2IO50"Mapping Frames from One Track to Another Tracksystem callsIRIXIDREF="Media2-4AL163"Real-time Programming Techniques for Audiosystemsaudio softwareIDREF="Media2-3SW1"Digital Audio System Softwaretable of contentsCDIDREF="Media2-6CD16"CD Frames, Samples, and SubcodesDATIDREF="Media2-7DAT13"DAT Frames, Samples, and SubcodesTailIDREF="Media5-1GS36"Using the Buffering InterfacetapesDDSIDREF="Media2-7DAT46"Recording Digital Audio over Digital Data Storage (DDS) TapestasksAF LibraryIDREF="Media2-5AF5"Programming with the Audio File Librarytelecommunicationschoosing a compression methodIDREF="Media5-2AP7"Choosing a Compression Library AlgorithmteleconferencingIDREF="Media5-0ZIN6"Compression Library ApplicationsIDREF="Media1-1ML17"About the Video LibrarytestingMovie Library I/OIDREF="Media6-1GS24"Emulating I/O Failuresthird-party audio softwareIDREF="Media2-3SW38"Third-party Audio Software and Sound LibrariesthresholdsaudioportsIDREF="Media2-4AL174"Setting and Getting the Fill Point for a QueueTIFF images in moviesIDREF="Media6-1GS152"Getting the Image Formattimerequired for audio hardware to play samplesIDREF="Media2-4AL113"Writing Samples to an Output ALporttime codesCDIDREF="Media2-6CD25"CD Tracks, Indices, and Time CodesconversionsIDREF="Media2-6CD94"Communicating CD Status to the End UserDATsIDREF="Media2-7DAT16"DAT Run Time, Absolute Time, and Program TimeIDREF="Media2-7DAT28"Getting DAT Locations from the End Userchecking and settingIDREF="Media2-7DAT44"Writing Audio Data to the DAT DrivetimestampingMIDIIDREF="Media2-8MI16"About MIDI EventstimestampsDATsIDREF="Media2-7DAT43"Writing Audio Data to the DAT DriveMIDIIDREF="Media2-8MI13"MIDI Library BasicstitlesmovieIDREF="Media6-1GS54"Setting and Getting the Movie TitletoolsaudioIDREF="Media2-3SW25"Graphical User Interface Audio ToolsVideo Library (VL)IDREF="Media3-2GS9"Generic Video ToolstracksaddingIDREF="Media6-2IO39"Adding an Audio or Image Track to a MovieexampleIDREF="Media6-2IO42"Adding an Audio or Image Track to a MovieaudiopropertiesIDREF="Media6-1GS95"Setting and Getting Audio Track Propertiesaudio fileIDREF="Media2-5AF14"Audio Tracks, Sample Frames, and Track MarkersdefinedIDREF="Media2-5AF11"About Audio FilesinitializingIDREF="Media2-5AF54"Initializing Audio Track Dataaudio filesIDREF="Media2-5AF104"Writing Audio Frames to an Audio TrackchannelsIDREF="Media2-5AF59"Initializing Audio Track ChannelsdefaultIDREF="Media2-5AF40"Creating an Audio File SetupCDIDREF="Media2-6CD22"CD Tracks, Indices, and Time CodesplayingIDREF="Media2-6CD67"Playing an Audio CD from the CD-ROM DriveseekingIDREF="Media2-6CD52"Seeking to a CD LocationsubdivisionsIDREF="Media2-6CD23"CD Tracks, Indices, and Time CodesCD lead-inIDREF="Media2-6CD15"CD Frames, Samples, and SubcodesdefinedIDREF="Media6-1GS3"DefinitionsfindingIDREF="Media6-2IO47"Locating an Existing Trackgeneral propertiesIDREF="Media6-1GS73"Setting and Getting Track PropertieslengthgettingIDREF="Media6-1GS85"Getting the Track LengthmappingIDREF="Media6-2IO49"Mapping Frames from One Track to Another Track illustratedIDREF="Media6-2IO50"Mapping Frames from One Track to Another TrackmediumgettingIDREF="Media6-1GS87"Getting the Track MediumoperationsIDREF="Media6-2IO38"Adding, Locating, and Deleting Audio and Image TracksremovingIDREF="Media6-2IO45"Removing an Audio or Image Track from a Moviesetting and gettinggeneral propertiesIDREF="Media6-1GS76"Setting and Getting Track Propertiestrappingaudio I/O errorsIDREF="Media2-4AL123"Detecting Errors in the Audio StreamtroubleshootingaudioconfigurationsIDREF="Media2-4AL49"Creating a New ALconfigdistortionIDREF="Media2-4AL155"Modifying the Values of the Global ParametersI/OIDREF="Media2-4AL117"Detecting Errors in the Audio Streamoverflow and underflowIDREF="Media2-4AL104"More Methods for Working with Queuespops and clicksIDREF="Media2-4AL57"Setting and Getting the Sample Queue Size for an ALconfigMovie Library I/OIDREF="Media6-1GS25"Emulating I/O Failurestuningaudio applications dynamicallyIDREF="Media2-4AL120"Detecting Errors in the Audio Streamaudio performanceIDREF="Media2-4AL164"Real-time Programming Techniques for Audiotwo's complement dataIDREF="Media2-4AL64"Setting and Getting the Sample Data Format for an ALconfigsample widthsIDREF="Media2-4AL71"Setting and Getting the Integer Sample Width for an ALconfigType B video formatIDREF="Media3-1VB53"Videotape FormatsType C (reel-to-reel) videotapeIDREF="Media3-1VB51"Videotape Formatstypesdigital media parametersIDREF="Media1-2DM8"Digital Media Type DefinitionsmediaIDREF="Media1-2DM12"Digital Media ParametersIDREF="Media1-2DM7"Digital Media Type DefinitionsparametersgettingIDREF="Media1-2DM76"Determining the Data Type of a Given ParameterU-Matic (SP)IDREF="Media3-1VB50"Videotape FormatsU-V signal. See chrominanceIDREF="Media3-1VB24"YUVuncompressed moviesIDREF="Media6-1GS165"Getting the Image Compression SchemeunderflowaudioIDREF="Media2-4AL104"More Methods for Working with QueuesIDREF="Media2-4AL123"Detecting Errors in the Audio StreamIDREF="Media2-4AL123"Detecting Errors in the Audio Streamuser interfaceIDREF="Media0-5ATB2"References for Adding a User Interface to Your ProgramverifyingmoviesIDREF="Media6-2IO25"Checking for the Presence of a MovieVHSIDREF="Media3-1VB49"Videotape FormatsvideoB-YIDREF="Media3-1VB26"YUVBetacamIDREF="Media3-1VB20"YUVbroadcast standardsIDREF="Media3-1VB6"Broadcast StandardscompositeIDREF="Media3-1VB37"Composite VideoillustratedIDREF="Media3-1VB43"Video SignalsD1IDREF="Media3-1VB21"YUVdaemonIDREF="Media3-2GS6"Video Daemondevice managementIDREF="Media3-2GS7"Device Managementdigital recordingIDREF="Media3-1VB12"Broadcast StandardsdrainsIDREF="Media3-2GS20"VL Architectural Model of Video DevicesencodingIDREF="Media3-1VB16"Color EncodingillustratedIDREF="Media3-1VB40"Composite VideoRGBIDREF="Media3-1VB17"RGBfieldsIDREF="Media3-1VB2"InterlacingformatsIDREF="Media3-1VB7"Broadcast StandardsillustratedIDREF="Media3-1VB40"Composite Videoframe rateIDREF="Media3-1VB3"InterlacingframesIDREF="Media3-1VB2"InterlacinghardwareIDREF="Media1-1ML11"About the Video LibraryinterlacingIDREF="Media3-1VB1"InterlacingluminanceIDREF="Media3-1VB22"YUVNTSCillustratedIDREF="Media3-1VB4"InterlacingPALillustratedIDREF="Media3-1VB4"InterlacingpathsblendingIDREF="Media3-2GS22"VL Architectural Model of Video DevicesR-YIDREF="Media3-1VB26"YUVresolutionIDREF="Media3-1VB11"Broadcast StandardsS-VideoIDREF="Media3-1VB35"YC, YC-358, YC-443, or S-VideosourcesIDREF="Media3-2GS19"VL Architectural Model of Video Devicessync burstIDREF="Media3-1VB42"Video SignalsteleconferencingIDREF="Media1-1ML17"About the Video LibraryYIQIDREF="Media3-1VB28"YIQYUVIDREF="Media3-1VB18"YUVvideo data formatsIDREF="Media4-1GS1"IndigoVideo Data Formatsvideo formatsand color encoding methodsIDREF="Media3-1VB41"Composite Videoand tape formatsIDREF="Media3-1VB45"Videotape FormatsVideo LibrarypurposeIDREF="Media1-1ML10"About the Video LibraryVideo Library (VL)device managementIDREF="Media3-2GS7"Device ManagementeventsIDREF="Media3-2GS2"Getting Started with the Video LibraryfeaturesIDREF="Media3-2GS1"Getting Started with the Video Libraryheader filesIDREF="Media3-2GS17"Library and Header Filesmultiple clientsIDREF="Media3-2GS8"Device ManagementpathsdefinedIDREF="Media3-2GS18"VL Architectural Model of Video Devicessystem software architectureIDREF="Media3-2GS5"VL System Software ArchitecturetoolsIDREF="Media3-2GS9"Generic Video ToolsVIDEO, GL pseudo devicein sample programIDREF="Media4-5EH4"IRIS GL Event Handlingvideo/voice mailIDREF="Media5-0ZIN5"Compression Library ApplicationsvideoinIDREF="Media3-2GS12"Generic Video ToolsvideooutIDREF="Media3-2GS13"Generic Video ToolsvideotapeformatsIDREF="Media3-1VB44"Videotape FormatsviewingsizemovieIDREF="Media6-3PB14"Setting and Getting the Viewing Area Sizeviewsmoviegetting sizeIDREF="Media6-3PB23"Setting and Getting the Viewing Area SizeguaranteeingIDREF="Media6-3PB18"Setting and Getting the Viewing Area SizeoffsetIDREF="Media6-3PB26"Setting and Getting the Viewing Location OffsetVLfeaturesIDREF="Media1-1ML16"About the Video LibraryvlBeginTransfer()IDREF="Media3-2GS28"Starting Data TransfervlCreatePath()IDREF="Media3-2GS24"Creating the PathvlEndTransfer()IDREF="Media3-2GS29"Ending Data TransfervlNextEvent()IDREF="Media3-4EH1"Querying VL EventsvlOpenVideo()IDREF="Media3-2GS23"Opening a Connection to the Video DaemonvlSelectEvents()IDREF="Media3-2GS26"Specifying the Path-related Events to Be CapturedvlSetControl()IDREF="Media3-2GS27"Setting Drain Node Controls for Data TransfervlSetupPaths()IDREF="Media3-2GS25"Setting Up the Data Pathvoice-quality audioIDREF="Media2-4AL13"Digital Audio Sample RatesvolumeIndyIDREF="Media2-2HW15"Indy Workstation Layoutwindowsmovie playbackconfiguringIDREF="Media6-3PB9"Configuring the Playback DisplayexampleIDREF="Media6-3PB8"Creating a Window for IRIS GL Playback workaroundsDAT drivesIDREF="Media2-7DAT47"Recording Digital Audio over Digital Data Storage (DDS) TapeswrapIDREF="Media5-1GS55"Producing and Consuming Data in Bufferswrite-only moviesIDREF="Media6-2IO11"Using File Descriptors with Movieswritingaudio filesIDREF="Media2-5AF104"Writing Audio Frames to an Audio Trackaudio samplesIDREF="Media2-4AL112"Writing Samples to an Output ALportDATsIDREF="Media2-7DAT42"Writing Audio Data to the DAT Drivemovie data. See inserting movie dataIDREF="Media6-2IO58"Inserting Raw Images and Audio from a Buffer into an Existing TrackX Window SystemcoordinatesIDREF="Media6-3PB27"Setting and Getting the Viewing Location OffsetX11, recommended readingIDREF="Media0-5ATB1"References for Using Digital Media with Other LibrariesY signal. See luminanceIDREF="Media3-1VB23"YUVYCIDREF="Media3-1VB31"YC, YC-358, YC-443, or S-VideoYC-358IDREF="Media3-1VB32"YC, YC-358, YC-443, or S-VideoYC-443IDREF="Media3-1VB33"YC, YC-358, YC-443, or S-VideoYIQIDREF="Media3-1VB28"YIQequationsIDREF="Media3-1VB30"YIQYUVIDREF="Media4-1GS4"4:1:1 YUVIDREF="Media3-1VB18"YUVdigitalIDREF="Media3-1VB27"YUVequationIDREF="Media3-1VB25"YUVzoomingmoviesIDREF="Media6-3PB15"Setting and Getting the Viewing Area Size